"""
This type stub file was generated by pyright.
"""

from typing import Optional, Tuple
from torch import Tensor

"""Various linear algebra utility methods for internal use.

"""
def is_sparse(A):
    """Check if tensor A is a sparse tensor"""
    ...

def get_floating_dtype(A):
    """Return the floating point dtype of tensor A.

    Integer types map to float32.
    """
    ...

def matmul(A: Optional[Tensor], B: Tensor) -> Tensor:
    """Multiply two matrices.

    If A is None, return B. A can be sparse or dense. B is always
    dense.
    """
    ...

def conjugate(A):
    """Return conjugate of tensor A.

    .. note:: If A's dtype is not complex, A is returned.
    """
    ...

def transpose(A):
    """Return transpose of a matrix or batches of matrices.
    """
    ...

def transjugate(A):
    """Return transpose conjugate of a matrix or batches of matrices.
    """
    ...

def bform(X: Tensor, A: Optional[Tensor], Y: Tensor) -> Tensor:
    """Return bilinear form of matrices: :math:`X^T A Y`.
    """
    ...

def qform(A: Optional[Tensor], S: Tensor) -> Tensor:
    """Return quadratic form :math:`S^T A S`.
    """
    ...

def basis(A):
    """Return orthogonal basis of A columns.
    """
    ...

def symeig(A: Tensor, largest: Optional[bool] = ..., eigenvectors: Optional[bool] = ...) -> Tuple[Tensor, Tensor]:
    """Return eigenpairs of A with specified ordering.
    """
    ...

"""
This type stub file was generated by pyright.
"""

class _StorageBase(object):
    is_cuda = ...
    is_sparse = ...
    def __str__(self) -> str:
        ...
    
    def __repr__(self):
        ...
    
    def __iter__(self):
        ...
    
    def __copy__(self):
        ...
    
    def __deepcopy__(self, memo):
        ...
    
    def __reduce__(self):
        ...
    
    def __sizeof__(self):
        ...
    
    def clone(self):
        """Returns a copy of this storage"""
        ...
    
    def tolist(self):
        """Returns a list containing the elements of this storage"""
        ...
    
    def cpu(self):
        """Returns a CPU copy of this storage if it's not already on the CPU"""
        ...
    
    def double(self):
        """Casts this storage to double type"""
        ...
    
    def float(self):
        """Casts this storage to float type"""
        ...
    
    def half(self):
        """Casts this storage to half type"""
        ...
    
    def long(self):
        """Casts this storage to long type"""
        ...
    
    def int(self):
        """Casts this storage to int type"""
        ...
    
    def short(self):
        """Casts this storage to short type"""
        ...
    
    def char(self):
        """Casts this storage to char type"""
        ...
    
    def byte(self):
        """Casts this storage to byte type"""
        ...
    
    def bool(self):
        """Casts this storage to bool type"""
        ...
    
    def bfloat16(self):
        """Casts this storage to bfloat16 type"""
        ...
    
    def complex_double(self):
        """Casts this storage to complex double type"""
        ...
    
    def complex_float(self):
        """Casts this storage to complex float type"""
        ...
    
    def pin_memory(self):
        """Copies the storage to pinned memory, if it's not already pinned."""
        ...
    
    def share_memory_(self):
        """Moves the storage to shared memory.

        This is a no-op for storages already in shared memory and for CUDA
        storages, which do not need to be moved for sharing across processes.
        Storages in shared memory cannot be resized.

        Returns: self
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

__version__ = '1.6.0'
debug = False
cuda = None
git_version = 'b31f58de6fa8bbda5353b3c77d9be4914399724d'
hip = None
"""
This type stub file was generated by pyright.
"""

import sys
import platform

"""
This file is directly from
https://github.com/ActiveState/appdirs/blob/3fe6a83776843a46f20c2e5587afcffe05e03b39/appdirs.py

The license of https://github.com/ActiveState/appdirs copied below:


# This is the MIT license

Copyright (c) 2010 ActiveState Software Inc.

Permission is hereby granted, free of charge, to any person obtaining a
copy of this software and associated documentation files (the
"Software"), to deal in the Software without restriction, including
without limitation the rights to use, copy, modify, merge, publish,
distribute, sublicense, and/or sell copies of the Software, and to
permit persons to whom the Software is furnished to do so, subject to
the following conditions:

The above copyright notice and this permission notice shall be included
in all copies or substantial portions of the Software.

THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS
OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
"""
__version__ = "1.4.4"
__version_info__ = tuple(int(segment) for segment in __version__.split("."))
unicode = str
if sys.platform.startswith('java'):
    os_name = platform.java_ver()[3][0]
else:
    system = sys.platform
def user_data_dir(appname=..., appauthor=..., version=..., roaming=...):
    r"""Return full path to the user-specific data dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "roaming" (boolean, default False) can be set True to use the Windows
            roaming appdata directory. That means that for users on a Windows
            network setup for roaming profiles, this user data will be
            sync'd on login. See
            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
            for a discussion of issues.

    Typical user data directories are:
        Mac OS X:               ~/Library/Application Support/<AppName>
        Unix:                   ~/.local/share/<AppName>    # or in $XDG_DATA_HOME, if defined
        Win XP (not roaming):   C:\Documents and Settings\<username>\Application Data\<AppAuthor>\<AppName>
        Win XP (roaming):       C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>
        Win 7  (not roaming):   C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>
        Win 7  (roaming):       C:\Users\<username>\AppData\Roaming\<AppAuthor>\<AppName>

    For Unix, we follow the XDG spec and support $XDG_DATA_HOME.
    That means, by default "~/.local/share/<AppName>".
    """
    ...

def site_data_dir(appname=..., appauthor=..., version=..., multipath=...):
    r"""Return full path to the user-shared data dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "multipath" is an optional parameter only applicable to *nix
            which indicates that the entire list of data dirs should be
            returned. By default, the first item from XDG_DATA_DIRS is
            returned, or '/usr/local/share/<AppName>',
            if XDG_DATA_DIRS is not set

    Typical site data directories are:
        Mac OS X:   /Library/Application Support/<AppName>
        Unix:       /usr/local/share/<AppName> or /usr/share/<AppName>
        Win XP:     C:\Documents and Settings\All Users\Application Data\<AppAuthor>\<AppName>
        Vista:      (Fail! "C:\ProgramData" is a hidden *system* directory on Vista.)
        Win 7:      C:\ProgramData\<AppAuthor>\<AppName>   # Hidden, but writeable on Win 7.

    For Unix, this is using the $XDG_DATA_DIRS[0] default.

    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.
    """
    ...

def user_config_dir(appname=..., appauthor=..., version=..., roaming=...):
    r"""Return full path to the user-specific config dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "roaming" (boolean, default False) can be set True to use the Windows
            roaming appdata directory. That means that for users on a Windows
            network setup for roaming profiles, this user data will be
            sync'd on login. See
            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
            for a discussion of issues.

    Typical user config directories are:
        Mac OS X:               ~/Library/Preferences/<AppName>
        Unix:                   ~/.config/<AppName>     # or in $XDG_CONFIG_HOME, if defined
        Win *:                  same as user_data_dir

    For Unix, we follow the XDG spec and support $XDG_CONFIG_HOME.
    That means, by default "~/.config/<AppName>".
    """
    ...

def site_config_dir(appname=..., appauthor=..., version=..., multipath=...):
    r"""Return full path to the user-shared data dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "multipath" is an optional parameter only applicable to *nix
            which indicates that the entire list of config dirs should be
            returned. By default, the first item from XDG_CONFIG_DIRS is
            returned, or '/etc/xdg/<AppName>', if XDG_CONFIG_DIRS is not set

    Typical site config directories are:
        Mac OS X:   same as site_data_dir
        Unix:       /etc/xdg/<AppName> or $XDG_CONFIG_DIRS[i]/<AppName> for each value in
                    $XDG_CONFIG_DIRS
        Win *:      same as site_data_dir
        Vista:      (Fail! "C:\ProgramData" is a hidden *system* directory on Vista.)

    For Unix, this is using the $XDG_CONFIG_DIRS[0] default, if multipath=False

    WARNING: Do not use this on Windows. See the Vista-Fail note above for why.
    """
    ...

def user_cache_dir(appname=..., appauthor=..., version=..., opinion=...):
    r"""Return full path to the user-specific cache dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "opinion" (boolean) can be False to disable the appending of
            "Cache" to the base app data dir for Windows. See
            discussion below.

    Typical user cache directories are:
        Mac OS X:   ~/Library/Caches/<AppName>
        Unix:       ~/.cache/<AppName> (XDG default)
        Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Cache
        Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Cache

    On Windows the only suggestion in the MSDN docs is that local settings go in
    the `CSIDL_LOCAL_APPDATA` directory. This is identical to the non-roaming
    app data dir (the default returned by `user_data_dir` above). Apps typically
    put cache data somewhere *under* the given dir here. Some examples:
        ...\Mozilla\Firefox\Profiles\<ProfileName>\Cache
        ...\Acme\SuperApp\Cache\1.0
    OPINION: This function appends "Cache" to the `CSIDL_LOCAL_APPDATA` value.
    This can be disabled with the `opinion=False` option.
    """
    ...

def user_state_dir(appname=..., appauthor=..., version=..., roaming=...):
    r"""Return full path to the user-specific state dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "roaming" (boolean, default False) can be set True to use the Windows
            roaming appdata directory. That means that for users on a Windows
            network setup for roaming profiles, this user data will be
            sync'd on login. See
            <http://technet.microsoft.com/en-us/library/cc766489(WS.10).aspx>
            for a discussion of issues.

    Typical user state directories are:
        Mac OS X:  same as user_data_dir
        Unix:      ~/.local/state/<AppName>   # or in $XDG_STATE_HOME, if defined
        Win *:     same as user_data_dir

    For Unix, we follow this Debian proposal <https://wiki.debian.org/XDGBaseDirectorySpecification#state>
    to extend the XDG spec and support $XDG_STATE_HOME.

    That means, by default "~/.local/state/<AppName>".
    """
    ...

def user_log_dir(appname=..., appauthor=..., version=..., opinion=...):
    r"""Return full path to the user-specific log dir for this application.

        "appname" is the name of application.
            If None, just the system directory is returned.
        "appauthor" (only used on Windows) is the name of the
            appauthor or distributing body for this application. Typically
            it is the owning company name. This falls back to appname. You may
            pass False to disable it.
        "version" is an optional version path element to append to the
            path. You might want to use this if you want multiple versions
            of your app to be able to run independently. If used, this
            would typically be "<major>.<minor>".
            Only applied when appname is present.
        "opinion" (boolean) can be False to disable the appending of
            "Logs" to the base app data dir for Windows, and "log" to the
            base cache dir for Unix. See discussion below.

    Typical user log directories are:
        Mac OS X:   ~/Library/Logs/<AppName>
        Unix:       ~/.cache/<AppName>/log  # or under $XDG_CACHE_HOME if defined
        Win XP:     C:\Documents and Settings\<username>\Local Settings\Application Data\<AppAuthor>\<AppName>\Logs
        Vista:      C:\Users\<username>\AppData\Local\<AppAuthor>\<AppName>\Logs

    On Windows the only suggestion in the MSDN docs is that local settings
    go in the `CSIDL_LOCAL_APPDATA` directory. (Note: I'm interested in
    examples of what some windows apps use for a logs dir.)

    OPINION: This function appends "Logs" to the `CSIDL_LOCAL_APPDATA`
    value for Windows and appends "log" to the user cache dir for Unix.
    This can be disabled with the `opinion=False` option.
    """
    ...

class AppDirs(object):
    """Convenience wrapper for getting application dirs."""
    def __init__(self, appname=..., appauthor=..., version=..., roaming=..., multipath=...) -> None:
        ...
    
    @property
    def user_data_dir(self):
        ...
    
    @property
    def site_data_dir(self):
        ...
    
    @property
    def user_config_dir(self):
        ...
    
    @property
    def site_config_dir(self):
        ...
    
    @property
    def user_cache_dir(self):
        ...
    
    @property
    def user_state_dir(self):
        ...
    
    @property
    def user_log_dir(self):
        ...
    


if system == "win32":
    ...
if __name__ == "__main__":
    appname = "MyApp"
    appauthor = "MyCompany"
    props = ("user_data_dir", "user_config_dir", "user_cache_dir", "user_state_dir", "user_log_dir", "site_data_dir", "site_config_dir")
    dirs = AppDirs(appname, appauthor, version="1.0")
    dirs = AppDirs(appname, appauthor)
    dirs = AppDirs(appname)
    dirs = AppDirs(appname, appauthor=False)
"""
This type stub file was generated by pyright.
"""

import weakref
import torch
from typing import Generic, TypeVar

"""
The weak_script annotation needs to be here instead of inside torch/jit/ so it
can be used in other places in torch/ (namely torch.nn) without running into
circular dependency problems
"""
boolean_dispatched = weakref.WeakKeyDictionary()
def createResolutionCallbackFromEnv(lookup_base):
    """
    Creates a resolution callback that will look up qualified names in an
    environment, starting with `lookup_base` for the base of any qualified
    names, then proceeding down the lookup chain with the resolved object.

    You should not use this directly, it should only be used from the other
    createResolutionCallbackFrom* functions.
    """
    ...

def createResolutionCallbackFromFrame(frames_up=...):
    """
    Creates a function which, given a string variable name,
    returns the value of the variable in the scope of the caller of
    the function which called createResolutionCallbackFromFrame (by default).

    This is used to enable access in-scope Python variables inside
    TorchScript fragments.

    frames_up is number of additional frames to go up on the stack.
    The default value is 0, which correspond to the frame of the caller
    of createResolutionCallbackFromFrame. Also for example, if frames_up is set
    to 1, then the frame of the caller's caller of createResolutionCallbackFromFrame
    will be taken.

    For example, the following program prints 2::

        def bar():
            cb = createResolutionCallbackFromFrame(1)
            print(cb("foo"))

        def baz():
            foo = 2
            bar()

        baz()
    """
    class env(object):
        ...
    
    

def get_closure(fn):
    """
    Get a dictionary of closed over variables from a function
    """
    ...

def createResolutionCallbackFromClosure(fn):
    """
    Create a resolutionCallback by introspecting the function instead of
    looking up the stack for the enclosing scope
    """
    class closure_lookup(object):
        ...
    
    

def can_compile_class(cls):
    ...

def createResolutionCallbackForClassMethods(cls):
    """
    This looks at all the methods defined in a class and pulls their closed-over
    variables into a dictionary and uses that to resolve variables.
    """
    ...

def boolean_dispatch(arg_name, arg_index, default, if_true, if_false, module_name, func_name):
    """
    Dispatches to either of 2 script functions based on a boolean argument.
    In TorchScript, the boolean argument must be constant so that the correct
    function to use can be determined at compile time.
    """
    ...

class FunctionModifiers(object):
    """
    Used to denote the behavior of a function in TorchScript. See export() and
    ignore() for details.
    """
    UNUSED = ...
    IGNORE = ...
    EXPORT = ...
    DEFAULT = ...
    COPY_TO_SCRIPT_WRAPPER = ...


def export(fn):
    """
    This decorator indicates that a method on an ``nn.Module`` is used as an entry point into a
    :class:`ScriptModule` and should be compiled.

    ``forward`` implicitly is assumed to be an entry point, so it does not need this decorator.
    Functions and methods called from ``forward`` are compiled as they are seen
    by the compiler, so they do not need this decorator either.

    Example (using ``@torch.jit.export`` on a method):

    .. testcode::

        import torch
        import torch.nn as nn

        class MyModule(nn.Module):
            def implicitly_compiled_method(self, x):
                return x + 99

            # `forward` is implicitly decorated with `@torch.jit.export`,
            # so adding it here would have no effect
            def forward(self, x):
                return x + 10

            @torch.jit.export
            def another_forward(self, x):
                # When the compiler sees this call, it will compile
                # `implicitly_compiled_method`
                return self.implicitly_compiled_method(x)

            def unused_method(self, x):
                return x - 20

        # `m` will contain compiled methods:
        #     `forward`
        #     `another_forward`
        #     `implicitly_compiled_method`
        # `unused_method` will not be compiled since it was not called from
        # any compiled methods and wasn't decorated with `@torch.jit.export`
        m = torch.jit.script(MyModule())
    """
    ...

def unused(fn):
    """
    This decorator indicates to the compiler that a function or method should
    be ignored and replaced with the raising of an exception. This allows you
    to leave code in your model that is not yet TorchScript compatible and still
    export your model.

        Example (using ``@torch.jit.unused`` on a method)::

            import torch
            import torch.nn as nn

            class MyModule(nn.Module):
                def __init__(self, use_memory_efficent):
                    super(MyModule, self).__init__()
                    self.use_memory_efficent = use_memory_efficent

                @torch.jit.unused
                def memory_efficient(self, x):
                    import pdb
                    pdb.set_trace()
                    return x + 10

                def forward(self, x):
                    # Use not-yet-scriptable memory efficient mode
                    if self.use_memory_efficient:
                        return self.memory_efficient(x)
                    else:
                        return x + 10

            m = torch.jit.script(MyModule(use_memory_efficent=False))
            m.save("m.pt")

            m = torch.jit.script(MyModule(use_memory_efficient=True))
            # exception raised
            m(torch.rand(100))
    """
    ...

def ignore(drop=..., **kwargs):
    """
    This decorator indicates to the compiler that a function or method should
    be ignored and left as a Python function. This allows you to leave code in
    your model that is not yet TorchScript compatible. If called from TorchScript,
    ignored functions will dispatch the call to the Python interpreter. Models with ignored
    functions cannot be exported; use :func:`@torch.jit.unused <torch.jit.unused>` instead.

    Example (using ``@torch.jit.ignore`` on a method)::

        import torch
        import torch.nn as nn

        class MyModule(nn.Module):
            @torch.jit.ignore
            def debugger(self, x):
                import pdb
                pdb.set_trace()

            def forward(self, x):
                x += 10
                # The compiler would normally try to compile `debugger`,
                # but since it is `@ignore`d, it will be left as a call
                # to Python
                self.debugger(x)
                return x

        m = torch.jit.script(MyModule())

        # Error! The call `debugger` cannot be saved since it calls into Python
        m.save("m.pt")

    Example (using ``@torch.jit.ignore(drop=True)`` on a method):

    .. testcode::

        import torch
        import torch.nn as nn

        class MyModule(nn.Module):
            @torch.jit.ignore(drop=True)
            def training_method(self, x):
                import pdb
                pdb.set_trace()

            def forward(self, x):
                if self.training:
                    self.training_method(x)
                return x

        m = torch.jit.script(MyModule())

        # This is OK since `training_method` is not saved, the call is replaced
        # with a `raise`.
        m.save("m.pt")

    .. testcleanup::

        import os
        os.remove('m.pt')
    """
    ...

def module_has_exports(mod):
    ...

def should_drop(fn):
    ...

def is_ignored_fn(fn):
    ...

def is_static_fn(cls, fn):
    ...

def get_static_fn(cls, fn):
    ...

def get_torchscript_modifier(fn):
    ...

def copy_torchscript_modifier(orig, new):
    ...

_overloaded_fns = {  }
def get_class_name_lineno(method):
    ...

_overloaded_methods = {  }
_overloaded_method_class_fileno = {  }
def is_tuple(ann):
    ...

def is_list(ann):
    ...

def is_dict(ann):
    ...

def is_optional(ann):
    ...

T = TypeVar('T')
class Future(Generic[T]):
    __slots__ = ...
    def __init__(self, types) -> None:
        ...
    


def is_future(ann):
    ...

if torch.distributed.rpc.is_available():
    def is_rref(ann):
        ...
    
class BroadcastingListCls(object):
    def __getitem__(self, types):
        ...
    


BroadcastingList1 = BroadcastingListCls()
class SourceContext(torch._C._jit_tree_views.SourceRangeFactory):
    def __init__(self, source, filename, file_lineno, leading_whitespace_len, uses_true_division=...) -> None:
        ...
    


def fake_range():
    ...

"""
This type stub file was generated by pyright.
"""

import torch

class Future(torch._C.Future):
    r"""
    Wrapper around a ``torch._C.Future`` which encapsulates an asynchronous
    execution of a callable, e.g. :meth:`~torch.distributed.rpc.rpc_async`. It
    also exposes a set of APIs to add callback functions and set results.
    """
    def __new__(cls):
        ...
    
    def wait(self):
        r"""
        Block until the value of this ``Future`` is ready.

        Returns:
            The value held by this ``Future``. If the function (callback or RPC)
            creating the value has thrown an error, this ``wait`` method will
            also throw an error.
        """
        ...
    
    def then(self, callback):
        r"""
        Append the given callback function to this ``Future``, which will be run
        when the ``Future`` is completed.  Multiple callbacks can be added to
        the same ``Future``, and will be invoked in the same order as they were
        added. The callback must take one argument, which is the reference to
        this ``Future``. The callback function can use the ``Future.wait()`` API
        to get the value.

        Arguments:
            callback(``Callable``): a ``Callable`` that takes this ``Future`` as
                                    the only argument.

        Returns:
            A new ``Future`` object that holds the return value of the
            ``callback`` and will be marked as completed when the given
            ``callback`` finishes.

        Example::
            >>> import torch
            >>>
            >>> def callback(fut):
            >>>     print(f"RPC return value is {fut.wait()}.")
            >>>
            >>> fut = torch.futures.Future()
            >>> # The inserted callback will print the return value when
            >>> # receiving the response from "worker1"
            >>> cb_fut = fut.then(callback)
            >>> chain_cb_fut = cb_fut.then(
            >>>     lambda x : print(f"Chained cb done. {x.wait()}")
            >>> )
            >>> fut.set_result(5)
            >>>
            >>> # Outputs are:
            >>> # RPC return value is 5.
            >>> # Chained cb done. None
        """
        ...
    
    def set_result(self, result):
        r"""
        Set the result for this ``Future``, which will mark this ``Future`` as
        completed and trigger all attached callbacks. Note that a ``Future``
        cannot be marked completed twice.

        Arguments:
            result (object): the result object of this ``Future``.

        Example::
            >>> import threading
            >>> import time
            >>> import torch
            >>>
            >>> def slow_set_future(fut, value):
            >>>     time.sleep(0.5)
            >>>     fut.set_result(value)
            >>>
            >>> fut = torch.futures.Future()
            >>> t = threading.Thread(
            >>>     target=slow_set_future,
            >>>     args=(fut, torch.ones(2) * 3)
            >>> )
            >>> t.start()
            >>>
            >>> print(fut.wait())  # tensor([3., 3.])
            >>> t.join()
        """
        ...
    


def collect_all(futures):
    r"""
    Collects the provided :class:`~torch.futures.Future` objects into a single
    combined :class:`~torch.futures.Future` that is completed when all of the
    sub-futures are completed.

    Arguments:
        futures (list): a list of :class:`~torch.futures.Future` objects.

    Returns:
        Returns a :class:`~torch.futures.Future` object to a list of the passed
        in Futures.

    Example::
        >>> import torch
        >>>
        >>> fut0 = torch.futures.Future()
        >>> fut1 = torch.futures.Future()
        >>>
        >>> fut = torch.futures.collect_all([fut0, fut1])
        >>>
        >>> fut0.set_result(0)
        >>> fut1.set_result(1)
        >>>
        >>> fut_list = fut.wait()
        >>> print(f"fut0 result = {fut_list[0].wait()}")
        >>> print(f"fut1 result = {fut_list[1].wait()}")
        >>> # outputs:
        >>> # fut0 result = 0
        >>> # fut1 result = 1
    """
    ...

def wait_all(futures):
    r"""
    Waits for all provided futures to be complete, and returns
    the list of completed values.

    Arguments:
        futures (list): a list of :class:`~torch.futures.Future` object.

    Returns:
        A list of the completed :class:`~torch.futures.Future` results. This
        method will throw an error if ``wait`` on any
        :class:`~torch.futures.Future` throws.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import contextlib

def set_rng_state(new_state):
    r"""Sets the random number generator state.

    Args:
        new_state (torch.ByteTensor): The desired state
    """
    ...

def get_rng_state():
    r"""Returns the random number generator state as a `torch.ByteTensor`."""
    ...

def manual_seed(seed):
    r"""Sets the seed for generating random numbers. Returns a
    `torch.Generator` object.

    Args:
        seed (int): The desired seed.
    """
    ...

def seed():
    r"""Sets the seed for generating random numbers to a non-deterministic
    random number. Returns a 64 bit number used to seed the RNG.
    """
    ...

def initial_seed():
    r"""Returns the initial seed for generating random numbers as a
    Python `long`.
    """
    ...

_fork_rng_warned_already = False
@contextlib.contextmanager
def fork_rng(devices=..., enabled=..., _caller=..., _devices_kw=...):
    """
    Forks the RNG, so that when you return, the RNG is reset
    to the state that it was previously in.

    Arguments:
        devices (iterable of CUDA IDs): CUDA devices for which to fork
            the RNG.  CPU RNG state is always forked.  By default, :meth:`fork_rng` operates
            on all devices, but will emit a warning if your machine has a lot
            of devices, since this function will run very slowly in that case.
            If you explicitly specify devices, this warning will be suppressed
        enabled (bool): if ``False``, the RNG is not forked.  This is a convenience
            argument for easily disabling the context manager without having
            to delete it and unindent your Python code under it.
    """
    ...

"""
This type stub file was generated by pyright.
"""

from typing import Any, Optional
from .common_types import _device_t, _devices_t
from ..modules import Module
from ... import Tensor

class DataParallel(Module):
    def __init__(self, module: Module, device_ids: Optional[_devices_t] = ..., output_device: Optional[_device_t] = ..., dim: int = ...) -> None:
        ...
    


def data_parallel(module: Module, inputs: Any, device_ids: Optional[_devices_t] = ..., output_device: Optional[_device_t] = ..., dim: int = ..., module_kwargs: Optional[Any] = ...) -> Tensor:
    ...

"""
This type stub file was generated by pyright.
"""

from typing import List, Sequence, Union
from ..modules import Module
from .common_types import _devices_t

def replicate(network: Module, devices: Union[_devices_t, Sequence[_devices_t]], detach: bool = ...) -> List[Module]:
    ...

"""
This type stub file was generated by pyright.
"""

from .data_parallel import DataParallel as DataParallel, data_parallel as data_parallel
from .distributed import DistributedDataParallel as DistributedDataParallel
from .parallel_apply import parallel_apply as parallel_apply
from .replicate import replicate as replicate
from .scatter_gather import gather as gather, scatter as scatter

"""
This type stub file was generated by pyright.
"""

from typing import Any, List, Optional, Sequence
from .common_types import _devices_t
from ..modules import Module

def parallel_apply(modules: Sequence[Module], inputs: Sequence[Any], kwargs_tup: Optional[Any] = ..., devices: Optional[_devices_t] = ...) -> List[Any]:
    ...

"""
This type stub file was generated by pyright.
"""

from ..modules import Module
from typing import Any, Optional
from .common_types import _device_t, _devices_t

class DistributedDataParallel(Module):
    def __init__(self, module: Module, device_ids: Optional[_devices_t] = ..., output_device: Optional[_device_t] = ..., dim: int = ..., broadcast_buffers: bool = ..., process_group: Optional[Any] = ..., bucket_cap_mb: float = ..., check_reduction: bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, List, Tuple, TypeVar, overload
from ... import Tensor
from .common_types import _device_t, _devices_t

T = TypeVar('T', Dict, List, Tuple)
@overload
def scatter(inputs: Tensor, target_gpus: _devices_t, dim: int = ...) -> Tuple[Tensor, ...]:
    ...

@overload
def scatter(inputs: T, target_gpus: _devices_t, dim: int = ...) -> List[T]:
    ...

def scatter_kwargs(inputs: Any, kwargs: Any, target_gpus: _devices_t, dim: int = ...) -> Any:
    ...

def gather(outputs: Any, target_device: _device_t, dim: int = ...) -> Any:
    ...

"""
This type stub file was generated by pyright.
"""

from typing import Sequence, Union
from ... import device

_device_t = Union[int, device]
_devices_t = Sequence[_device_t]
"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .modules import *

"""
This type stub file was generated by pyright.
"""

from .linear import Linear
from .conv import Conv2d

"""
This type stub file was generated by pyright.
"""

import torch.nn as nn

class Linear(nn.Linear):
    r"""
    A linear module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.

    We adopt the same interface as `torch.nn.Linear`, please see
    https://pytorch.org/docs/stable/nn.html#torch.nn.Linear
    for documentation.

    Similar to `torch.nn.Linear`, with FakeQuantize modules initialized to
    default.

    Attributes:
        activation_post_process: fake quant module for output activation
        weight: fake quant module for weight
    """
    _FLOAT_MODULE = ...
    def __init__(self, in_features, out_features, bias=..., qconfig=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        r"""Create a qat module from a float module or qparams_dict

            Args: `mod` a float module, either produced by torch.quantization utilities
            or directly from user
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch.nn as nn

class Conv2d(nn.Conv2d):
    r"""
    A Conv2d module attached with FakeQuantize modules for both output
    activation and weight, used for quantization aware training.

    We adopt the same interface as `torch.nn.Conv2d`, please see
    https://pytorch.org/docs/stable/nn.html?highlight=conv2d#torch.nn.Conv2d
    for documentation.

    Similar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to
    default.

    Attributes:
        activation_post_process: fake quant module for output activation
        weight_fake_quant: fake quant module for weight
    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=..., qconfig=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        r"""Create a qat module from a float module or qparams_dict

            Args: `mod` a float module, either produced by torch.quantization utilities
            or directly from user
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from .modules import *
from .parameter import Parameter as Parameter
from .parallel import DataParallel as DataParallel
from . import functional as functional, init as init, parallel as parallel, utils as utils

"""
This type stub file was generated by pyright.
"""

from typing import Optional

def get_enum(reduction: str) -> int:
    ...

def legacy_get_string(size_average: Optional[bool], reduce: Optional[bool], emit_warning: bool = ...) -> str:
    ...

def legacy_get_enum(size_average: Optional[bool], reduce: Optional[bool], emit_warning: bool = ...) -> int:
    ...

"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .modules import *

"""
This type stub file was generated by pyright.
"""

from .linear import Linear
from .rnn import GRUCell, LSTM, LSTMCell, RNNCell

"""
This type stub file was generated by pyright.
"""

import torch.nn.quantized as nnq

class Linear(nnq.Linear):
    r"""
    A dynamic quantized linear module with floating point tensor as inputs and outputs.
    We adopt the same interface as `torch.nn.Linear`, please see
    https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.

    Similar to :class:`torch.nn.Linear`, attributes will be randomly
    initialized at module creation time and will be overwritten later

    Attributes:
        weight (Tensor): the non-learnable quantized weights of the module which are of
                         shape :math:`(\text{out\_features}, \text{in\_features})`.
        bias (Tensor): the non-learnable floating point bias of the module of shape
                       :math:`(\text{out\_features})`. If :attr:`bias` is ``True``,
                       the values are initialized to zero.

    Examples::

        >>> m = nn.quantized.dynamic.Linear(20, 30)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 30])
    """
    _version = ...
    def __init__(self, in_features, out_features, bias_=..., dtype=...) -> None:
        ...
    
    def forward(self, x):
        ...
    
    def extra_repr(self):
        ...
    
    @classmethod
    def from_float(cls, mod):
        r"""Create a dynamic quantized module from a float module or qparams_dict

        Args:
            mod (Module): a float module, either produced by torch.quantization
                          utilities or provided by the user
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from torch import Tensor
from torch._jit_internal import Optional, Tuple
from torch.nn.utils.rnn import PackedSequence

def apply_permutation(tensor: Tensor, permutation: Tensor, dim: int = ...) -> Tensor:
    ...

class PackedParameter(torch.nn.Module):
    def __init__(self, param) -> None:
        ...
    


class RNNBase(torch.nn.Module):
    _FLOAT_MODULE = ...
    _version = ...
    def __init__(self, mode, input_size, hidden_size, num_layers=..., bias=..., batch_first=..., dropout=..., bidirectional=..., dtype=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def __repr__(self):
        ...
    
    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None:
        ...
    
    def get_expected_hidden_size(self, input: Tensor, batch_sizes: Optional[Tensor]) -> Tuple[int, int, int]:
        ...
    
    def check_hidden_size(self, hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = ...) -> None:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]) -> None:
        ...
    
    def permute_hidden(self, hx: Tensor, permutation: Optional[Tensor]) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    
    def get_weight(self):
        ...
    
    def get_bias(self):
        ...
    


class LSTM(RNNBase):
    r"""
    A dynamic quantized LSTM module with floating point tensor as inputs and outputs.
    We adopt the same interface as `torch.nn.LSTM`, please see
    https://pytorch.org/docs/stable/nn.html#torch.nn.LSTM for documentation.

    Examples::

        >>> rnn = nn.LSTM(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> c0 = torch.randn(2, 3, 20)
        >>> output, (hn, cn) = rnn(input, (h0, c0))
    """
    _FLOAT_MODULE = ...
    __overloads__ = ...
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def forward_impl(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]], batch_sizes: Optional[Tensor], max_batch_size: int, sorted_indices: Optional[Tensor]) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        ...
    
    @torch.jit.export
    def forward_tensor(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        ...
    
    @torch.jit.export
    def forward_packed(self, input: PackedSequence, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[PackedSequence, Tuple[Tensor, Tensor]]:
        ...
    
    def permute_hidden(self, hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> Tuple[Tensor, Tensor]:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]) -> None:
        ...
    
    @torch.jit.ignore
    def forward(self, input, hx=...):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class RNNCellBase(torch.nn.Module):
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., num_chunks=..., dtype=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def check_forward_input(self, input):
        ...
    
    def check_forward_hidden(self, input: Tensor, hx: Tensor, hidden_label: str = ...) -> None:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    
    def get_weight(self):
        ...
    
    def get_bias(self):
        ...
    


class RNNCell(RNNCellBase):
    r"""An Elman RNN cell with tanh or ReLU non-linearity.
    A dynamic quantized RNNCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.RNNCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.RNNCell for documentation.

    Examples::

        >>> rnn = nn.RNNCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    __constants__ = ...
    def __init__(self, input_size, hidden_size, bias=..., nonlinearity=..., dtype=...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class LSTMCell(RNNCellBase):
    r"""A long short-term memory (LSTM) cell.

    A dynamic quantized LSTMCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.LSTMCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.LSTMCell for documentation.

    Examples::

        >>> rnn = nn.LSTMCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> cx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx, cx = rnn(input[i], (hx, cx))
                output.append(hx)
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tensor]:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class GRUCell(RNNCellBase):
    r"""A gated recurrent unit (GRU) cell

    A dynamic quantized GRUCell module with floating point tensor as inputs and outputs.
    Weights are quantized to 8 bits. We adopt the same interface as `torch.nn.GRUCell`,
    please see https://pytorch.org/docs/stable/nn.html#torch.nn.GRUCell for documentation.

    Examples::

        >>> rnn = nn.GRUCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    def __init__(self, input_size, hidden_size, bias=..., dtype=...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .modules import *

"""
This type stub file was generated by pyright.
"""

from typing import Optional
from torch import Tensor

r""" Functional interface (quantized)."""
def avg_pool2d(input, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...):
    r"""
    Applies 2D average-pooling operation in :math:`kH \times kW` regions by step size
    :math:`sH \times sW` steps. The number of output features is equal to the number of
    input planes.

    .. note:: The input quantization parameters propagate to the output.

    See :class:`~torch.nn.quantized.AvgPool2d` for details and output shape.

    Args:
        input: quantized input tensor :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
        kernel_size: size of the pooling region. Can be a single number or a
          tuple `(kH, kW)`
        stride: stride of the pooling operation. Can be a single number or a
          tuple `(sH, sW)`. Default: :attr:`kernel_size`
        padding: implicit zero paddings on both sides of the input. Can be a
          single number or a tuple `(padH, padW)`. Default: 0
        ceil_mode: when True, will use `ceil` instead of `floor` in the formula
            to compute the output shape. Default: ``False``
        count_include_pad: when True, will include the zero-padding in the
            averaging calculation. Default: ``True``
        divisor_override: if specified, it will be used as divisor, otherwise
             size of the pooling region will be used. Default: None
    """
    ...

def avg_pool3d(input, kernel_size, stride=..., padding=..., ceil_mode=..., count_include_pad=..., divisor_override=...):
    r"""
    Applies 3D average-pooling operation in :math:`kD \ times kH \times kW` regions by step size
    :math:`sD \times sH \times sW` steps. The number of output features is equal to the number of
    input planes.

    .. note:: The input quantization parameters propagate to the output.

    Args:
        input: quantized input tensor :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
        kernel_size: size of the pooling region. Can be a single number or a
          tuple `(kD, kH, kW)`
        stride: stride of the pooling operation. Can be a single number or a
          tuple `(sD, sH, sW)`. Default: :attr:`kernel_size`
        padding: implicit zero paddings on both sides of the input. Can be a
          single number or a tuple `(padD, padH, padW)`. Default: 0
        ceil_mode: when True, will use `ceil` instead of `floor` in the formula
            to compute the output shape. Default: ``False``
        count_include_pad: when True, will include the zero-padding in the
            averaging calculation. Default: ``True``
        divisor_override: if specified, it will be used as divisor, otherwise
             size of the pooling region will be used. Default: None
    """
    ...

def adaptive_avg_pool2d(input: Tensor, output_size: BroadcastingList2[int]) -> Tensor:
    r"""
    Applies a 2D adaptive average pooling over a quantized input signal composed
    of several quantized input planes.

    .. note:: The input quantization paramteres propagate to the output.

    See :class:`~torch.nn.quantized.AdaptiveAvgPool2d` for details and output shape.

    Args:
        output_size: the target output size (single integer or
                     double-integer tuple)
    """
    ...

def conv1d(input, weight, bias, stride=..., padding=..., dilation=..., groups=..., padding_mode=..., scale=..., zero_point=..., dtype=...):
    r"""
    Applies a 1D convolution over a quantized 1D input composed of several input
    planes.

    See :class:`~torch.nn.quantized.Conv1d` for details and output shape.

    Args:
        input: quantized input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iW)`
        weight: quantized filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , iW)`
        bias: **non-quantized** bias tensor of shape :math:`(\text{out\_channels})`. The tensor type must be `torch.float`.
        stride: the stride of the convolving kernel. Can be a single number or a
          tuple `(sW,)`. Default: 1
        padding: implicit paddings on both sides of the input. Can be a
          single number or a tuple `(padW,)`. Default: 0
        dilation: the spacing between kernel elements. Can be a single number or
          a tuple `(dW,)`. Default: 1
        groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
          number of groups. Default: 1
        padding_mode: the padding mode to use. Only "zeros" is supported for quantized convolution at the moment. Default: "zeros"
        scale: quantization scale for the output. Default: 1.0
        zero_point: quantization zero_point for the output. Default: 0
        dtype: quantization data type to use. Default: ``torch.quint8``

    Examples::

        >>> from torch.nn.quantized import functional as qF
        >>> filters = torch.randn(33, 16, 3, dtype=torch.float)
        >>> inputs = torch.randn(20, 16, 50, dtype=torch.float)
        >>> bias = torch.randn(33, dtype=torch.float)
        >>>
        >>> scale, zero_point = 1.0, 0
        >>> dtype_inputs = torch.quint8
        >>> dtype_filters = torch.qint8
        >>>
        >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
        >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
        >>> qF.conv1d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    """
    ...

def conv2d(input, weight, bias, stride=..., padding=..., dilation=..., groups=..., padding_mode=..., scale=..., zero_point=..., dtype=...):
    r"""
    Applies a 2D convolution over a quantized 2D input composed of several input
    planes.

    See :class:`~torch.nn.quantized.Conv2d` for details and output shape.

    Args:
        input: quantized input tensor of shape :math:`(\text{minibatch} , \text{in\_channels} , iH , iW)`
        weight: quantized filters of shape :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kH , kW)`
        bias: **non-quantized** bias tensor of shape :math:`(\text{out\_channels})`. The tensor type must be `torch.float`.
        stride: the stride of the convolving kernel. Can be a single number or a
          tuple `(sH, sW)`. Default: 1
        padding: implicit paddings on both sides of the input. Can be a
          single number or a tuple `(padH, padW)`. Default: 0
        dilation: the spacing between kernel elements. Can be a single number or
          a tuple `(dH, dW)`. Default: 1
        groups: split input into groups, :math:`\text{in\_channels}` should be divisible by the
          number of groups. Default: 1
        padding_mode: the padding mode to use. Only "zeros" is supported for quantized convolution at the moment. Default: "zeros"
        scale: quantization scale for the output. Default: 1.0
        zero_point: quantization zero_point for the output. Default: 0
        dtype: quantization data type to use. Default: ``torch.quint8``

    Examples::

        >>> from torch.nn.quantized import functional as qF
        >>> filters = torch.randn(8, 4, 3, 3, dtype=torch.float)
        >>> inputs = torch.randn(1, 4, 5, 5, dtype=torch.float)
        >>> bias = torch.randn(8, dtype=torch.float)
        >>>
        >>> scale, zero_point = 1.0, 0
        >>> dtype_inputs = torch.quint8
        >>> dtype_filters = torch.qint8
        >>>
        >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
        >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
        >>> qF.conv2d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    """
    ...

def conv3d(input, weight, bias, stride=..., padding=..., dilation=..., groups=..., padding_mode=..., scale=..., zero_point=..., dtype=...):
    r"""
    Applies a 3D convolution over a quantized 3D input composed of several input
    planes.

    See :class:`~torch.nn.quantized.Conv3d` for details and output shape.

    Args:
        input: quantized input tensor of shape
          :math:`(\text{minibatch} , \text{in\_channels} , iD , iH , iW)`
        weight: quantized filters of shape
          :math:`(\text{out\_channels} , \frac{\text{in\_channels}}{\text{groups}} , kD , kH , kW)`
        bias: **non-quantized** bias tensor of shape
          :math:`(\text{out\_channels})`. The tensor type must be `torch.float`.
        stride: the stride of the convolving kernel. Can be a single number or a
          tuple `(sD, sH, sW)`. Default: 1
        padding: implicit paddings on both sides of the input. Can be a
          single number or a tuple `(padD, padH, padW)`. Default: 0
        dilation: the spacing between kernel elements. Can be a single number or
          a tuple `(dD, dH, dW)`. Default: 1
        groups: split input into groups, :math:`\text{in\_channels}` should be
          divisible by the number of groups. Default: 1
        padding_mode: the padding mode to use. Only "zeros" is supported for
          quantized convolution at the moment. Default: "zeros"
        scale: quantization scale for the output. Default: 1.0
        zero_point: quantization zero_point for the output. Default: 0
        dtype: quantization data type to use. Default: ``torch.quint8``

    Examples::

        >>> from torch.nn.quantized import functional as qF
        >>> filters = torch.randn(8, 4, 3, 3, 3, dtype=torch.float)
        >>> inputs = torch.randn(1, 4, 5, 5, 5, dtype=torch.float)
        >>> bias = torch.randn(8, dtype=torch.float)
        >>>
        >>> scale, zero_point = 1.0, 0
        >>> dtype_inputs = torch.quint8
        >>> dtype_filters = torch.qint8
        >>>
        >>> q_filters = torch.quantize_per_tensor(filters, scale, zero_point, dtype_filters)
        >>> q_inputs = torch.quantize_per_tensor(inputs, scale, zero_point, dtype_inputs)
        >>> qF.conv3d(q_inputs, q_filters, bias, padding=1, scale=scale, zero_point=zero_point)
    """
    ...

def interpolate(input, size=..., scale_factor=..., mode=..., align_corners=...):
    r"""Down/up samples the input to either the given :attr:`size` or the given
    :attr:`scale_factor`

    See :func:`torch.nn.functional.interpolate` for implementation details.

    The input dimensions are interpreted in the form:
    `mini-batch x channels x [optional depth] x [optional height] x width`.

    .. note:: The input quantization parameters propagate to the output.

    .. note:: Only 2D/3D input is supported for quantized inputs

    .. note:: Only the following modes are supported for the quantized inputs:

        - `bilinear`
        - `nearest`

    Args:
        input (Tensor): the input tensor
        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):
            output spatial size.
        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to match input size if it is a tuple.
        mode (str): algorithm used for upsampling:
            ``'nearest'`` | ``'bilinear'``
        align_corners (bool, optional): Geometrically, we consider the pixels of the
            input and output as squares rather than points.
            If set to ``True``, the input and output tensors are aligned by the
            center points of their corner pixels, preserving the values at the corner pixels.
            If set to ``False``, the input and output tensors are aligned by the corner
            points of their corner pixels, and the interpolation uses edge value padding
            for out-of-boundary values, making this operation *independent* of input size
            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`
            is ``'bilinear'``.
            Default: ``False``
    """
    ...

def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., scale: Optional[float] = ..., zero_point: Optional[int] = ...) -> Tensor:
    r"""
    Applies a linear transformation to the incoming quantized data:
    :math:`y = xA^T + b`.
    See :class:`~torch.nn.quantized.Linear`

    .. note::

      Current implementation packs weights on every call, which has penalty on performance.
      If you want to avoid the overhead, use :class:`~torch.nn.quantized.Linear`.

    Args:
      input (Tensor): Quantized input of type `torch.quint8`
      weight (Tensor): Quantized weight of type `torch.qint8`
      bias (Tensor): None or fp32 bias of type `torch.float`
      scale (double): output scale. If None, derived from the input scale
      zero_point (long): output zero point. If None, derived from the input zero_point

    Shape:
        - Input: :math:`(N, *, in\_features)` where `*` means any number of
          additional dimensions
        - Weight: :math:`(out\_features, in\_features)`
        - Bias: :math:`(out\_features)`
        - Output: :math:`(N, *, out\_features)`
    """
    ...

def max_pool2d(input, kernel_size, stride=..., padding=..., dilation=..., ceil_mode=..., return_indices=...):
    r"""Applies a 2D max pooling over a quantized input signal composed of
    several quantized input planes.

    .. note:: The input quantization parameters are propagated to the output.

    See :class:`~torch.nn.quantized.MaxPool2d` for details.
    """
    ...

def relu(input: Tensor, inplace: bool = ...) -> Tensor:
    r"""relu(input, inplace=False) -> Tensor

    Applies the rectified linear unit function element-wise.
    See :class:`~torch.nn.quantized.ReLU` for more details.

    Args:
        input: quantized input
        inplace: perform the computation inplace
    """
    ...

def leaky_relu(input: Tensor, negative_slope: float = ..., inplace: bool = ..., scale: float = ..., zero_point: int = ...) -> Tensor:
    r"""
    Quantized version of the.
    leaky_relu(input, negative_slope=0.01, inplace=False, scale, zero_point) -> Tensor

    Applies element-wise,
    :math:`\text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)`

    Args:
        input: Quaintized input
        negative_slope: The slope of the negative input
        inplace: Inplace modification of the input tensor
        scale, zero_point: Scale and zero point of thhe output tensor.

    See :class:`~torch.nn.LeakyReLU` for more details.
    """
    ...

def hardtanh(input: Tensor, min_val: float = ..., max_val: float = ..., inplace: bool = ...) -> Tensor:
    r"""This is the quantized version of :func:`~torch.nn.functional.hardtanh`.
    """
    ...

def hardswish(input: Tensor, scale: float, zero_point: int) -> Tensor:
    r"""This is the quantized version of :func:`~torch.nn.functional.hardswish`.

    Args:
        input: quantized input
        scale: quantization scale of the output tensor
        zero_point: quantization zero point of the output tensor
    """
    ...

def threshold(input: Tensor, threshold: float, value: float) -> Tensor:
    r"""Applies the quantized version of the threshold function element-wise:

    .. math::
        x = \begin{cases}
                x & \text{if~} x > \text{threshold} \\
                \text{value} & \text{otherwise}
            \end{cases}

    See :class:`~torch.nn.Threshold` for more details.
    """
    ...

def elu(input: Tensor, scale: float, zero_point: int, alpha: float = ...) -> Tensor:
    r"""This is the quantized version of :func:`~torch.nn.functional.elu`.

    Args:
        input: quantized input
        scale: quantization scale of the output tensor
        zero_point: quantization zero point of the output tensor
        alpha: the alpha constant
    """
    ...

def hardsigmoid(input: Tensor) -> Tensor:
    r"""This is the quantized version of :func:`~torch.nn.functional.hardsigmoid`.
    """
    ...

def clamp(input: Tensor, min_: float, max_: float) -> Tensor:
    r"""float(input, min_, max_) -> Tensor

    Applies the clamp function element-wise.
    See :class:`~torch.nn.quantized.clamp` for more details.

    Args:
        input: quantized input
        min_: minimum value for clamping
        max_: maximum value for clamping
    """
    ...

def upsample(input, size=..., scale_factor=..., mode=..., align_corners=...):
    r"""Upsamples the input to either the given :attr:`size` or the given
    :attr:`scale_factor`

    .. warning::
        This function is deprecated in favor of
        :func:`torch.nn.quantized.functional.interpolate`.
        This is equivalent with ``nn.quantized.functional.interpolate(...)``.

    See :func:`torch.nn.functional.interpolate` for implementation details.

    The input dimensions are interpreted in the form:
    `mini-batch x channels x [optional depth] x [optional height] x width`.

    .. note:: The input quantization parameters propagate to the output.

    .. note:: Only 2D input is supported for quantized inputs

    .. note:: Only the following modes are supported for the quantized inputs:

        - `bilinear`
        - `nearest`

    Args:
        input (Tensor): quantized input tensor
        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int]):
            output spatial size.
        scale_factor (float or Tuple[float]): multiplier for spatial size. Has to be an integer.
        mode (string): algorithm used for upsampling:
            ``'nearest'`` | ``'bilinear'``
        align_corners (bool, optional): Geometrically, we consider the pixels of the
            input and output as squares rather than points.
            If set to ``True``, the input and output tensors are aligned by the
            center points of their corner pixels, preserving the values at the corner pixels.
            If set to ``False``, the input and output tensors are aligned by the corner
            points of their corner pixels, and the interpolation uses edge value padding
            for out-of-boundary values, making this operation *independent* of input size
            when :attr:`scale_factor` is kept the same. This only has an effect when :attr:`mode`
            is ``'bilinear'``.
            Default: ``False``

    .. warning::
        With ``align_corners = True``, the linearly interpolating modes
        (`bilinear`) don't proportionally align the
        output and input pixels, and thus the output values can depend on the
        input size. This was the default behavior for these modes up to version
        0.3.1. Since then, the default behavior is ``align_corners = False``.
        See :class:`~torch.nn.Upsample` for concrete examples on how this
        affects the outputs.
    """
    ...

def upsample_bilinear(input, size=..., scale_factor=...):
    r"""Upsamples the input, using bilinear upsampling.

    .. warning::
        This function is deprecated in favor of
        :func:`torch.nn.quantized.functional.interpolate`.
        This is equivalent with
        ``nn.quantized.functional.interpolate(..., mode='bilinear', align_corners=True)``.

    .. note:: The input quantization parameters propagate to the output.

    .. note:: Only 2D inputs are supported

    Args:
        input (Tensor): quantized input
        size (int or Tuple[int, int]): output spatial size.
        scale_factor (int or Tuple[int, int]): multiplier for spatial size
    """
    ...

def upsample_nearest(input, size=..., scale_factor=...):
    r"""Upsamples the input, using nearest neighbours' pixel values.

    .. warning::
        This function is deprecated in favor of
        :func:`torch.nn.quantized.functional.interpolate`.
        This is equivalent with ``nn.quantized.functional.interpolate(..., mode='nearest')``.

    .. note:: The input quantization parameters propagate to the output.

    .. note:: Only 2D inputs are supported

    Args:
        input (Tensor): quantized input
        size (int or Tuple[int, int] or Tuple[int, int, int]): output spatial
            size.
        scale_factor (int): multiplier for spatial size. Has to be an integer.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from typing import List
from torch import Tensor

class FloatFunctional(torch.nn.Module):
    r"""State collector class for float operatitons.

    The instance of this class can be used instead of the ``torch.`` prefix for
    some operations. See example usage below.

    .. note::

        This class does not provide a ``forward`` hook. Instead, you must use
        one of the underlying functions (e.g. ``add``).

    Examples::

        >>> f_add = FloatFunctional()
        >>> a = torch.tensor(3.0)
        >>> b = torch.tensor(4.0)
        >>> f_add.add(a, b)  # Equivalent to ``torch.add(a, b)``

    Valid operation names:
        - add
        - cat
        - mul
        - add_relu
        - add_scalar
        - mul_scalar
    """
    def __init__(self) -> None:
        ...
    
    def forward(self, x):
        ...
    
    def add(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def add_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def mul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def mul_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def cat(self, x: List[Tensor], dim: int = ...) -> Tensor:
        ...
    
    def add_relu(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    


class QFunctional(torch.nn.Module):
    r"""Wrapper class for quantized operatitons.

    The instance of this class can be used instead of the
    ``torch.ops.quantized`` prefix. See example usage below.

    .. note::

        This class does not provide a ``forward`` hook. Instead, you must use
        one of the underlying functions (e.g. ``add``).

    Examples::

        >>> q_add = QFunctional()
        >>> a = torch.quantize_per_tensor(torch.tensor(3.0), 1.0, 0, torch.qint32)
        >>> b = torch.quantize_per_tensor(torch.tensor(4.0), 1.0, 0, torch.qint32)
        >>> q_add.add(a, b)  # Equivalent to ``torch.ops.quantized.add(a, b, 1.0, 0)``

    Valid operation names:
        - add
        - cat
        - mul
        - add_relu
        - add_scalar
        - mul_scalar
    """
    def __init__(self) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def forward(self, x):
        ...
    
    def add(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def add_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def mul(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    def mul_scalar(self, x: Tensor, y: float) -> Tensor:
        ...
    
    def cat(self, x: List[Tensor], dim: int = ...) -> Tensor:
        ...
    
    def add_relu(self, x: Tensor, y: Tensor) -> Tensor:
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
from torch.nn.modules.pooling import MaxPool2d
from .activation import ELU, Hardswish, ReLU, ReLU6
from .batchnorm import BatchNorm2d, BatchNorm3d
from .normalization import GroupNorm, InstanceNorm1d, InstanceNorm2d, InstanceNorm3d, LayerNorm
from .conv import Conv1d, Conv2d, Conv3d
from .linear import Linear
from .functional_modules import FloatFunctional, QFunctional

class Quantize(torch.nn.Module):
    r"""Quantizes an incoming tensor

    Args:
     `scale`: scale of the output Quantized Tensor
     `zero_point`: zero_point of output Quantized Tensor
     `dtype`: data type of output Quantized Tensor

    Attributes:
      `scale`, `zero_point`, `dtype`

    Examples::
        >>> t = torch.tensor([[1., -1.], [1., -1.]])
        >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
        >>> qm = Quantize(scale, zero_point, dtype)
        >>> qt = qm(t)
        >>> print(qt)
        tensor([[ 1., -1.],
                [ 1., -1.]], size=(2, 2), dtype=torch.qint8, scale=1.0, zero_point=2)
    """
    def __init__(self, scale, zero_point, dtype) -> None:
        ...
    
    def forward(self, X):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    
    def extra_repr(self):
        ...
    


class DeQuantize(torch.nn.Module):
    r"""Dequantizes an incoming tensor

    Examples::
        >>> input = torch.tensor([[1., -1.], [1., -1.]])
        >>> scale, zero_point, dtype = 1.0, 2, torch.qint8
        >>> qm = Quantize(scale, zero_point, dtype)
        >>> quantized_input = qm(input)
        >>> dqm = DeQuantize()
        >>> dequantized = dqm(quantized_input)
        >>> print(dequantized)
        tensor([[ 1., -1.],
                [ 1., -1.]], dtype=torch.float32)
    """
    def __init__(self) -> None:
        ...
    
    def forward(self, Xq):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch

class LayerNorm(torch.nn.LayerNorm):
    r"""This is the quantized version of :class:`~torch.nn.LayerNorm`.

    Additional args:
        * **scale** - quantization scale of the output, type: double.
        * **zero_point** - quantization zero point of the output, type: long.

    """
    def __init__(self, normalized_shape, weight, bias, scale, zero_point, eps=..., elementwise_affine=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class GroupNorm(torch.nn.GroupNorm):
    r"""This is the quantized version of :class:`~torch.nn.GroupNorm`.

    Additional args:
        * **scale** - quantization scale of the output, type: double.
        * **zero_point** - quantization zero point of the output, type: long.

    """
    __constants__ = ...
    def __init__(self, num_groups, num_channels, weight, bias, scale, zero_point, eps=..., affine=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class InstanceNorm1d(torch.nn.InstanceNorm1d):
    r"""This is the quantized version of :class:`~torch.nn.InstanceNorm1d`.

    Additional args:
        * **scale** - quantization scale of the output, type: double.
        * **zero_point** - quantization zero point of the output, type: long.

    """
    def __init__(self, num_features, weight, bias, scale, zero_point, eps=..., momentum=..., affine=..., track_running_stats=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class InstanceNorm2d(torch.nn.InstanceNorm2d):
    r"""This is the quantized version of :class:`~torch.nn.InstanceNorm2d`.

    Additional args:
        * **scale** - quantization scale of the output, type: double.
        * **zero_point** - quantization zero point of the output, type: long.

    """
    def __init__(self, num_features, weight, bias, scale, zero_point, eps=..., momentum=..., affine=..., track_running_stats=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class InstanceNorm3d(torch.nn.InstanceNorm3d):
    r"""This is the quantized version of :class:`~torch.nn.InstanceNorm3d`.

    Additional args:
        * **scale** - quantization scale of the output, type: double.
        * **zero_point** - quantization zero point of the output, type: long.

    """
    def __init__(self, num_features, weight, bias, scale, zero_point, eps=..., momentum=..., affine=..., track_running_stats=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch

class BatchNorm2d(torch.nn.BatchNorm2d):
    r"""This is the quantized version of :class:`~torch.nn.BatchNorm2d`.
    """
    def __init__(self, num_features, eps=..., momentum=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class BatchNorm3d(torch.nn.BatchNorm3d):
    r"""This is the quantized version of :class:`~torch.nn.BatchNorm3d`.
    """
    def __init__(self, num_features, eps=..., momentum=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from torch._jit_internal import Optional

class LinearPackedParams(torch.nn.Module):
    _version = ...
    def __init__(self, dtype=...) -> None:
        ...
    
    @torch.jit.export
    def set_weight_bias(self, weight: torch.Tensor, bias: Optional[torch.Tensor]) -> None:
        ...
    
    def forward(self, x):
        ...
    
    @torch.jit.export
    def __getstate__(self):
        ...
    
    @torch.jit.export
    def __setstate__(self, state):
        ...
    
    def __repr__(self):
        ...
    


class Linear(torch.nn.Module):
    r"""
    A quantized linear module with quantized tensor as inputs and outputs.
    We adopt the same interface as `torch.nn.Linear`, please see
    https://pytorch.org/docs/stable/nn.html#torch.nn.Linear for documentation.

    Similar to :class:`~torch.nn.Linear`, attributes will be randomly
    initialized at module creation time and will be overwritten later

    Attributes:
        weight (Tensor): the non-learnable quantized weights of the module of
                         shape :math:`(\text{out\_features}, \text{in\_features})`.
        bias (Tensor): the non-learnable bias of the module of shape :math:`(\text{out\_features})`.
                If :attr:`bias` is ``True``, the values are initialized to zero.
        scale: `scale` parameter of output Quantized Tensor, type: double
        zero_point: `zero_point` parameter for output Quantized Tensor, type: long

    Examples::

        >>> m = nn.quantized.Linear(20, 30)
        >>> input = torch.randn(128, 20)
        >>> input = torch.quantize_per_tensor(input, 1.0, 0, torch.quint8)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 30])
    """
    _version = ...
    _FLOAT_MODULE = ...
    def __init__(self, in_features, out_features, bias_=..., dtype=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def __repr__(self):
        ...
    
    def forward(self, x):
        ...
    
    def weight(self):
        ...
    
    def bias(self):
        ...
    
    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor]) -> None:
        ...
    
    @classmethod
    def from_float(cls, mod):
        r"""Create a quantized module from a float module or qparams_dict

        Args:
            mod (Module): a float module, either produced by torch.quantization
                          utilities or provided by the user
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

_pair_from_first = _ntuple_from_first(2)
"""
This type stub file was generated by pyright.
"""

import torch

class ReLU(torch.nn.ReLU):
    r"""Applies quantized rectified linear unit function element-wise:

    :math:`\text{ReLU}(x)= \max(x_0, x)`, where :math:`x_0` is the zero point.

    Please see https://pytorch.org/docs/stable/nn.html#torch.nn.ReLU
    for more documentation on ReLU.

    Args:
        inplace: (Currently not supported) can optionally do the operation in-place.

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Examples::

        >>> m = nn.quantized.ReLU()
        >>> input = torch.randn(2)
        >>> input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)
        >>> output = m(input)
    """
    def __init__(self, inplace=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    


class ReLU6(torch.nn.ReLU):
    r"""Applies the element-wise function:

    :math:`\text{ReLU6}(x) = \min(\max(x_0, x), q(6))`, where :math:`x_0` is the
    zero_point, and :math:`q(6)` is the quantized representation of number 6.

    Args:
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: scripts/activation_images/ReLU6.png

    Examples::

        >>> m = nn.quantized.ReLU6()
        >>> input = torch.randn(2)
        >>> input = torch.quantize_per_tensor(input, 1.0, 0, dtype=torch.qint32)
        >>> output = m(input)
    """
    def __init__(self, inplace=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    


class Hardswish(torch.nn.Hardswish):
    r"""This is the quantized version of :class:`~torch.nn.Hardswish`.

    Args:
        scale: quantization scale of the output tensor
        zero_point: quantization zero point of the output tensor
    """
    def __init__(self, scale, zero_point) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    


class ELU(torch.nn.ELU):
    r"""This is the quantized equivalent of :class:`~torch.nn.ELU`.

    Args:
        scale: quantization scale of the output tensor
        zero_point: quantization zero point of the output tensor
        alpha: the alpha constant
    """
    def __init__(self, scale, zero_point, alpha=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @staticmethod
    def from_float(mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from typing import Optional

r"""Quantized convolution modules."""
class _ConvNd(nn.Module):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode=...) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    @torch.jit.export
    def __getstate__(self):
        ...
    
    @torch.jit.export
    def __setstate__(self, state):
        ...
    


class Conv1d(_ConvNd):
    r"""Applies a 1D convolution over a quantized input signal composed of
    several quantized input planes.

    For details on input arguments, parameters, and implementation see
    :class:`~torch.nn.Conv1d`.

    .. note::
        Only `zeros` is supported for the :attr:`padding_mode` argument.

    .. note::
        Only `torch.quint8` is supported for the input data type.


    Attributes:
        weight (Tensor):     packed tensor derived from the learnable weight
                             parameter.
        scale (Tensor):      scalar for the output scale
        zero_point (Tensor): scalar for the output zero point

    See :class:`~torch.nn.Conv1d` for other attributes.

    Examples::

        >>> m = nn.quantized.Conv1d(16, 33, 3, stride=2)
        >>> input = torch.randn(20, 16, 100)
        >>> # quantize input to quint8
        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0,
                                                dtype=torch.quint8)
        >>> output = m(q_input)

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor]) -> None:
        ...
    
    def weight(self):
        ...
    
    def bias(self):
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        r"""Creates a quantized module from a float module or qparams_dict.

        Args:
            mod (Module): a float module, either produced by torch.quantization
              utilities or provided by the user
        """
        ...
    


class Conv2d(_ConvNd):
    r"""Applies a 2D convolution over a quantized input signal composed of
    several quantized input planes.

    For details on input arguments, parameters, and implementation see
    :class:`~torch.nn.Conv2d`.

    .. note::
        Only `zeros` is supported for the :attr:`padding_mode` argument.

    .. note::
        Only `torch.quint8` is supported for the input data type.


    Attributes:
        weight (Tensor):     packed tensor derived from the learnable weight
                             parameter.
        scale (Tensor):      scalar for the output scale
        zero_point (Tensor): scalar for the output zero point

    See :class:`~torch.nn.Conv2d` for other attributes.

    Examples::

        >>> # With square kernels and equal stride
        >>> m = nn.quantized.Conv2d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
        >>> # non-square kernels and unequal stride and with padding and dilation
        >>> m = nn.quantized.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
        >>> input = torch.randn(20, 16, 50, 100)
        >>> # quantize input to quint8
        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
        >>> output = m(q_input)

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor]) -> None:
        ...
    
    def weight(self):
        ...
    
    def bias(self):
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        r"""Creates a quantized module from a float module or qparams_dict.

        Args:
            mod (Module): a float module, either produced by torch.quantization
              utilities or provided by the user
        """
        ...
    


class Conv3d(_ConvNd):
    r"""Applies a 3D convolution over a quantized input signal composed of
    several quantized input planes.

    For details on input arguments, parameters, and implementation see
    :class:`~torch.nn.Conv3d`.

    .. note::
        Only `zeros` is supported for the :attr:`padding_mode` argument.

    .. note::
        Only `torch.quint8` is supported for the input data type.


    Attributes:
        weight (Tensor):     packed tensor derived from the learnable weight
                             parameter.
        scale (Tensor):      scalar for the output scale
        zero_point (Tensor): scalar for the output zero point

    See :class:`~torch.nn.Conv3d` for other attributes.

    Examples::

        >>> # With square kernels and equal stride
        >>> m = nn.quantized.Conv3d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2))
        >>> # non-square kernels and unequal stride and with padding and dilation
        >>> m = nn.quantized.Conv3d(16, 33, (3, 5, 5), stride=(1, 2, 2), padding=(1, 2, 2), dilation=(1, 2, 2))
        >>> input = torch.randn(20, 16, 56, 56, 56)
        >>> # quantize input to quint8
        >>> q_input = torch.quantize_per_tensor(input, scale=1.0, zero_point=0, dtype=torch.quint8)
        >>> output = m(q_input)

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def set_weight_bias(self, w: torch.Tensor, b: Optional[torch.Tensor]) -> None:
        ...
    
    def weight(self):
        ...
    
    def bias(self):
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        r"""Creates a quantized module from a float module or qparams_dict.

        Args:
            mod (Module): a float module, either produced by torch.quantization
              utilities or provided by the user
        """
        ...
    


class _ConvTransposeNd(_ConvNd, nn.modules.conv._ConvTransposeNd):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

import builtins
from .. import Tensor

class Parameter(Tensor):
    def __init__(self, data: Tensor = ..., requires_grad: builtins.bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from . import rnn
from .clip_grad import clip_grad_norm, clip_grad_norm_, clip_grad_value_
from .weight_norm import remove_weight_norm, weight_norm
from .convert_parameters import parameters_to_vector, vector_to_parameters
from .spectral_norm import remove_spectral_norm, spectral_norm
from .fusion import fuse_conv_bn_eval, fuse_conv_bn_weights
from .memory_format import convert_conv2d_weight_memory_format

"""
This type stub file was generated by pyright.
"""

def fuse_conv_bn_eval(conv, bn):
    ...

def fuse_conv_bn_weights(conv_w, conv_b, bn_rm, bn_rv, bn_eps, bn_w, bn_b):
    ...

"""
This type stub file was generated by pyright.
"""

def convert_conv2d_weight_memory_format(module, memory_format):
    r"""Convert ``memory_format`` of ``nn.Conv2d.weight`` to ``memory_format``
    The conversion recursively applies to nested ``nn.Module``, including ``module``.
    Note that it only changes the memory_format, but not the semantics of each dimensions.
    This function is used to facilitate the computation to adopt NHWC kernels, which
    provides considerable speed up for fp16 data on CUDA devices with compute capability >= 7.0

    .. note::
        Calling ``model.to(memory_format=torch.channels_last)`` is more aggressive
        than the utility function ``convert_conv2d_weight_memory_format``. Any
        layer with 4d weight will be affected by ``model.to``, which does not
        necessarily benefit from conversion to specified ``memory_format``.
        One place we are confident in is that NHWC(channels_last) conversion for
        convolution in cuDNN, As it is beneficial to run convolution in NHWC,
        even in cases where we have to apply permutation to input tensors.

        Hence our strategy here is to convert only the weight of convolution to
        channels_last. This ensures that;
        1. Fast convolution kernels will be used, the benefit of which could
           outweigh overhead of permutation (if input is not in the same format)
        2. No unnecessary permutations are applied on layers that do not benefit
           from memory_format conversion.

        The optimal case is that, layers between convolution layers are channels
        last compatible. Input tensor would be permuted to channels last when it
        encounters the first convolution layer and stay in that memory format.
        Hence following convolutions will not need to permute its input tensor.

        In case where a channels last incompatible layer is between convolution
        layers, we need to permute the input tensor back to contiguous format
        for that layer. The input tensor will go through the remaining layers in
        contiguous format and be permuted to channels last when it encounters
        another convolution layer. There's no point in propagating that
        permutation to an earlier layer, as most layers are quite agnostic to
        ``memory_format``.

        This claim might change when PyTorch supports fusion of permutation, as
        there might have been a better spot to fuse the permutation other than
        immediately before a convolution.

    Args:
        module (nn.Module): ``nn.Conv2d`` & ``nn.ConvTranspose2d``  or container
                            ``nn.Module``
        format: user specified ``memory_format``,
            e.g. ``torch.channels_last`` or ``torch.contiguous_format``

    Returns:
        The original module with updated ``nn.Conv2d``

    Example:
        >>>  input = torch.randint(1, 10, (2, 8, 4, 4), dtype=torch.float16, device="cuda")
        >>>  model = nn.Sequential(
        >>>      nn.Conv2d(8, 4, 3)).cuda().half()
        >>>  # This is identical to:
        >>>  # nn.utils.convert_conv2d_weight_memory_format(model, torch.channels_last)
        >>>  model = nn.utils.convert_conv2d_weight_memory_format(model, torch.channels_last)
        >>>  out = model(input)
    """
    ...

"""
This type stub file was generated by pyright.
"""

"""
Spectral Normalization from https://arxiv.org/abs/1802.05957
"""
class SpectralNorm(object):
    _version = ...
    def __init__(self, name=..., n_power_iterations=..., dim=..., eps=...) -> None:
        ...
    
    def reshape_weight_to_matrix(self, weight):
        ...
    
    def compute_weight(self, module, do_power_iteration):
        ...
    
    def remove(self, module):
        ...
    
    def __call__(self, module, inputs):
        ...
    
    @staticmethod
    def apply(module, name, n_power_iterations, dim, eps):
        ...
    


class SpectralNormLoadStateDictPreHook(object):
    def __init__(self, fn) -> None:
        ...
    
    def __call__(self, state_dict, prefix, local_metadata, strict, missing_keys, unexpected_keys, error_msgs):
        ...
    


class SpectralNormStateDictHook(object):
    def __init__(self, fn) -> None:
        ...
    
    def __call__(self, module, state_dict, prefix, local_metadata):
        ...
    


def spectral_norm(module, name=..., n_power_iterations=..., eps=..., dim=...):
    r"""Applies spectral normalization to a parameter in the given module.

    .. math::
        \mathbf{W}_{SN} = \dfrac{\mathbf{W}}{\sigma(\mathbf{W})},
        \sigma(\mathbf{W}) = \max_{\mathbf{h}: \mathbf{h} \ne 0} \dfrac{\|\mathbf{W} \mathbf{h}\|_2}{\|\mathbf{h}\|_2}

    Spectral normalization stabilizes the training of discriminators (critics)
    in Generative Adversarial Networks (GANs) by rescaling the weight tensor
    with spectral norm :math:`\sigma` of the weight matrix calculated using
    power iteration method. If the dimension of the weight tensor is greater
    than 2, it is reshaped to 2D in power iteration method to get spectral
    norm. This is implemented via a hook that calculates spectral norm and
    rescales weight before every :meth:`~Module.forward` call.

    See `Spectral Normalization for Generative Adversarial Networks`_ .

    .. _`Spectral Normalization for Generative Adversarial Networks`: https://arxiv.org/abs/1802.05957

    Args:
        module (nn.Module): containing module
        name (str, optional): name of weight parameter
        n_power_iterations (int, optional): number of power iterations to
            calculate spectral norm
        eps (float, optional): epsilon for numerical stability in
            calculating norms
        dim (int, optional): dimension corresponding to number of outputs,
            the default is ``0``, except for modules that are instances of
            ConvTranspose{1,2,3}d, when it is ``1``

    Returns:
        The original module with the spectral norm hook

    Example::

        >>> m = spectral_norm(nn.Linear(20, 40))
        >>> m
        Linear(in_features=20, out_features=40, bias=True)
        >>> m.weight_u.size()
        torch.Size([40])

    """
    ...

def remove_spectral_norm(module, name=...):
    r"""Removes the spectral normalization reparameterization from a module.

    Args:
        module (Module): containing module
        name (str, optional): name of weight parameter

    Example:
        >>> m = spectral_norm(nn.Linear(40, 10))
        >>> remove_spectral_norm(m)
    """
    ...

"""
This type stub file was generated by pyright.
"""

def clip_grad_norm_(parameters, max_norm, norm_type=...):
    r"""Clips gradient norm of an iterable of parameters.

    The norm is computed over all gradients together, as if they were
    concatenated into a single vector. Gradients are modified in-place.

    Arguments:
        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
            single Tensor that will have gradients normalized
        max_norm (float or int): max norm of the gradients
        norm_type (float or int): type of the used p-norm. Can be ``'inf'`` for
            infinity norm.

    Returns:
        Total norm of the parameters (viewed as a single vector).
    """
    ...

def clip_grad_norm(parameters, max_norm, norm_type=...):
    r"""Clips gradient norm of an iterable of parameters.

    .. warning::
        This method is now deprecated in favor of
        :func:`torch.nn.utils.clip_grad_norm_`.
    """
    ...

def clip_grad_value_(parameters, clip_value):
    r"""Clips gradient of an iterable of parameters at specified value.

    Gradients are modified in-place.

    Arguments:
        parameters (Iterable[Tensor] or Tensor): an iterable of Tensors or a
            single Tensor that will have gradients normalized
        clip_value (float or int): maximum allowed value of the gradients.
            The gradients are clipped in the range
            :math:`\left[\text{-clip\_value}, \text{clip\_value}\right]`
    """
    ...

"""
This type stub file was generated by pyright.
"""

r"""
Weight Normalization from https://arxiv.org/abs/1602.07868
"""
class WeightNorm(object):
    def __init__(self, name, dim) -> None:
        ...
    
    def compute_weight(self, module):
        ...
    
    @staticmethod
    def apply(module, name, dim):
        ...
    
    def remove(self, module):
        ...
    
    def __call__(self, module, inputs):
        ...
    


def weight_norm(module, name=..., dim=...):
    r"""Applies weight normalization to a parameter in the given module.

    .. math::
         \mathbf{w} = g \dfrac{\mathbf{v}}{\|\mathbf{v}\|}

    Weight normalization is a reparameterization that decouples the magnitude
    of a weight tensor from its direction. This replaces the parameter specified
    by :attr:`name` (e.g. ``'weight'``) with two parameters: one specifying the magnitude
    (e.g. ``'weight_g'``) and one specifying the direction (e.g. ``'weight_v'``).
    Weight normalization is implemented via a hook that recomputes the weight
    tensor from the magnitude and direction before every :meth:`~Module.forward`
    call.

    By default, with ``dim=0``, the norm is computed independently per output
    channel/plane. To compute a norm over the entire weight tensor, use
    ``dim=None``.

    See https://arxiv.org/abs/1602.07868

    Args:
        module (Module): containing module
        name (str, optional): name of weight parameter
        dim (int, optional): dimension over which to compute the norm

    Returns:
        The original module with the weight norm hook

    Example::

        >>> m = weight_norm(nn.Linear(20, 40), name='weight')
        >>> m
        Linear(in_features=20, out_features=40, bias=True)
        >>> m.weight_g.size()
        torch.Size([40, 1])
        >>> m.weight_v.size()
        torch.Size([40, 20])

    """
    ...

def remove_weight_norm(module, name=...):
    r"""Removes the weight normalization reparameterization from a module.

    Args:
        module (Module): containing module
        name (str, optional): name of weight parameter

    Example:
        >>> m = weight_norm(nn.Linear(20, 40))
        >>> remove_weight_norm(m)
    """
    ...

"""
This type stub file was generated by pyright.
"""

def parameters_to_vector(parameters):
    r"""Convert parameters to one vector

    Arguments:
        parameters (Iterable[Tensor]): an iterator of Tensors that are the
            parameters of a model.

    Returns:
        The parameters represented by a single vector
    """
    ...

def vector_to_parameters(vec, parameters):
    r"""Convert one vector to the parameters

    Arguments:
        vec (Tensor): a single vector represents the parameters of a model.
        parameters (Iterable[Tensor]): an iterator of Tensors that are the
            parameters of a model.
    """
    ...

"""
This type stub file was generated by pyright.
"""

from collections import namedtuple
from ..._jit_internal import Optional

PackedSequence_ = namedtuple('PackedSequence', ['data', 'batch_sizes', 'sorted_indices', 'unsorted_indices'])
def bind(optional, fn):
    ...

class PackedSequence(PackedSequence_):
    r"""Holds the data and list of :attr:`batch_sizes` of a packed sequence.

    All RNN modules accept packed sequences as inputs.

    Note:
        Instances of this class should never be created manually. They are meant
        to be instantiated by functions like :func:`pack_padded_sequence`.

        Batch sizes represent the number elements at each sequence step in
        the batch, not the varying sequence lengths passed to
        :func:`pack_padded_sequence`.  For instance, given data ``abc`` and ``x``
        the :class:`PackedSequence` would contain data ``axbc`` with
        ``batch_sizes=[2,1,1]``.

    Attributes:
        data (Tensor): Tensor containing packed sequence
        batch_sizes (Tensor): Tensor of integers holding
            information about the batch size at each sequence step
        sorted_indices (Tensor, optional): Tensor of integers holding how this
            :class:`PackedSequence` is constructed from sequences.
        unsorted_indices (Tensor, optional): Tensor of integers holding how this
            to recover the original sequences with correct order.

    .. note::
        :attr:`data` can be on arbitrary device and of arbitrary dtype.
        :attr:`sorted_indices` and :attr:`unsorted_indices` must be ``torch.int64``
        tensors on the same device as :attr:`data`.

        However, :attr:`batch_sizes` should always be a CPU ``torch.int64`` tensor.

        This invariant is maintained throughout :class:`PackedSequence` class,
        and all functions that construct a `:class:PackedSequence` in PyTorch
        (i.e., they only pass in tensors conforming to this constraint).

    """
    def __new__(cls, data, batch_sizes=..., sorted_indices=..., unsorted_indices=...):
        ...
    
    def pin_memory(self):
        ...
    
    def cuda(self, *args, **kwargs):
        ...
    
    def cpu(self, *args, **kwargs):
        ...
    
    def double(self):
        ...
    
    def float(self):
        ...
    
    def half(self):
        ...
    
    def long(self):
        ...
    
    def int(self):
        ...
    
    def short(self):
        ...
    
    def char(self):
        ...
    
    def byte(self):
        ...
    
    def to(self, *args, **kwargs):
        r"""Performs dtype and/or device conversion on `self.data`.

        It has similar signature as :meth:`torch.Tensor.to`, except optional
        arguments like `non_blocking` and `copy` should be passed as kwargs,
        not args, or they will not apply to the index tensors.

        .. note::

            If the ``self.data`` Tensor already has the correct :class:`torch.dtype`
            and :class:`torch.device`, then ``self`` is returned.
            Otherwise, returns a copy with the desired configuration.
        """
        ...
    
    @property
    def is_cuda(self):
        r"""Returns true if `self.data` stored on a gpu"""
        ...
    
    def is_pinned(self):
        r"""Returns true if `self.data` stored on in pinned memory"""
        ...
    


def invert_permutation(permutation: Optional[Tensor]) -> Optional[Tensor]:
    ...

def pack_padded_sequence(input: Tensor, lengths: Tensor, batch_first: bool = ..., enforce_sorted: bool = ...) -> PackedSequence:
    r"""Packs a Tensor containing padded sequences of variable length.

    :attr:`input` can be of size ``T x B x *`` where `T` is the length of the
    longest sequence (equal to ``lengths[0]``), ``B`` is the batch size, and
    ``*`` is any number of dimensions (including 0). If ``batch_first`` is
    ``True``, ``B x T x *`` :attr:`input` is expected.

    For unsorted sequences, use `enforce_sorted = False`. If :attr:`enforce_sorted` is
    ``True``, the sequences should be sorted by length in a decreasing order, i.e.
    ``input[:,0]`` should be the longest sequence, and ``input[:,B-1]`` the shortest
    one. `enforce_sorted = True` is only necessary for ONNX export.

    Note:
        This function accepts any input that has at least two dimensions. You
        can apply it to pack the labels, and use the output of the RNN with
        them to compute the loss directly. A Tensor can be retrieved from
        a :class:`PackedSequence` object by accessing its ``.data`` attribute.

    Arguments:
        input (Tensor): padded batch of variable length sequences.
        lengths (Tensor): list of sequences lengths of each batch element.
        batch_first (bool, optional): if ``True``, the input is expected in ``B x T x *``
            format.
        enforce_sorted (bool, optional): if ``True``, the input is expected to
            contain sequences sorted by length in a decreasing order. If
            ``False``, the input will get sorted unconditionally. Default: ``True``.

    Returns:
        a :class:`PackedSequence` object
    """
    ...

def pad_packed_sequence(sequence: PackedSequence, batch_first: bool = ..., padding_value: float = ..., total_length: Optional[int] = ...) -> Tuple[Tensor, Tensor]:
    r"""Pads a packed batch of variable length sequences.

    It is an inverse operation to :func:`pack_padded_sequence`.

    The returned Tensor's data will be of size ``T x B x *``, where `T` is the length
    of the longest sequence and `B` is the batch size. If ``batch_first`` is True,
    the data will be transposed into ``B x T x *`` format.

    Example:
        >>> from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence
        >>> seq = torch.tensor([[1,2,0], [3,0,0], [4,5,6]])
        >>> lens = [2, 1, 3]
        >>> packed = pack_padded_sequence(seq, lens, batch_first=True, enforce_sorted=False)
        >>> packed
        PackedSequence(data=tensor([4, 1, 3, 5, 2, 6]), batch_sizes=tensor([3, 2, 1]),
                       sorted_indices=tensor([2, 0, 1]), unsorted_indices=tensor([1, 2, 0]))
        >>> seq_unpacked, lens_unpacked = pad_packed_sequence(packed, batch_first=True)
        >>> seq_unpacked
        tensor([[1, 2, 0],
                [3, 0, 0],
                [4, 5, 6]])
        >>> lens_unpacked
        tensor([2, 1, 3])

    .. note::
        :attr:`total_length` is useful to implement the
        ``pack sequence -> recurrent network -> unpack sequence`` pattern in a
        :class:`~torch.nn.Module` wrapped in :class:`~torch.nn.DataParallel`.
        See :ref:`this FAQ section <pack-rnn-unpack-with-data-parallelism>` for
        details.

    Arguments:
        sequence (PackedSequence): batch to pad
        batch_first (bool, optional): if ``True``, the output will be in ``B x T x *``
            format.
        padding_value (float, optional): values for padded elements.
        total_length (int, optional): if not ``None``, the output will be padded to
            have length :attr:`total_length`. This method will throw :class:`ValueError`
            if :attr:`total_length` is less than the max sequence length in
            :attr:`sequence`.

    Returns:
        Tuple of Tensor containing the padded sequence, and a Tensor
        containing the list of lengths of each sequence in the batch.
        Batch elements will be re-ordered as they were ordered originally when
        the batch was passed to ``pack_padded_sequence`` or ``pack_sequence``.




    """
    ...

def pad_sequence(sequences: List[Tensor], batch_first: bool = ..., padding_value: float = ...) -> Tensor:
    r"""Pad a list of variable length Tensors with ``padding_value``

    ``pad_sequence`` stacks a list of Tensors along a new dimension,
    and pads them to equal length. For example, if the input is list of
    sequences with size ``L x *`` and if batch_first is False, and ``T x B x *``
    otherwise.

    `B` is batch size. It is equal to the number of elements in ``sequences``.
    `T` is length of the longest sequence.
    `L` is length of the sequence.
    `*` is any number of trailing dimensions, including none.

    Example:
        >>> from torch.nn.utils.rnn import pad_sequence
        >>> a = torch.ones(25, 300)
        >>> b = torch.ones(22, 300)
        >>> c = torch.ones(15, 300)
        >>> pad_sequence([a, b, c]).size()
        torch.Size([25, 3, 300])

    Note:
        This function returns a Tensor of size ``T x B x *`` or ``B x T x *``
        where `T` is the length of the longest sequence. This function assumes
        trailing dimensions and type of all the Tensors in sequences are same.

    Arguments:
        sequences (list[Tensor]): list of variable length sequences.
        batch_first (bool, optional): output will be in ``B x T x *`` if True, or in
            ``T x B x *`` otherwise
        padding_value (float, optional): value for padded elements. Default: 0.

    Returns:
        Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.
        Tensor of size ``B x T x *`` otherwise
    """
    ...

def pack_sequence(sequences: List[Tensor], enforce_sorted: bool = ...) -> PackedSequence:
    r"""Packs a list of variable length Tensors

    ``sequences`` should be a list of Tensors of size ``L x *``, where `L` is
    the length of a sequence and `*` is any number of trailing dimensions,
    including zero.

    For unsorted sequences, use `enforce_sorted = False`. If ``enforce_sorted``
    is ``True``, the sequences should be sorted in the order of decreasing length.
    ``enforce_sorted = True`` is only necessary for ONNX export.


    Example:
        >>> from torch.nn.utils.rnn import pack_sequence
        >>> a = torch.tensor([1,2,3])
        >>> b = torch.tensor([4,5])
        >>> c = torch.tensor([6])
        >>> pack_sequence([a, b, c])
        PackedSequence(data=tensor([ 1,  4,  6,  2,  5,  3]), batch_sizes=tensor([ 3,  2,  1]))


    Arguments:
        sequences (list[Tensor]): A list of sequences of decreasing length.
        enforce_sorted (bool, optional): if ``True``, checks that the input
            contains sequences sorted by length in a decreasing order. If
            ``False``, this condition is not checked. Default: ``True``.

    Returns:
        a :class:`PackedSequence` object
    """
    ...

"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .modules import ConvBn2d, ConvBnReLU2d, ConvReLU2d, LinearReLU, freeze_bn_stats, update_bn_stats

"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .linear_relu import LinearReLU
from .conv_fused import ConvBn2d, ConvBnReLU2d, ConvReLU2d, freeze_bn_stats, update_bn_stats

"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
import torch.nn.qat as nnqat

class _ConvBnNd(nn.modules.conv._ConvNd):
    _version = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode, eps=..., momentum=..., freeze_bn=..., qconfig=...) -> None:
        ...
    
    def reset_running_stats(self):
        ...
    
    def reset_bn_parameters(self):
        ...
    
    def reset_parameters(self):
        ...
    
    def update_bn_stats(self):
        ...
    
    def freeze_bn_stats(self):
        ...
    
    def extra_repr(self):
        ...
    
    def forward(self, input):
        ...
    
    def train(self, mode=...):
        """
        Batchnorm's training behavior is using the self.training flag. Prevent
        changing it if BN is frozen. This makes sure that calling `model.train()`
        on a model with a frozen BN will behave properly.
        """
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        r"""Create a qat module from a float module or qparams_dict

            Args: `mod` a float module, either produced by torch.quantization utilities
            or directly from user
        """
        ...
    


class ConvBn2d(_ConvBnNd, nn.Conv2d):
    r"""
    A ConvBn2d module is a module fused from Conv2d and BatchNorm2d,
    attached with FakeQuantize modules for both output activation and weight,
    used in quantization aware training.

    We combined the interface of :class:`torch.nn.Conv2d` and
    :class:`torch.nn.BatchNorm2d`.

    Implementation details: https://arxiv.org/pdf/1806.08342.pdf section 3.2.2

    Similar to :class:`torch.nn.Conv2d`, with FakeQuantize modules initialized
    to default.

    Attributes:
        freeze_bn:
        activation_post_process: fake quant module for output activation
        weight_fake_quant: fake quant module for weight

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=..., eps=..., momentum=..., freeze_bn=..., qconfig=...) -> None:
        ...
    


class ConvBnReLU2d(ConvBn2d):
    r"""
    A ConvBnReLU2d module is a module fused from Conv2d, BatchNorm2d and ReLU,
    attached with FakeQuantize modules for both output activation and weight,
    used in quantization aware training.

    We combined the interface of :class:`torch.nn.Conv2d` and
    :class:`torch.nn.BatchNorm2d` and :class:`torch.nn.ReLU`.

    Implementation details: https://arxiv.org/pdf/1806.08342.pdf

    Similar to `torch.nn.Conv2d`, with FakeQuantize modules initialized to
    default.

    Attributes:
        observer: fake quant module for output activation, it's called observer
            to align with post training flow
        weight_fake_quant: fake quant module for weight

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=..., eps=..., momentum=..., freeze_bn=..., qconfig=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        ...
    


class ConvReLU2d(nnqat.Conv2d):
    r"""
    A ConvReLU2d module is a fused module of Conv2d and ReLU, attached with
    FakeQuantize modules for both output activation and weight for
    quantization aware training.

    We combined the interface of :class:`~torch.nn.Conv2d` and
    :class:`~torch.nn.BatchNorm2d`.

    Attributes:
        activation_post_process: fake quant module for output activation
        weight_fake_quant: fake quant module for weight

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=..., qconfig=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        ...
    


def update_bn_stats(mod):
    ...

def freeze_bn_stats(mod):
    ...

"""
This type stub file was generated by pyright.
"""

import torch.nn.qat as nnqat

class LinearReLU(nnqat.Linear):
    r"""
    A LinearReLU module fused from Linear and ReLU modules, attached with
    FakeQuantize modules for output activation and weight, used in
    quantization aware training.

    We adopt the same interface as :class:`torch.nn.Linear`.

    Similar to `torch.nn.intrinsic.LinearReLU`, with FakeQuantize modules initialized to
    default.

    Attributes:
        activation_post_process: fake quant module for output activation
        weight: fake quant module for weight

    Examples::

        >>> m = nn.qat.LinearReLU(20, 30)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 30])
    """
    _FLOAT_MODULE = ...
    def __init__(self, in_features, out_features, bias=..., qconfig=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod, qconfig=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .modules import BNReLU2d, BNReLU3d, ConvBn1d, ConvBn2d, ConvBn3d, ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvReLU1d, ConvReLU2d, ConvReLU3d, LinearReLU

"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .modules import BNReLU2d, BNReLU3d, ConvReLU1d, ConvReLU2d, ConvReLU3d, LinearReLU

"""
This type stub file was generated by pyright.
"""

import torch.nn.quantized as nnq

class ConvReLU1d(nnq.Conv1d):
    r"""
    A ConvReLU1d module is a fused module of Conv1d and ReLU

    We adopt the same interface as :class:`torch.nn.quantized.Conv1d`.

    Attributes:
        Same as torch.nn.quantized.Conv1d

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class ConvReLU2d(nnq.Conv2d):
    r"""
    A ConvReLU2d module is a fused module of Conv2d and ReLU

    We adopt the same interface as :class:`torch.nn.quantized.Conv2d`.

    Attributes:
        Same as torch.nn.quantized.Conv2d

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class ConvReLU3d(nnq.Conv3d):
    r"""
    A ConvReLU3d module is a fused module of Conv3d and ReLU

    We adopt the same interface as :class:`torch.nn.quantized.Conv3d`.

    Attributes: Same as torch.nn.quantized.Conv3d

    """
    _FLOAT_MODULE = ...
    def __init__(self, in_channels, out_channels, kernel_size, stride=..., padding=..., dilation=..., groups=..., bias=..., padding_mode=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .linear_relu import LinearReLU
from .conv_relu import ConvReLU1d, ConvReLU2d, ConvReLU3d
from .bn_relu import BNReLU2d, BNReLU3d

"""
This type stub file was generated by pyright.
"""

import torch.nn.quantized as nnq

class BNReLU2d(nnq.BatchNorm2d):
    r"""
    A BNReLU2d module is a fused module of BatchNorm2d and ReLU

    We adopt the same interface as :class:`torch.nn.quantized.BatchNorm2d`.

    Attributes:
        Same as torch.nn.quantized.BatchNorm2d

    """
    _FLOAT_MODULE = ...
    def __init__(self, num_features, eps=..., momentum=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


class BNReLU3d(nnq.BatchNorm3d):
    r"""
    A BNReLU3d module is a fused module of BatchNorm3d and ReLU

    We adopt the same interface as :class:`torch.nn.quantized.BatchNorm3d`.

    .. note::
    Attributes: Same as torch.nn.quantized.BatchNorm3d

    """
    _FLOAT_MODULE = ...
    def __init__(self, num_features, eps=..., momentum=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch.nn.quantized as nnq

class LinearReLU(nnq.Linear):
    r"""
    A LinearReLU module fused from Linear and ReLU modules

    We adopt the same interface as :class:`torch.nn.quantized.Linear`.

    Attributes:
        Same as torch.nn.quantized.Linear

    Examples::

        >>> m = nn.intrinsic.LinearReLU(20, 30)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 30])
    """
    _FLOAT_MODULE = ...
    def __init__(self, in_features, out_features, bias=..., dtype=...) -> None:
        ...
    
    def forward(self, input):
        ...
    
    @classmethod
    def from_float(cls, mod):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .fused import BNReLU2d, BNReLU3d, ConvBn1d, ConvBn2d, ConvBn3d, ConvBnReLU1d, ConvBnReLU2d, ConvBnReLU3d, ConvReLU1d, ConvReLU2d, ConvReLU3d, LinearReLU

"""
This type stub file was generated by pyright.
"""

import torch

class ConvReLU1d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 1d and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, relu) -> None:
        ...
    


class ConvReLU2d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 2d and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, relu) -> None:
        ...
    


class ConvReLU3d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 3d and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, relu) -> None:
        ...
    


class LinearReLU(torch.nn.Sequential):
    r"""This is a sequential container which calls the Linear and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, linear, relu) -> None:
        ...
    


class ConvBn1d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 1d and Batch Norm 1d modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn) -> None:
        ...
    


class ConvBn2d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 2d and Batch Norm 2d modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn) -> None:
        ...
    


class ConvBnReLU1d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 1d, Batch Norm 1d, and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn, relu) -> None:
        ...
    


class ConvBnReLU2d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 2d, Batch Norm 2d, and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn, relu) -> None:
        ...
    


class ConvBn3d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 3d and Batch Norm 3d modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn) -> None:
        ...
    


class ConvBnReLU3d(torch.nn.Sequential):
    r"""This is a sequential container which calls the Conv 3d, Batch Norm 3d, and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, conv, bn, relu) -> None:
        ...
    


class BNReLU2d(torch.nn.Sequential):
    r"""This is a sequential container which calls the BatchNorm 2d and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, batch_norm, relu) -> None:
        ...
    


class BNReLU3d(torch.nn.Sequential):
    r"""This is a sequential container which calls the BatchNorm 3d and ReLU modules.
    During quantization this will be replaced with the corresponding fused module."""
    def __init__(self, batch_norm, relu) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch import Tensor

def calculate_gain(nonlinearity, param=...):
    r"""Return the recommended gain value for the given nonlinearity function.
    The values are as follows:

    ================= ====================================================
    nonlinearity      gain
    ================= ====================================================
    Linear / Identity :math:`1`
    Conv{1,2,3}D      :math:`1`
    Sigmoid           :math:`1`
    Tanh              :math:`\frac{5}{3}`
    ReLU              :math:`\sqrt{2}`
    Leaky Relu        :math:`\sqrt{\frac{2}{1 + \text{negative\_slope}^2}}`
    ================= ====================================================

    Args:
        nonlinearity: the non-linear function (`nn.functional` name)
        param: optional parameter for the non-linear function

    Examples:
        >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2
    """
    ...

def uniform_(tensor: Tensor, a: float = ..., b: float = ...) -> Tensor:
    r"""Fills the input Tensor with values drawn from the uniform
    distribution :math:`\mathcal{U}(a, b)`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        a: the lower bound of the uniform distribution
        b: the upper bound of the uniform distribution

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.uniform_(w)
    """
    ...

def normal_(tensor: Tensor, mean: float = ..., std: float = ...) -> Tensor:
    r"""Fills the input Tensor with values drawn from the normal
    distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.normal_(w)
    """
    ...

def trunc_normal_(tensor: Tensor, mean: float = ..., std: float = ..., a: float = ..., b: float = ...) -> Tensor:
    r"""Fills the input Tensor with values drawn from a truncated
    normal distribution. The values are effectively drawn from the
    normal distribution :math:`\mathcal{N}(\text{mean}, \text{std}^2)`
    with values outside :math:`[a, b]` redrawn until they are within
    the bounds. The method used for generating the random values works
    best when :math:`a \leq \text{mean} \leq b`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        mean: the mean of the normal distribution
        std: the standard deviation of the normal distribution
        a: the minimum cutoff value
        b: the maximum cutoff value

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.trunc_normal_(w)
    """
    ...

def constant_(tensor: Tensor, val: float) -> Tensor:
    r"""Fills the input Tensor with the value :math:`\text{val}`.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        val: the value to fill the tensor with

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.constant_(w, 0.3)
    """
    ...

def ones_(tensor: Tensor) -> Tensor:
    r"""Fills the input Tensor with the scalar value `1`.

    Args:
        tensor: an n-dimensional `torch.Tensor`

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.ones_(w)
    """
    ...

def zeros_(tensor: Tensor) -> Tensor:
    r"""Fills the input Tensor with the scalar value `0`.

    Args:
        tensor: an n-dimensional `torch.Tensor`

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.zeros_(w)
    """
    ...

def eye_(tensor):
    r"""Fills the 2-dimensional input `Tensor` with the identity
    matrix. Preserves the identity of the inputs in `Linear` layers, where as
    many inputs are preserved as possible.

    Args:
        tensor: a 2-dimensional `torch.Tensor`

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.eye_(w)
    """
    ...

def dirac_(tensor, groups=...):
    r"""Fills the {3, 4, 5}-dimensional input `Tensor` with the Dirac
    delta function. Preserves the identity of the inputs in `Convolutional`
    layers, where as many input channels are preserved as possible. In case
    of groups>1, each group of channels preserves identity

    Args:
        tensor: a {3, 4, 5}-dimensional `torch.Tensor`
        groups (optional): number of groups in the conv layer (default: 1)
    Examples:
        >>> w = torch.empty(3, 16, 5, 5)
        >>> nn.init.dirac_(w)
        >>> w = torch.empty(3, 24, 5, 5)
        >>> nn.init.dirac_(w, 3)
    """
    ...

def xavier_uniform_(tensor: Tensor, gain: float = ...) -> Tensor:
    r"""Fills the input `Tensor` with values according to the method
    described in `Understanding the difficulty of training deep feedforward
    neural networks` - Glorot, X. & Bengio, Y. (2010), using a uniform
    distribution. The resulting tensor will have values sampled from
    :math:`\mathcal{U}(-a, a)` where

    .. math::
        a = \text{gain} \times \sqrt{\frac{6}{\text{fan\_in} + \text{fan\_out}}}

    Also known as Glorot initialization.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        gain: an optional scaling factor

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.xavier_uniform_(w, gain=nn.init.calculate_gain('relu'))
    """
    ...

def xavier_normal_(tensor: Tensor, gain: float = ...) -> Tensor:
    r"""Fills the input `Tensor` with values according to the method
    described in `Understanding the difficulty of training deep feedforward
    neural networks` - Glorot, X. & Bengio, Y. (2010), using a normal
    distribution. The resulting tensor will have values sampled from
    :math:`\mathcal{N}(0, \text{std}^2)` where

    .. math::
        \text{std} = \text{gain} \times \sqrt{\frac{2}{\text{fan\_in} + \text{fan\_out}}}

    Also known as Glorot initialization.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        gain: an optional scaling factor

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.xavier_normal_(w)
    """
    ...

def kaiming_uniform_(tensor, a=..., mode=..., nonlinearity=...):
    r"""Fills the input `Tensor` with values according to the method
    described in `Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification` - He, K. et al. (2015), using a
    uniform distribution. The resulting tensor will have values sampled from
    :math:`\mathcal{U}(-\text{bound}, \text{bound})` where

    .. math::
        \text{bound} = \text{gain} \times \sqrt{\frac{3}{\text{fan\_mode}}}

    Also known as He initialization.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        a: the negative slope of the rectifier used after this layer (only
            used with ``'leaky_relu'``)
        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``
            preserves the magnitude of the variance of the weights in the
            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the
            backwards pass.
        nonlinearity: the non-linear function (`nn.functional` name),
            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.kaiming_uniform_(w, mode='fan_in', nonlinearity='relu')
    """
    ...

def kaiming_normal_(tensor, a=..., mode=..., nonlinearity=...):
    r"""Fills the input `Tensor` with values according to the method
    described in `Delving deep into rectifiers: Surpassing human-level
    performance on ImageNet classification` - He, K. et al. (2015), using a
    normal distribution. The resulting tensor will have values sampled from
    :math:`\mathcal{N}(0, \text{std}^2)` where

    .. math::
        \text{std} = \frac{\text{gain}}{\sqrt{\text{fan\_mode}}}

    Also known as He initialization.

    Args:
        tensor: an n-dimensional `torch.Tensor`
        a: the negative slope of the rectifier used after this layer (only
            used with ``'leaky_relu'``)
        mode: either ``'fan_in'`` (default) or ``'fan_out'``. Choosing ``'fan_in'``
            preserves the magnitude of the variance of the weights in the
            forward pass. Choosing ``'fan_out'`` preserves the magnitudes in the
            backwards pass.
        nonlinearity: the non-linear function (`nn.functional` name),
            recommended to use only with ``'relu'`` or ``'leaky_relu'`` (default).

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.kaiming_normal_(w, mode='fan_out', nonlinearity='relu')
    """
    ...

def orthogonal_(tensor, gain=...):
    r"""Fills the input `Tensor` with a (semi) orthogonal matrix, as
    described in `Exact solutions to the nonlinear dynamics of learning in deep
    linear neural networks` - Saxe, A. et al. (2013). The input tensor must have
    at least 2 dimensions, and for tensors with more than 2 dimensions the
    trailing dimensions are flattened.

    Args:
        tensor: an n-dimensional `torch.Tensor`, where :math:`n \geq 2`
        gain: optional scaling factor

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.orthogonal_(w)
    """
    ...

def sparse_(tensor, sparsity, std=...):
    r"""Fills the 2D input `Tensor` as a sparse matrix, where the
    non-zero elements will be drawn from the normal distribution
    :math:`\mathcal{N}(0, 0.01)`, as described in `Deep learning via
    Hessian-free optimization` - Martens, J. (2010).

    Args:
        tensor: an n-dimensional `torch.Tensor`
        sparsity: The fraction of elements in each column to be set to zero
        std: the standard deviation of the normal distribution used to generate
            the non-zero values

    Examples:
        >>> w = torch.empty(3, 5)
        >>> nn.init.sparse_(w, sparsity=0.1)
    """
    ...

uniform = _make_deprecate(uniform_)
normal = _make_deprecate(normal_)
constant = _make_deprecate(constant_)
eye = _make_deprecate(eye_)
dirac = _make_deprecate(dirac_)
xavier_uniform = _make_deprecate(xavier_uniform_)
xavier_normal = _make_deprecate(xavier_normal_)
kaiming_uniform = _make_deprecate(kaiming_uniform_)
kaiming_normal = _make_deprecate(kaiming_normal_)
orthogonal = _make_deprecate(orthogonal_)
sparse = _make_deprecate(sparse_)
"""
This type stub file was generated by pyright.
"""

from typing import Tuple, TypeVar, Union
from .. import Tensor

T = TypeVar('T')
_scalar_or_tuple_any_t = Union[T, Tuple[T, ...]]
_scalar_or_tuple_1_t = Union[T, Tuple[T]]
_scalar_or_tuple_2_t = Union[T, Tuple[T, T]]
_scalar_or_tuple_3_t = Union[T, Tuple[T, T, T]]
_scalar_or_tuple_4_t = Union[T, Tuple[T, T, T, T]]
_scalar_or_tuple_5_t = Union[T, Tuple[T, T, T, T, T]]
_scalar_or_tuple_6_t = Union[T, Tuple[T, T, T, T, T, T]]
_size_any_t = _scalar_or_tuple_any_t[int]
_size_1_t = _scalar_or_tuple_1_t[int]
_size_2_t = _scalar_or_tuple_2_t[int]
_size_3_t = _scalar_or_tuple_3_t[int]
_size_4_t = _scalar_or_tuple_4_t[int]
_size_5_t = _scalar_or_tuple_5_t[int]
_size_6_t = _scalar_or_tuple_6_t[int]
_ratio_2_t = _scalar_or_tuple_2_t[float]
_ratio_3_t = _scalar_or_tuple_3_t[float]
_ratio_any_t = _scalar_or_tuple_any_t[float]
_tensor_list_t = _scalar_or_tuple_any_t[Tensor]
_maybe_indices_t = _scalar_or_tuple_2_t[Tensor]
"""
This type stub file was generated by pyright.
"""

from torch import Tensor
from torch.types import _size
from typing import Any, Callable, Dict, List, Optional, Tuple
from .common_types import _ratio_any_t

GRID_SAMPLE_INTERPOLATION_MODES = Dict[str, int]
GRID_SAMPLE_PADDING_MODES = Dict[str, int]
def fractional_max_pool2d_with_indices(input: Tensor, kernel_size: _size, output_size: Optional[_size] = ..., output_ratio: Optional[_ratio_any_t] = ..., return_indices: bool = ..., _random_samples: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
    ...

def fractional_max_pool3d_with_indices(input: Tensor, kernel_size: _size, output_size: Optional[_size] = ..., output_ratio: Optional[_ratio_any_t] = ..., return_indices: bool = ..., _random_samples: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
    ...

def max_pool1d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def max_pool2d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def max_pool3d_with_indices(input: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., dilation: _size = ..., ceil_mode: bool = ..., return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def max_unpool1d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., output_size: Optional[_size] = ...) -> Tensor:
    ...

def max_unpool2d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., output_size: Optional[_size] = ...) -> Tensor:
    ...

def max_unpool3d(input: Tensor, indices: Tensor, kernel_size: _size, stride: Optional[_size] = ..., padding: _size = ..., output_size: Optional[_size] = ...) -> Tensor:
    ...

def lp_pool2d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[_size] = ..., ceil_mode: bool = ...) -> Tensor:
    ...

def lp_pool1d(input: Tensor, norm_type: float, kernel_size: int, stride: Optional[_size] = ..., ceil_mode: bool = ...) -> Tensor:
    ...

def adaptive_max_pool1d_with_indices(input: Tensor, output_size: _size, return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def adaptive_max_pool2d_with_indices(input: Tensor, output_size: _size, return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def adaptive_max_pool3d_with_indices(input: Tensor, output_size: _size, return_indices: bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def adaptive_avg_pool2d(input: Tensor, output_size: _size) -> Tensor:
    ...

def adaptive_avg_pool3d(input: Tensor, output_size: _size) -> Tensor:
    ...

def dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def alpha_dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def dropout2d(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def dropout3d(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def feature_alpha_dropout(input: Tensor, p: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def threshold(input: Tensor, threshold: float, value: float, inplace: bool = ...) -> Tensor:
    ...

def relu(input: Tensor, inplace: bool = ...) -> Tensor:
    ...

def glu(input: Tensor, dim: int = ...) -> Tensor:
    ...

def hardtanh(input: Tensor, min_val: float = ..., max_val: float = ..., inplace: bool = ...) -> Tensor:
    ...

def relu6(input: Tensor, inplace: bool = ...) -> Tensor:
    ...

def elu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor:
    ...

def selu(input: Tensor, inplace: bool = ...) -> Tensor:
    ...

def celu(input: Tensor, alpha: float = ..., inplace: bool = ...) -> Tensor:
    ...

def leaky_relu(input: Tensor, negative_slope: float = ..., inplace: bool = ...) -> Tensor:
    ...

def prelu(input: Tensor, weight: Tensor) -> Tensor:
    ...

def rrelu(input: Tensor, lower: float = ..., upper: float = ..., training: bool = ..., inplace: bool = ...) -> Tensor:
    ...

def gelu(input: Any):
    ...

def hardshrink(input: Tensor, lambd: float = ...) -> Tensor:
    ...

def tanhshrink(input: Any):
    ...

def softsign(input: Any):
    ...

def softmin(input: Tensor, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

def softmax(input: Tensor, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

def gumbel_softmax(logits: Tensor, tau: float = ..., hard: bool = ..., eps: float = ..., dim: int = ...) -> Tensor:
    ...

def log_softmax(input: Tensor, dim: Optional[int] = ..., _stacklevel: int = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

def tanh(input: Any):
    ...

def sigmoid(input: Any):
    ...

def linear(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ...) -> Tensor:
    ...

def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor] = ...) -> Tensor:
    ...

def embedding(input: Tensor, weight: Tensor, padding_idx: Optional[int] = ..., max_norm: Optional[float] = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., sparse: bool = ...) -> Tensor:
    ...

def embedding_bag(input: Tensor, weight: Tensor, offsets: Optional[Tensor] = ..., max_norm: Optional[float] = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., mode: str = ..., sparse: bool = ...) -> Tensor:
    ...

def batch_norm(input: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., training: bool = ..., momentum: float = ..., eps: float = ...) -> Tensor:
    ...

def instance_norm(input: Tensor, running_mean: Optional[Tensor] = ..., running_var: Optional[Tensor] = ..., weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., use_input_stats: bool = ..., momentum: float = ..., eps: float = ...) -> Tensor:
    ...

def layer_norm(input: Tensor, normalized_shape: List[int], weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: float = ...) -> Tensor:
    ...

def group_norm(input: Tensor, num_groups: int, weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: float = ...) -> Tensor:
    ...

def local_response_norm(input: Tensor, size: int, alpha: float = ..., beta: float = ..., k: float = ...) -> Tensor:
    ...

def ctc_loss(log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor, blank: int = ..., reduction: str = ..., zero_infinity: bool = ...) -> Tensor:
    ...

def nll_loss(input: Tensor, target: Tensor, weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., ignore_index: int = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def poisson_nll_loss(input: Tensor, target: Tensor, log_input: bool = ..., full: bool = ..., size_average: Optional[bool] = ..., eps: float = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def kl_div(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ..., log_target: bool = ...) -> Tensor:
    ...

def cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., ignore_index: int = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def binary_cross_entropy(input: Tensor, target: Tensor, weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def binary_cross_entropy_with_logits(input: Tensor, target: Tensor, weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ..., pos_weight: Optional[Tensor] = ...) -> Tensor:
    ...

def smooth_l1_loss(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def l1_loss(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def mse_loss(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def margin_ranking_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def hinge_embedding_loss(input: Tensor, target: Tensor, margin: float = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def multilabel_margin_loss(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def soft_margin_loss(input: Tensor, target: Tensor, size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def multilabel_soft_margin_loss(input: Tensor, target: Tensor, weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def cosine_embedding_loss(input1: Tensor, input2: Tensor, target: Tensor, margin: float = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def multi_margin_loss(input: Tensor, target: Tensor, p: int = ..., margin: float = ..., weight: Optional[Tensor] = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def upsample(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ..., mode: str = ..., align_corners: Optional[Any] = ...):
    ...

def interpolate(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ..., mode: str = ..., align_corners: Optional[Any] = ...):
    ...

def upsample_nearest(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ...):
    ...

def upsample_bilinear(input: Any, size: Optional[Any] = ..., scale_factor: Optional[Any] = ...):
    ...

def grid_sample(input: Tensor, grid: Tensor, mode: str = ..., padding_mode: str = ..., align_corners: Optional[Any] = ...) -> Tensor:
    ...

def affine_grid(theta: Tensor, size: List[int], align_corners: Optional[Any] = ...) -> Tensor:
    ...

def pad(input: Tensor, pad: List[int], mode: str = ..., value: float = ...) -> Tensor:
    ...

def pairwise_distance(x1: Tensor, x2: Tensor, p: float = ..., eps: float = ..., keepdim: bool = ...) -> Tensor:
    ...

def triplet_margin_loss(anchor: Tensor, positive: Tensor, negative: Tensor, margin: float = ..., p: float = ..., eps: float = ..., swap: bool = ..., size_average: Optional[bool] = ..., reduce: Optional[bool] = ..., reduction: str = ...) -> Tensor:
    ...

def normalize(input: Tensor, p: float = ..., dim: int = ..., eps: float = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def assert_int_or_pair(arg: Any, arg_name: Any, message: Any) -> None:
    ...

def unfold(input: Tensor, kernel_size: _size, dilation: _size = ..., padding: _size = ..., stride: _size = ...) -> Tensor:
    ...

def fold(input: Tensor, output_size: _size, kernel_size: _size, dilation: _size = ..., padding: _size = ..., stride: _size = ...) -> Tensor:
    ...

fractional_max_pool2d: Callable
fractional_max_pool3d: Callable
max_pool1d: Callable
max_pool2d: Callable
max_pool3d: Callable
adaptive_max_pool1d: Callable
adaptive_max_pool2d: Callable
adaptive_max_pool3d: Callable
avg_pool2d: Callable
avg_pool3d: Callable
hardtanh_: Callable
elu_: Callable
leaky_relu_: Callable
logsigmoid: Callable
softplus: Callable
softshrink: Callable
one_hot: Callable
"""
This type stub file was generated by pyright.
"""

from typing import List, Optional
from torch import Tensor
from .module import Module
from ..common_types import _ratio_2_t, _ratio_3_t, _size_1_t, _size_2_t, _size_3_t, _size_any_t

class _MaxPoolNd(Module):
    __constants__ = ...
    return_indices: bool
    ceil_mode: bool
    def __init__(self, kernel_size: _size_any_t, stride: Optional[_size_any_t] = ..., padding: _size_any_t = ..., dilation: _size_any_t = ..., return_indices: bool = ..., ceil_mode: bool = ...) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class MaxPool1d(_MaxPoolNd):
    r"""Applies a 1D max pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`
    and output :math:`(N, C, L_{out})` can be precisely described as:

    .. math::
        out(N_i, C_j, k) = \max_{m=0, \ldots, \text{kernel\_size} - 1}
                input(N_i, C_j, stride \times k + m)

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    Args:
        kernel_size: the size of the window to take a max over
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on both sides
        dilation: a parameter that controls the stride of elements in the window
        return_indices: if ``True``, will return the max indices along with the outputs.
                        Useful for :class:`torch.nn.MaxUnpool1d` later
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape

    Shape:
        - Input: :math:`(N, C, L_{in})`
        - Output: :math:`(N, C, L_{out})`, where

          .. math::
              L_{out} = \left\lfloor \frac{L_{in} + 2 \times \text{padding} - \text{dilation}
                    \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

    Examples::

        >>> # pool of size=3, stride=2
        >>> m = nn.MaxPool1d(3, stride=2)
        >>> input = torch.randn(20, 16, 50)
        >>> output = m(input)

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    kernel_size: _size_1_t
    stride: _size_1_t
    padding: _size_1_t
    dilation: _size_1_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class MaxPool2d(_MaxPoolNd):
    r"""Applies a 2D max pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,
    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`
    can be precisely described as:

    .. math::
        \begin{aligned}
            out(N_i, C_j, h, w) ={} & \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                    & \text{input}(N_i, C_j, \text{stride[0]} \times h + m,
                                                   \text{stride[1]} \times w + n)
        \end{aligned}

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:

        - a single ``int`` -- in which case the same value is used for the height and width dimension
        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
          and the second `int` for the width dimension

    Args:
        kernel_size: the size of the window to take a max over
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on both sides
        dilation: a parameter that controls the stride of elements in the window
        return_indices: if ``True``, will return the max indices along with the outputs.
                        Useful for :class:`torch.nn.MaxUnpool2d` later
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})`, where

          .. math::
              H_{out} = \left\lfloor\frac{H_{in} + 2 * \text{padding[0]} - \text{dilation[0]}
                    \times (\text{kernel\_size[0]} - 1) - 1}{\text{stride[0]}} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in} + 2 * \text{padding[1]} - \text{dilation[1]}
                    \times (\text{kernel\_size[1]} - 1) - 1}{\text{stride[1]}} + 1\right\rfloor

    Examples::

        >>> # pool of square window of size=3, stride=2
        >>> m = nn.MaxPool2d(3, stride=2)
        >>> # pool of non-square window
        >>> m = nn.MaxPool2d((3, 2), stride=(2, 1))
        >>> input = torch.randn(20, 16, 50, 32)
        >>> output = m(input)

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    kernel_size: _size_2_t
    stride: _size_2_t
    padding: _size_2_t
    dilation: _size_2_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class MaxPool3d(_MaxPoolNd):
    r"""Applies a 3D max pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,
    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`
    can be precisely described as:

    .. math::
        \begin{aligned}
            \text{out}(N_i, C_j, d, h, w) ={} & \max_{k=0, \ldots, kD-1} \max_{m=0, \ldots, kH-1} \max_{n=0, \ldots, kW-1} \\
                                              & \text{input}(N_i, C_j, \text{stride[0]} \times d + k,
                                                             \text{stride[1]} \times h + m, \text{stride[2]} \times w + n)
        \end{aligned}

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
    for :attr:`padding` number of points. :attr:`dilation` controls the spacing between the kernel points.
    It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:

        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
          the second `int` for the height dimension and the third `int` for the width dimension

    Args:
        kernel_size: the size of the window to take a max over
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on all three sides
        dilation: a parameter that controls the stride of elements in the window
        return_indices: if ``True``, will return the max indices along with the outputs.
                        Useful for :class:`torch.nn.MaxUnpool3d` later
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape

    Shape:
        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where

          .. math::
              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0] \times
                (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1] \times
                (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2] \times
                (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

    Examples::

        >>> # pool of square window of size=3, stride=2
        >>> m = nn.MaxPool3d(3, stride=2)
        >>> # pool of non-square window
        >>> m = nn.MaxPool3d((3, 2, 2), stride=(2, 1, 2))
        >>> input = torch.randn(20, 16, 50,44, 31)
        >>> output = m(input)

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    kernel_size: _size_3_t
    stride: _size_3_t
    padding: _size_3_t
    dilation: _size_3_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class _MaxUnpoolNd(Module):
    def extra_repr(self) -> str:
        ...
    


class MaxUnpool1d(_MaxUnpoolNd):
    r"""Computes a partial inverse of :class:`MaxPool1d`.

    :class:`MaxPool1d` is not fully invertible, since the non-maximal values are lost.

    :class:`MaxUnpool1d` takes in as input the output of :class:`MaxPool1d`
    including the indices of the maximal values and computes a partial inverse
    in which all non-maximal values are set to zero.

    .. note:: :class:`MaxPool1d` can map several input sizes to the same output
              sizes. Hence, the inversion process can get ambiguous.
              To accommodate this, you can provide the needed output size
              as an additional argument :attr:`output_size` in the forward call.
              See the Inputs and Example below.

    Args:
        kernel_size (int or tuple): Size of the max pooling window.
        stride (int or tuple): Stride of the max pooling window.
            It is set to :attr:`kernel_size` by default.
        padding (int or tuple): Padding that was added to the input

    Inputs:
        - `input`: the input Tensor to invert
        - `indices`: the indices given out by :class:`~torch.nn.MaxPool1d`
        - `output_size` (optional): the targeted output size

    Shape:
        - Input: :math:`(N, C, H_{in})`
        - Output: :math:`(N, C, H_{out})`, where

          .. math::
              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{kernel\_size}[0]

          or as given by :attr:`output_size` in the call operator

    Example::

        >>> pool = nn.MaxPool1d(2, stride=2, return_indices=True)
        >>> unpool = nn.MaxUnpool1d(2, stride=2)
        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8]]])
        >>> output, indices = pool(input)
        >>> unpool(output, indices)
        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])

        >>> # Example showcasing the use of output_size
        >>> input = torch.tensor([[[1., 2, 3, 4, 5, 6, 7, 8, 9]]])
        >>> output, indices = pool(input)
        >>> unpool(output, indices, output_size=input.size())
        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.,  0.]]])

        >>> unpool(output, indices)
        tensor([[[ 0.,  2.,  0.,  4.,  0.,  6.,  0., 8.]]])
    """
    kernel_size: _size_1_t
    stride: _size_1_t
    padding: _size_1_t
    def __init__(self, kernel_size: _size_1_t, stride: Optional[_size_1_t] = ..., padding: _size_1_t = ...) -> None:
        ...
    
    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class MaxUnpool2d(_MaxUnpoolNd):
    r"""Computes a partial inverse of :class:`MaxPool2d`.

    :class:`MaxPool2d` is not fully invertible, since the non-maximal values are lost.

    :class:`MaxUnpool2d` takes in as input the output of :class:`MaxPool2d`
    including the indices of the maximal values and computes a partial inverse
    in which all non-maximal values are set to zero.

    .. note:: :class:`MaxPool2d` can map several input sizes to the same output
              sizes. Hence, the inversion process can get ambiguous.
              To accommodate this, you can provide the needed output size
              as an additional argument :attr:`output_size` in the forward call.
              See the Inputs and Example below.

    Args:
        kernel_size (int or tuple): Size of the max pooling window.
        stride (int or tuple): Stride of the max pooling window.
            It is set to :attr:`kernel_size` by default.
        padding (int or tuple): Padding that was added to the input

    Inputs:
        - `input`: the input Tensor to invert
        - `indices`: the indices given out by :class:`~torch.nn.MaxPool2d`
        - `output_size` (optional): the targeted output size

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})`, where

          .. math::
            H_{out} = (H_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

          .. math::
            W_{out} = (W_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

          or as given by :attr:`output_size` in the call operator

    Example::

        >>> pool = nn.MaxPool2d(2, stride=2, return_indices=True)
        >>> unpool = nn.MaxUnpool2d(2, stride=2)
        >>> input = torch.tensor([[[[ 1.,  2,  3,  4],
                                    [ 5,  6,  7,  8],
                                    [ 9, 10, 11, 12],
                                    [13, 14, 15, 16]]]])
        >>> output, indices = pool(input)
        >>> unpool(output, indices)
        tensor([[[[  0.,   0.,   0.,   0.],
                  [  0.,   6.,   0.,   8.],
                  [  0.,   0.,   0.,   0.],
                  [  0.,  14.,   0.,  16.]]]])

        >>> # specify a different output size than input size
        >>> unpool(output, indices, output_size=torch.Size([1, 1, 5, 5]))
        tensor([[[[  0.,   0.,   0.,   0.,   0.],
                  [  6.,   0.,   8.,   0.,   0.],
                  [  0.,   0.,   0.,  14.,   0.],
                  [ 16.,   0.,   0.,   0.,   0.],
                  [  0.,   0.,   0.,   0.,   0.]]]])
    """
    kernel_size: _size_2_t
    stride: _size_2_t
    padding: _size_2_t
    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = ..., padding: _size_2_t = ...) -> None:
        ...
    
    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class MaxUnpool3d(_MaxUnpoolNd):
    r"""Computes a partial inverse of :class:`MaxPool3d`.

    :class:`MaxPool3d` is not fully invertible, since the non-maximal values are lost.
    :class:`MaxUnpool3d` takes in as input the output of :class:`MaxPool3d`
    including the indices of the maximal values and computes a partial inverse
    in which all non-maximal values are set to zero.

    .. note:: :class:`MaxPool3d` can map several input sizes to the same output
              sizes. Hence, the inversion process can get ambiguous.
              To accommodate this, you can provide the needed output size
              as an additional argument :attr:`output_size` in the forward call.
              See the Inputs section below.

    Args:
        kernel_size (int or tuple): Size of the max pooling window.
        stride (int or tuple): Stride of the max pooling window.
            It is set to :attr:`kernel_size` by default.
        padding (int or tuple): Padding that was added to the input

    Inputs:
        - `input`: the input Tensor to invert
        - `indices`: the indices given out by :class:`~torch.nn.MaxPool3d`
        - `output_size` (optional): the targeted output size

    Shape:
        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where

          .. math::
              D_{out} = (D_{in} - 1) \times \text{stride[0]} - 2 \times \text{padding[0]} + \text{kernel\_size[0]}

          .. math::
              H_{out} = (H_{in} - 1) \times \text{stride[1]} - 2 \times \text{padding[1]} + \text{kernel\_size[1]}

          .. math::
              W_{out} = (W_{in} - 1) \times \text{stride[2]} - 2 \times \text{padding[2]} + \text{kernel\_size[2]}

          or as given by :attr:`output_size` in the call operator

    Example::

        >>> # pool of square window of size=3, stride=2
        >>> pool = nn.MaxPool3d(3, stride=2, return_indices=True)
        >>> unpool = nn.MaxUnpool3d(3, stride=2)
        >>> output, indices = pool(torch.randn(20, 16, 51, 33, 15))
        >>> unpooled_output = unpool(output, indices)
        >>> unpooled_output.size()
        torch.Size([20, 16, 51, 33, 15])
    """
    kernel_size: _size_3_t
    stride: _size_3_t
    padding: _size_3_t
    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = ..., padding: _size_3_t = ...) -> None:
        ...
    
    def forward(self, input: Tensor, indices: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class _AvgPoolNd(Module):
    __constants__ = ...
    def extra_repr(self) -> str:
        ...
    


class AvgPool1d(_AvgPoolNd):
    r"""Applies a 1D average pooling over an input signal composed of several
    input planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, L)`,
    output :math:`(N, C, L_{out})` and :attr:`kernel_size` :math:`k`
    can be precisely described as:

    .. math::

        \text{out}(N_i, C_j, l) = \frac{1}{k} \sum_{m=0}^{k-1}
                               \text{input}(N_i, C_j, \text{stride} \times l + m)

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
    for :attr:`padding` number of points.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can each be
    an ``int`` or a one-element tuple.

    Args:
        kernel_size: the size of the window
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on both sides
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
        count_include_pad: when True, will include the zero-padding in the averaging calculation

    Shape:
        - Input: :math:`(N, C, L_{in})`
        - Output: :math:`(N, C, L_{out})`, where

          .. math::
              L_{out} = \left\lfloor \frac{L_{in} +
              2 \times \text{padding} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

    Examples::

        >>> # pool with window of size=3, stride=2
        >>> m = nn.AvgPool1d(3, stride=2)
        >>> m(torch.tensor([[[1.,2,3,4,5,6,7]]]))
        tensor([[[ 2.,  4.,  6.]]])
    """
    kernel_size: _size_1_t
    stride: _size_1_t
    padding: _size_1_t
    ceil_mode: bool
    count_include_pad: bool
    def __init__(self, kernel_size: _size_1_t, stride: _size_1_t = ..., padding: _size_1_t = ..., ceil_mode: bool = ..., count_include_pad: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AvgPool2d(_AvgPoolNd):
    r"""Applies a 2D average pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, H, W)`,
    output :math:`(N, C, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kH, kW)`
    can be precisely described as:

    .. math::

        out(N_i, C_j, h, w)  = \frac{1}{kH * kW} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1}
                               input(N_i, C_j, stride[0] \times h + m, stride[1] \times w + n)

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on both sides
    for :attr:`padding` number of points.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding` can either be:

        - a single ``int`` -- in which case the same value is used for the height and width dimension
        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
          and the second `int` for the width dimension

    Args:
        kernel_size: the size of the window
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on both sides
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
        count_include_pad: when True, will include the zero-padding in the averaging calculation
        divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})`, where

          .. math::
              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] -
                \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] -
                \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

    Examples::

        >>> # pool of square window of size=3, stride=2
        >>> m = nn.AvgPool2d(3, stride=2)
        >>> # pool of non-square window
        >>> m = nn.AvgPool2d((3, 2), stride=(2, 1))
        >>> input = torch.randn(20, 16, 50, 32)
        >>> output = m(input)
    """
    __constants__ = ...
    kernel_size: _size_2_t
    stride: _size_2_t
    padding: _size_2_t
    ceil_mode: bool
    count_include_pad: bool
    def __init__(self, kernel_size: _size_2_t, stride: Optional[_size_2_t] = ..., padding: _size_2_t = ..., ceil_mode: bool = ..., count_include_pad: bool = ..., divisor_override: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AvgPool3d(_AvgPoolNd):
    r"""Applies a 3D average pooling over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C, D, H, W)`,
    output :math:`(N, C, D_{out}, H_{out}, W_{out})` and :attr:`kernel_size` :math:`(kD, kH, kW)`
    can be precisely described as:

    .. math::
        \begin{aligned}
            \text{out}(N_i, C_j, d, h, w) ={} & \sum_{k=0}^{kD-1} \sum_{m=0}^{kH-1} \sum_{n=0}^{kW-1} \\
                                              & \frac{\text{input}(N_i, C_j, \text{stride}[0] \times d + k,
                                                      \text{stride}[1] \times h + m, \text{stride}[2] \times w + n)}
                                                     {kD \times kH \times kW}
        \end{aligned}

    If :attr:`padding` is non-zero, then the input is implicitly zero-padded on all three sides
    for :attr:`padding` number of points.

    The parameters :attr:`kernel_size`, :attr:`stride` can either be:

        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
          the second `int` for the height dimension and the third `int` for the width dimension

    Args:
        kernel_size: the size of the window
        stride: the stride of the window. Default value is :attr:`kernel_size`
        padding: implicit zero padding to be added on all three sides
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape
        count_include_pad: when True, will include the zero-padding in the averaging calculation
        divisor_override: if specified, it will be used as divisor, otherwise :attr:`kernel_size` will be used

    Shape:
        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})`, where

          .. math::
              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] -
                    \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] -
                    \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] -
                    \text{kernel\_size}[2]}{\text{stride}[2]} + 1\right\rfloor

    Examples::

        >>> # pool of square window of size=3, stride=2
        >>> m = nn.AvgPool3d(3, stride=2)
        >>> # pool of non-square window
        >>> m = nn.AvgPool3d((3, 2, 2), stride=(2, 1, 2))
        >>> input = torch.randn(20, 16, 50,44, 31)
        >>> output = m(input)
    """
    __constants__ = ...
    kernel_size: _size_3_t
    stride: _size_3_t
    padding: _size_3_t
    ceil_mode: bool
    count_include_pad: bool
    def __init__(self, kernel_size: _size_3_t, stride: Optional[_size_3_t] = ..., padding: _size_3_t = ..., ceil_mode: bool = ..., count_include_pad: bool = ..., divisor_override=...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def __setstate__(self, d):
        ...
    


class FractionalMaxPool2d(Module):
    r"""Applies a 2D fractional max pooling over an input signal composed of several input planes.

    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham

    The max-pooling operation is applied in :math:`kH \times kW` regions by a stochastic
    step size determined by the target output size.
    The number of output features is equal to the number of input planes.

    Args:
        kernel_size: the size of the window to take a max over.
                     Can be a single number k (for a square kernel of k x k) or a tuple `(kh, kw)`
        output_size: the target output size of the image of the form `oH x oW`.
                     Can be a tuple `(oH, oW)` or a single number oH for a square image `oH x oH`
        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
                      This has to be a number or tuple in the range (0, 1)
        return_indices: if ``True``, will return the indices along with the outputs.
                        Useful to pass to :meth:`nn.MaxUnpool2d`. Default: ``False``

    Examples:
        >>> # pool of square window of size=3, and target output size 13x12
        >>> m = nn.FractionalMaxPool2d(3, output_size=(13, 12))
        >>> # pool of square window and target output size being half of input image size
        >>> m = nn.FractionalMaxPool2d(3, output_ratio=(0.5, 0.5))
        >>> input = torch.randn(20, 16, 50, 32)
        >>> output = m(input)

    .. _Fractional MaxPooling:
        http://arxiv.org/abs/1412.6071
    """
    __constants__ = ...
    kernel_size: _size_2_t
    return_indices: bool
    output_size: _size_2_t
    output_ratio: _ratio_2_t
    def __init__(self, kernel_size: _size_2_t, output_size: Optional[_size_2_t] = ..., output_ratio: Optional[_ratio_2_t] = ..., return_indices: bool = ..., _random_samples=...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class FractionalMaxPool3d(Module):
    r"""Applies a 3D fractional max pooling over an input signal composed of several input planes.

    Fractional MaxPooling is described in detail in the paper `Fractional MaxPooling`_ by Ben Graham

    The max-pooling operation is applied in :math:`kTxkHxkW` regions by a stochastic
    step size determined by the target output size.
    The number of output features is equal to the number of input planes.

    Args:
        kernel_size: the size of the window to take a max over.
                     Can be a single number k (for a square kernel of k x k x k) or a tuple `(kt x kh x kw)`
        output_size: the target output size of the image of the form `oT x oH x oW`.
                     Can be a tuple `(oT, oH, oW)` or a single number oH for a square image `oH x oH x oH`
        output_ratio: If one wants to have an output size as a ratio of the input size, this option can be given.
                      This has to be a number or tuple in the range (0, 1)
        return_indices: if ``True``, will return the indices along with the outputs.
                        Useful to pass to :meth:`nn.MaxUnpool3d`. Default: ``False``

    Examples:
        >>> # pool of cubic window of size=3, and target output size 13x12x11
        >>> m = nn.FractionalMaxPool3d(3, output_size=(13, 12, 11))
        >>> # pool of cubic window and target output size being half of input size
        >>> m = nn.FractionalMaxPool3d(3, output_ratio=(0.5, 0.5, 0.5))
        >>> input = torch.randn(20, 16, 50, 32, 16)
        >>> output = m(input)

    .. _Fractional MaxPooling:
        http://arxiv.org/abs/1412.6071
    """
    __constants__ = ...
    kernel_size: _size_3_t
    return_indices: bool
    output_size: _size_3_t
    output_ratio: _ratio_3_t
    def __init__(self, kernel_size: _size_3_t, output_size: Optional[_size_3_t] = ..., output_ratio: Optional[_ratio_3_t] = ..., return_indices: bool = ..., _random_samples=...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class _LPPoolNd(Module):
    __constants__ = ...
    norm_type: float
    ceil_mode: bool
    def __init__(self, norm_type: float, kernel_size: _size_any_t, stride: Optional[_size_any_t] = ..., ceil_mode: bool = ...) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class LPPool1d(_LPPoolNd):
    r"""Applies a 1D power-average pooling over an input signal composed of several input
    planes.

    On each window, the function computed is:

    .. math::
        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

    - At p = :math:`\infty`, one gets Max Pooling
    - At p = 1, one gets Sum Pooling (which is proportional to Average Pooling)

    .. note:: If the sum to the power of `p` is zero, the gradient of this function is
              not defined. This implementation will set the gradient to zero in this case.

    Args:
        kernel_size: a single int, the size of the window
        stride: a single int, the stride of the window. Default value is :attr:`kernel_size`
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape

    Shape:
        - Input: :math:`(N, C, L_{in})`
        - Output: :math:`(N, C, L_{out})`, where

          .. math::
              L_{out} = \left\lfloor\frac{L_{in} - \text{kernel\_size}}{\text{stride}} + 1\right\rfloor

    Examples::
        >>> # power-2 pool of window of length 3, with stride 2.
        >>> m = nn.LPPool1d(2, 3, stride=2)
        >>> input = torch.randn(20, 16, 50)
        >>> output = m(input)
    """
    kernel_size: _size_1_t
    stride: _size_1_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class LPPool2d(_LPPoolNd):
    r"""Applies a 2D power-average pooling over an input signal composed of several input
    planes.

    On each window, the function computed is:

    .. math::
        f(X) = \sqrt[p]{\sum_{x \in X} x^{p}}

    - At p = :math:`\infty`, one gets Max Pooling
    - At p = 1, one gets Sum Pooling (which is proportional to average pooling)

    The parameters :attr:`kernel_size`, :attr:`stride` can either be:

        - a single ``int`` -- in which case the same value is used for the height and width dimension
        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
          and the second `int` for the width dimension

    .. note:: If the sum to the power of `p` is zero, the gradient of this function is
              not defined. This implementation will set the gradient to zero in this case.

    Args:
        kernel_size: the size of the window
        stride: the stride of the window. Default value is :attr:`kernel_size`
        ceil_mode: when True, will use `ceil` instead of `floor` to compute the output shape

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})`, where

          .. math::
              H_{out} = \left\lfloor\frac{H_{in} - \text{kernel\_size}[0]}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in} - \text{kernel\_size}[1]}{\text{stride}[1]} + 1\right\rfloor

    Examples::

        >>> # power-2 pool of square window of size=3, stride=2
        >>> m = nn.LPPool2d(2, 3, stride=2)
        >>> # pool of non-square window of power 1.2
        >>> m = nn.LPPool2d(1.2, (3, 2), stride=(2, 1))
        >>> input = torch.randn(20, 16, 50, 32)
        >>> output = m(input)

    """
    kernel_size: _size_2_t
    stride: _size_2_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class _AdaptiveMaxPoolNd(Module):
    __constants__ = ...
    return_indices: bool
    def __init__(self, output_size: _size_any_t, return_indices: bool = ...) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class AdaptiveMaxPool1d(_AdaptiveMaxPoolNd):
    r"""Applies a 1D adaptive max pooling over an input signal composed of several input planes.

    The output size is H, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size H
        return_indices: if ``True``, will return the indices along with the outputs.
                        Useful to pass to nn.MaxUnpool1d. Default: ``False``

    Examples:
        >>> # target output size of 5
        >>> m = nn.AdaptiveMaxPool1d(5)
        >>> input = torch.randn(1, 64, 8)
        >>> output = m(input)

    """
    output_size: _size_1_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AdaptiveMaxPool2d(_AdaptiveMaxPoolNd):
    r"""Applies a 2D adaptive max pooling over an input signal composed of several input planes.

    The output is of size H x W, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size of the image of the form H x W.
                     Can be a tuple (H, W) or a single H for a square image H x H.
                     H and W can be either a ``int``, or ``None`` which means the size will
                     be the same as that of the input.
        return_indices: if ``True``, will return the indices along with the outputs.
                        Useful to pass to nn.MaxUnpool2d. Default: ``False``

    Examples:
        >>> # target output size of 5x7
        >>> m = nn.AdaptiveMaxPool2d((5,7))
        >>> input = torch.randn(1, 64, 8, 9)
        >>> output = m(input)
        >>> # target output size of 7x7 (square)
        >>> m = nn.AdaptiveMaxPool2d(7)
        >>> input = torch.randn(1, 64, 10, 9)
        >>> output = m(input)
        >>> # target output size of 10x7
        >>> m = nn.AdaptiveMaxPool2d((None, 7))
        >>> input = torch.randn(1, 64, 10, 9)
        >>> output = m(input)

    """
    output_size: _size_2_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AdaptiveMaxPool3d(_AdaptiveMaxPoolNd):
    r"""Applies a 3D adaptive max pooling over an input signal composed of several input planes.

    The output is of size D x H x W, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size of the image of the form D x H x W.
                     Can be a tuple (D, H, W) or a single D for a cube D x D x D.
                     D, H and W can be either a ``int``, or ``None`` which means the size will
                     be the same as that of the input.

        return_indices: if ``True``, will return the indices along with the outputs.
                        Useful to pass to nn.MaxUnpool3d. Default: ``False``

    Examples:
        >>> # target output size of 5x7x9
        >>> m = nn.AdaptiveMaxPool3d((5,7,9))
        >>> input = torch.randn(1, 64, 8, 9, 10)
        >>> output = m(input)
        >>> # target output size of 7x7x7 (cube)
        >>> m = nn.AdaptiveMaxPool3d(7)
        >>> input = torch.randn(1, 64, 10, 9, 8)
        >>> output = m(input)
        >>> # target output size of 7x9x8
        >>> m = nn.AdaptiveMaxPool3d((7, None, None))
        >>> input = torch.randn(1, 64, 10, 9, 8)
        >>> output = m(input)

    """
    output_size: _size_3_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class _AdaptiveAvgPoolNd(Module):
    __constants__ = ...
    def __init__(self, output_size: _size_any_t) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class AdaptiveAvgPool1d(_AdaptiveAvgPoolNd):
    r"""Applies a 1D adaptive average pooling over an input signal composed of several input planes.

    The output size is H, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size H

    Examples:
        >>> # target output size of 5
        >>> m = nn.AdaptiveAvgPool1d(5)
        >>> input = torch.randn(1, 64, 8)
        >>> output = m(input)

    """
    output_size: _size_1_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AdaptiveAvgPool2d(_AdaptiveAvgPoolNd):
    r"""Applies a 2D adaptive average pooling over an input signal composed of several input planes.

    The output is of size H x W, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size of the image of the form H x W.
                     Can be a tuple (H, W) or a single H for a square image H x H.
                     H and W can be either a ``int``, or ``None`` which means the size will
                     be the same as that of the input.

    Examples:
        >>> # target output size of 5x7
        >>> m = nn.AdaptiveAvgPool2d((5,7))
        >>> input = torch.randn(1, 64, 8, 9)
        >>> output = m(input)
        >>> # target output size of 7x7 (square)
        >>> m = nn.AdaptiveAvgPool2d(7)
        >>> input = torch.randn(1, 64, 10, 9)
        >>> output = m(input)
        >>> # target output size of 10x7
        >>> m = nn.AdaptiveMaxPool2d((None, 7))
        >>> input = torch.randn(1, 64, 10, 9)
        >>> output = m(input)

    """
    output_size: _size_2_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AdaptiveAvgPool3d(_AdaptiveAvgPoolNd):
    r"""Applies a 3D adaptive average pooling over an input signal composed of several input planes.

    The output is of size D x H x W, for any input size.
    The number of output features is equal to the number of input planes.

    Args:
        output_size: the target output size of the form D x H x W.
                     Can be a tuple (D, H, W) or a single number D for a cube D x D x D.
                     D, H and W can be either a ``int``, or ``None`` which means the size will
                     be the same as that of the input.

    Examples:
        >>> # target output size of 5x7x9
        >>> m = nn.AdaptiveAvgPool3d((5,7,9))
        >>> input = torch.randn(1, 64, 8, 9, 10)
        >>> output = m(input)
        >>> # target output size of 7x7x7 (cube)
        >>> m = nn.AdaptiveAvgPool3d(7)
        >>> input = torch.randn(1, 64, 10, 9, 8)
        >>> output = m(input)
        >>> # target output size of 7x9x8
        >>> m = nn.AdaptiveMaxPool3d((7, None, None))
        >>> input = torch.randn(1, 64, 10, 9, 8)
        >>> output = m(input)

    """
    output_size: _size_3_t
    def forward(self, input: Tensor) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module

class ChannelShuffle(Module):
    r"""Divide the channels in a tensor of shape :math:`(*, C , H, W)`
    into g groups and rearrange them as :math:`(*, C \frac g, g, H, W)`,
    while keeping the original tensor shape.

    Args:
        groups (int): number of groups to divide channels in.

    Examples::

        >>> channel_shuffle = nn.ChannelShuffle(2)
        >>> input = torch.randn(1, 4, 2, 2)
        >>> print(input)
        [[[[1, 2],
           [3, 4]],
          [[5, 6],
           [7, 8]],
          [[9, 10],
           [11, 12]],
          [[13, 14],
           [15, 16]],
         ]]
        >>> output = channel_shuffle(input)
        >>> print(output)
        [[[[1, 2],
           [3, 4]],
          [[9, 10],
           [11, 12]],
          [[5, 6],
           [7, 8]],
          [[13, 14],
           [15, 16]],
         ]]
    """
    __constants__ = ...
    def __init__(self, groups) -> None:
        ...
    
    def forward(self, input):
        ...
    
    def extra_repr(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from collections import OrderedDict, namedtuple
from ..parameter import Parameter
from torch import Tensor, device, dtype
from typing import Callable, Dict, Iterator, Optional, Set, Tuple, TypeVar, Union, overload
from ...utils.hooks import RemovableHandle

_grad_t = Union[Tuple[Tensor, ...], Tensor]
T = TypeVar('T', bound='Module')
class _IncompatibleKeys(namedtuple('IncompatibleKeys', ['missing_keys', 'unexpected_keys'])):
    def __repr__(self):
        ...
    
    __str__ = ...


class ModuleAttributeError(AttributeError):
    """ When `__getattr__` raises AttributeError inside a property,
    AttributeError is raised with the property name instead of the
    attribute that initially raised AttributeError, making the error
    message uninformative. Using `ModuleAttributeError` instead
    fixes this issue."""
    ...


_global_backward_hooks = OrderedDict()
_global_forward_pre_hooks = OrderedDict()
_global_forward_hooks = OrderedDict()
def register_module_forward_pre_hook(hook: Callable[..., None]) -> RemovableHandle:
    r"""Registers a forward pre-hook common to all modules.

    .. warning ::

        This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes. 

    The hook will be called every time before :func:`forward` is invoked.
    It should have the following signature::

        hook(module, input) -> None or modified input

    The input contains only the positional arguments given to the module.
    Keyword arguments won't be passed to the hooks and only to the ``forward``.
    The hook can modify the input. User can either return a tuple or a
    single modified value in the hook. We will wrap the value into a tuple
    if a single value is returned(unless that value is already a tuple).

    This hook has precedence over the specific module hooks registered with
    ``register_forward_pre_hook``.

    Returns:
        :class:`torch.utils.hooks.RemovableHandle`:
            a handle that can be used to remove the added hook by calling
            ``handle.remove()``
    """
    ...

def register_module_forward_hook(hook: Callable[..., None]) -> RemovableHandle:
    r"""Registers a global forward hook for all the modules

    .. warning ::

        This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes. 

    The hook will be called every time after :func:`forward` has computed an output.
    It should have the following signature::

        hook(module, input, output) -> None or modified output

    The input contains only the positional arguments given to the module.
    Keyword arguments won't be passed to the hooks and only to the ``forward``.
    The hook can modify the output. It can modify the input inplace but
    it will not have effect on forward since this is called after
    :func:`forward` is called.

    Returns:
        :class:`torch.utils.hooks.RemovableHandle`:
            a handle that can be used to remove the added hook by calling
            ``handle.remove()``

    This hook will be executed before specific module hooks registered with
    ``register_forward_hook``.
    """
    ...

def register_module_backward_hook(hook: Callable[[Module, _grad_t, _grad_t], Union[None, Tensor]]) -> RemovableHandle:
    r"""Registers a backward hook common to all the modules.

    .. warning ::
        This adds global state to the `nn.module` module
        and it is only intended for debugging/profiling purposes. 

        The current implementation will not have the presented behavior
        for complex :class:`Module` that perform many operations.
        In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only
        contain the gradients for a subset of the inputs and outputs.
        For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`
        directly on a specific input or output to get the required gradients.

    The hook will be called every time the gradients with respect to module
    inputs are computed. The hook should have the following signature::

        hook(module, grad_input, grad_output) -> Tensor or None

    The :attr:`grad_input` and :attr:`grad_output` may be tuples if the
    module has multiple inputs or outputs. The hook should not modify its
    arguments, but it can optionally return a new gradient with respect to
    input that will be used in place of :attr:`grad_input` in subsequent
    computations. :attr:`grad_input` will only correspond to the inputs given
    as positional arguments.

    Global hooks are called before hooks registered with `register_backward_hook`

    Returns:
        :class:`torch.utils.hooks.RemovableHandle`:
            a handle that can be used to remove the added hook by calling
            ``handle.remove()``

    """
    ...

class Module:
    r"""Base class for all neural network modules.

    Your models should also subclass this class.

    Modules can also contain other Modules, allowing to nest them in
    a tree structure. You can assign the submodules as regular attributes::

        import torch.nn as nn
        import torch.nn.functional as F

        class Model(nn.Module):
            def __init__(self):
                super(Model, self).__init__()
                self.conv1 = nn.Conv2d(1, 20, 5)
                self.conv2 = nn.Conv2d(20, 20, 5)

            def forward(self, x):
                x = F.relu(self.conv1(x))
                return F.relu(self.conv2(x))

    Submodules assigned in this way will be registered, and will have their
    parameters converted too when you call :meth:`to`, etc.
    """
    training: bool
    def __init__(self) -> None:
        """
        Initializes internal Module state, shared by both nn.Module and ScriptModule.
        """
        ...
    
    def register_buffer(self, name: str, tensor: Tensor, persistent: bool = ...) -> None:
        r"""Adds a buffer to the module.

        This is typically used to register a buffer that should not to be
        considered a model parameter. For example, BatchNorm's ``running_mean``
        is not a parameter, but is part of the module's state. Buffers, by
        default, are persistent and will be saved alongside parameters. This
        behavior can be changed by setting :attr:`persistent` to ``False``. The
        only difference between a persistent buffer and a non-persistent buffer
        is that the latter will not be a part of this module's
        :attr:`state_dict`.

        Buffers can be accessed as attributes using given names.

        Args:
            name (string): name of the buffer. The buffer can be accessed
                from this module using the given name
            tensor (Tensor): buffer to be registered.
            persistent (bool): whether the buffer is part of this module's
                :attr:`state_dict`.

        Example::

            >>> self.register_buffer('running_mean', torch.zeros(num_features))

        """
        ...
    
    def register_parameter(self, name: str, param: Parameter) -> None:
        r"""Adds a parameter to the module.

        The parameter can be accessed as an attribute using given name.

        Args:
            name (string): name of the parameter. The parameter can be accessed
                from this module using the given name
            param (Parameter): parameter to be added to the module.
        """
        ...
    
    def add_module(self, name: str, module: Module) -> None:
        r"""Adds a child module to the current module.

        The module can be accessed as an attribute using the given name.

        Args:
            name (string): name of the child module. The child module can be
                accessed from this module using the given name
            module (Module): child module to be added to the module.
        """
        ...
    
    def apply(self: T, fn: Callable[[Module], None]) -> T:
        r"""Applies ``fn`` recursively to every submodule (as returned by ``.children()``)
        as well as self. Typical use includes initializing the parameters of a model
        (see also :ref:`nn-init-doc`).

        Args:
            fn (:class:`Module` -> None): function to be applied to each submodule

        Returns:
            Module: self

        Example::

            >>> @torch.no_grad()
            >>> def init_weights(m):
            >>>     print(m)
            >>>     if type(m) == nn.Linear:
            >>>         m.weight.fill_(1.0)
            >>>         print(m.weight)
            >>> net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))
            >>> net.apply(init_weights)
            Linear(in_features=2, out_features=2, bias=True)
            Parameter containing:
            tensor([[ 1.,  1.],
                    [ 1.,  1.]])
            Linear(in_features=2, out_features=2, bias=True)
            Parameter containing:
            tensor([[ 1.,  1.],
                    [ 1.,  1.]])
            Sequential(
              (0): Linear(in_features=2, out_features=2, bias=True)
              (1): Linear(in_features=2, out_features=2, bias=True)
            )
            Sequential(
              (0): Linear(in_features=2, out_features=2, bias=True)
              (1): Linear(in_features=2, out_features=2, bias=True)
            )
        """
        ...
    
    def cuda(self: T, device: Optional[Union[int, device]] = ...) -> T:
        r"""Moves all model parameters and buffers to the GPU.

        This also makes associated parameters and buffers different objects. So
        it should be called before constructing optimizer if the module will
        live on GPU while being optimized.

        Arguments:
            device (int, optional): if specified, all parameters will be
                copied to that device

        Returns:
            Module: self
        """
        ...
    
    def cpu(self: T) -> T:
        r"""Moves all model parameters and buffers to the CPU.

        Returns:
            Module: self
        """
        ...
    
    def type(self: T, dst_type: Union[dtype, str]) -> T:
        r"""Casts all parameters and buffers to :attr:`dst_type`.

        Arguments:
            dst_type (type or string): the desired type

        Returns:
            Module: self
        """
        ...
    
    def float(self: T) -> T:
        r"""Casts all floating point parameters and buffers to float datatype.

        Returns:
            Module: self
        """
        ...
    
    def double(self: T) -> T:
        r"""Casts all floating point parameters and buffers to ``double`` datatype.

        Returns:
            Module: self
        """
        ...
    
    def half(self: T) -> T:
        r"""Casts all floating point parameters and buffers to ``half`` datatype.

        Returns:
            Module: self
        """
        ...
    
    def bfloat16(self: T) -> T:
        r"""Casts all floating point parameters and buffers to ``bfloat16`` datatype.

        Returns:
            Module: self
        """
        ...
    
    @overload
    def to(self: T, device: Optional[Union[int, device]] = ..., dtype: Optional[Union[dtype, str]] = ..., non_blocking: bool = ...) -> T:
        ...
    
    @overload
    def to(self: T, dtype: Union[dtype, str], non_blocking: bool = ...) -> T:
        ...
    
    @overload
    def to(self: T, tensor: Tensor, non_blocking: bool = ...) -> T:
        ...
    
    def to(self, *args, **kwargs):
        r"""Moves and/or casts the parameters and buffers.

        This can be called as

        .. function:: to(device=None, dtype=None, non_blocking=False)

        .. function:: to(dtype, non_blocking=False)

        .. function:: to(tensor, non_blocking=False)

        .. function:: to(memory_format=torch.channels_last)

        Its signature is similar to :meth:`torch.Tensor.to`, but only accepts
        floating point desired :attr:`dtype` s. In addition, this method will
        only cast the floating point parameters and buffers to :attr:`dtype`
        (if given). The integral parameters and buffers will be moved
        :attr:`device`, if that is given, but with dtypes unchanged. When
        :attr:`non_blocking` is set, it tries to convert/move asynchronously
        with respect to the host if possible, e.g., moving CPU Tensors with
        pinned memory to CUDA devices.

        See below for examples.

        .. note::
            This method modifies the module in-place.

        Args:
            device (:class:`torch.device`): the desired device of the parameters
                and buffers in this module
            dtype (:class:`torch.dtype`): the desired floating point type of
                the floating point parameters and buffers in this module
            tensor (torch.Tensor): Tensor whose dtype and device are the desired
                dtype and device for all parameters and buffers in this module
            memory_format (:class:`torch.memory_format`): the desired memory
                format for 4D parameters and buffers in this module (keyword
                only argument)

        Returns:
            Module: self

        Example::

            >>> linear = nn.Linear(2, 2)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]])
            >>> linear.to(torch.double)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1913, -0.3420],
                    [-0.5113, -0.2325]], dtype=torch.float64)
            >>> gpu1 = torch.device("cuda:1")
            >>> linear.to(gpu1, dtype=torch.half, non_blocking=True)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16, device='cuda:1')
            >>> cpu = torch.device("cpu")
            >>> linear.to(cpu)
            Linear(in_features=2, out_features=2, bias=True)
            >>> linear.weight
            Parameter containing:
            tensor([[ 0.1914, -0.3420],
                    [-0.5112, -0.2324]], dtype=torch.float16)

        """
        ...
    
    def register_backward_hook(self, hook: Callable[[Module, _grad_t, _grad_t], Union[None, Tensor]]) -> RemovableHandle:
        r"""Registers a backward hook on the module.

        .. warning ::

            The current implementation will not have the presented behavior
            for complex :class:`Module` that perform many operations.
            In some failure cases, :attr:`grad_input` and :attr:`grad_output` will only
            contain the gradients for a subset of the inputs and outputs.
            For such :class:`Module`, you should use :func:`torch.Tensor.register_hook`
            directly on a specific input or output to get the required gradients.

        The hook will be called every time the gradients with respect to module
        inputs are computed. The hook should have the following signature::

            hook(module, grad_input, grad_output) -> Tensor or None

        The :attr:`grad_input` and :attr:`grad_output` may be tuples if the
        module has multiple inputs or outputs. The hook should not modify its
        arguments, but it can optionally return a new gradient with respect to
        input that will be used in place of :attr:`grad_input` in subsequent
        computations. :attr:`grad_input` will only correspond to the inputs given
        as positional arguments.

        Returns:
            :class:`torch.utils.hooks.RemovableHandle`:
                a handle that can be used to remove the added hook by calling
                ``handle.remove()``

        """
        ...
    
    def register_forward_pre_hook(self, hook: Callable[..., None]) -> RemovableHandle:
        r"""Registers a forward pre-hook on the module.

        The hook will be called every time before :func:`forward` is invoked.
        It should have the following signature::

            hook(module, input) -> None or modified input

        The input contains only the positional arguments given to the module.
        Keyword arguments won't be passed to the hooks and only to the ``forward``.
        The hook can modify the input. User can either return a tuple or a
        single modified value in the hook. We will wrap the value into a tuple
        if a single value is returned(unless that value is already a tuple).

        Returns:
            :class:`torch.utils.hooks.RemovableHandle`:
                a handle that can be used to remove the added hook by calling
                ``handle.remove()``
        """
        ...
    
    def register_forward_hook(self, hook: Callable[..., None]) -> RemovableHandle:
        r"""Registers a forward hook on the module.

        The hook will be called every time after :func:`forward` has computed an output.
        It should have the following signature::

            hook(module, input, output) -> None or modified output

        The input contains only the positional arguments given to the module.
        Keyword arguments won't be passed to the hooks and only to the ``forward``.
        The hook can modify the output. It can modify the input inplace but
        it will not have effect on forward since this is called after
        :func:`forward` is called.

        Returns:
            :class:`torch.utils.hooks.RemovableHandle`:
                a handle that can be used to remove the added hook by calling
                ``handle.remove()``
        """
        ...
    
    def __setstate__(self, state):
        ...
    
    def __getattr__(self, name: str) -> Union[Tensor, Module]:
        ...
    
    def __setattr__(self, name: str, value: Union[Tensor, Module]) -> None:
        ...
    
    def __delattr__(self, name):
        ...
    
    T_destination = ...
    @overload
    def state_dict(self, destination: T_destination, prefix: str = ..., keep_vars: bool = ...) -> T_destination:
        ...
    
    @overload
    def state_dict(self, prefix: str = ..., keep_vars: bool = ...) -> Dict[str, Tensor]:
        ...
    
    def state_dict(self, destination=..., prefix=..., keep_vars=...):
        r"""Returns a dictionary containing a whole state of the module.

        Both parameters and persistent buffers (e.g. running averages) are
        included. Keys are corresponding parameter and buffer names.

        Returns:
            dict:
                a dictionary containing a whole state of the module

        Example::

            >>> module.state_dict().keys()
            ['bias', 'weight']

        """
        ...
    
    def load_state_dict(self, state_dict: Union[Dict[str, Tensor], Dict[str, Tensor]], strict: bool = ...):
        r"""Copies parameters and buffers from :attr:`state_dict` into
        this module and its descendants. If :attr:`strict` is ``True``, then
        the keys of :attr:`state_dict` must exactly match the keys returned
        by this module's :meth:`~torch.nn.Module.state_dict` function.

        Arguments:
            state_dict (dict): a dict containing parameters and
                persistent buffers.
            strict (bool, optional): whether to strictly enforce that the keys
                in :attr:`state_dict` match the keys returned by this module's
                :meth:`~torch.nn.Module.state_dict` function. Default: ``True``

        Returns:
            ``NamedTuple`` with ``missing_keys`` and ``unexpected_keys`` fields:
                * **missing_keys** is a list of str containing the missing keys
                * **unexpected_keys** is a list of str containing the unexpected keys
        """
        ...
    
    def parameters(self, recurse: bool = ...) -> Iterator[Parameter]:
        r"""Returns an iterator over module parameters.

        This is typically passed to an optimizer.

        Args:
            recurse (bool): if True, then yields parameters of this module
                and all submodules. Otherwise, yields only parameters that
                are direct members of this module.

        Yields:
            Parameter: module parameter

        Example::

            >>> for param in model.parameters():
            >>>     print(type(param), param.size())
            <class 'torch.Tensor'> (20L,)
            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)

        """
        ...
    
    def named_parameters(self, prefix: str = ..., recurse: bool = ...) -> Iterator[Tuple[str, Tensor]]:
        r"""Returns an iterator over module parameters, yielding both the
        name of the parameter as well as the parameter itself.

        Args:
            prefix (str): prefix to prepend to all parameter names.
            recurse (bool): if True, then yields parameters of this module
                and all submodules. Otherwise, yields only parameters that
                are direct members of this module.

        Yields:
            (string, Parameter): Tuple containing the name and parameter

        Example::

            >>> for name, param in self.named_parameters():
            >>>    if name in ['bias']:
            >>>        print(param.size())

        """
        ...
    
    def buffers(self, recurse: bool = ...) -> Iterator[Tensor]:
        r"""Returns an iterator over module buffers.

        Args:
            recurse (bool): if True, then yields buffers of this module
                and all submodules. Otherwise, yields only buffers that
                are direct members of this module.

        Yields:
            torch.Tensor: module buffer

        Example::

            >>> for buf in model.buffers():
            >>>     print(type(buf), buf.size())
            <class 'torch.Tensor'> (20L,)
            <class 'torch.Tensor'> (20L, 1L, 5L, 5L)

        """
        ...
    
    def named_buffers(self, prefix: str = ..., recurse: bool = ...) -> Iterator[Tuple[str, Tensor]]:
        r"""Returns an iterator over module buffers, yielding both the
        name of the buffer as well as the buffer itself.

        Args:
            prefix (str): prefix to prepend to all buffer names.
            recurse (bool): if True, then yields buffers of this module
                and all submodules. Otherwise, yields only buffers that
                are direct members of this module.

        Yields:
            (string, torch.Tensor): Tuple containing the name and buffer

        Example::

            >>> for name, buf in self.named_buffers():
            >>>    if name in ['running_var']:
            >>>        print(buf.size())

        """
        ...
    
    def children(self) -> Iterator[Module]:
        r"""Returns an iterator over immediate children modules.

        Yields:
            Module: a child module
        """
        ...
    
    def named_children(self) -> Iterator[Tuple[str, Module]]:
        r"""Returns an iterator over immediate children modules, yielding both
        the name of the module as well as the module itself.

        Yields:
            (string, Module): Tuple containing a name and child module

        Example::

            >>> for name, module in model.named_children():
            >>>     if name in ['conv4', 'conv5']:
            >>>         print(module)

        """
        ...
    
    def modules(self) -> Iterator[Module]:
        r"""Returns an iterator over all modules in the network.

        Yields:
            Module: a module in the network

        Note:
            Duplicate modules are returned only once. In the following
            example, ``l`` will be returned only once.

        Example::

            >>> l = nn.Linear(2, 2)
            >>> net = nn.Sequential(l, l)
            >>> for idx, m in enumerate(net.modules()):
                    print(idx, '->', m)

            0 -> Sequential(
              (0): Linear(in_features=2, out_features=2, bias=True)
              (1): Linear(in_features=2, out_features=2, bias=True)
            )
            1 -> Linear(in_features=2, out_features=2, bias=True)

        """
        ...
    
    def named_modules(self, memo: Optional[Set[Module]] = ..., prefix: str = ...):
        r"""Returns an iterator over all modules in the network, yielding
        both the name of the module as well as the module itself.

        Yields:
            (string, Module): Tuple of name and module

        Note:
            Duplicate modules are returned only once. In the following
            example, ``l`` will be returned only once.

        Example::

            >>> l = nn.Linear(2, 2)
            >>> net = nn.Sequential(l, l)
            >>> for idx, m in enumerate(net.named_modules()):
                    print(idx, '->', m)

            0 -> ('', Sequential(
              (0): Linear(in_features=2, out_features=2, bias=True)
              (1): Linear(in_features=2, out_features=2, bias=True)
            ))
            1 -> ('0', Linear(in_features=2, out_features=2, bias=True))

        """
        ...
    
    def train(self: T, mode: bool = ...) -> T:
        r"""Sets the module in training mode.

        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.

        Args:
            mode (bool): whether to set training mode (``True``) or evaluation
                         mode (``False``). Default: ``True``.

        Returns:
            Module: self
        """
        ...
    
    def eval(self: T) -> T:
        r"""Sets the module in evaluation mode.

        This has any effect only on certain modules. See documentations of
        particular modules for details of their behaviors in training/evaluation
        mode, if they are affected, e.g. :class:`Dropout`, :class:`BatchNorm`,
        etc.

        This is equivalent with :meth:`self.train(False) <torch.nn.Module.train>`.

        Returns:
            Module: self
        """
        ...
    
    def requires_grad_(self: T, requires_grad: bool = ...) -> T:
        r"""Change if autograd should record operations on parameters in this
        module.

        This method sets the parameters' :attr:`requires_grad` attributes
        in-place.

        This method is helpful for freezing part of the module for finetuning
        or training parts of a model individually (e.g., GAN training).

        Args:
            requires_grad (bool): whether autograd should record operations on
                                  parameters in this module. Default: ``True``.

        Returns:
            Module: self
        """
        ...
    
    def zero_grad(self) -> None:
        r"""Sets gradients of all model parameters to zero."""
        ...
    
    def share_memory(self: T) -> T:
        ...
    
    def extra_repr(self) -> str:
        r"""Set the extra representation of the module

        To print customized extra information, you should reimplement
        this method in your own modules. Both single-line and multi-line
        strings are acceptable.
        """
        ...
    
    def __repr__(self):
        ...
    
    def __dir__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from .linear import Bilinear, Identity, Linear
from .conv import Conv1d, Conv2d, Conv3d, ConvTranspose1d, ConvTranspose2d, ConvTranspose3d
from .activation import CELU, ELU, GELU, GLU, Hardshrink, Hardsigmoid, Hardswish, Hardtanh, LeakyReLU, LogSigmoid, LogSoftmax, MultiheadAttention, PReLU, RReLU, ReLU, ReLU6, SELU, Sigmoid, Softmax, Softmax2d, Softmin, Softplus, Softshrink, Softsign, Tanh, Tanhshrink, Threshold
from .loss import BCELoss, BCEWithLogitsLoss, CTCLoss, CosineEmbeddingLoss, CrossEntropyLoss, HingeEmbeddingLoss, KLDivLoss, L1Loss, MSELoss, MarginRankingLoss, MultiLabelMarginLoss, MultiLabelSoftMarginLoss, MultiMarginLoss, NLLLoss, NLLLoss2d, PoissonNLLLoss, SmoothL1Loss, SoftMarginLoss, TripletMarginLoss
from .container import Container, ModuleDict, ModuleList, ParameterDict, ParameterList, Sequential
from .pooling import AdaptiveAvgPool1d, AdaptiveAvgPool2d, AdaptiveAvgPool3d, AdaptiveMaxPool1d, AdaptiveMaxPool2d, AdaptiveMaxPool3d, AvgPool1d, AvgPool2d, AvgPool3d, FractionalMaxPool2d, FractionalMaxPool3d, LPPool1d, LPPool2d, MaxPool1d, MaxPool2d, MaxPool3d, MaxUnpool1d, MaxUnpool2d, MaxUnpool3d
from .batchnorm import BatchNorm1d, BatchNorm2d, BatchNorm3d, SyncBatchNorm
from .instancenorm import InstanceNorm1d, InstanceNorm2d, InstanceNorm3d
from .normalization import CrossMapLRN2d, GroupNorm, LayerNorm, LocalResponseNorm
from .dropout import AlphaDropout, Dropout, Dropout2d, Dropout3d, FeatureAlphaDropout
from .padding import ConstantPad1d, ConstantPad2d, ConstantPad3d, ReflectionPad1d, ReflectionPad2d, ReplicationPad1d, ReplicationPad2d, ReplicationPad3d, ZeroPad2d
from .sparse import Embedding, EmbeddingBag
from .rnn import GRU, GRUCell, LSTM, LSTMCell, RNN, RNNBase, RNNCell, RNNCellBase
from .pixelshuffle import PixelShuffle
from .upsampling import Upsample, UpsamplingBilinear2d, UpsamplingNearest2d
from .distance import CosineSimilarity, PairwiseDistance
from .fold import Fold, Unfold
from .adaptive import AdaptiveLogSoftmaxWithLoss
from .transformer import Transformer, TransformerDecoder, TransformerDecoderLayer, TransformerEncoder, TransformerEncoderLayer
from .flatten import Flatten

"""
This type stub file was generated by pyright.
"""

from .batchnorm import _NormBase
from torch import Tensor

class _InstanceNorm(_NormBase):
    def __init__(self, num_features: int, eps: float = ..., momentum: float = ..., affine: bool = ..., track_running_stats: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class InstanceNorm1d(_InstanceNorm):
    r"""Applies Instance Normalization over a 3D input (a mini-batch of 1D
    inputs with optional additional channel dimension) as described in the paper
    `Instance Normalization: The Missing Ingredient for Fast Stylization
    <https://arxiv.org/abs/1607.08022>`__.

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension separately
    for each object in a mini-batch. :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size `C` (where `C` is the input size) if :attr:`affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    By default, this layer uses instance statistics computed from input data in
    both training and evaluation modes.

    If :attr:`track_running_stats` is set to ``True``, during training this
    layer keeps running estimates of its computed mean and variance, which are
    then used for normalization during evaluation. The running estimates are
    kept with a default :attr:`momentum` of 0.1.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    .. note::
        :class:`InstanceNorm1d` and :class:`LayerNorm` are very similar, but
        have some subtle differences. :class:`InstanceNorm1d` is applied
        on each channel of channeled data like multidimensional time series, but
        :class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
        transform, while :class:`InstanceNorm1d` usually don't apply affine
        transform.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`
        eps: a value added to the denominator for numerical stability. Default: 1e-5
        momentum: the value used for the running_mean and running_var computation. Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters, initialized the same way as done for batch normalization.
            Default: ``False``.
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and always uses batch
            statistics in both training and eval modes. Default: ``False``

    Shape:
        - Input: :math:`(N, C, L)`
        - Output: :math:`(N, C, L)` (same shape as input)

    Examples::

        >>> # Without Learnable Parameters
        >>> m = nn.InstanceNorm1d(100)
        >>> # With Learnable Parameters
        >>> m = nn.InstanceNorm1d(100, affine=True)
        >>> input = torch.randn(20, 100, 40)
        >>> output = m(input)
    """
    ...


class InstanceNorm2d(_InstanceNorm):
    r"""Applies Instance Normalization over a 4D input (a mini-batch of 2D inputs
    with additional channel dimension) as described in the paper
    `Instance Normalization: The Missing Ingredient for Fast Stylization
    <https://arxiv.org/abs/1607.08022>`__.

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension separately
    for each object in a mini-batch. :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size `C` (where `C` is the input size) if :attr:`affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    By default, this layer uses instance statistics computed from input data in
    both training and evaluation modes.

    If :attr:`track_running_stats` is set to ``True``, during training this
    layer keeps running estimates of its computed mean and variance, which are
    then used for normalization during evaluation. The running estimates are
    kept with a default :attr:`momentum` of 0.1.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    .. note::
        :class:`InstanceNorm2d` and :class:`LayerNorm` are very similar, but
        have some subtle differences. :class:`InstanceNorm2d` is applied
        on each channel of channeled data like RGB images, but
        :class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
        transform, while :class:`InstanceNorm2d` usually don't apply affine
        transform.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, H, W)`
        eps: a value added to the denominator for numerical stability. Default: 1e-5
        momentum: the value used for the running_mean and running_var computation. Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters, initialized the same way as done for batch normalization.
            Default: ``False``.
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and always uses batch
            statistics in both training and eval modes. Default: ``False``

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Examples::

        >>> # Without Learnable Parameters
        >>> m = nn.InstanceNorm2d(100)
        >>> # With Learnable Parameters
        >>> m = nn.InstanceNorm2d(100, affine=True)
        >>> input = torch.randn(20, 100, 35, 45)
        >>> output = m(input)
    """
    ...


class InstanceNorm3d(_InstanceNorm):
    r"""Applies Instance Normalization over a 5D input (a mini-batch of 3D inputs
    with additional channel dimension) as described in the paper
    `Instance Normalization: The Missing Ingredient for Fast Stylization
    <https://arxiv.org/abs/1607.08022>`__.

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension separately
    for each object in a mini-batch. :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size C (where C is the input size) if :attr:`affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    By default, this layer uses instance statistics computed from input data in
    both training and evaluation modes.

    If :attr:`track_running_stats` is set to ``True``, during training this
    layer keeps running estimates of its computed mean and variance, which are
    then used for normalization during evaluation. The running estimates are
    kept with a default :attr:`momentum` of 0.1.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    .. note::
        :class:`InstanceNorm3d` and :class:`LayerNorm` are very similar, but
        have some subtle differences. :class:`InstanceNorm3d` is applied
        on each channel of channeled data like 3D models with RGB color, but
        :class:`LayerNorm` is usually applied on entire sample and often in NLP
        tasks. Additionally, :class:`LayerNorm` applies elementwise affine
        transform, while :class:`InstanceNorm3d` usually don't apply affine
        transform.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, D, H, W)`
        eps: a value added to the denominator for numerical stability. Default: 1e-5
        momentum: the value used for the running_mean and running_var computation. Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters, initialized the same way as done for batch normalization.
            Default: ``False``.
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and always uses batch
            statistics in both training and eval modes. Default: ``False``

    Shape:
        - Input: :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, D, H, W)` (same shape as input)

    Examples::

        >>> # Without Learnable Parameters
        >>> m = nn.InstanceNorm3d(100)
        >>> # With Learnable Parameters
        >>> m = nn.InstanceNorm3d(100, affine=True)
        >>> input = torch.randn(20, 100, 35, 45, 10)
        >>> output = m(input)
    """
    ...


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor

class _DropoutNd(Module):
    __constants__ = ...
    p: float
    inplace: bool
    def __init__(self, p: float = ..., inplace: bool = ...) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Dropout(_DropoutNd):
    r"""During training, randomly zeroes some of the elements of the input
    tensor with probability :attr:`p` using samples from a Bernoulli
    distribution. Each channel will be zeroed out independently on every forward
    call.

    This has proven to be an effective technique for regularization and
    preventing the co-adaptation of neurons as described in the paper
    `Improving neural networks by preventing co-adaptation of feature
    detectors`_ .

    Furthermore, the outputs are scaled by a factor of :math:`\frac{1}{1-p}` during
    training. This means that during evaluation the module simply computes an
    identity function.

    Args:
        p: probability of an element to be zeroed. Default: 0.5
        inplace: If set to ``True``, will do this operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(*)`. Input can be of any shape
        - Output: :math:`(*)`. Output is of the same shape as input

    Examples::

        >>> m = nn.Dropout(p=0.2)
        >>> input = torch.randn(20, 16)
        >>> output = m(input)

    .. _Improving neural networks by preventing co-adaptation of feature
        detectors: https://arxiv.org/abs/1207.0580
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Dropout2d(_DropoutNd):
    r"""Randomly zero out entire channels (a channel is a 2D feature map,
    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the
    batched input is a 2D tensor :math:`\text{input}[i, j]`).
    Each channel will be zeroed out independently on every forward call with
    probability :attr:`p` using samples from a Bernoulli distribution.

    Usually the input comes from :class:`nn.Conv2d` modules.

    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.

    In this case, :func:`nn.Dropout2d` will help promote independence between
    feature maps and should be used instead.

    Args:
        p (float, optional): probability of an element to be zero-ed.
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Examples::

        >>> m = nn.Dropout2d(p=0.2)
        >>> input = torch.randn(20, 16, 32, 32)
        >>> output = m(input)

    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Dropout3d(_DropoutNd):
    r"""Randomly zero out entire channels (a channel is a 3D feature map,
    e.g., the :math:`j`-th channel of the :math:`i`-th sample in the
    batched input is a 3D tensor :math:`\text{input}[i, j]`).
    Each channel will be zeroed out independently on every forward call with
    probability :attr:`p` using samples from a Bernoulli distribution.

    Usually the input comes from :class:`nn.Conv3d` modules.

    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.

    In this case, :func:`nn.Dropout3d` will help promote independence between
    feature maps and should be used instead.

    Args:
        p (float, optional): probability of an element to be zeroed.
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, D, H, W)` (same shape as input)

    Examples::

        >>> m = nn.Dropout3d(p=0.2)
        >>> input = torch.randn(20, 16, 4, 32, 32)
        >>> output = m(input)

    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class AlphaDropout(_DropoutNd):
    r"""Applies Alpha Dropout over the input.

    Alpha Dropout is a type of Dropout that maintains the self-normalizing
    property.
    For an input with zero mean and unit standard deviation, the output of
    Alpha Dropout maintains the original mean and standard deviation of the
    input.
    Alpha Dropout goes hand-in-hand with SELU activation function, which ensures
    that the outputs have zero mean and unit standard deviation.

    During training, it randomly masks some of the elements of the input
    tensor with probability *p* using samples from a bernoulli distribution.
    The elements to masked are randomized on every forward call, and scaled
    and shifted to maintain zero mean and unit standard deviation.

    During evaluation the module simply computes an identity function.

    More details can be found in the paper `Self-Normalizing Neural Networks`_ .

    Args:
        p (float): probability of an element to be dropped. Default: 0.5
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(*)`. Input can be of any shape
        - Output: :math:`(*)`. Output is of the same shape as input

    Examples::

        >>> m = nn.AlphaDropout(p=0.2)
        >>> input = torch.randn(20, 16)
        >>> output = m(input)

    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class FeatureAlphaDropout(_DropoutNd):
    r"""Randomly masks out entire channels (a channel is a feature map, 
    e.g. the :math:`j`-th channel of the :math:`i`-th sample in the batch input 
    is a tensor :math:`\text{input}[i, j]`) of the input tensor). Instead of 
    setting activations to zero, as in regular Dropout, the activations are set 
    to the negative saturation value of the SELU activation function. More details
    can be found in the paper `Self-Normalizing Neural Networks`_ .

    Each element will be masked independently for each sample on every forward 
    call with probability :attr:`p` using samples from a Bernoulli distribution.
    The elements to be masked are randomized on every forward call, and scaled
    and shifted to maintain zero mean and unit variance.

    Usually the input comes from :class:`nn.AlphaDropout` modules.

    As described in the paper
    `Efficient Object Localization Using Convolutional Networks`_ ,
    if adjacent pixels within feature maps are strongly correlated
    (as is normally the case in early convolution layers) then i.i.d. dropout
    will not regularize the activations and will otherwise just result
    in an effective learning rate decrease.

    In this case, :func:`nn.AlphaDropout` will help promote independence between
    feature maps and should be used instead.

    Args:
        p (float, optional): probability of an element to be zeroed. Default: 0.5
        inplace (bool, optional): If set to ``True``, will do this operation
            in-place

    Shape:
        - Input: :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, D, H, W)` (same shape as input)

    Examples::

        >>> m = nn.FeatureAlphaDropout(p=0.2)
        >>> input = torch.randn(20, 16, 4, 32, 32)
        >>> output = m(input)

    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
    .. _Efficient Object Localization Using Convolutional Networks:
       http://arxiv.org/abs/1411.4280
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor

class PixelShuffle(Module):
    r"""Rearranges elements in a tensor of shape :math:`(*, C \times r^2, H, W)`
    to a tensor of shape :math:`(*, C, H \times r, W \times r)`.

    This is useful for implementing efficient sub-pixel convolution
    with a stride of :math:`1/r`.

    Look at the paper:
    `Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network`_
    by Shi et. al (2016) for more details.

    Args:
        upscale_factor (int): factor to increase spatial resolution by

    Shape:
        - Input: :math:`(N, L, H_{in}, W_{in})` where :math:`L=C \times \text{upscale\_factor}^2`
        - Output: :math:`(N, C, H_{out}, W_{out})` where
          :math:`H_{out} = H_{in} \times \text{upscale\_factor}`
          and :math:`W_{out} = W_{in} \times \text{upscale\_factor}`

    Examples::

        >>> pixel_shuffle = nn.PixelShuffle(3)
        >>> input = torch.randn(1, 9, 4, 4)
        >>> output = pixel_shuffle(input)
        >>> print(output.size())
        torch.Size([1, 1, 12, 12])

    .. _Real-Time Single Image and Video Super-Resolution Using an Efficient Sub-Pixel Convolutional Neural Network:
        https://arxiv.org/abs/1609.05158
    """
    __constants__ = ...
    upscale_factor: int
    def __init__(self, upscale_factor: int) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor

class Flatten(Module):
    r"""
    Flattens a contiguous range of dims into a tensor. For use with :class:`~nn.Sequential`.
    Args:
        start_dim: first dim to flatten (default = 1).
        end_dim: last dim to flatten (default = -1).

    Shape:
        - Input: :math:`(N, *dims)`
        - Output: :math:`(N, \prod *dims)` (for the default case).


    Examples::
        >>> m = nn.Sequential(
        >>>     nn.Conv2d(1, 32, 5, 1, 1),
        >>>     nn.Flatten()
        >>> )
    """
    __constants__ = ...
    start_dim: int
    end_dim: int
    def __init__(self, start_dim: int = ..., end_dim: int = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Size, Tensor
from typing import List, Union

class LocalResponseNorm(Module):
    r"""Applies local response normalization over an input signal composed
    of several input planes, where channels occupy the second dimension.
    Applies normalization across channels.

    .. math::
        b_{c} = a_{c}\left(k + \frac{\alpha}{n}
        \sum_{c'=\max(0, c-n/2)}^{\min(N-1,c+n/2)}a_{c'}^2\right)^{-\beta}

    Args:
        size: amount of neighbouring channels used for normalization
        alpha: multiplicative factor. Default: 0.0001
        beta: exponent. Default: 0.75
        k: additive factor. Default: 1

    Shape:
        - Input: :math:`(N, C, *)`
        - Output: :math:`(N, C, *)` (same shape as input)

    Examples::

        >>> lrn = nn.LocalResponseNorm(2)
        >>> signal_2d = torch.randn(32, 5, 24, 24)
        >>> signal_4d = torch.randn(16, 5, 7, 7, 7, 7)
        >>> output_2d = lrn(signal_2d)
        >>> output_4d = lrn(signal_4d)

    """
    __constants__ = ...
    size: int
    alpha: float
    beta: float
    k: float
    def __init__(self, size: int, alpha: float = ..., beta: float = ..., k: float = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self):
        ...
    


class CrossMapLRN2d(Module):
    size: int
    alpha: float
    beta: float
    k: float
    def __init__(self, size: int, alpha: float = ..., beta: float = ..., k: float = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


_shape_t = Union[int, List[int], Size]
class LayerNorm(Module):
    r"""Applies Layer Normalization over a mini-batch of inputs as described in
    the paper `Layer Normalization <https://arxiv.org/abs/1607.06450>`__

    .. math::
        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated separately over the last
    certain number dimensions which have to be of the shape specified by
    :attr:`normalized_shape`.
    :math:`\gamma` and :math:`\beta` are learnable affine transform parameters of
    :attr:`normalized_shape` if :attr:`elementwise_affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    .. note::
        Unlike Batch Normalization and Instance Normalization, which applies
        scalar scale and bias for each entire channel/plane with the
        :attr:`affine` option, Layer Normalization applies per-element scale and
        bias with :attr:`elementwise_affine`.

    This layer uses statistics computed from input data in both training and
    evaluation modes.

    Args:
        normalized_shape (int or list or torch.Size): input shape from an expected input
            of size

            .. math::
                [* \times \text{normalized\_shape}[0] \times \text{normalized\_shape}[1]
                    \times \ldots \times \text{normalized\_shape}[-1]]

            If a single integer is used, it is treated as a singleton list, and this module will
            normalize over the last dimension which is expected to be of that specific size.
        eps: a value added to the denominator for numerical stability. Default: 1e-5
        elementwise_affine: a boolean value that when set to ``True``, this module
            has learnable per-element affine parameters initialized to ones (for weights)
            and zeros (for biases). Default: ``True``.

    Shape:
        - Input: :math:`(N, *)`
        - Output: :math:`(N, *)` (same shape as input)

    Examples::

        >>> input = torch.randn(20, 5, 10, 10)
        >>> # With Learnable Parameters
        >>> m = nn.LayerNorm(input.size()[1:])
        >>> # Without Learnable Parameters
        >>> m = nn.LayerNorm(input.size()[1:], elementwise_affine=False)
        >>> # Normalize over last two dimensions
        >>> m = nn.LayerNorm([10, 10])
        >>> # Normalize over last dimension of size 10
        >>> m = nn.LayerNorm(10)
        >>> # Activating the module
        >>> output = m(input)
    """
    __constants__ = ...
    normalized_shape: _shape_t
    eps: float
    elementwise_affine: bool
    def __init__(self, normalized_shape: _shape_t, eps: float = ..., elementwise_affine: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> Tensor:
        ...
    


class GroupNorm(Module):
    r"""Applies Group Normalization over a mini-batch of inputs as described in
    the paper `Group Normalization <https://arxiv.org/abs/1803.08494>`__

    .. math::
        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The input channels are separated into :attr:`num_groups` groups, each containing
    ``num_channels / num_groups`` channels. The mean and standard-deviation are calculated
    separately over the each group. :math:`\gamma` and :math:`\beta` are learnable
    per-channel affine transform parameter vectors of size :attr:`num_channels` if
    :attr:`affine` is ``True``.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    This layer uses statistics computed from input data in both training and
    evaluation modes.

    Args:
        num_groups (int): number of groups to separate the channels into
        num_channels (int): number of channels expected in input
        eps: a value added to the denominator for numerical stability. Default: 1e-5
        affine: a boolean value that when set to ``True``, this module
            has learnable per-channel affine parameters initialized to ones (for weights)
            and zeros (for biases). Default: ``True``.

    Shape:
        - Input: :math:`(N, C, *)` where :math:`C=\text{num\_channels}`
        - Output: :math:`(N, C, *)` (same shape as input)

    Examples::

        >>> input = torch.randn(20, 6, 10, 10)
        >>> # Separate 6 channels into 3 groups
        >>> m = nn.GroupNorm(3, 6)
        >>> # Separate 6 channels into 6 groups (equivalent with InstanceNorm)
        >>> m = nn.GroupNorm(6, 6)
        >>> # Put all 6 channels into a single group (equivalent with LayerNorm)
        >>> m = nn.GroupNorm(1, 6)
        >>> # Activating the module
        >>> output = m(input)
    """
    __constants__ = ...
    num_groups: int
    num_channels: int
    eps: float
    affine: bool
    def __init__(self, num_groups: int, num_channels: int, eps: float = ..., affine: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch import Tensor
from .module import Module
from typing import Any, Optional

class _NormBase(Module):
    """Common base of _InstanceNorm and _BatchNorm"""
    _version = ...
    __constants__ = ...
    num_features: int
    eps: float
    momentum: float
    affine: bool
    track_running_stats: bool
    def __init__(self, num_features: int, eps: float = ..., momentum: float = ..., affine: bool = ..., track_running_stats: bool = ...) -> None:
        ...
    
    def reset_running_stats(self) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def extra_repr(self):
        ...
    


class _BatchNorm(_NormBase):
    def __init__(self, num_features, eps=..., momentum=..., affine=..., track_running_stats=...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class BatchNorm1d(_BatchNorm):
    r"""Applies Batch Normalization over a 2D or 3D input (a mini-batch of 1D
    inputs with optional additional channel dimension) as described in the paper
    `Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{\sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over
    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.

    Also by default, during training this layer keeps running estimates of its
    computed mean and variance, which are then used for normalization during
    evaluation. The running estimates are kept with a default :attr:`momentum`
    of 0.1.

    If :attr:`track_running_stats` is set to ``False``, this layer then does not
    keep running estimates, and batch statistics are instead used during
    evaluation time as well.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    Because the Batch Normalization is done over the `C` dimension, computing statistics
    on `(N, L)` slices, it's common terminology to call this Temporal Batch Normalization.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, L)` or :math:`L` from input of size :math:`(N, L)`
        eps: a value added to the denominator for numerical stability.
            Default: 1e-5
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and uses batch statistics instead
            in both training and eval modes if the running mean and variance are ``None``. Default: ``True``

    Shape:
        - Input: :math:`(N, C)` or :math:`(N, C, L)`
        - Output: :math:`(N, C)` or :math:`(N, C, L)` (same shape as input)

    Examples::

        >>> # With Learnable Parameters
        >>> m = nn.BatchNorm1d(100)
        >>> # Without Learnable Parameters
        >>> m = nn.BatchNorm1d(100, affine=False)
        >>> input = torch.randn(20, 100)
        >>> output = m(input)
    """
    ...


class BatchNorm2d(_BatchNorm):
    r"""Applies Batch Normalization over a 4D input (a mini-batch of 2D inputs
    with additional channel dimension) as described in the paper
    `Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over
    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.

    Also by default, during training this layer keeps running estimates of its
    computed mean and variance, which are then used for normalization during
    evaluation. The running estimates are kept with a default :attr:`momentum`
    of 0.1.

    If :attr:`track_running_stats` is set to ``False``, this layer then does not
    keep running estimates, and batch statistics are instead used during
    evaluation time as well.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    Because the Batch Normalization is done over the `C` dimension, computing statistics
    on `(N, H, W)` slices, it's common terminology to call this Spatial Batch Normalization.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, H, W)`
        eps: a value added to the denominator for numerical stability.
            Default: 1e-5
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and uses batch statistics instead
            in both training and eval modes if the running mean and variance are ``None``. Default: ``True``

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Examples::

        >>> # With Learnable Parameters
        >>> m = nn.BatchNorm2d(100)
        >>> # Without Learnable Parameters
        >>> m = nn.BatchNorm2d(100, affine=False)
        >>> input = torch.randn(20, 100, 35, 45)
        >>> output = m(input)
    """
    ...


class BatchNorm3d(_BatchNorm):
    r"""Applies Batch Normalization over a 5D input (a mini-batch of 3D inputs
    with additional channel dimension) as described in the paper
    `Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over
    the mini-batches and :math:`\gamma` and :math:`\beta` are learnable parameter vectors
    of size `C` (where `C` is the input size). By default, the elements of :math:`\gamma` are set
    to 1 and the elements of :math:`\beta` are set to 0. The standard-deviation is calculated
    via the biased estimator, equivalent to `torch.var(input, unbiased=False)`.

    Also by default, during training this layer keeps running estimates of its
    computed mean and variance, which are then used for normalization during
    evaluation. The running estimates are kept with a default :attr:`momentum`
    of 0.1.

    If :attr:`track_running_stats` is set to ``False``, this layer then does not
    keep running estimates, and batch statistics are instead used during
    evaluation time as well.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momentum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    Because the Batch Normalization is done over the `C` dimension, computing statistics
    on `(N, D, H, W)` slices, it's common terminology to call this Volumetric Batch Normalization
    or Spatio-temporal Batch Normalization.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, D, H, W)`
        eps: a value added to the denominator for numerical stability.
            Default: 1e-5
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and uses batch statistics instead
            in both training and eval modes if the running mean and variance are ``None``. Default: ``True``

    Shape:
        - Input: :math:`(N, C, D, H, W)`
        - Output: :math:`(N, C, D, H, W)` (same shape as input)

    Examples::

        >>> # With Learnable Parameters
        >>> m = nn.BatchNorm3d(100)
        >>> # Without Learnable Parameters
        >>> m = nn.BatchNorm3d(100, affine=False)
        >>> input = torch.randn(20, 100, 35, 45, 10)
        >>> output = m(input)
    """
    ...


class SyncBatchNorm(_BatchNorm):
    r"""Applies Batch Normalization over a N-Dimensional input (a mini-batch of [N-2]D inputs
    with additional channel dimension) as described in the paper
    `Batch Normalization: Accelerating Deep Network Training by Reducing
    Internal Covariate Shift <https://arxiv.org/abs/1502.03167>`__ .

    .. math::

        y = \frac{x - \mathrm{E}[x]}{ \sqrt{\mathrm{Var}[x] + \epsilon}} * \gamma + \beta

    The mean and standard-deviation are calculated per-dimension over all
    mini-batches of the same process groups. :math:`\gamma` and :math:`\beta`
    are learnable parameter vectors of size `C` (where `C` is the input size).
    By default, the elements of :math:`\gamma` are sampled from
    :math:`\mathcal{U}(0, 1)` and the elements of :math:`\beta` are set to 0.
    The standard-deviation is calculated via the biased estimator, equivalent to
    `torch.var(input, unbiased=False)`.

    Also by default, during training this layer keeps running estimates of its
    computed mean and variance, which are then used for normalization during
    evaluation. The running estimates are kept with a default :attr:`momentum`
    of 0.1.

    If :attr:`track_running_stats` is set to ``False``, this layer then does not
    keep running estimates, and batch statistics are instead used during
    evaluation time as well.

    .. note::
        This :attr:`momentum` argument is different from one used in optimizer
        classes and the conventional notion of momentum. Mathematically, the
        update rule for running statistics here is
        :math:`\hat{x}_\text{new} = (1 - \text{momentum}) \times \hat{x} + \text{momemtum} \times x_t`,
        where :math:`\hat{x}` is the estimated statistic and :math:`x_t` is the
        new observed value.

    Because the Batch Normalization is done for each channel in the ``C`` dimension, computing
    statistics on ``(N, +)`` slices, it's common terminology to call this Volumetric Batch
    Normalization or Spatio-temporal Batch Normalization.

    Currently :class:`SyncBatchNorm` only supports
    :class:`~torch.nn.DistributedDataParallel` (DDP) with single GPU per process. Use
    :meth:`torch.nn.SyncBatchNorm.convert_sync_batchnorm()` to convert
    :attr:`BatchNorm*D` layer to :class:`SyncBatchNorm` before wrapping
    Network with DDP.

    Args:
        num_features: :math:`C` from an expected input of size
            :math:`(N, C, +)`
        eps: a value added to the denominator for numerical stability.
            Default: ``1e-5``
        momentum: the value used for the running_mean and running_var
            computation. Can be set to ``None`` for cumulative moving average
            (i.e. simple average). Default: 0.1
        affine: a boolean value that when set to ``True``, this module has
            learnable affine parameters. Default: ``True``
        track_running_stats: a boolean value that when set to ``True``, this
            module tracks the running mean and variance, and when set to ``False``,
            this module does not track such statistics and uses batch statistics instead
            in both training and eval modes if the running mean and variance are ``None``. Default: ``True``
        process_group: synchronization of stats happen within each process group
            individually. Default behavior is synchronization across the whole
            world

    Shape:
        - Input: :math:`(N, C, +)`
        - Output: :math:`(N, C, +)` (same shape as input)

    Examples::

        >>> # With Learnable Parameters
        >>> m = nn.SyncBatchNorm(100)
        >>> # creating process group (optional)
        >>> # process_ids is a list of int identifying rank ids.
        >>> process_group = torch.distributed.new_group(process_ids)
        >>> # Without Learnable Parameters
        >>> m = nn.BatchNorm3d(100, affine=False, process_group=process_group)
        >>> input = torch.randn(20, 100, 35, 45, 10)
        >>> output = m(input)

        >>> # network is nn.BatchNorm layer
        >>> sync_bn_network = nn.SyncBatchNorm.convert_sync_batchnorm(network, process_group)
        >>> # only single gpu per process is currently supported
        >>> ddp_sync_bn_network = torch.nn.parallel.DistributedDataParallel(
        >>>                         sync_bn_network,
        >>>                         device_ids=[args.local_rank],
        >>>                         output_device=args.local_rank)
    """
    def __init__(self, num_features: int, eps: float = ..., momentum: float = ..., affine: bool = ..., track_running_stats: bool = ..., process_group: Optional[Any] = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    @classmethod
    def convert_sync_batchnorm(cls, module, process_group=...):
        r"""Helper function to convert all :attr:`BatchNorm*D` layers in the model to
        :class:`torch.nn.SyncBatchNorm` layers.

        Args:
            module (nn.Module): module containing one or more attr:`BatchNorm*D` layers
            process_group (optional): process group to scope synchronization,
                default is the whole world

        Returns:
            The original :attr:`module` with the converted :class:`torch.nn.SyncBatchNorm`
            layers. If the original :attr:`module` is a :attr:`BatchNorm*D` layer,
            a new :class:`torch.nn.SyncBatchNorm` layer object will be returned
            instead.

        Example::

            >>> # Network with nn.BatchNorm layer
            >>> module = torch.nn.Sequential(
            >>>            torch.nn.Linear(20, 100),
            >>>            torch.nn.BatchNorm1d(100),
            >>>          ).cuda()
            >>> # creating process group (optional)
            >>> # process_ids is a list of int identifying rank ids.
            >>> process_group = torch.distributed.new_group(process_ids)
            >>> sync_bn_module = torch.nn.SyncBatchNorm.convert_sync_batchnorm(module, process_group)

        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch import Tensor
from .module import Module

class Identity(Module):
    r"""A placeholder identity operator that is argument-insensitive.

    Args:
        args: any argument (unused)
        kwargs: any keyword argument (unused)

    Examples::

        >>> m = nn.Identity(54, unused_argument1=0.1, unused_argument2=False)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 20])

    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Linear(Module):
    r"""Applies a linear transformation to the incoming data: :math:`y = xA^T + b`

    Args:
        in_features: size of each input sample
        out_features: size of each output sample
        bias: If set to ``False``, the layer will not learn an additive bias.
            Default: ``True``

    Shape:
        - Input: :math:`(N, *, H_{in})` where :math:`*` means any number of
          additional dimensions and :math:`H_{in} = \text{in\_features}`
        - Output: :math:`(N, *, H_{out})` where all but the last dimension
          are the same shape as the input and :math:`H_{out} = \text{out\_features}`.

    Attributes:
        weight: the learnable weights of the module of shape
            :math:`(\text{out\_features}, \text{in\_features})`. The values are
            initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
            :math:`k = \frac{1}{\text{in\_features}}`
        bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
                If :attr:`bias` is ``True``, the values are initialized from
                :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                :math:`k = \frac{1}{\text{in\_features}}`

    Examples::

        >>> m = nn.Linear(20, 30)
        >>> input = torch.randn(128, 20)
        >>> output = m(input)
        >>> print(output.size())
        torch.Size([128, 30])
    """
    __constants__ = ...
    in_features: int
    out_features: int
    weight: Tensor
    def __init__(self, in_features: int, out_features: int, bias: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class _LinearWithBias(Linear):
    bias: Tensor
    def __init__(self, in_features: int, out_features: int) -> None:
        ...
    


class Bilinear(Module):
    r"""Applies a bilinear transformation to the incoming data:
    :math:`y = x_1^T A x_2 + b`

    Args:
        in1_features: size of each first input sample
        in2_features: size of each second input sample
        out_features: size of each output sample
        bias: If set to False, the layer will not learn an additive bias.
            Default: ``True``

    Shape:
        - Input1: :math:`(N, *, H_{in1})` where :math:`H_{in1}=\text{in1\_features}` and
          :math:`*` means any number of additional dimensions. All but the last dimension
          of the inputs should be the same.
        - Input2: :math:`(N, *, H_{in2})` where :math:`H_{in2}=\text{in2\_features}`.
        - Output: :math:`(N, *, H_{out})` where :math:`H_{out}=\text{out\_features}`
          and all but the last dimension are the same shape as the input.

    Attributes:
        weight: the learnable weights of the module of shape
            :math:`(\text{out\_features}, \text{in1\_features}, \text{in2\_features})`.
            The values are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
            :math:`k = \frac{1}{\text{in1\_features}}`
        bias:   the learnable bias of the module of shape :math:`(\text{out\_features})`.
                If :attr:`bias` is ``True``, the values are initialized from
                :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`, where
                :math:`k = \frac{1}{\text{in1\_features}}`

    Examples::

        >>> m = nn.Bilinear(20, 30, 40)
        >>> input1 = torch.randn(128, 20)
        >>> input2 = torch.randn(128, 30)
        >>> output = m(input1, input2)
        >>> print(output.size())
        torch.Size([128, 40])
    """
    __constants__ = ...
    in1_features: int
    in2_features: int
    out_features: int
    weight: Tensor
    def __init__(self, in1_features: int, in2_features: int, out_features: int, bias: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input1: Tensor, input2: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


"""
This type stub file was generated by pyright.
"""

_single = _ntuple(1)
_pair = _ntuple(2)
_triple = _ntuple(3)
_quadruple = _ntuple(4)
"""
This type stub file was generated by pyright.
"""

from typing import Optional
from torch import Tensor
from .module import Module

class Embedding(Module):
    r"""A simple lookup table that stores embeddings of a fixed dictionary and size.

    This module is often used to store word embeddings and retrieve them using indices.
    The input to the module is a list of indices, and the output is the corresponding
    word embeddings.

    Args:
        num_embeddings (int): size of the dictionary of embeddings
        embedding_dim (int): the size of each embedding vector
        padding_idx (int, optional): If given, pads the output with the embedding vector at :attr:`padding_idx`
                                         (initialized to zeros) whenever it encounters the index.
        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                    is renormalized to have norm :attr:`max_norm`.
        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
        scale_grad_by_freq (boolean, optional): If given, this will scale gradients by the inverse of frequency of
                                                the words in the mini-batch. Default ``False``.
        sparse (bool, optional): If ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor.
                                 See Notes for more details regarding sparse gradients.

    Attributes:
        weight (Tensor): the learnable weights of the module of shape (num_embeddings, embedding_dim)
                         initialized from :math:`\mathcal{N}(0, 1)`

    Shape:
        - Input: :math:`(*)`, LongTensor of arbitrary shape containing the indices to extract
        - Output: :math:`(*, H)`, where `*` is the input shape and :math:`H=\text{embedding\_dim}`

    .. note::
        Keep in mind that only a limited number of optimizers support
        sparse gradients: currently it's :class:`optim.SGD` (`CUDA` and `CPU`),
        :class:`optim.SparseAdam` (`CUDA` and `CPU`) and :class:`optim.Adagrad` (`CPU`)

    .. note::
        With :attr:`padding_idx` set, the embedding vector at
        :attr:`padding_idx` is initialized to all zeros. However, note that this
        vector can be modified afterwards, e.g., using a customized
        initialization method, and thus changing the vector used to pad the
        output. The gradient for this vector from :class:`~torch.nn.Embedding`
        is always zero.

    Examples::

        >>> # an Embedding module containing 10 tensors of size 3
        >>> embedding = nn.Embedding(10, 3)
        >>> # a batch of 2 samples of 4 indices each
        >>> input = torch.LongTensor([[1,2,4,5],[4,3,2,9]])
        >>> embedding(input)
        tensor([[[-0.0251, -1.6902,  0.7172],
                 [-0.6431,  0.0748,  0.6969],
                 [ 1.4970,  1.3448, -0.9685],
                 [-0.3677, -2.7265, -0.1685]],

                [[ 1.4970,  1.3448, -0.9685],
                 [ 0.4362, -0.4004,  0.9400],
                 [-0.6431,  0.0748,  0.6969],
                 [ 0.9124, -2.3616,  1.1151]]])


        >>> # example with padding_idx
        >>> embedding = nn.Embedding(10, 3, padding_idx=0)
        >>> input = torch.LongTensor([[0,2,0,5]])
        >>> embedding(input)
        tensor([[[ 0.0000,  0.0000,  0.0000],
                 [ 0.1535, -2.0309,  0.9315],
                 [ 0.0000,  0.0000,  0.0000],
                 [-0.1655,  0.9897,  0.0635]]])
    """
    __constants__ = ...
    num_embeddings: int
    embedding_dim: int
    padding_idx: int
    max_norm: float
    norm_type: float
    scale_grad_by_freq: bool
    weight: Tensor
    sparse: bool
    def __init__(self, num_embeddings: int, embedding_dim: int, padding_idx: Optional[int] = ..., max_norm: Optional[float] = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., sparse: bool = ..., _weight: Optional[Tensor] = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    
    @classmethod
    def from_pretrained(cls, embeddings, freeze=..., padding_idx=..., max_norm=..., norm_type=..., scale_grad_by_freq=..., sparse=...):
        r"""Creates Embedding instance from given 2-dimensional FloatTensor.

        Args:
            embeddings (Tensor): FloatTensor containing weights for the Embedding.
                First dimension is being passed to Embedding as ``num_embeddings``, second as ``embedding_dim``.
            freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.
                Equivalent to ``embedding.weight.requires_grad = False``. Default: ``True``
            padding_idx (int, optional): See module initialization documentation.
            max_norm (float, optional): See module initialization documentation.
            norm_type (float, optional): See module initialization documentation. Default ``2``.
            scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.
            sparse (bool, optional): See module initialization documentation.

        Examples::

            >>> # FloatTensor containing pretrained weights
            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
            >>> embedding = nn.Embedding.from_pretrained(weight)
            >>> # Get embeddings for index 1
            >>> input = torch.LongTensor([1])
            >>> embedding(input)
            tensor([[ 4.0000,  5.1000,  6.3000]])
        """
        ...
    


class EmbeddingBag(Module):
    r"""Computes sums or means of 'bags' of embeddings, without instantiating the
    intermediate embeddings.

    For bags of constant length and no :attr:`per_sample_weights`, this class

        * with ``mode="sum"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.sum(dim=0)``,
        * with ``mode="mean"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.mean(dim=0)``,
        * with ``mode="max"`` is equivalent to :class:`~torch.nn.Embedding` followed by ``torch.max(dim=0)``.

    However, :class:`~torch.nn.EmbeddingBag` is much more time and memory efficient than using a chain of these
    operations.

    EmbeddingBag also supports per-sample weights as an argument to the forward
    pass. This scales the output of the Embedding before performing a weighted
    reduction as specified by ``mode``. If :attr:`per_sample_weights`` is passed, the
    only supported ``mode`` is ``"sum"``, which computes a weighted sum according to
    :attr:`per_sample_weights`.

    Args:
        num_embeddings (int): size of the dictionary of embeddings
        embedding_dim (int): the size of each embedding vector
        max_norm (float, optional): If given, each embedding vector with norm larger than :attr:`max_norm`
                                    is renormalized to have norm :attr:`max_norm`.
        norm_type (float, optional): The p of the p-norm to compute for the :attr:`max_norm` option. Default ``2``.
        scale_grad_by_freq (boolean, optional): if given, this will scale gradients by the inverse of frequency of
                                                the words in the mini-batch. Default ``False``.
                                                Note: this option is not supported when ``mode="max"``.
        mode (string, optional): ``"sum"``, ``"mean"`` or ``"max"``. Specifies the way to reduce the bag.
                                 ``"sum"`` computes the weighted sum, taking :attr:`per_sample_weights`
                                 into consideration. ``"mean"`` computes the average of the values
                                 in the bag, ``"max"`` computes the max value over each bag.
                                 Default: ``"mean"``
        sparse (bool, optional): if ``True``, gradient w.r.t. :attr:`weight` matrix will be a sparse tensor. See
                                 Notes for more details regarding sparse gradients. Note: this option is not
                                 supported when ``mode="max"``.
        include_last_offset (bool, optional): if ``True``, :attr:`offsets` has one additional element, where the last element
                                      is equivalent to the size of `indices`. This matches the CSR format. Note:
                                      this option is currently only supported when ``mode="sum"``.

    Attributes:
        weight (Tensor): the learnable weights of the module of shape `(num_embeddings, embedding_dim)`
                         initialized from :math:`\mathcal{N}(0, 1)`.

    Inputs: :attr:`input` (LongTensor), :attr:`offsets` (LongTensor, optional), and
        :attr:`per_index_weights` (Tensor, optional)

        - If :attr:`input` is 2D of shape `(B, N)`,

          it will be treated as ``B`` bags (sequences) each of fixed length ``N``, and
          this will return ``B`` values aggregated in a way depending on the :attr:`mode`.
          :attr:`offsets` is ignored and required to be ``None`` in this case.

        - If :attr:`input` is 1D of shape `(N)`,

          it will be treated as a concatenation of multiple bags (sequences).
          :attr:`offsets` is required to be a 1D tensor containing the
          starting index positions of each bag in :attr:`input`. Therefore,
          for :attr:`offsets` of shape `(B)`, :attr:`input` will be viewed as
          having ``B`` bags. Empty bags (i.e., having 0-length) will have
          returned vectors filled by zeros.

        per_sample_weights (Tensor, optional): a tensor of float / double weights, or None
            to indicate all weights should be taken to be ``1``. If specified, :attr:`per_sample_weights`
            must have exactly the same shape as input and is treated as having the same
            :attr:`offsets`, if those are not ``None``. Only supported for ``mode='sum'``.


    Output shape: `(B, embedding_dim)`

    Examples::

        >>> # an Embedding module containing 10 tensors of size 3
        >>> embedding_sum = nn.EmbeddingBag(10, 3, mode='sum')
        >>> # a batch of 2 samples of 4 indices each
        >>> input = torch.LongTensor([1,2,4,5,4,3,2,9])
        >>> offsets = torch.LongTensor([0,4])
        >>> embedding_sum(input, offsets)
        tensor([[-0.8861, -5.4350, -0.0523],
                [ 1.1306, -2.5798, -1.0044]])
    """
    __constants__ = ...
    num_embeddings: int
    embedding_dim: int
    max_norm: float
    norm_type: float
    scale_grad_by_freq: bool
    weight: Tensor
    mode: str
    sparse: bool
    include_last_offset: bool
    def __init__(self, num_embeddings: int, embedding_dim: int, max_norm: Optional[float] = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., mode: str = ..., sparse: bool = ..., _weight: Optional[Tensor] = ..., include_last_offset: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor, offsets: Optional[Tensor] = ..., per_sample_weights: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    
    @classmethod
    def from_pretrained(cls, embeddings: Tensor, freeze: bool = ..., max_norm: Optional[float] = ..., norm_type: float = ..., scale_grad_by_freq: bool = ..., mode: str = ..., sparse: bool = ..., include_last_offset: bool = ...) -> EmbeddingBag:
        r"""Creates EmbeddingBag instance from given 2-dimensional FloatTensor.

        Args:
            embeddings (Tensor): FloatTensor containing weights for the EmbeddingBag.
                First dimension is being passed to EmbeddingBag as 'num_embeddings', second as 'embedding_dim'.
            freeze (boolean, optional): If ``True``, the tensor does not get updated in the learning process.
                Equivalent to ``embeddingbag.weight.requires_grad = False``. Default: ``True``
            max_norm (float, optional): See module initialization documentation. Default: ``None``
            norm_type (float, optional): See module initialization documentation. Default ``2``.
            scale_grad_by_freq (boolean, optional): See module initialization documentation. Default ``False``.
            mode (string, optional): See module initialization documentation. Default: ``"mean"``
            sparse (bool, optional): See module initialization documentation. Default: ``False``.
            include_last_offset (bool, optional): See module initialization documentation. Default: ``False``.

        Examples::

            >>> # FloatTensor containing pretrained weights
            >>> weight = torch.FloatTensor([[1, 2.3, 3], [4, 5.1, 6.3]])
            >>> embeddingbag = nn.EmbeddingBag.from_pretrained(weight)
            >>> # Get embeddings for index 1
            >>> input = torch.LongTensor([[1, 0]])
            >>> embeddingbag(input)
            tensor([[ 2.5000,  3.7000,  4.6500]])
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor
from typing import Optional
from ..common_types import _ratio_2_t, _ratio_any_t, _size_2_t, _size_any_t

class Upsample(Module):
    r"""Upsamples a given multi-channel 1D (temporal), 2D (spatial) or 3D (volumetric) data.

    The input data is assumed to be of the form
    `minibatch x channels x [optional depth] x [optional height] x width`.
    Hence, for spatial inputs, we expect a 4D Tensor and for volumetric inputs, we expect a 5D Tensor.

    The algorithms available for upsampling are nearest neighbor and linear,
    bilinear, bicubic and trilinear for 3D, 4D and 5D input Tensor,
    respectively.

    One can either give a :attr:`scale_factor` or the target output :attr:`size` to
    calculate the output size. (You cannot give both, as it is ambiguous)

    Args:
        size (int or Tuple[int] or Tuple[int, int] or Tuple[int, int, int], optional):
            output spatial sizes
        scale_factor (float or Tuple[float] or Tuple[float, float] or Tuple[float, float, float], optional):
            multiplier for spatial size. Has to match input size if it is a tuple.
        mode (str, optional): the upsampling algorithm: one of ``'nearest'``,
            ``'linear'``, ``'bilinear'``, ``'bicubic'`` and ``'trilinear'``.
            Default: ``'nearest'``
        align_corners (bool, optional): if ``True``, the corner pixels of the input
            and output tensors are aligned, and thus preserving the values at
            those pixels. This only has effect when :attr:`mode` is
            ``'linear'``, ``'bilinear'``, or ``'trilinear'``. Default: ``False``

    Shape:
        - Input: :math:`(N, C, W_{in})`, :math:`(N, C, H_{in}, W_{in})` or :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, W_{out})`, :math:`(N, C, H_{out}, W_{out})`
          or :math:`(N, C, D_{out}, H_{out}, W_{out})`, where

    .. math::
        D_{out} = \left\lfloor D_{in} \times \text{scale\_factor} \right\rfloor

    .. math::
        H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

    .. math::
        W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

    .. warning::
        With ``align_corners = True``, the linearly interpolating modes
        (`linear`, `bilinear`, `bicubic`, and `trilinear`) don't proportionally
        align the output and input pixels, and thus the output values can depend
        on the input size. This was the default behavior for these modes up to
        version 0.3.1. Since then, the default behavior is
        ``align_corners = False``. See below for concrete examples on how this
        affects the outputs.

    .. note::
        If you want downsampling/general resizing, you should use :func:`~nn.functional.interpolate`.

    Examples::

        >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
        >>> input
        tensor([[[[ 1.,  2.],
                  [ 3.,  4.]]]])

        >>> m = nn.Upsample(scale_factor=2, mode='nearest')
        >>> m(input)
        tensor([[[[ 1.,  1.,  2.,  2.],
                  [ 1.,  1.,  2.,  2.],
                  [ 3.,  3.,  4.,  4.],
                  [ 3.,  3.,  4.,  4.]]]])

        >>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False
        >>> m(input)
        tensor([[[[ 1.0000,  1.2500,  1.7500,  2.0000],
                  [ 1.5000,  1.7500,  2.2500,  2.5000],
                  [ 2.5000,  2.7500,  3.2500,  3.5000],
                  [ 3.0000,  3.2500,  3.7500,  4.0000]]]])

        >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        >>> m(input)
        tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
                  [ 1.6667,  2.0000,  2.3333,  2.6667],
                  [ 2.3333,  2.6667,  3.0000,  3.3333],
                  [ 3.0000,  3.3333,  3.6667,  4.0000]]]])

        >>> # Try scaling the same data in a larger tensor
        >>>
        >>> input_3x3 = torch.zeros(3, 3).view(1, 1, 3, 3)
        >>> input_3x3[:, :, :2, :2].copy_(input)
        tensor([[[[ 1.,  2.],
                  [ 3.,  4.]]]])
        >>> input_3x3
        tensor([[[[ 1.,  2.,  0.],
                  [ 3.,  4.,  0.],
                  [ 0.,  0.,  0.]]]])

        >>> m = nn.Upsample(scale_factor=2, mode='bilinear')  # align_corners=False
        >>> # Notice that values in top left corner are the same with the small input (except at boundary)
        >>> m(input_3x3)
        tensor([[[[ 1.0000,  1.2500,  1.7500,  1.5000,  0.5000,  0.0000],
                  [ 1.5000,  1.7500,  2.2500,  1.8750,  0.6250,  0.0000],
                  [ 2.5000,  2.7500,  3.2500,  2.6250,  0.8750,  0.0000],
                  [ 2.2500,  2.4375,  2.8125,  2.2500,  0.7500,  0.0000],
                  [ 0.7500,  0.8125,  0.9375,  0.7500,  0.2500,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])

        >>> m = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)
        >>> # Notice that values in top left corner are now changed
        >>> m(input_3x3)
        tensor([[[[ 1.0000,  1.4000,  1.8000,  1.6000,  0.8000,  0.0000],
                  [ 1.8000,  2.2000,  2.6000,  2.2400,  1.1200,  0.0000],
                  [ 2.6000,  3.0000,  3.4000,  2.8800,  1.4400,  0.0000],
                  [ 2.4000,  2.7200,  3.0400,  2.5600,  1.2800,  0.0000],
                  [ 1.2000,  1.3600,  1.5200,  1.2800,  0.6400,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])
    """
    __constants__ = ...
    name: str
    size: _size_any_t
    scale_factor: _ratio_any_t
    mode: str
    align_corners: bool
    def __init__(self, size: Optional[_size_any_t] = ..., scale_factor: Optional[_ratio_any_t] = ..., mode: str = ..., align_corners: Optional[bool] = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class UpsamplingNearest2d(Upsample):
    r"""Applies a 2D nearest neighbor upsampling to an input signal composed of several input
    channels.

    To specify the scale, it takes either the :attr:`size` or the :attr:`scale_factor`
    as it's constructor argument.

    When :attr:`size` is given, it is the output size of the image `(h, w)`.

    Args:
        size (int or Tuple[int, int], optional): output spatial sizes
        scale_factor (float or Tuple[float, float], optional): multiplier for
            spatial size.

    .. warning::
        This class is deprecated in favor of :func:`~nn.functional.interpolate`.

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

    .. math::
          H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

    .. math::
          W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

    Examples::

        >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
        >>> input
        tensor([[[[ 1.,  2.],
                  [ 3.,  4.]]]])

        >>> m = nn.UpsamplingNearest2d(scale_factor=2)
        >>> m(input)
        tensor([[[[ 1.,  1.,  2.,  2.],
                  [ 1.,  1.,  2.,  2.],
                  [ 3.,  3.,  4.,  4.],
                  [ 3.,  3.,  4.,  4.]]]])
    """
    def __init__(self, size: Optional[_size_2_t] = ..., scale_factor: Optional[_ratio_2_t] = ...) -> None:
        ...
    


class UpsamplingBilinear2d(Upsample):
    r"""Applies a 2D bilinear upsampling to an input signal composed of several input
    channels.

    To specify the scale, it takes either the :attr:`size` or the :attr:`scale_factor`
    as it's constructor argument.

    When :attr:`size` is given, it is the output size of the image `(h, w)`.

    Args:
        size (int or Tuple[int, int], optional): output spatial sizes
        scale_factor (float or Tuple[float, float], optional): multiplier for
            spatial size.

    .. warning::
        This class is deprecated in favor of :func:`~nn.functional.interpolate`. It is
        equivalent to ``nn.functional.interpolate(..., mode='bilinear', align_corners=True)``.

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

    .. math::
        H_{out} = \left\lfloor H_{in} \times \text{scale\_factor} \right\rfloor

    .. math::
        W_{out} = \left\lfloor W_{in} \times \text{scale\_factor} \right\rfloor

    Examples::

        >>> input = torch.arange(1, 5, dtype=torch.float32).view(1, 1, 2, 2)
        >>> input
        tensor([[[[ 1.,  2.],
                  [ 3.,  4.]]]])

        >>> m = nn.UpsamplingBilinear2d(scale_factor=2)
        >>> m(input)
        tensor([[[[ 1.0000,  1.3333,  1.6667,  2.0000],
                  [ 1.6667,  2.0000,  2.3333,  2.6667],
                  [ 2.3333,  2.6667,  3.0000,  3.3333],
                  [ 3.0000,  3.3333,  3.6667,  4.0000]]]])
    """
    def __init__(self, size: Optional[_size_2_t] = ..., scale_factor: Optional[_ratio_2_t] = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from collections import namedtuple
from torch import Tensor
from typing import List, Sequence
from . import Linear, ModuleList
from .module import Module

_ASMoutput = namedtuple('ASMoutput', ['output', 'loss'])
class AdaptiveLogSoftmaxWithLoss(Module):
    r"""Efficient softmax approximation as described in
    `Efficient softmax approximation for GPUs by Edouard Grave, Armand Joulin,
    Moustapha Cissé, David Grangier, and Hervé Jégou
    <https://arxiv.org/abs/1609.04309>`__.

    Adaptive softmax is an approximate strategy for training models with large
    output spaces. It is most effective when the label distribution is highly
    imbalanced, for example in natural language modelling, where the word
    frequency distribution approximately follows the `Zipf's law`_.

    Adaptive softmax partitions the labels into several clusters, according to
    their frequency. These clusters may contain different number of targets
    each.
    Additionally, clusters containing less frequent labels assign lower
    dimensional embeddings to those labels, which speeds up the computation.
    For each minibatch, only clusters for which at least one target is
    present are evaluated.

    The idea is that the clusters which are accessed frequently
    (like the first one, containing most frequent labels), should also be cheap
    to compute -- that is, contain a small number of assigned labels.

    We highly recommend taking a look at the original paper for more details.

    * :attr:`cutoffs` should be an ordered Sequence of integers sorted
      in the increasing order.
      It controls number of clusters and the partitioning of targets into
      clusters. For example setting ``cutoffs = [10, 100, 1000]``
      means that first `10` targets will be assigned
      to the 'head' of the adaptive softmax, targets `11, 12, ..., 100` will be
      assigned to the first cluster, and targets `101, 102, ..., 1000` will be
      assigned to the second cluster, while targets
      `1001, 1002, ..., n_classes - 1` will be assigned
      to the last, third cluster.

    * :attr:`div_value` is used to compute the size of each additional cluster,
      which is given as
      :math:`\left\lfloor\frac{\texttt{in\_features}}{\texttt{div\_value}^{idx}}\right\rfloor`,
      where :math:`idx` is the cluster index (with clusters
      for less frequent words having larger indices,
      and indices starting from :math:`1`).

    * :attr:`head_bias` if set to True, adds a bias term to the 'head' of the
      adaptive softmax. See paper for details. Set to False in the official
      implementation.

    .. warning::
        Labels passed as inputs to this module should be sorted according to
        their frequency. This means that the most frequent label should be
        represented by the index `0`, and the least frequent
        label should be represented by the index `n_classes - 1`.

    .. note::
        This module returns a ``NamedTuple`` with ``output``
        and ``loss`` fields. See further documentation for details.

    .. note::
        To compute log-probabilities for all classes, the ``log_prob``
        method can be used.

    Args:
        in_features (int): Number of features in the input tensor
        n_classes (int): Number of classes in the dataset
        cutoffs (Sequence): Cutoffs used to assign targets to their buckets
        div_value (float, optional): value used as an exponent to compute sizes
            of the clusters. Default: 4.0
        head_bias (bool, optional): If ``True``, adds a bias term to the 'head' of the
            adaptive softmax. Default: ``False``

    Returns:
        ``NamedTuple`` with ``output`` and ``loss`` fields:
            * **output** is a Tensor of size ``N`` containing computed target
              log probabilities for each example
            * **loss** is a Scalar representing the computed negative
              log likelihood loss

    Shape:
        - input: :math:`(N, \texttt{in\_features})`
        - target: :math:`(N)` where each value satisfies :math:`0 <= \texttt{target[i]} <= \texttt{n\_classes}`
        - output1: :math:`(N)`
        - output2: ``Scalar``

    .. _Zipf's law: https://en.wikipedia.org/wiki/Zipf%27s_law
    """
    in_features: int
    n_classes: int
    cutoffs: List[int]
    div_value: float
    head_bias: bool
    head: Linear
    tail: ModuleList
    def __init__(self, in_features: int, n_classes: int, cutoffs: Sequence[int], div_value: float = ..., head_bias: bool = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> _ASMoutput:
        ...
    
    def log_prob(self, input: Tensor) -> Tensor:
        r""" Computes log probabilities for all :math:`\texttt{n\_classes}`

        Args:
            input (Tensor): a minibatch of examples

        Returns:
            log-probabilities of for each class :math:`c`
            in range :math:`0 <= c <= \texttt{n\_classes}`, where :math:`\texttt{n\_classes}` is a
            parameter passed to ``AdaptiveLogSoftmaxWithLoss`` constructor.

        Shape:
            - Input: :math:`(N, \texttt{in\_features})`
            - Output: :math:`(N, \texttt{n\_classes})`

        """
        ...
    
    def predict(self, input: Tensor) -> Tensor:
        r""" This is equivalent to `self.log_pob(input).argmax(dim=1)`,
        but is more efficient in some cases.

        Args:
            input (Tensor): a minibatch of examples

        Returns:
            output (Tensor): a class with the highest probability for each example

        Shape:
            - Input: :math:`(N, \texttt{in\_features})`
            - Output: :math:`(N)`
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Optional, Tuple
from torch import Tensor
from .module import Module

class Threshold(Module):
    r"""Thresholds each element of the input Tensor.

    Threshold is defined as:

    .. math::
        y =
        \begin{cases}
        x, &\text{ if } x > \text{threshold} \\
        \text{value}, &\text{ otherwise }
        \end{cases}

    Args:
        threshold: The value to threshold at
        value: The value to replace with
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Examples::

        >>> m = nn.Threshold(0.1, 20)
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    threshold: float
    value: float
    inplace: bool
    def __init__(self, threshold: float, value: float, inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self):
        ...
    


class ReLU(Module):
    r"""Applies the rectified linear unit function element-wise:

    :math:`\text{ReLU}(x) = (x)^+ = \max(0, x)`

    Args:
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/ReLU.png

    Examples::

        >>> m = nn.ReLU()
        >>> input = torch.randn(2)
        >>> output = m(input)


      An implementation of CReLU - https://arxiv.org/abs/1603.05201

        >>> m = nn.ReLU()
        >>> input = torch.randn(2).unsqueeze(0)
        >>> output = torch.cat((m(input),m(-input)))
    """
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class RReLU(Module):
    r"""Applies the randomized leaky rectified liner unit function, element-wise,
    as described in the paper:

    `Empirical Evaluation of Rectified Activations in Convolutional Network`_.

    The function is defined as:

    .. math::
        \text{RReLU}(x) =
        \begin{cases}
            x & \text{if } x \geq 0 \\
            ax & \text{ otherwise }
        \end{cases}

    where :math:`a` is randomly sampled from uniform distribution
    :math:`\mathcal{U}(\text{lower}, \text{upper})`.

     See: https://arxiv.org/pdf/1505.00853.pdf

    Args:
        lower: lower bound of the uniform distribution. Default: :math:`\frac{1}{8}`
        upper: upper bound of the uniform distribution. Default: :math:`\frac{1}{3}`
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Examples::

        >>> m = nn.RReLU(0.1, 0.3)
        >>> input = torch.randn(2)
        >>> output = m(input)

    .. _`Empirical Evaluation of Rectified Activations in Convolutional Network`:
        https://arxiv.org/abs/1505.00853
    """
    __constants__ = ...
    lower: float
    upper: float
    inplace: bool
    def __init__(self, lower: float = ..., upper: float = ..., inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self):
        ...
    


class Hardtanh(Module):
    r"""Applies the HardTanh function element-wise

    HardTanh is defined as:

    .. math::
        \text{HardTanh}(x) = \begin{cases}
            1 & \text{ if } x > 1 \\
            -1 & \text{ if } x < -1 \\
            x & \text{ otherwise } \\
        \end{cases}

    The range of the linear region :math:`[-1, 1]` can be adjusted using
    :attr:`min_val` and :attr:`max_val`.

    Args:
        min_val: minimum value of the linear region range. Default: -1
        max_val: maximum value of the linear region range. Default: 1
        inplace: can optionally do the operation in-place. Default: ``False``

    Keyword arguments :attr:`min_value` and :attr:`max_value`
    have been deprecated in favor of :attr:`min_val` and :attr:`max_val`.

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Hardtanh.png

    Examples::

        >>> m = nn.Hardtanh(-2, 2)
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    min_val: float
    max_val: float
    inplace: bool
    def __init__(self, min_val: float = ..., max_val: float = ..., inplace: bool = ..., min_value: Optional[float] = ..., max_value: Optional[float] = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class ReLU6(Hardtanh):
    r"""Applies the element-wise function:

    .. math::
        \text{ReLU6}(x) = \min(\max(0,x), 6)

    Args:
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/ReLU6.png

    Examples::

        >>> m = nn.ReLU6()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def __init__(self, inplace: bool = ...) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Sigmoid(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Sigmoid}(x) = \sigma(x) = \frac{1}{1 + \exp(-x)}


    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Sigmoid.png

    Examples::

        >>> m = nn.Sigmoid()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Hardsigmoid(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Hardsigmoid}(x) = \begin{cases}
            0 & \text{if~} x \le -3, \\
            1 & \text{if~} x \ge +3, \\
            x / 6 + 1 / 2 & \text{otherwise}
        \end{cases}


    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Examples::

        >>> m = nn.Hardsigmoid()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Tanh(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Tanh}(x) = \tanh(x) = \frac{\exp(x) - \exp(-x)} {\exp(x) + \exp(-x)}

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Tanh.png

    Examples::

        >>> m = nn.Tanh()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Hardswish(Module):
    r"""Applies the hardswish function, element-wise, as described in the paper:

    `Searching for MobileNetV3`_.

    .. math::
        \text{Hardswish}(x) = \begin{cases}
            0 & \text{if~} x \le -3, \\
            x & \text{if~} x \ge +3, \\
            x \cdot (x + 3) /6 & \text{otherwise}
        \end{cases}

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Examples::

        >>> m = nn.Hardswish()
        >>> input = torch.randn(2)
        >>> output = m(input)

    .. _`Searching for MobileNetV3`:
        https://arxiv.org/abs/1905.02244
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class ELU(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{ELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x) - 1))

    Args:
        alpha: the :math:`\alpha` value for the ELU formulation. Default: 1.0
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/ELU.png

    Examples::

        >>> m = nn.ELU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    alpha: float
    inplace: bool
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class CELU(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{CELU}(x) = \max(0,x) + \min(0, \alpha * (\exp(x/\alpha) - 1))

    More details can be found in the paper `Continuously Differentiable Exponential Linear Units`_ .

    Args:
        alpha: the :math:`\alpha` value for the CELU formulation. Default: 1.0
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/CELU.png

    Examples::

        >>> m = nn.CELU()
        >>> input = torch.randn(2)
        >>> output = m(input)

    .. _`Continuously Differentiable Exponential Linear Units`:
        https://arxiv.org/abs/1704.07483
    """
    __constants__ = ...
    alpha: float
    inplace: bool
    def __init__(self, alpha: float = ..., inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class SELU(Module):
    r"""Applied element-wise, as:

    .. math::
        \text{SELU}(x) = \text{scale} * (\max(0,x) + \min(0, \alpha * (\exp(x) - 1)))

    with :math:`\alpha = 1.6732632423543772848170429916717` and
    :math:`\text{scale} = 1.0507009873554804934193349852946`.

    More details can be found in the paper `Self-Normalizing Neural Networks`_ .

    Args:
        inplace (bool, optional): can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/SELU.png

    Examples::

        >>> m = nn.SELU()
        >>> input = torch.randn(2)
        >>> output = m(input)

    .. _Self-Normalizing Neural Networks: https://arxiv.org/abs/1706.02515
    """
    __constants__ = ...
    inplace: bool
    def __init__(self, inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class GLU(Module):
    r"""Applies the gated linear unit function
    :math:`{GLU}(a, b)= a \otimes \sigma(b)` where :math:`a` is the first half
    of the input matrices and :math:`b` is the second half.

    Args:
        dim (int): the dimension on which to split the input. Default: -1

    Shape:
        - Input: :math:`(\ast_1, N, \ast_2)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(\ast_1, M, \ast_2)` where :math:`M=N/2`

    Examples::

        >>> m = nn.GLU()
        >>> input = torch.randn(4, 2)
        >>> output = m(input)
    """
    __constants__ = ...
    dim: int
    def __init__(self, dim: int = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class GELU(Module):
    r"""Applies the Gaussian Error Linear Units function:

    .. math:: \text{GELU}(x) = x * \Phi(x)

    where :math:`\Phi(x)` is the Cumulative Distribution Function for Gaussian Distribution.

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/GELU.png

    Examples::

        >>> m = nn.GELU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Hardshrink(Module):
    r"""Applies the hard shrinkage function element-wise:

    .. math::
        \text{HardShrink}(x) =
        \begin{cases}
        x, & \text{ if } x > \lambda \\
        x, & \text{ if } x < -\lambda \\
        0, & \text{ otherwise }
        \end{cases}

    Args:
        lambd: the :math:`\lambda` value for the Hardshrink formulation. Default: 0.5

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Hardshrink.png

    Examples::

        >>> m = nn.Hardshrink()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    lambd: float
    def __init__(self, lambd: float = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class LeakyReLU(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{LeakyReLU}(x) = \max(0, x) + \text{negative\_slope} * \min(0, x)


    or

    .. math::
        \text{LeakyRELU}(x) =
        \begin{cases}
        x, & \text{ if } x \geq 0 \\
        \text{negative\_slope} \times x, & \text{ otherwise }
        \end{cases}

    Args:
        negative_slope: Controls the angle of the negative slope. Default: 1e-2
        inplace: can optionally do the operation in-place. Default: ``False``

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/LeakyReLU.png

    Examples::

        >>> m = nn.LeakyReLU(0.1)
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    inplace: bool
    negative_slope: float
    def __init__(self, negative_slope: float = ..., inplace: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class LogSigmoid(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{LogSigmoid}(x) = \log\left(\frac{ 1 }{ 1 + \exp(-x)}\right)

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/LogSigmoid.png

    Examples::

        >>> m = nn.LogSigmoid()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Softplus(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Softplus}(x) = \frac{1}{\beta} * \log(1 + \exp(\beta * x))

    SoftPlus is a smooth approximation to the ReLU function and can be used
    to constrain the output of a machine to always be positive.

    For numerical stability the implementation reverts to the linear function
    when :math:`input \times \beta > threshold`.

    Args:
        beta: the :math:`\beta` value for the Softplus formulation. Default: 1
        threshold: values above this revert to a linear function. Default: 20

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Softplus.png

    Examples::

        >>> m = nn.Softplus()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    beta: int
    threshold: int
    def __init__(self, beta: int = ..., threshold: int = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Softshrink(Module):
    r"""Applies the soft shrinkage function elementwise:

    .. math::
        \text{SoftShrinkage}(x) =
        \begin{cases}
        x - \lambda, & \text{ if } x > \lambda \\
        x + \lambda, & \text{ if } x < -\lambda \\
        0, & \text{ otherwise }
        \end{cases}

    Args:
        lambd: the :math:`\lambda` (must be no less than zero) value for the Softshrink formulation. Default: 0.5

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Softshrink.png

    Examples::

        >>> m = nn.Softshrink()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    lambd: float
    def __init__(self, lambd: float = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class MultiheadAttention(Module):
    r"""Allows the model to jointly attend to information
    from different representation subspaces.
    See reference: Attention Is All You Need

    .. math::
        \text{MultiHead}(Q, K, V) = \text{Concat}(head_1,\dots,head_h)W^O
        \text{where} head_i = \text{Attention}(QW_i^Q, KW_i^K, VW_i^V)

    Args:
        embed_dim: total dimension of the model.
        num_heads: parallel attention heads.
        dropout: a Dropout layer on attn_output_weights. Default: 0.0.
        bias: add bias as module parameter. Default: True.
        add_bias_kv: add bias to the key and value sequences at dim=0.
        add_zero_attn: add a new batch of zeros to the key and
                       value sequences at dim=1.
        kdim: total number of features in key. Default: None.
        vdim: total number of features in value. Default: None.

        Note: if kdim and vdim are None, they will be set to embed_dim such that
        query, key, and value have the same number of features.

    Examples::

        >>> multihead_attn = nn.MultiheadAttention(embed_dim, num_heads)
        >>> attn_output, attn_output_weights = multihead_attn(query, key, value)
    """
    __annotations__ = ...
    def __init__(self, embed_dim, num_heads, dropout=..., bias=..., add_bias_kv=..., add_zero_attn=..., kdim=..., vdim=...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, query: Tensor, key: Tensor, value: Tensor, key_padding_mask: Optional[Tensor] = ..., need_weights: bool = ..., attn_mask: Optional[Tensor] = ...) -> Tuple[Tensor, Optional[Tensor]]:
        r"""
    Args:
        query, key, value: map a query and a set of key-value pairs to an output.
            See "Attention Is All You Need" for more details.
        key_padding_mask: if provided, specified padding elements in the key will
            be ignored by the attention. When given a binary mask and a value is True,
            the corresponding value on the attention layer will be ignored. When given
            a byte mask and a value is non-zero, the corresponding value on the attention
            layer will be ignored
        need_weights: output attn_output_weights.
        attn_mask: 2D or 3D mask that prevents attention to certain positions. A 2D mask will be broadcasted for all
            the batches while a 3D mask allows to specify a different mask for the entries of each batch.

    Shape:
        - Inputs:
        - query: :math:`(L, N, E)` where L is the target sequence length, N is the batch size, E is
          the embedding dimension.
        - key: :math:`(S, N, E)`, where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - value: :math:`(S, N, E)` where S is the source sequence length, N is the batch size, E is
          the embedding dimension.
        - key_padding_mask: :math:`(N, S)` where N is the batch size, S is the source sequence length.
          If a ByteTensor is provided, the non-zero positions will be ignored while the position
          with the zero positions will be unchanged. If a BoolTensor is provided, the positions with the
          value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.
        - attn_mask: 2D mask :math:`(L, S)` where L is the target sequence length, S is the source sequence length.
          3D mask :math:`(N*num_heads, L, S)` where N is the batch size, L is the target sequence length,
          S is the source sequence length. attn_mask ensure that position i is allowed to attend the unmasked
          positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
          while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
          is not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
          is provided, it will be added to the attention weight.

        - Outputs:
        - attn_output: :math:`(L, N, E)` where L is the target sequence length, N is the batch size,
          E is the embedding dimension.
        - attn_output_weights: :math:`(N, L, S)` where N is the batch size,
          L is the target sequence length, S is the source sequence length.
        """
        ...
    


class PReLU(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{PReLU}(x) = \max(0,x) + a * \min(0,x)

    or

    .. math::
        \text{PReLU}(x) =
        \begin{cases}
        x, & \text{ if } x \geq 0 \\
        ax, & \text{ otherwise }
        \end{cases}

    Here :math:`a` is a learnable parameter. When called without arguments, `nn.PReLU()` uses a single
    parameter :math:`a` across all input channels. If called with `nn.PReLU(nChannels)`,
    a separate :math:`a` is used for each input channel.


    .. note::
        weight decay should not be used when learning :math:`a` for good performance.

    .. note::
        Channel dim is the 2nd dim of input. When input has dims < 2, then there is
        no channel dim and the number of channels = 1.

    Args:
        num_parameters (int): number of :math:`a` to learn.
            Although it takes an int as input, there is only two values are legitimate:
            1, or the number of channels at input. Default: 1
        init (float): the initial value of :math:`a`. Default: 0.25

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    Attributes:
        weight (Tensor): the learnable weights of shape (:attr:`num_parameters`).

    .. image:: ../scripts/activation_images/PReLU.png

    Examples::

        >>> m = nn.PReLU()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    __constants__ = ...
    num_parameters: int
    def __init__(self, num_parameters: int = ..., init: float = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Softsign(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{SoftSign}(x) = \frac{x}{ 1 + |x|}

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Softsign.png

    Examples::

        >>> m = nn.Softsign()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Tanhshrink(Module):
    r"""Applies the element-wise function:

    .. math::
        \text{Tanhshrink}(x) = x - \tanh(x)

    Shape:
        - Input: :math:`(N, *)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(N, *)`, same shape as the input

    .. image:: ../scripts/activation_images/Tanhshrink.png

    Examples::

        >>> m = nn.Tanhshrink()
        >>> input = torch.randn(2)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Softmin(Module):
    r"""Applies the Softmin function to an n-dimensional input Tensor
    rescaling them so that the elements of the n-dimensional output Tensor
    lie in the range `[0, 1]` and sum to 1.

    Softmin is defined as:

    .. math::
        \text{Softmin}(x_{i}) = \frac{\exp(-x_i)}{\sum_j \exp(-x_j)}

    Shape:
        - Input: :math:`(*)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(*)`, same shape as the input

    Arguments:
        dim (int): A dimension along which Softmin will be computed (so every slice
            along dim will sum to 1).

    Returns:
        a Tensor of the same dimension and shape as the input, with
        values in the range [0, 1]

    Examples::

        >>> m = nn.Softmin()
        >>> input = torch.randn(2, 3)
        >>> output = m(input)
    """
    __constants__ = ...
    dim: Optional[int]
    def __init__(self, dim: Optional[int] = ...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self):
        ...
    


class Softmax(Module):
    r"""Applies the Softmax function to an n-dimensional input Tensor
    rescaling them so that the elements of the n-dimensional output Tensor
    lie in the range [0,1] and sum to 1.

    Softmax is defined as:

    .. math::
        \text{Softmax}(x_{i}) = \frac{\exp(x_i)}{\sum_j \exp(x_j)}

    When the input Tensor is a sparse tensor then the unspecifed
    values are treated as ``-inf``.

    Shape:
        - Input: :math:`(*)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(*)`, same shape as the input

    Returns:
        a Tensor of the same dimension and shape as the input with
        values in the range [0, 1]

    Arguments:
        dim (int): A dimension along which Softmax will be computed (so every slice
            along dim will sum to 1).

    .. note::
        This module doesn't work directly with NLLLoss,
        which expects the Log to be computed between the Softmax and itself.
        Use `LogSoftmax` instead (it's faster and has better numerical properties).

    Examples::

        >>> m = nn.Softmax(dim=1)
        >>> input = torch.randn(2, 3)
        >>> output = m(input)

    """
    __constants__ = ...
    dim: Optional[int]
    def __init__(self, dim: Optional[int] = ...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Softmax2d(Module):
    r"""Applies SoftMax over features to each spatial location.

    When given an image of ``Channels x Height x Width``, it will
    apply `Softmax` to each location :math:`(Channels, h_i, w_j)`

    Shape:
        - Input: :math:`(N, C, H, W)`
        - Output: :math:`(N, C, H, W)` (same shape as input)

    Returns:
        a Tensor of the same dimension and shape as the input with
        values in the range [0, 1]

    Examples::

        >>> m = nn.Softmax2d()
        >>> # you softmax over the 2nd dimension
        >>> input = torch.randn(2, 3, 12, 13)
        >>> output = m(input)
    """
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class LogSoftmax(Module):
    r"""Applies the :math:`\log(\text{Softmax}(x))` function to an n-dimensional
    input Tensor. The LogSoftmax formulation can be simplified as:

    .. math::
        \text{LogSoftmax}(x_{i}) = \log\left(\frac{\exp(x_i) }{ \sum_j \exp(x_j)} \right)

    Shape:
        - Input: :math:`(*)` where `*` means, any number of additional
          dimensions
        - Output: :math:`(*)`, same shape as the input

    Arguments:
        dim (int): A dimension along which LogSoftmax will be computed.

    Returns:
        a Tensor of the same dimension and shape as the input with
        values in the range [-inf, 0)

    Examples::

        >>> m = nn.LogSoftmax()
        >>> input = torch.randn(2, 3)
        >>> output = m(input)
    """
    __constants__ = ...
    dim: Optional[int]
    def __init__(self, dim: Optional[int] = ...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from collections import OrderedDict
from .module import Module
from torch._jit_internal import _copy_to_script_wrapper
from typing import Any, Iterable, Iterator, Mapping, Optional, Tuple, TypeVar, Union, overload

T = TypeVar('T')
class Container(Module):
    def __init__(self, **kwargs: Any) -> None:
        ...
    


class Sequential(Module):
    r"""A sequential container.
    Modules will be added to it in the order they are passed in the constructor.
    Alternatively, an ordered dict of modules can also be passed in.

    To make it easier to understand, here is a small example::

        # Example of using Sequential
        model = nn.Sequential(
                  nn.Conv2d(1,20,5),
                  nn.ReLU(),
                  nn.Conv2d(20,64,5),
                  nn.ReLU()
                )

        # Example of using Sequential with OrderedDict
        model = nn.Sequential(OrderedDict([
                  ('conv1', nn.Conv2d(1,20,5)),
                  ('relu1', nn.ReLU()),
                  ('conv2', nn.Conv2d(20,64,5)),
                  ('relu2', nn.ReLU())
                ]))
    """
    @overload
    def __init__(self, *args: Module) -> None:
        ...
    
    @overload
    def __init__(self, arg: OrderedDict[str, Module]) -> None:
        ...
    
    def __init__(self, *args: Any) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __getitem__(self: T, idx) -> T:
        ...
    
    def __setitem__(self, idx: int, module: Module) -> None:
        ...
    
    def __delitem__(self, idx: Union[slice, int]) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __len__(self) -> int:
        ...
    
    @_copy_to_script_wrapper
    def __dir__(self):
        ...
    
    @_copy_to_script_wrapper
    def __iter__(self) -> Iterator[Module]:
        ...
    
    def forward(self, input):
        ...
    


class ModuleList(Module):
    r"""Holds submodules in a list.

    :class:`~torch.nn.ModuleList` can be indexed like a regular Python list, but
    modules it contains are properly registered, and will be visible by all
    :class:`~torch.nn.Module` methods.

    Arguments:
        modules (iterable, optional): an iterable of modules to add

    Example::

        class MyModule(nn.Module):
            def __init__(self):
                super(MyModule, self).__init__()
                self.linears = nn.ModuleList([nn.Linear(10, 10) for i in range(10)])

            def forward(self, x):
                # ModuleList can act as an iterable, or be indexed using ints
                for i, l in enumerate(self.linears):
                    x = self.linears[i // 2](x) + l(x)
                return x
    """
    def __init__(self, modules: Optional[Iterable[Module]] = ...) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __getitem__(self, idx: int) -> Module:
        ...
    
    def __setitem__(self, idx: int, module: Module) -> None:
        ...
    
    def __delitem__(self, idx: Union[int, slice]) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __len__(self) -> int:
        ...
    
    @_copy_to_script_wrapper
    def __iter__(self) -> Iterator[Module]:
        ...
    
    def __iadd__(self: T, modules: Iterable[Module]) -> T:
        ...
    
    @_copy_to_script_wrapper
    def __dir__(self):
        ...
    
    def insert(self, index: int, module: Module) -> None:
        r"""Insert a given module before a given index in the list.

        Arguments:
            index (int): index to insert.
            module (nn.Module): module to insert
        """
        ...
    
    def append(self: T, module: Module) -> T:
        r"""Appends a given module to the end of the list.

        Arguments:
            module (nn.Module): module to append
        """
        ...
    
    def extend(self: T, modules: Iterable[Module]) -> T:
        r"""Appends modules from a Python iterable to the end of the list.

        Arguments:
            modules (iterable): iterable of modules to append
        """
        ...
    
    def forward(self):
        ...
    


class ModuleDict(Module):
    r"""Holds submodules in a dictionary.

    :class:`~torch.nn.ModuleDict` can be indexed like a regular Python dictionary,
    but modules it contains are properly registered, and will be visible by all
    :class:`~torch.nn.Module` methods.

    :class:`~torch.nn.ModuleDict` is an **ordered** dictionary that respects

    * the order of insertion, and

    * in :meth:`~torch.nn.ModuleDict.update`, the order of the merged ``OrderedDict``
      or another :class:`~torch.nn.ModuleDict` (the argument to :meth:`~torch.nn.ModuleDict.update`).

    Note that :meth:`~torch.nn.ModuleDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict``) does not preserve the order of the
    merged mapping.

    Arguments:
        modules (iterable, optional): a mapping (dictionary) of (string: module)
            or an iterable of key-value pairs of type (string, module)

    Example::

        class MyModule(nn.Module):
            def __init__(self):
                super(MyModule, self).__init__()
                self.choices = nn.ModuleDict({
                        'conv': nn.Conv2d(10, 10, 3),
                        'pool': nn.MaxPool2d(3)
                })
                self.activations = nn.ModuleDict([
                        ['lrelu', nn.LeakyReLU()],
                        ['prelu', nn.PReLU()]
                ])

            def forward(self, x, choice, act):
                x = self.choices[choice](x)
                x = self.activations[act](x)
                return x
    """
    def __init__(self, modules: Optional[Mapping[str, Module]] = ...) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __getitem__(self, key: str) -> Module:
        ...
    
    def __setitem__(self, key: str, module: Module) -> None:
        ...
    
    def __delitem__(self, key: str) -> None:
        ...
    
    @_copy_to_script_wrapper
    def __len__(self) -> int:
        ...
    
    @_copy_to_script_wrapper
    def __iter__(self) -> Iterator[str]:
        ...
    
    @_copy_to_script_wrapper
    def __contains__(self, key: str) -> bool:
        ...
    
    def clear(self) -> None:
        """Remove all items from the ModuleDict.
        """
        ...
    
    def pop(self, key: str) -> Module:
        r"""Remove key from the ModuleDict and return its module.

        Arguments:
            key (string): key to pop from the ModuleDict
        """
        ...
    
    @_copy_to_script_wrapper
    def keys(self) -> Iterable[str]:
        r"""Return an iterable of the ModuleDict keys.
        """
        ...
    
    @_copy_to_script_wrapper
    def items(self) -> Iterable[Tuple[str, Module]]:
        r"""Return an iterable of the ModuleDict key/value pairs.
        """
        ...
    
    @_copy_to_script_wrapper
    def values(self) -> Iterable[Module]:
        r"""Return an iterable of the ModuleDict values.
        """
        ...
    
    def update(self, modules: Mapping[str, Module]) -> None:
        r"""Update the :class:`~torch.nn.ModuleDict` with the key-value pairs from a
        mapping or an iterable, overwriting existing keys.

        .. note::
            If :attr:`modules` is an ``OrderedDict``, a :class:`~torch.nn.ModuleDict`, or
            an iterable of key-value pairs, the order of new elements in it is preserved.

        Arguments:
            modules (iterable): a mapping (dictionary) from string to :class:`~torch.nn.Module`,
                or an iterable of key-value pairs of type (string, :class:`~torch.nn.Module`)
        """
        ...
    
    def forward(self):
        ...
    


class ParameterList(Module):
    r"""Holds parameters in a list.

    :class:`~torch.nn.ParameterList` can be indexed like a regular Python
    list, but parameters it contains are properly registered, and will be
    visible by all :class:`~torch.nn.Module` methods.

    Arguments:
        parameters (iterable, optional): an iterable of :class:`~torch.nn.Parameter` to add

    Example::

        class MyModule(nn.Module):
            def __init__(self):
                super(MyModule, self).__init__()
                self.params = nn.ParameterList([nn.Parameter(torch.randn(10, 10)) for i in range(10)])

            def forward(self, x):
                # ParameterList can act as an iterable, or be indexed using ints
                for i, p in enumerate(self.params):
                    x = self.params[i // 2].mm(x) + p.mm(x)
                return x
    """
    def __init__(self, parameters: Optional[Iterable[Parameter]] = ...) -> None:
        ...
    
    @overload
    def __getitem__(self, idx: int) -> Parameter:
        ...
    
    @overload
    def __getitem__(self: T, idx: slice) -> T:
        ...
    
    def __getitem__(self, idx):
        ...
    
    def __setitem__(self, idx: int, param: Parameter) -> None:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __iter__(self) -> Iterator[Parameter]:
        ...
    
    def __iadd__(self: T, parameters: Iterable[Parameter]) -> T:
        ...
    
    def __dir__(self):
        ...
    
    def append(self: T, parameter: Parameter) -> T:
        """Appends a given parameter at the end of the list.

        Arguments:
            parameter (nn.Parameter): parameter to append
        """
        ...
    
    def extend(self: T, parameters: Iterable[Parameter]) -> T:
        """Appends parameters from a Python iterable to the end of the list.

        Arguments:
            parameters (iterable): iterable of parameters to append
        """
        ...
    
    def extra_repr(self) -> str:
        ...
    
    def __call__(self, input):
        ...
    


class ParameterDict(Module):
    r"""Holds parameters in a dictionary.

    ParameterDict can be indexed like a regular Python dictionary, but parameters it
    contains are properly registered, and will be visible by all Module methods.

    :class:`~torch.nn.ParameterDict` is an **ordered** dictionary that respects

    * the order of insertion, and

    * in :meth:`~torch.nn.ParameterDict.update`, the order of the merged ``OrderedDict``
      or another :class:`~torch.nn.ParameterDict` (the argument to
      :meth:`~torch.nn.ParameterDict.update`).

    Note that :meth:`~torch.nn.ParameterDict.update` with other unordered mapping
    types (e.g., Python's plain ``dict``) does not preserve the order of the
    merged mapping.

    Arguments:
        parameters (iterable, optional): a mapping (dictionary) of
            (string : :class:`~torch.nn.Parameter`) or an iterable of key-value pairs
            of type (string, :class:`~torch.nn.Parameter`)

    Example::

        class MyModule(nn.Module):
            def __init__(self):
                super(MyModule, self).__init__()
                self.params = nn.ParameterDict({
                        'left': nn.Parameter(torch.randn(5, 10)),
                        'right': nn.Parameter(torch.randn(5, 10))
                })

            def forward(self, x, choice):
                x = self.params[choice].mm(x)
                return x
    """
    def __init__(self, parameters: Optional[Mapping[str, Parameter]] = ...) -> None:
        ...
    
    def __getitem__(self, key: str) -> Parameter:
        ...
    
    def __setitem__(self, key: str, parameter: Parameter) -> None:
        ...
    
    def __delitem__(self, key: str) -> None:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __iter__(self) -> Iterator[str]:
        ...
    
    def __contains__(self, key: str) -> bool:
        ...
    
    def clear(self) -> None:
        """Remove all items from the ParameterDict.
        """
        ...
    
    def pop(self, key: str) -> Parameter:
        r"""Remove key from the ParameterDict and return its parameter.

        Arguments:
            key (string): key to pop from the ParameterDict
        """
        ...
    
    def keys(self) -> Iterable[str]:
        r"""Return an iterable of the ParameterDict keys.
        """
        ...
    
    def items(self) -> Iterable[Tuple[str, Parameter]]:
        r"""Return an iterable of the ParameterDict key/value pairs.
        """
        ...
    
    def values(self) -> Iterable[Parameter]:
        r"""Return an iterable of the ParameterDict values.
        """
        ...
    
    def update(self, parameters: Mapping[str, Parameter]) -> None:
        r"""Update the :class:`~torch.nn.ParameterDict` with the key-value pairs from a
        mapping or an iterable, overwriting existing keys.

        .. note::
            If :attr:`parameters` is an ``OrderedDict``, a :class:`~torch.nn.ParameterDict`, or
            an iterable of key-value pairs, the order of new elements in it is preserved.

        Arguments:
            parameters (iterable): a mapping (dictionary) from string to
                :class:`~torch.nn.Parameter`, or an iterable of
                key-value pairs of type (string, :class:`~torch.nn.Parameter`)
        """
        ...
    
    def extra_repr(self) -> str:
        ...
    
    def __call__(self, input):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor
from typing import Optional

class _Loss(Module):
    reduction: str
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    


class _WeightedLoss(_Loss):
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    


class L1Loss(_Loss):
    r"""Creates a criterion that measures the mean absolute error (MAE) between each element in
    the input :math:`x` and target :math:`y`.

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left| x_n - y_n \right|,

    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then:

    .. math::
        \ell(x, y) =
        \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total
    of :math:`n` elements each.

    The sum operation still operates over all the elements, and divides by :math:`n`.

    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then
          :math:`(N, *)`, same shape as the input

    Examples::

        >>> loss = nn.L1Loss()
        >>> input = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.randn(3, 5)
        >>> output = loss(input, target)
        >>> output.backward()
    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class NLLLoss(_WeightedLoss):
    r"""The negative log likelihood loss. It is useful to train a classification
    problem with `C` classes.

    If provided, the optional argument :attr:`weight` should be a 1D Tensor assigning
    weight to each of the classes. This is particularly useful when you have an
    unbalanced training set.

    The `input` given through a forward call is expected to contain
    log-probabilities of each class. `input` has to be a Tensor of size either
    :math:`(minibatch, C)` or :math:`(minibatch, C, d_1, d_2, ..., d_K)`
    with :math:`K \geq 1` for the `K`-dimensional case (described later).

    Obtaining log-probabilities in a neural network is easily achieved by
    adding a  `LogSoftmax`  layer in the last layer of your network.
    You may use `CrossEntropyLoss` instead, if you prefer not to add an extra
    layer.

    The `target` that this loss expects should be a class index in the range :math:`[0, C-1]`
    where `C = number of classes`; if `ignore_index` is specified, this loss also accepts
    this class index (this index may not necessarily be in the class range).

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_{y_n} x_{n,y_n}, \quad
        w_{c} = \text{weight}[c] \cdot \mathbb{1}\{c \not= \text{ignore\_index}\},

    where :math:`x` is the input, :math:`y` is the target, :math:`w` is the weight, and
    :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then

    .. math::
        \ell(x, y) = \begin{cases}
            \sum_{n=1}^N \frac{1}{\sum_{n=1}^N w_{y_n}} l_n, &
            \text{if reduction} = \text{'mean';}\\
            \sum_{n=1}^N l_n,  &
            \text{if reduction} = \text{'sum'.}
        \end{cases}

    Can also be used for higher dimension inputs, such as 2D images, by providing
    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,
    where :math:`K` is the number of dimensions, and a target of appropriate shape
    (see below). In the case of images, it computes NLL loss per-pixel.

    Args:
        weight (Tensor, optional): a manual rescaling weight given to each
            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
            treated as if having all ones.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        ignore_index (int, optional): Specifies a target value that is ignored
            and does not contribute to the input gradient. When
            :attr:`size_average` is ``True``, the loss is averaged over
            non-ignored targets.
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, C)` where `C = number of classes`, or
          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
          in the case of `K`-dimensional loss.
        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
          K-dimensional loss.
        - Output: scalar.
          If :attr:`reduction` is ``'none'``, then the same size as the target: :math:`(N)`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
          of K-dimensional loss.

    Examples::

        >>> m = nn.LogSoftmax(dim=1)
        >>> loss = nn.NLLLoss()
        >>> # input is of size N x C = 3 x 5
        >>> input = torch.randn(3, 5, requires_grad=True)
        >>> # each element in target has to have 0 <= value < C
        >>> target = torch.tensor([1, 0, 4])
        >>> output = loss(m(input), target)
        >>> output.backward()
        >>>
        >>>
        >>> # 2D loss example (used, for example, with image inputs)
        >>> N, C = 5, 4
        >>> loss = nn.NLLLoss()
        >>> # input is of size N x C x height x width
        >>> data = torch.randn(N, 16, 10, 10)
        >>> conv = nn.Conv2d(16, C, (3, 3))
        >>> m = nn.LogSoftmax(dim=1)
        >>> # each element in target has to have 0 <= value < C
        >>> target = torch.empty(N, 8, 8, dtype=torch.long).random_(0, C)
        >>> output = loss(m(conv(data)), target)
        >>> output.backward()
    """
    __constants__ = ...
    ignore_index: int
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., ignore_index: int = ..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class NLLLoss2d(NLLLoss):
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., ignore_index: int = ..., reduce=..., reduction: str = ...) -> None:
        ...
    


class PoissonNLLLoss(_Loss):
    r"""Negative log likelihood loss with Poisson distribution of target.

    The loss can be described as:

    .. math::
        \text{target} \sim \mathrm{Poisson}(\text{input})

        \text{loss}(\text{input}, \text{target}) = \text{input} - \text{target} * \log(\text{input})
                                    + \log(\text{target!})

    The last term can be omitted or approximated with Stirling formula. The
    approximation is used for target values more than 1. For targets less or
    equal to 1 zeros are added to the loss.

    Args:
        log_input (bool, optional): if ``True`` the loss is computed as
            :math:`\exp(\text{input}) - \text{target}*\text{input}`, if ``False`` the loss is
            :math:`\text{input} - \text{target}*\log(\text{input}+\text{eps})`.
        full (bool, optional): whether to compute full loss, i. e. to add the
            Stirling approximation term

            .. math::
                \text{target}*\log(\text{target}) - \text{target} + 0.5 * \log(2\pi\text{target}).
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        eps (float, optional): Small value to avoid evaluation of :math:`\log(0)` when
            :attr:`log_input = False`. Default: 1e-8
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Examples::

        >>> loss = nn.PoissonNLLLoss()
        >>> log_input = torch.randn(5, 2, requires_grad=True)
        >>> target = torch.randn(5, 2)
        >>> output = loss(log_input, target)
        >>> output.backward()

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar by default. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`,
          the same shape as the input
    """
    __constants__ = ...
    log_input: bool
    full: bool
    eps: float
    def __init__(self, log_input: bool = ..., full: bool = ..., size_average=..., eps: float = ..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, log_input: Tensor, target: Tensor) -> Tensor:
        ...
    


class KLDivLoss(_Loss):
    r"""The `Kullback-Leibler divergence`_ Loss

    KL divergence is a useful distance measure for continuous distributions
    and is often useful when performing direct regression over the space of
    (discretely sampled) continuous output distributions.

    As with :class:`~torch.nn.NLLLoss`, the `input` given is expected to contain
    *log-probabilities* and is not restricted to a 2D Tensor.
    The targets are interpreted as *probabilities* by default, but could be considered
    as *log-probabilities* with :attr:`log_target` set to ``True``.

    This criterion expects a `target` `Tensor` of the same size as the
    `input` `Tensor`.

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        l(x,y) = L = \{ l_1,\dots,l_N \}, \quad
        l_n = y_n \cdot \left( \log y_n - x_n \right)

    where the index :math:`N` spans all dimensions of ``input`` and :math:`L` has the same
    shape as ``input``. If :attr:`reduction` is not ``'none'`` (default ``'mean'``), then:

    .. math::
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';} \\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    In default :attr:`reduction` mode ``'mean'``, the losses are averaged for each minibatch over observations
    **as well as** over dimensions. ``'batchmean'`` mode gives the correct KL divergence where losses
    are averaged over batch dimension only. ``'mean'`` mode's behavior will be changed to the same as
    ``'batchmean'`` in the next major release.

    .. _Kullback-Leibler divergence:
        https://en.wikipedia.org/wiki/Kullback-Leibler_divergence

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'batchmean'`` | ``'sum'`` | ``'mean'``.
            ``'none'``: no reduction will be applied.
            ``'batchmean'``: the sum of the output will be divided by batchsize.
            ``'sum'``: the output will be summed.
            ``'mean'``: the output will be divided by the number of elements in the output.
            Default: ``'mean'``
        log_target (bool, optional): Specifies whether `target` is passed in the log space.
            Default: ``False``

    .. note::
        :attr:`size_average` and :attr:`reduce` are in the process of being deprecated,
        and in the meantime, specifying either of those two args will override :attr:`reduction`.

    .. note::
        :attr:`reduction` = ``'mean'`` doesn't return the true kl divergence value, please use
        :attr:`reduction` = ``'batchmean'`` which aligns with KL math definition.
        In the next major release, ``'mean'`` will be changed to be the same as ``'batchmean'``.

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar by default. If :attr:``reduction`` is ``'none'``, then :math:`(N, *)`,
          the same shape as the input

    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ..., log_target: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class MSELoss(_Loss):
    r"""Creates a criterion that measures the mean squared error (squared L2 norm) between
    each element in the input :math:`x` and target :math:`y`.

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = \left( x_n - y_n \right)^2,

    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then:

    .. math::
        \ell(x, y) =
        \begin{cases}
            \operatorname{mean}(L), &  \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  &  \text{if reduction} = \text{'sum'.}
        \end{cases}

    :math:`x` and :math:`y` are tensors of arbitrary shapes with a total
    of :math:`n` elements each.

    The mean operation still operates over all the elements, and divides by :math:`n`.

    The division by :math:`n` can be avoided if one sets ``reduction = 'sum'``.

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input

    Examples::

        >>> loss = nn.MSELoss()
        >>> input = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.randn(3, 5)
        >>> output = loss(input, target)
        >>> output.backward()
    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class BCELoss(_WeightedLoss):
    r"""Creates a criterion that measures the Binary Cross Entropy
    between the target and the output:

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log x_n + (1 - y_n) \cdot \log (1 - x_n) \right],

    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then

    .. math::
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    This is used for measuring the error of a reconstruction in for example
    an auto-encoder. Note that the targets :math:`y` should be numbers
    between 0 and 1.

    Notice that if :math:`x_n` is either 0 or 1, one of the log terms would be
    mathematically undefined in the above loss equation. PyTorch chooses to set
    :math:`\log (0) = -\infty`, since :math:`\lim_{x\to 0} \log (x) = -\infty`.
    However, an infinite term in the loss equation is not desirable for several reasons.

    For one, if either :math:`y_n = 0` or :math:`(1 - y_n) = 0`, then we would be
    multipying 0 with infinity. Secondly, if we have an infinite loss value, then
    we would also have an infinite term in our gradient, since
    :math:`\lim_{x\to 0} \frac{d}{dx} \log (x) = \infty`.
    This would make BCELoss's backward method nonlinear with respect to :math:`x_n`,
    and using it for things like linear regression would not be straight-forward.

    Our solution is that BCELoss clamps its log function outputs to be greater than
    or equal to -100. This way, we can always have a finite loss value and a linear
    backward method.


    Args:
        weight (Tensor, optional): a manual rescaling weight given to the loss
            of each batch element. If given, has to be a Tensor of size `nbatch`.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
          shape as input.

    Examples::

        >>> m = nn.Sigmoid()
        >>> loss = nn.BCELoss()
        >>> input = torch.randn(3, requires_grad=True)
        >>> target = torch.empty(3).random_(2)
        >>> output = loss(m(input), target)
        >>> output.backward()
    """
    __constants__ = ...
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class BCEWithLogitsLoss(_Loss):
    r"""This loss combines a `Sigmoid` layer and the `BCELoss` in one single
    class. This version is more numerically stable than using a plain `Sigmoid`
    followed by a `BCELoss` as, by combining the operations into one layer,
    we take advantage of the log-sum-exp trick for numerical stability.

    The unreduced (i.e. with :attr:`reduction` set to ``'none'``) loss can be described as:

    .. math::
        \ell(x, y) = L = \{l_1,\dots,l_N\}^\top, \quad
        l_n = - w_n \left[ y_n \cdot \log \sigma(x_n)
        + (1 - y_n) \cdot \log (1 - \sigma(x_n)) \right],

    where :math:`N` is the batch size. If :attr:`reduction` is not ``'none'``
    (default ``'mean'``), then

    .. math::
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    This is used for measuring the error of a reconstruction in for example
    an auto-encoder. Note that the targets `t[i]` should be numbers
    between 0 and 1.

    It's possible to trade off recall and precision by adding weights to positive examples.
    In the case of multi-label classification the loss can be described as:

    .. math::
        \ell_c(x, y) = L_c = \{l_{1,c},\dots,l_{N,c}\}^\top, \quad
        l_{n,c} = - w_{n,c} \left[ p_c y_{n,c} \cdot \log \sigma(x_{n,c})
        + (1 - y_{n,c}) \cdot \log (1 - \sigma(x_{n,c})) \right],

    where :math:`c` is the class number (:math:`c > 1` for multi-label binary classification,
    :math:`c = 1` for single-label binary classification),
    :math:`n` is the number of the sample in the batch and
    :math:`p_c` is the weight of the positive answer for the class :math:`c`.

    :math:`p_c > 1` increases the recall, :math:`p_c < 1` increases the precision.

    For example, if a dataset contains 100 positive and 300 negative examples of a single class,
    then `pos_weight` for the class should be equal to :math:`\frac{300}{100}=3`.
    The loss would act as if the dataset contains :math:`3\times 100=300` positive examples.

    Examples::

        >>> target = torch.ones([10, 64], dtype=torch.float32)  # 64 classes, batch size = 10
        >>> output = torch.full([10, 64], 1.5)  # A prediction (logit)
        >>> pos_weight = torch.ones([64])  # All weights are equal to 1
        >>> criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight)
        >>> criterion(output, target)  # -log(sigmoid(1.5))
        tensor(0.2014)

    Args:
        weight (Tensor, optional): a manual rescaling weight given to the loss
            of each batch element. If given, has to be a Tensor of size `nbatch`.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
        pos_weight (Tensor, optional): a weight of positive examples.
                Must be a vector with length equal to the number of classes.

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N, *)`, same
          shape as input.

     Examples::

        >>> loss = nn.BCEWithLogitsLoss()
        >>> input = torch.randn(3, requires_grad=True)
        >>> target = torch.empty(3).random_(2)
        >>> output = loss(input, target)
        >>> output.backward()
    """
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., reduce=..., reduction: str = ..., pos_weight: Optional[Tensor] = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class HingeEmbeddingLoss(_Loss):
    r"""Measures the loss given an input tensor :math:`x` and a labels tensor :math:`y`
    (containing 1 or -1).
    This is usually used for measuring whether two inputs are similar or
    dissimilar, e.g. using the L1 pairwise distance as :math:`x`, and is typically
    used for learning nonlinear embeddings or semi-supervised learning.

    The loss function for :math:`n`-th sample in the mini-batch is

    .. math::
        l_n = \begin{cases}
            x_n, & \text{if}\; y_n = 1,\\
            \max \{0, \Delta - x_n\}, & \text{if}\; y_n = -1,
        \end{cases}

    and the total loss functions is

    .. math::
        \ell(x, y) = \begin{cases}
            \operatorname{mean}(L), & \text{if reduction} = \text{'mean';}\\
            \operatorname{sum}(L),  & \text{if reduction} = \text{'sum'.}
        \end{cases}

    where :math:`L = \{l_1,\dots,l_N\}^\top`.

    Args:
        margin (float, optional): Has a default value of `1`.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(*)` where :math:`*` means, any number of dimensions. The sum operation
          operates over all the elements.
        - Target: :math:`(*)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input
    """
    __constants__ = ...
    margin: float
    def __init__(self, margin: float = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class MultiLabelMarginLoss(_Loss):
    r"""Creates a criterion that optimizes a multi-class multi-classification
    hinge loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`)
    and output :math:`y` (which is a 2D `Tensor` of target class indices).
    For each sample in the mini-batch:

    .. math::
        \text{loss}(x, y) = \sum_{ij}\frac{\max(0, 1 - (x[y[j]] - x[i]))}{\text{x.size}(0)}

    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`, \
    :math:`y \in \left\{0, \; \cdots , \; \text{y.size}(0) - 1\right\}`, \
    :math:`0 \leq y[j] \leq \text{x.size}(0)-1`, \
    and :math:`i \neq y[j]` for all :math:`i` and :math:`j`.

    :math:`y` and :math:`x` must have the same size.

    The criterion only considers a contiguous block of non-negative targets that
    starts at the front.

    This allows for different samples to have variable amounts of target classes.

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(C)` or :math:`(N, C)` where `N` is the batch size and `C`
          is the number of classes.
        - Target: :math:`(C)` or :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.

    Examples::

        >>> loss = nn.MultiLabelMarginLoss()
        >>> x = torch.FloatTensor([[0.1, 0.2, 0.4, 0.8]])
        >>> # for target y, only consider labels 3 and 0, not after label -1
        >>> y = torch.LongTensor([[3, 0, -1, 1]])
        >>> loss(x, y)
        >>> # 0.25 * ((1-(0.1-0.2)) + (1-(0.1-0.4)) + (1-(0.8-0.2)) + (1-(0.8-0.4)))
        tensor(0.8500)

    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class SmoothL1Loss(_Loss):
    r"""Creates a criterion that uses a squared term if the absolute
    element-wise error falls below 1 and an L1 term otherwise.
    It is less sensitive to outliers than the `MSELoss` and in some cases
    prevents exploding gradients (e.g. see `Fast R-CNN` paper by Ross Girshick).
    Also known as the Huber loss:

    .. math::
        \text{loss}(x, y) = \frac{1}{n} \sum_{i} z_{i}

    where :math:`z_{i}` is given by:

    .. math::
        z_{i} =
        \begin{cases}
        0.5 (x_i - y_i)^2, & \text{if } |x_i - y_i| < 1 \\
        |x_i - y_i| - 0.5, & \text{otherwise }
        \end{cases}

    :math:`x` and :math:`y` arbitrary shapes with a total of :math:`n` elements each
    the sum operation still operates over all the elements, and divides by :math:`n`.

    The division by :math:`n` can be avoided if sets ``reduction = 'sum'``.

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, *)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(N, *)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then
          :math:`(N, *)`, same shape as the input

    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class SoftMarginLoss(_Loss):
    r"""Creates a criterion that optimizes a two-class classification
    logistic loss between input tensor :math:`x` and target tensor :math:`y`
    (containing 1 or -1).

    .. math::
        \text{loss}(x, y) = \sum_i \frac{\log(1 + \exp(-y[i]*x[i]))}{\text{x.nelement}()}

    Args:
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(*)` where :math:`*` means, any number of additional
          dimensions
        - Target: :math:`(*)`, same shape as the input
        - Output: scalar. If :attr:`reduction` is ``'none'``, then same shape as the input

    """
    __constants__ = ...
    def __init__(self, size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class CrossEntropyLoss(_WeightedLoss):
    r"""This criterion combines :func:`nn.LogSoftmax` and :func:`nn.NLLLoss` in one single class.

    It is useful when training a classification problem with `C` classes.
    If provided, the optional argument :attr:`weight` should be a 1D `Tensor`
    assigning weight to each of the classes.
    This is particularly useful when you have an unbalanced training set.

    The `input` is expected to contain raw, unnormalized scores for each class.

    `input` has to be a Tensor of size either :math:`(minibatch, C)` or
    :math:`(minibatch, C, d_1, d_2, ..., d_K)`
    with :math:`K \geq 1` for the `K`-dimensional case (described later).

    This criterion expects a class index in the range :math:`[0, C-1]` as the
    `target` for each value of a 1D tensor of size `minibatch`; if `ignore_index`
    is specified, this criterion also accepts this class index (this index may not
    necessarily be in the class range).

    The loss can be described as:

    .. math::
        \text{loss}(x, class) = -\log\left(\frac{\exp(x[class])}{\sum_j \exp(x[j])}\right)
                       = -x[class] + \log\left(\sum_j \exp(x[j])\right)

    or in the case of the :attr:`weight` argument being specified:

    .. math::
        \text{loss}(x, class) = weight[class] \left(-x[class] + \log\left(\sum_j \exp(x[j])\right)\right)

    The losses are averaged across observations for each minibatch.

    Can also be used for higher dimension inputs, such as 2D images, by providing
    an input of size :math:`(minibatch, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`,
    where :math:`K` is the number of dimensions, and a target of appropriate shape
    (see below).


    Args:
        weight (Tensor, optional): a manual rescaling weight given to each class.
            If given, has to be a Tensor of size `C`
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        ignore_index (int, optional): Specifies a target value that is ignored
            and does not contribute to the input gradient. When :attr:`size_average` is
            ``True``, the loss is averaged over non-ignored targets.
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, C)` where `C = number of classes`, or
          :math:`(N, C, d_1, d_2, ..., d_K)` with :math:`K \geq 1`
          in the case of `K`-dimensional loss.
        - Target: :math:`(N)` where each value is :math:`0 \leq \text{targets}[i] \leq C-1`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case of
          K-dimensional loss.
        - Output: scalar.
          If :attr:`reduction` is ``'none'``, then the same size as the target:
          :math:`(N)`, or
          :math:`(N, d_1, d_2, ..., d_K)` with :math:`K \geq 1` in the case
          of K-dimensional loss.

    Examples::

        >>> loss = nn.CrossEntropyLoss()
        >>> input = torch.randn(3, 5, requires_grad=True)
        >>> target = torch.empty(3, dtype=torch.long).random_(5)
        >>> output = loss(input, target)
        >>> output.backward()
    """
    __constants__ = ...
    ignore_index: int
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., ignore_index: int = ..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class MultiLabelSoftMarginLoss(_WeightedLoss):
    r"""Creates a criterion that optimizes a multi-label one-versus-all
    loss based on max-entropy, between input :math:`x` and target :math:`y` of size
    :math:`(N, C)`.
    For each sample in the minibatch:

    .. math::
        loss(x, y) = - \frac{1}{C} * \sum_i y[i] * \log((1 + \exp(-x[i]))^{-1})
                         + (1-y[i]) * \log\left(\frac{\exp(-x[i])}{(1 + \exp(-x[i]))}\right)

    where :math:`i \in \left\{0, \; \cdots , \; \text{x.nElement}() - 1\right\}`,
    :math:`y[i] \in \left\{0, \; 1\right\}`.

    Args:
        weight (Tensor, optional): a manual rescaling weight given to each
            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
            treated as if having all ones.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, C)` where `N` is the batch size and `C` is the number of classes.
        - Target: :math:`(N, C)`, label targets padded by -1 ensuring same shape as the input.
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.
    """
    __constants__ = ...
    def __init__(self, weight: Optional[Tensor] = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class CosineEmbeddingLoss(_Loss):
    r"""Creates a criterion that measures the loss given input tensors
    :math:`x_1`, :math:`x_2` and a `Tensor` label :math:`y` with values 1 or -1.
    This is used for measuring whether two inputs are similar or dissimilar,
    using the cosine distance, and is typically used for learning nonlinear
    embeddings or semi-supervised learning.

    The loss function for each sample is:

    .. math::
        \text{loss}(x, y) =
        \begin{cases}
        1 - \cos(x_1, x_2), & \text{if } y = 1 \\
        \max(0, \cos(x_1, x_2) - \text{margin}), & \text{if } y = -1
        \end{cases}

    Args:
        margin (float, optional): Should be a number from :math:`-1` to :math:`1`,
            :math:`0` to :math:`0.5` is suggested. If :attr:`margin` is missing, the
            default value is :math:`0`.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
    """
    __constants__ = ...
    margin: float
    def __init__(self, margin: float = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input1: Tensor, input2: Tensor, target: Tensor) -> Tensor:
        ...
    


class MarginRankingLoss(_Loss):
    r"""Creates a criterion that measures the loss given
    inputs :math:`x1`, :math:`x2`, two 1D mini-batch `Tensors`,
    and a label 1D mini-batch tensor :math:`y` (containing 1 or -1).

    If :math:`y = 1` then it assumed the first input should be ranked higher
    (have a larger value) than the second input, and vice-versa for :math:`y = -1`.

    The loss function for each sample in the mini-batch is:

    .. math::
        \text{loss}(x, y) = \max(0, -y * (x1 - x2) + \text{margin})

    Args:
        margin (float, optional): Has a default value of :math:`0`.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, D)` where `N` is the batch size and `D` is the size of a sample.
        - Target: :math:`(N)`
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.
    """
    __constants__ = ...
    margin: float
    def __init__(self, margin: float = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input1: Tensor, input2: Tensor, target: Tensor) -> Tensor:
        ...
    


class MultiMarginLoss(_WeightedLoss):
    r"""Creates a criterion that optimizes a multi-class classification hinge
    loss (margin-based loss) between input :math:`x` (a 2D mini-batch `Tensor`) and
    output :math:`y` (which is a 1D tensor of target class indices,
    :math:`0 \leq y \leq \text{x.size}(1)-1`):

    For each mini-batch sample, the loss in terms of the 1D input :math:`x` and scalar
    output :math:`y` is:

    .. math::
        \text{loss}(x, y) = \frac{\sum_i \max(0, \text{margin} - x[y] + x[i]))^p}{\text{x.size}(0)}

    where :math:`x \in \left\{0, \; \cdots , \; \text{x.size}(0) - 1\right\}`
    and :math:`i \neq y`.

    Optionally, you can give non-equal weighting on the classes by passing
    a 1D :attr:`weight` tensor into the constructor.

    The loss function then becomes:

    .. math::
        \text{loss}(x, y) = \frac{\sum_i \max(0, w[y] * (\text{margin} - x[y] + x[i]))^p)}{\text{x.size}(0)}

    Args:
        p (int, optional): Has a default value of :math:`1`. :math:`1` and :math:`2`
            are the only supported values.
        margin (float, optional): Has a default value of :math:`1`.
        weight (Tensor, optional): a manual rescaling weight given to each
            class. If given, it has to be a Tensor of size `C`. Otherwise, it is
            treated as if having all ones.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``
    """
    __constants__ = ...
    margin: float
    p: int
    def __init__(self, p: int = ..., margin: float = ..., weight: Optional[Tensor] = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, target: Tensor) -> Tensor:
        ...
    


class TripletMarginLoss(_Loss):
    r"""Creates a criterion that measures the triplet loss given an input
    tensors :math:`x1`, :math:`x2`, :math:`x3` and a margin with a value greater than :math:`0`.
    This is used for measuring a relative similarity between samples. A triplet
    is composed by `a`, `p` and `n` (i.e., `anchor`, `positive examples` and `negative
    examples` respectively). The shapes of all input tensors should be
    :math:`(N, D)`.

    The distance swap is described in detail in the paper `Learning shallow
    convolutional feature descriptors with triplet losses`_ by
    V. Balntas, E. Riba et al.

    The loss function for each sample in the mini-batch is:

    .. math::
        L(a, p, n) = \max \{d(a_i, p_i) - d(a_i, n_i) + {\rm margin}, 0\}


    where

    .. math::
        d(x_i, y_i) = \left\lVert {\bf x}_i - {\bf y}_i \right\rVert_p

    Args:
        margin (float, optional): Default: :math:`1`.
        p (int, optional): The norm degree for pairwise distance. Default: :math:`2`.
        swap (bool, optional): The distance swap is described in detail in the paper
            `Learning shallow convolutional feature descriptors with triplet losses` by
            V. Balntas, E. Riba et al. Default: ``False``.
        size_average (bool, optional): Deprecated (see :attr:`reduction`). By default,
            the losses are averaged over each loss element in the batch. Note that for
            some losses, there are multiple elements per sample. If the field :attr:`size_average`
            is set to ``False``, the losses are instead summed for each minibatch. Ignored
            when reduce is ``False``. Default: ``True``
        reduce (bool, optional): Deprecated (see :attr:`reduction`). By default, the
            losses are averaged or summed over observations for each minibatch depending
            on :attr:`size_average`. When :attr:`reduce` is ``False``, returns a loss per
            batch element instead and ignores :attr:`size_average`. Default: ``True``
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the sum of the output will be divided by the number of
            elements in the output, ``'sum'``: the output will be summed. Note: :attr:`size_average`
            and :attr:`reduce` are in the process of being deprecated, and in the meantime,
            specifying either of those two args will override :attr:`reduction`. Default: ``'mean'``

    Shape:
        - Input: :math:`(N, D)` where :math:`D` is the vector dimension.
        - Output: scalar. If :attr:`reduction` is ``'none'``, then :math:`(N)`.

    >>> triplet_loss = nn.TripletMarginLoss(margin=1.0, p=2)
    >>> anchor = torch.randn(100, 128, requires_grad=True)
    >>> positive = torch.randn(100, 128, requires_grad=True)
    >>> negative = torch.randn(100, 128, requires_grad=True)
    >>> output = triplet_loss(anchor, positive, negative)
    >>> output.backward()

    .. _Learning shallow convolutional feature descriptors with triplet losses:
        http://www.bmva.org/bmvc/2016/papers/paper119/index.html
    """
    __constants__ = ...
    margin: float
    p: float
    eps: float
    swap: bool
    def __init__(self, margin: float = ..., p: float = ..., eps: float = ..., swap: bool = ..., size_average=..., reduce=..., reduction: str = ...) -> None:
        ...
    
    def forward(self, anchor: Tensor, positive: Tensor, negative: Tensor) -> Tensor:
        ...
    


class CTCLoss(_Loss):
    r"""The Connectionist Temporal Classification loss.

    Calculates loss between a continuous (unsegmented) time series and a target sequence. CTCLoss sums over the
    probability of possible alignments of input to target, producing a loss value which is differentiable
    with respect to each input node. The alignment of input to target is assumed to be "many-to-one", which
    limits the length of the target sequence such that it must be :math:`\leq` the input length.

    Args:
        blank (int, optional): blank label. Default :math:`0`.
        reduction (string, optional): Specifies the reduction to apply to the output:
            ``'none'`` | ``'mean'`` | ``'sum'``. ``'none'``: no reduction will be applied,
            ``'mean'``: the output losses will be divided by the target lengths and
            then the mean over the batch is taken. Default: ``'mean'``
        zero_infinity (bool, optional):
            Whether to zero infinite losses and the associated gradients.
            Default: ``False``
            Infinite losses mainly occur when the inputs are too short
            to be aligned to the targets.

    Shape:
        - Log_probs: Tensor of size :math:`(T, N, C)`,
          where :math:`T = \text{input length}`,
          :math:`N = \text{batch size}`, and
          :math:`C = \text{number of classes (including blank)}`.
          The logarithmized probabilities of the outputs (e.g. obtained with
          :func:`torch.nn.functional.log_softmax`).
        - Targets: Tensor of size :math:`(N, S)` or
          :math:`(\operatorname{sum}(\text{target\_lengths}))`,
          where :math:`N = \text{batch size}` and
          :math:`S = \text{max target length, if shape is } (N, S)`.
          It represent the target sequences. Each element in the target
          sequence is a class index. And the target index cannot be blank (default=0).
          In the :math:`(N, S)` form, targets are padded to the
          length of the longest sequence, and stacked.
          In the :math:`(\operatorname{sum}(\text{target\_lengths}))` form,
          the targets are assumed to be un-padded and
          concatenated within 1 dimension.
        - Input_lengths: Tuple or tensor of size :math:`(N)`,
          where :math:`N = \text{batch size}`. It represent the lengths of the
          inputs (must each be :math:`\leq T`). And the lengths are specified
          for each sequence to achieve masking under the assumption that sequences
          are padded to equal lengths.
        - Target_lengths: Tuple or tensor of size :math:`(N)`,
          where :math:`N = \text{batch size}`. It represent lengths of the targets.
          Lengths are specified for each sequence to achieve masking under the
          assumption that sequences are padded to equal lengths. If target shape is
          :math:`(N,S)`, target_lengths are effectively the stop index
          :math:`s_n` for each target sequence, such that ``target_n = targets[n,0:s_n]`` for
          each target in a batch. Lengths must each be :math:`\leq S`
          If the targets are given as a 1d tensor that is the concatenation of individual
          targets, the target_lengths must add up to the total length of the tensor.
        - Output: scalar. If :attr:`reduction` is ``'none'``, then
          :math:`(N)`, where :math:`N = \text{batch size}`.

    Examples::

        >>> # Target are to be padded
        >>> T = 50      # Input sequence length
        >>> C = 20      # Number of classes (including blank)
        >>> N = 16      # Batch size
        >>> S = 30      # Target sequence length of longest target in batch (padding length)
        >>> S_min = 10  # Minimum target length, for demonstration purposes
        >>>
        >>> # Initialize random batch of input vectors, for *size = (T,N,C)
        >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
        >>>
        >>> # Initialize random batch of targets (0 = blank, 1:C = classes)
        >>> target = torch.randint(low=1, high=C, size=(N, S), dtype=torch.long)
        >>>
        >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
        >>> target_lengths = torch.randint(low=S_min, high=S, size=(N,), dtype=torch.long)
        >>> ctc_loss = nn.CTCLoss()
        >>> loss = ctc_loss(input, target, input_lengths, target_lengths)
        >>> loss.backward()
        >>>
        >>>
        >>> # Target are to be un-padded
        >>> T = 50      # Input sequence length
        >>> C = 20      # Number of classes (including blank)
        >>> N = 16      # Batch size
        >>>
        >>> # Initialize random batch of input vectors, for *size = (T,N,C)
        >>> input = torch.randn(T, N, C).log_softmax(2).detach().requires_grad_()
        >>> input_lengths = torch.full(size=(N,), fill_value=T, dtype=torch.long)
        >>>
        >>> # Initialize random batch of targets (0 = blank, 1:C = classes)
        >>> target_lengths = torch.randint(low=1, high=T, size=(N,), dtype=torch.long)
        >>> target = torch.randint(low=1, high=C, size=(sum(target_lengths),), dtype=torch.long)
        >>> ctc_loss = nn.CTCLoss()
        >>> loss = ctc_loss(input, target, input_lengths, target_lengths)
        >>> loss.backward()

    Reference:
        A. Graves et al.: Connectionist Temporal Classification:
        Labelling Unsegmented Sequence Data with Recurrent Neural Networks:
        https://www.cs.toronto.edu/~graves/icml_2006.pdf

    Note:
        In order to use CuDNN, the following must be satisfied: :attr:`targets` must be
        in concatenated format, all :attr:`input_lengths` must be `T`.  :math:`blank=0`,
        :attr:`target_lengths` :math:`\leq 256`, the integer arguments must be of
        dtype :attr:`torch.int32`.

        The regular implementation uses the (more common in PyTorch) `torch.long` dtype.


    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.
    """
    __constants__ = ...
    blank: int
    zero_infinity: bool
    def __init__(self, blank: int = ..., reduction: str = ..., zero_infinity: bool = ...) -> None:
        ...
    
    def forward(self, log_probs: Tensor, targets: Tensor, input_lengths: Tensor, target_lengths: Tensor) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor
from ..common_types import _size_2_t, _size_4_t, _size_6_t

class _ConstantPadNd(Module):
    __constants__ = ...
    value: float
    def __init__(self, value: float) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class ConstantPad1d(_ConstantPadNd):
    r"""Pads the input tensor boundaries with a constant value.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in both boundaries. If a 2-`tuple`, uses
            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)

    Shape:
        - Input: :math:`(N, C, W_{in})`
        - Output: :math:`(N, C, W_{out})` where

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ConstantPad1d(2, 3.5)
        >>> input = torch.randn(1, 2, 4)
        >>> input
        tensor([[[-1.0491, -0.7152, -0.0749,  0.8530],
                 [-1.3287,  1.8966,  0.1466, -0.2771]]])
        >>> m(input)
        tensor([[[ 3.5000,  3.5000, -1.0491, -0.7152, -0.0749,  0.8530,  3.5000,
                   3.5000],
                 [ 3.5000,  3.5000, -1.3287,  1.8966,  0.1466, -0.2771,  3.5000,
                   3.5000]]])
        >>> m = nn.ConstantPad1d(2, 3.5)
        >>> input = torch.randn(1, 2, 3)
        >>> input
        tensor([[[ 1.6616,  1.4523, -1.1255],
                 [-3.6372,  0.1182, -1.8652]]])
        >>> m(input)
        tensor([[[ 3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000,  3.5000],
                 [ 3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000,  3.5000]]])
        >>> # using different paddings for different sides
        >>> m = nn.ConstantPad1d((3, 1), 3.5)
        >>> m(input)
        tensor([[[ 3.5000,  3.5000,  3.5000,  1.6616,  1.4523, -1.1255,  3.5000],
                 [ 3.5000,  3.5000,  3.5000, -3.6372,  0.1182, -1.8652,  3.5000]]])

    """
    padding: _size_2_t
    def __init__(self, padding: _size_2_t, value: float) -> None:
        ...
    


class ConstantPad2d(_ConstantPadNd):
    r"""Pads the input tensor boundaries with a constant value.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,
            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ConstantPad2d(2, 3.5)
        >>> input = torch.randn(1, 2, 2)
        >>> input
        tensor([[[ 1.6585,  0.4320],
                 [-0.8701, -0.4649]]])
        >>> m(input)
        tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  1.6585,  0.4320,  3.5000,  3.5000],
                 [ 3.5000,  3.5000, -0.8701, -0.4649,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])
        >>> # using different paddings for different sides
        >>> m = nn.ConstantPad2d((3, 0, 2, 1), 3.5)
        >>> m(input)
        tensor([[[ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000],
                 [ 3.5000,  3.5000,  3.5000,  1.6585,  0.4320],
                 [ 3.5000,  3.5000,  3.5000, -0.8701, -0.4649],
                 [ 3.5000,  3.5000,  3.5000,  3.5000,  3.5000]]])

    """
    __constants__ = ...
    padding: _size_4_t
    def __init__(self, padding: _size_4_t, value: float) -> None:
        ...
    


class ConstantPad3d(_ConstantPadNd):
    r"""Pads the input tensor boundaries with a constant value.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 6-`tuple`, uses
            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`,
            :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`,
            :math:`\text{padding\_front}`, :math:`\text{padding\_back}`)

    Shape:
        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` where

          :math:`D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}`

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ConstantPad3d(3, 3.5)
        >>> input = torch.randn(16, 3, 10, 20, 30)
        >>> output = m(input)
        >>> # using different paddings for different sides
        >>> m = nn.ConstantPad3d((3, 3, 6, 6, 0, 1), 3.5)
        >>> output = m(input)

    """
    padding: _size_6_t
    def __init__(self, padding: _size_6_t, value: float) -> None:
        ...
    


class _ReflectionPadNd(Module):
    __constants__ = ...
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class ReflectionPad1d(_ReflectionPadNd):
    r"""Pads the input tensor using the reflection of the input boundary.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 2-`tuple`, uses
            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)

    Shape:
        - Input: :math:`(N, C, W_{in})`
        - Output: :math:`(N, C, W_{out})` where

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ReflectionPad1d(2)
        >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
        >>> input
        tensor([[[0., 1., 2., 3.],
                 [4., 5., 6., 7.]]])
        >>> m(input)
        tensor([[[2., 1., 0., 1., 2., 3., 2., 1.],
                 [6., 5., 4., 5., 6., 7., 6., 5.]]])
        >>> # using different paddings for different sides
        >>> m = nn.ReflectionPad1d((3, 1))
        >>> m(input)
        tensor([[[3., 2., 1., 0., 1., 2., 3., 2.],
                 [7., 6., 5., 4., 5., 6., 7., 6.]]])

    """
    padding: _size_2_t
    def __init__(self, padding: _size_2_t) -> None:
        ...
    


class ReflectionPad2d(_ReflectionPadNd):
    r"""Pads the input tensor using the reflection of the input boundary.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,
            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ReflectionPad2d(2)
        >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
        >>> input
        tensor([[[[0., 1., 2.],
                  [3., 4., 5.],
                  [6., 7., 8.]]]])
        >>> m(input)
        tensor([[[[8., 7., 6., 7., 8., 7., 6.],
                  [5., 4., 3., 4., 5., 4., 3.],
                  [2., 1., 0., 1., 2., 1., 0.],
                  [5., 4., 3., 4., 5., 4., 3.],
                  [8., 7., 6., 7., 8., 7., 6.],
                  [5., 4., 3., 4., 5., 4., 3.],
                  [2., 1., 0., 1., 2., 1., 0.]]]])
        >>> # using different paddings for different sides
        >>> m = nn.ReflectionPad2d((1, 1, 2, 0))
        >>> m(input)
        tensor([[[[7., 6., 7., 8., 7.],
                  [4., 3., 4., 5., 4.],
                  [1., 0., 1., 2., 1.],
                  [4., 3., 4., 5., 4.],
                  [7., 6., 7., 8., 7.]]]])

    """
    padding: _size_4_t
    def __init__(self, padding: _size_4_t) -> None:
        ...
    


class _ReplicationPadNd(Module):
    __constants__ = ...
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class ReplicationPad1d(_ReplicationPadNd):
    r"""Pads the input tensor using replication of the input boundary.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 2-`tuple`, uses
            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`)

    Shape:
        - Input: :math:`(N, C, W_{in})`
        - Output: :math:`(N, C, W_{out})` where

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ReplicationPad1d(2)
        >>> input = torch.arange(8, dtype=torch.float).reshape(1, 2, 4)
        >>> input
        tensor([[[0., 1., 2., 3.],
                 [4., 5., 6., 7.]]])
        >>> m(input)
        tensor([[[0., 0., 0., 1., 2., 3., 3., 3.],
                 [4., 4., 4., 5., 6., 7., 7., 7.]]])
        >>> # using different paddings for different sides
        >>> m = nn.ReplicationPad1d((3, 1))
        >>> m(input)
        tensor([[[0., 0., 0., 0., 1., 2., 3., 3.],
                 [4., 4., 4., 4., 5., 6., 7., 7.]]])

    """
    padding: _size_2_t
    def __init__(self, padding: _size_2_t) -> None:
        ...
    


class ReplicationPad2d(_ReplicationPadNd):
    r"""Pads the input tensor using replication of the input boundary.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,
            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ReplicationPad2d(2)
        >>> input = torch.arange(9, dtype=torch.float).reshape(1, 1, 3, 3)
        >>> input
        tensor([[[[0., 1., 2.],
                  [3., 4., 5.],
                  [6., 7., 8.]]]])
        >>> m(input)
        tensor([[[[0., 0., 0., 1., 2., 2., 2.],
                  [0., 0., 0., 1., 2., 2., 2.],
                  [0., 0., 0., 1., 2., 2., 2.],
                  [3., 3., 3., 4., 5., 5., 5.],
                  [6., 6., 6., 7., 8., 8., 8.],
                  [6., 6., 6., 7., 8., 8., 8.],
                  [6., 6., 6., 7., 8., 8., 8.]]]])
        >>> # using different paddings for different sides
        >>> m = nn.ReplicationPad2d((1, 1, 2, 0))
        >>> m(input)
        tensor([[[[0., 0., 1., 2., 2.],
                  [0., 0., 1., 2., 2.],
                  [0., 0., 1., 2., 2.],
                  [3., 3., 4., 5., 5.],
                  [6., 6., 7., 8., 8.]]]])

    """
    padding: _size_4_t
    def __init__(self, padding: _size_4_t) -> None:
        ...
    


class ReplicationPad3d(_ReplicationPadNd):
    r"""Pads the input tensor using replication of the input boundary.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 6-`tuple`, uses
            (:math:`\text{padding\_left}`, :math:`\text{padding\_right}`,
            :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`,
            :math:`\text{padding\_front}`, :math:`\text{padding\_back}`)

    Shape:
        - Input: :math:`(N, C, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C, D_{out}, H_{out}, W_{out})` where

          :math:`D_{out} = D_{in} + \text{padding\_front} + \text{padding\_back}`

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ReplicationPad3d(3)
        >>> input = torch.randn(16, 3, 8, 320, 480)
        >>> output = m(input)
        >>> # using different paddings for different sides
        >>> m = nn.ReplicationPad3d((3, 3, 6, 6, 1, 1))
        >>> output = m(input)

    """
    padding: _size_6_t
    def __init__(self, padding: _size_6_t) -> None:
        ...
    


class ZeroPad2d(ConstantPad2d):
    r"""Pads the input tensor boundaries with zero.

    For `N`-dimensional padding, use :func:`torch.nn.functional.pad()`.

    Args:
        padding (int, tuple): the size of the padding. If is `int`, uses the same
            padding in all boundaries. If a 4-`tuple`, uses (:math:`\text{padding\_left}`,
            :math:`\text{padding\_right}`, :math:`\text{padding\_top}`, :math:`\text{padding\_bottom}`)

    Shape:
        - Input: :math:`(N, C, H_{in}, W_{in})`
        - Output: :math:`(N, C, H_{out}, W_{out})` where

          :math:`H_{out} = H_{in} + \text{padding\_top} + \text{padding\_bottom}`

          :math:`W_{out} = W_{in} + \text{padding\_left} + \text{padding\_right}`

    Examples::

        >>> m = nn.ZeroPad2d(2)
        >>> input = torch.randn(1, 1, 3, 3)
        >>> input
        tensor([[[[-0.1678, -0.4418,  1.9466],
                  [ 0.9604, -0.4219, -0.5241],
                  [-0.9162, -0.5436, -0.6446]]]])
        >>> m(input)
        tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000, -0.1678, -0.4418,  1.9466,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.9604, -0.4219, -0.5241,  0.0000,  0.0000],
                  [ 0.0000,  0.0000, -0.9162, -0.5436, -0.6446,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000]]]])
        >>> # using different paddings for different sides
        >>> m = nn.ZeroPad2d((1, 1, 2, 0))
        >>> m(input)
        tensor([[[[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
                  [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000],
                  [ 0.0000, -0.1678, -0.4418,  1.9466,  0.0000],
                  [ 0.0000,  0.9604, -0.4219, -0.5241,  0.0000],
                  [ 0.0000, -0.9162, -0.5436, -0.6446,  0.0000]]]])

    """
    padding: _size_4_t
    def __init__(self, padding: _size_4_t) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor

class PairwiseDistance(Module):
    r"""
    Computes the batchwise pairwise distance between vectors :math:`v_1`, :math:`v_2` using the p-norm:

    .. math ::
        \Vert x \Vert _p = \left( \sum_{i=1}^n  \vert x_i \vert ^ p \right) ^ {1/p}.

    Args:
        p (real): the norm degree. Default: 2
        eps (float, optional): Small value to avoid division by zero.
            Default: 1e-6
        keepdim (bool, optional): Determines whether or not to keep the vector dimension.
            Default: False
    Shape:
        - Input1: :math:`(N, D)` where `D = vector dimension`
        - Input2: :math:`(N, D)`, same shape as the Input1
        - Output: :math:`(N)`. If :attr:`keepdim` is ``True``, then :math:`(N, 1)`.
    Examples::
        >>> pdist = nn.PairwiseDistance(p=2)
        >>> input1 = torch.randn(100, 128)
        >>> input2 = torch.randn(100, 128)
        >>> output = pdist(input1, input2)
    """
    __constants__ = ...
    norm: float
    eps: float
    keepdim: bool
    def __init__(self, p: float = ..., eps: float = ..., keepdim: bool = ...) -> None:
        ...
    
    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:
        ...
    


class CosineSimilarity(Module):
    r"""Returns cosine similarity between :math:`x_1` and :math:`x_2`, computed along dim.

    .. math ::
        \text{similarity} = \dfrac{x_1 \cdot x_2}{\max(\Vert x_1 \Vert _2 \cdot \Vert x_2 \Vert _2, \epsilon)}.

    Args:
        dim (int, optional): Dimension where cosine similarity is computed. Default: 1
        eps (float, optional): Small value to avoid division by zero.
            Default: 1e-8
    Shape:
        - Input1: :math:`(\ast_1, D, \ast_2)` where D is at position `dim`
        - Input2: :math:`(\ast_1, D, \ast_2)`, same shape as the Input1
        - Output: :math:`(\ast_1, \ast_2)`
    Examples::
        >>> input1 = torch.randn(100, 128)
        >>> input2 = torch.randn(100, 128)
        >>> cos = nn.CosineSimilarity(dim=1, eps=1e-6)
        >>> output = cos(input1, input2)
    """
    __constants__ = ...
    dim: int
    eps: float
    def __init__(self, dim: int = ..., eps: float = ...) -> None:
        ...
    
    def forward(self, x1: Tensor, x2: Tensor) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch import Tensor
from .module import Module
from ..common_types import _size_1_t, _size_2_t, _size_3_t
from typing import List, Optional, Tuple

class _ConvNd(Module):
    __constants__ = ...
    __annotations__ = ...
    _in_channels: int
    out_channels: int
    kernel_size: Tuple[int, ...]
    stride: Tuple[int, ...]
    padding: Tuple[int, ...]
    dilation: Tuple[int, ...]
    transposed: bool
    output_padding: Tuple[int, ...]
    groups: int
    padding_mode: str
    weight: Tensor
    bias: Optional[Tensor]
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_1_t, stride: _size_1_t, padding: _size_1_t, dilation: _size_1_t, transposed: bool, output_padding: _size_1_t, groups: int, bias: Optional[Tensor], padding_mode: str) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def extra_repr(self):
        ...
    
    def __setstate__(self, state):
        ...
    


class Conv1d(_ConvNd):
    r"""Applies a 1D convolution over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size
    :math:`(N, C_{\text{in}}, L)` and output :math:`(N, C_{\text{out}}, L_{\text{out}})` can be
    precisely described as:

    .. math::
        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
        \sum_{k = 0}^{C_{in} - 1} \text{weight}(C_{\text{out}_j}, k)
        \star \text{input}(N_i, k)

    where :math:`\star` is the valid `cross-correlation`_ operator,
    :math:`N` is a batch size, :math:`C` denotes a number of channels,
    :math:`L` is a length of signal sequence.

    * :attr:`stride` controls the stride for the cross-correlation, a single
      number or a one-element tuple.

    * :attr:`padding` controls the amount of implicit zero-paddings on both sides
      for :attr:`padding` number of points.

    * :attr:`dilation` controls the spacing between the kernel points; also
      known as the à trous algorithm. It is harder to describe, but this `link`_
      has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters,
          of size
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.

    Note:

        Depending of the size of your kernel, several (of the last)
        columns of the input might be lost, because it is a valid
        `cross-correlation`_, and not a full `cross-correlation`_.
        It is up to the user to add proper padding.

    Note:

        When `groups == in_channels` and `out_channels == K * in_channels`,
        where `K` is a positive integer, this operation is also termed in
        literature as depthwise convolution.

        In other words, for an input of size :math:`(N, C_{in}, L_{in})`,
        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments
        :math:`(C_\text{in}=C_{in}, C_\text{out}=C_{in} \times K, ..., \text{groups}=C_{in})`.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.


    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to both sides of
            the input. Default: 0
        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
        dilation (int or tuple, optional): Spacing between kernel
            elements. Default: 1
        groups (int, optional): Number of blocked connections from input
            channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the
            output. Default: ``True``

    Shape:
        - Input: :math:`(N, C_{in}, L_{in})`
        - Output: :math:`(N, C_{out}, L_{out})` where

          .. math::
              L_{out} = \left\lfloor\frac{L_{in} + 2 \times \text{padding} - \text{dilation}
                        \times (\text{kernel\_size} - 1) - 1}{\text{stride}} + 1\right\rfloor

    Attributes:
        weight (Tensor): the learnable weights of the module of shape
            :math:`(\text{out\_channels},
            \frac{\text{in\_channels}}{\text{groups}}, \text{kernel\_size})`.
            The values of these weights are sampled from
            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`
        bias (Tensor):   the learnable bias of the module of shape
            (out_channels). If :attr:`bias` is ``True``, then the values of these weights are
            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
            :math:`k = \frac{groups}{C_\text{in} * \text{kernel\_size}}`

    Examples::

        >>> m = nn.Conv1d(16, 33, 3, stride=2)
        >>> input = torch.randn(20, 16, 50)
        >>> output = m(input)

    .. _cross-correlation:
        https://en.wikipedia.org/wiki/Cross-correlation

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_1_t, stride: _size_1_t = ..., padding: _size_1_t = ..., dilation: _size_1_t = ..., groups: int = ..., bias: bool = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Conv2d(_ConvNd):
    r"""Applies a 2D convolution over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size
    :math:`(N, C_{\text{in}}, H, W)` and output :math:`(N, C_{\text{out}}, H_{\text{out}}, W_{\text{out}})`
    can be precisely described as:

    .. math::
        \text{out}(N_i, C_{\text{out}_j}) = \text{bias}(C_{\text{out}_j}) +
        \sum_{k = 0}^{C_{\text{in}} - 1} \text{weight}(C_{\text{out}_j}, k) \star \text{input}(N_i, k)


    where :math:`\star` is the valid 2D `cross-correlation`_ operator,
    :math:`N` is a batch size, :math:`C` denotes a number of channels,
    :math:`H` is a height of input planes in pixels, and :math:`W` is
    width in pixels.

    * :attr:`stride` controls the stride for the cross-correlation, a single
      number or a tuple.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for :attr:`padding` number of points for each dimension.

    * :attr:`dilation` controls the spacing between the kernel points; also
      known as the à trous algorithm. It is harder to describe, but this `link`_
      has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters, of size:
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:

        - a single ``int`` -- in which case the same value is used for the height and width dimension
        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
          and the second `int` for the width dimension

    Note:

         Depending of the size of your kernel, several (of the last)
         columns of the input might be lost, because it is a valid `cross-correlation`_,
         and not a full `cross-correlation`_.
         It is up to the user to add proper padding.

    Note:

        When `groups == in_channels` and `out_channels == K * in_channels`,
        where `K` is a positive integer, this operation is also termed in
        literature as depthwise convolution.

        In other words, for an input of size :math:`(N, C_{in}, H_{in}, W_{in})`,
        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments
        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.


    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to both sides of
            the input. Default: 0
        padding_mode (string, optional): ``'zeros'``, ``'reflect'``,
            ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
        groups (int, optional): Number of blocked connections from input
            channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the
            output. Default: ``True``

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where

          .. math::
              H_{out} = \left\lfloor\frac{H_{in}  + 2 \times \text{padding}[0] - \text{dilation}[0]
                        \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in}  + 2 \times \text{padding}[1] - \text{dilation}[1]
                        \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

    Attributes:
        weight (Tensor): the learnable weights of the module of shape
            :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
            :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
            The values of these weights are sampled from
            :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
        bias (Tensor):   the learnable bias of the module of shape
            (out_channels). If :attr:`bias` is ``True``,
            then the values of these weights are
            sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
            :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`

    Examples:

        >>> # With square kernels and equal stride
        >>> m = nn.Conv2d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
        >>> # non-square kernels and unequal stride and with padding and dilation
        >>> m = nn.Conv2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2), dilation=(3, 1))
        >>> input = torch.randn(20, 16, 50, 100)
        >>> output = m(input)

    .. _cross-correlation:
        https://en.wikipedia.org/wiki/Cross-correlation

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t = ..., padding: _size_2_t = ..., dilation: _size_2_t = ..., groups: int = ..., bias: bool = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class Conv3d(_ConvNd):
    r"""Applies a 3D convolution over an input signal composed of several input
    planes.

    In the simplest case, the output value of the layer with input size :math:`(N, C_{in}, D, H, W)`
    and output :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` can be precisely described as:

    .. math::
        out(N_i, C_{out_j}) = bias(C_{out_j}) +
                                \sum_{k = 0}^{C_{in} - 1} weight(C_{out_j}, k) \star input(N_i, k)

    where :math:`\star` is the valid 3D `cross-correlation`_ operator

    * :attr:`stride` controls the stride for the cross-correlation.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for :attr:`padding` number of points for each dimension.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters, of size
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`.

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`dilation` can either be:

        - a single ``int`` -- in which case the same value is used for the depth, height and width dimension
        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
          the second `int` for the height dimension and the third `int` for the width dimension

    Note:

         Depending of the size of your kernel, several (of the last)
         columns of the input might be lost, because it is a valid `cross-correlation`_,
         and not a full `cross-correlation`_.
         It is up to the user to add proper padding.

    Note:

        When `groups == in_channels` and `out_channels == K * in_channels`,
        where `K` is a positive integer, this operation is also termed in
        literature as depthwise convolution.

        In other words, for an input of size :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`,
        a depthwise convolution with a depthwise multiplier `K`, can be constructed by arguments
        :math:`(in\_channels=C_{in}, out\_channels=C_{in} \times K, ..., groups=C_{in})`.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.


    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): Zero-padding added to all three sides of the input. Default: 0
        padding_mode (string, optional): ``'zeros'``, ``'reflect'``, ``'replicate'`` or ``'circular'``. Default: ``'zeros'``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1
        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``

    Shape:
        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where

          .. math::
              D_{out} = \left\lfloor\frac{D_{in} + 2 \times \text{padding}[0] - \text{dilation}[0]
                    \times (\text{kernel\_size}[0] - 1) - 1}{\text{stride}[0]} + 1\right\rfloor

          .. math::
              H_{out} = \left\lfloor\frac{H_{in} + 2 \times \text{padding}[1] - \text{dilation}[1]
                    \times (\text{kernel\_size}[1] - 1) - 1}{\text{stride}[1]} + 1\right\rfloor

          .. math::
              W_{out} = \left\lfloor\frac{W_{in} + 2 \times \text{padding}[2] - \text{dilation}[2]
                    \times (\text{kernel\_size}[2] - 1) - 1}{\text{stride}[2]} + 1\right\rfloor

    Attributes:
        weight (Tensor): the learnable weights of the module of shape
                         :math:`(\text{out\_channels}, \frac{\text{in\_channels}}{\text{groups}},`
                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
                         The values of these weights are sampled from
                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
        bias (Tensor):   the learnable bias of the module of shape (out_channels). If :attr:`bias` is ``True``,
                         then the values of these weights are
                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{in} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`

    Examples::

        >>> # With square kernels and equal stride
        >>> m = nn.Conv3d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.Conv3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(4, 2, 0))
        >>> input = torch.randn(20, 16, 10, 50, 100)
        >>> output = m(input)

    .. _cross-correlation:
        https://en.wikipedia.org/wiki/Cross-correlation

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_3_t, stride: _size_3_t = ..., padding: _size_3_t = ..., dilation: _size_3_t = ..., groups: int = ..., bias: bool = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    


class _ConvTransposeNd(_ConvNd):
    def __init__(self, in_channels, out_channels, kernel_size, stride, padding, dilation, transposed, output_padding, groups, bias, padding_mode) -> None:
        ...
    


class ConvTranspose1d(_ConvTransposeNd):
    r"""Applies a 1D transposed convolution operator over an input image
    composed of several input planes.

    This module can be seen as the gradient of Conv1d with respect to its input.
    It is also known as a fractionally-strided convolution or
    a deconvolution (although it is not an actual deconvolution operation).

    * :attr:`stride` controls the stride for the cross-correlation.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
      below for details.

    * :attr:`output_padding` controls the additional size added to one side
      of the output shape. See note below for details.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters (of size
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).

    Note:

         Depending of the size of your kernel, several (of the last)
         columns of the input might be lost, because it is a valid `cross-correlation`_,
         and not a full `cross-correlation`_.
         It is up to the user to add proper padding.

    Note:
        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
        amount of zero padding to both sizes of the input. This is set so that
        when a :class:`~torch.nn.Conv1d` and a :class:`~torch.nn.ConvTranspose1d`
        are initialized with same parameters, they are inverses of each other in
        regard to the input and output shapes. However, when ``stride > 1``,
        :class:`~torch.nn.Conv1d` maps multiple input shapes to the same output
        shape. :attr:`output_padding` is provided to resolve this ambiguity by
        effectively increasing the calculated output shape on one side. Note
        that :attr:`output_padding` is only used to find output shape, but does
        not actually add zero-padding to output.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.

    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
            will be added to both sides of the input. Default: 0
        output_padding (int or tuple, optional): Additional size added to one side
            of the output shape. Default: 0
        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1

    Shape:
        - Input: :math:`(N, C_{in}, L_{in})`
        - Output: :math:`(N, C_{out}, L_{out})` where

          .. math::
              L_{out} = (L_{in} - 1) \times \text{stride} - 2 \times \text{padding} + \text{dilation}
                        \times (\text{kernel\_size} - 1) + \text{output\_padding} + 1

    Attributes:
        weight (Tensor): the learnable weights of the module of shape
                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
                         :math:`\text{kernel\_size})`.
                         The values of these weights are sampled from
                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
        bias (Tensor):   the learnable bias of the module of shape (out_channels).
                         If :attr:`bias` is ``True``, then the values of these weights are
                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \text{kernel\_size}}`
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_1_t, stride: _size_1_t = ..., padding: _size_1_t = ..., output_padding: _size_1_t = ..., groups: int = ..., bias: bool = ..., dilation: _size_1_t = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class ConvTranspose2d(_ConvTransposeNd):
    r"""Applies a 2D transposed convolution operator over an input image
    composed of several input planes.

    This module can be seen as the gradient of Conv2d with respect to its input.
    It is also known as a fractionally-strided convolution or
    a deconvolution (although it is not an actual deconvolution operation).

    * :attr:`stride` controls the stride for the cross-correlation.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
      below for details.

    * :attr:`output_padding` controls the additional size added to one side
      of the output shape. See note below for details.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters (of size
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
    can either be:

        - a single ``int`` -- in which case the same value is used for the height and width dimensions
        - a ``tuple`` of two ints -- in which case, the first `int` is used for the height dimension,
          and the second `int` for the width dimension

    .. note::

         Depending of the size of your kernel, several (of the last)
         columns of the input might be lost, because it is a valid `cross-correlation`_,
         and not a full `cross-correlation`_.
         It is up to the user to add proper padding.

    Note:
        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
        amount of zero padding to both sizes of the input. This is set so that
        when a :class:`~torch.nn.Conv2d` and a :class:`~torch.nn.ConvTranspose2d`
        are initialized with same parameters, they are inverses of each other in
        regard to the input and output shapes. However, when ``stride > 1``,
        :class:`~torch.nn.Conv2d` maps multiple input shapes to the same output
        shape. :attr:`output_padding` is provided to resolve this ambiguity by
        effectively increasing the calculated output shape on one side. Note
        that :attr:`output_padding` is only used to find output shape, but does
        not actually add zero-padding to output.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.


    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
            will be added to both sides of each dimension in the input. Default: 0
        output_padding (int or tuple, optional): Additional size added to one side
            of each dimension in the output shape. Default: 0
        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1

    Shape:
        - Input: :math:`(N, C_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, H_{out}, W_{out})` where

        .. math::
              H_{out} = (H_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
        .. math::
              W_{out} = (W_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1

    Attributes:
        weight (Tensor): the learnable weights of the module of shape
                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]})`.
                         The values of these weights are sampled from
                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`
        bias (Tensor):   the learnable bias of the module of shape (out_channels)
                         If :attr:`bias` is ``True``, then the values of these weights are
                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{1}\text{kernel\_size}[i]}`

    Examples::

        >>> # With square kernels and equal stride
        >>> m = nn.ConvTranspose2d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.ConvTranspose2d(16, 33, (3, 5), stride=(2, 1), padding=(4, 2))
        >>> input = torch.randn(20, 16, 50, 100)
        >>> output = m(input)
        >>> # exact output size can be also specified as an argument
        >>> input = torch.randn(1, 16, 12, 12)
        >>> downsample = nn.Conv2d(16, 16, 3, stride=2, padding=1)
        >>> upsample = nn.ConvTranspose2d(16, 16, 3, stride=2, padding=1)
        >>> h = downsample(input)
        >>> h.size()
        torch.Size([1, 16, 6, 6])
        >>> output = upsample(h, output_size=input.size())
        >>> output.size()
        torch.Size([1, 16, 12, 12])

    .. _cross-correlation:
        https://en.wikipedia.org/wiki/Cross-correlation

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_2_t, stride: _size_2_t = ..., padding: _size_2_t = ..., output_padding: _size_2_t = ..., groups: int = ..., bias: bool = ..., dilation: int = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class ConvTranspose3d(_ConvTransposeNd):
    r"""Applies a 3D transposed convolution operator over an input image composed of several input
    planes.
    The transposed convolution operator multiplies each input value element-wise by a learnable kernel,
    and sums over the outputs from all input feature planes.

    This module can be seen as the gradient of Conv3d with respect to its input.
    It is also known as a fractionally-strided convolution or
    a deconvolution (although it is not an actual deconvolution operation).

    * :attr:`stride` controls the stride for the cross-correlation.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for ``dilation * (kernel_size - 1) - padding`` number of points. See note
      below for details.

    * :attr:`output_padding` controls the additional size added to one side
      of the output shape. See note below for details.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    * :attr:`groups` controls the connections between inputs and outputs.
      :attr:`in_channels` and :attr:`out_channels` must both be divisible by
      :attr:`groups`. For example,

        * At groups=1, all inputs are convolved to all outputs.
        * At groups=2, the operation becomes equivalent to having two conv
          layers side by side, each seeing half the input channels,
          and producing half the output channels, and both subsequently
          concatenated.
        * At groups= :attr:`in_channels`, each input channel is convolved with
          its own set of filters (of size
          :math:`\left\lfloor\frac{out\_channels}{in\_channels}\right\rfloor`).

    The parameters :attr:`kernel_size`, :attr:`stride`, :attr:`padding`, :attr:`output_padding`
    can either be:

        - a single ``int`` -- in which case the same value is used for the depth, height and width dimensions
        - a ``tuple`` of three ints -- in which case, the first `int` is used for the depth dimension,
          the second `int` for the height dimension and the third `int` for the width dimension

    Note:

         Depending of the size of your kernel, several (of the last)
         columns of the input might be lost, because it is a valid `cross-correlation`_,
         and not a full `cross-correlation`_.
         It is up to the user to add proper padding.

    Note:
        The :attr:`padding` argument effectively adds ``dilation * (kernel_size - 1) - padding``
        amount of zero padding to both sizes of the input. This is set so that
        when a :class:`~torch.nn.Conv3d` and a :class:`~torch.nn.ConvTranspose3d`
        are initialized with same parameters, they are inverses of each other in
        regard to the input and output shapes. However, when ``stride > 1``,
        :class:`~torch.nn.Conv3d` maps multiple input shapes to the same output
        shape. :attr:`output_padding` is provided to resolve this ambiguity by
        effectively increasing the calculated output shape on one side. Note
        that :attr:`output_padding` is only used to find output shape, but does
        not actually add zero-padding to output.

    Note:
        In some circumstances when using the CUDA backend with CuDNN, this operator
        may select a nondeterministic algorithm to increase performance. If this is
        undesirable, you can try to make the operation deterministic (potentially at
        a performance cost) by setting ``torch.backends.cudnn.deterministic =
        True``.
        Please see the notes on :doc:`/notes/randomness` for background.


    Args:
        in_channels (int): Number of channels in the input image
        out_channels (int): Number of channels produced by the convolution
        kernel_size (int or tuple): Size of the convolving kernel
        stride (int or tuple, optional): Stride of the convolution. Default: 1
        padding (int or tuple, optional): ``dilation * (kernel_size - 1) - padding`` zero-padding
            will be added to both sides of each dimension in the input. Default: 0
        output_padding (int or tuple, optional): Additional size added to one side
            of each dimension in the output shape. Default: 0
        groups (int, optional): Number of blocked connections from input channels to output channels. Default: 1
        bias (bool, optional): If ``True``, adds a learnable bias to the output. Default: ``True``
        dilation (int or tuple, optional): Spacing between kernel elements. Default: 1

    Shape:
        - Input: :math:`(N, C_{in}, D_{in}, H_{in}, W_{in})`
        - Output: :math:`(N, C_{out}, D_{out}, H_{out}, W_{out})` where

        .. math::
              D_{out} = (D_{in} - 1) \times \text{stride}[0] - 2 \times \text{padding}[0] + \text{dilation}[0]
                        \times (\text{kernel\_size}[0] - 1) + \text{output\_padding}[0] + 1
        .. math::
              H_{out} = (H_{in} - 1) \times \text{stride}[1] - 2 \times \text{padding}[1] + \text{dilation}[1]
                        \times (\text{kernel\_size}[1] - 1) + \text{output\_padding}[1] + 1
        .. math::
              W_{out} = (W_{in} - 1) \times \text{stride}[2] - 2 \times \text{padding}[2] + \text{dilation}[2]
                        \times (\text{kernel\_size}[2] - 1) + \text{output\_padding}[2] + 1


    Attributes:
        weight (Tensor): the learnable weights of the module of shape
                         :math:`(\text{in\_channels}, \frac{\text{out\_channels}}{\text{groups}},`
                         :math:`\text{kernel\_size[0]}, \text{kernel\_size[1]}, \text{kernel\_size[2]})`.
                         The values of these weights are sampled from
                         :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`
        bias (Tensor):   the learnable bias of the module of shape (out_channels)
                         If :attr:`bias` is ``True``, then the values of these weights are
                         sampled from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})` where
                         :math:`k = \frac{groups}{C_\text{out} * \prod_{i=0}^{2}\text{kernel\_size}[i]}`

    Examples::

        >>> # With square kernels and equal stride
        >>> m = nn.ConvTranspose3d(16, 33, 3, stride=2)
        >>> # non-square kernels and unequal stride and with padding
        >>> m = nn.ConvTranspose3d(16, 33, (3, 5, 2), stride=(2, 1, 1), padding=(0, 4, 2))
        >>> input = torch.randn(20, 16, 10, 50, 100)
        >>> output = m(input)

    .. _cross-correlation:
        https://en.wikipedia.org/wiki/Cross-correlation

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md
    """
    def __init__(self, in_channels: int, out_channels: int, kernel_size: _size_3_t, stride: _size_3_t = ..., padding: _size_3_t = ..., output_padding: _size_3_t = ..., groups: int = ..., bias: bool = ..., dilation: _size_3_t = ..., padding_mode: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, output_size: Optional[List[int]] = ...) -> Tensor:
        ...
    


class _ConvTransposeMixin(_ConvTransposeNd):
    def __init__(self, *args, **kwargs) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .module import Module
from torch import Tensor
from ..common_types import _size_any_t

class Fold(Module):
    r"""Combines an array of sliding local blocks into a large containing
    tensor.

    Consider a batched :attr:`input` tensor containing sliding local blocks,
    e.g., patches of images, of shape :math:`(N, C \times  \prod(\text{kernel\_size}), L)`,
    where :math:`N` is batch dimension, :math:`C \times \prod(\text{kernel\_size})`
    is the number of values within a block (a block has :math:`\prod(\text{kernel\_size})`
    spatial locations each containing a :math:`C`-channeled vector), and
    :math:`L` is the total number of blocks. (This is exactly the
    same specification as the output shape of :class:`~torch.nn.Unfold`.) This
    operation combines these local blocks into the large :attr:`output` tensor
    of shape :math:`(N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)`
    by summing the overlapping values. Similar to :class:`~torch.nn.Unfold`, the
    arguments must satisfy

    .. math::
        L = \prod_d \left\lfloor\frac{\text{output\_size}[d] + 2 \times \text{padding}[d] %
            - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

    where :math:`d` is over all spatial dimensions.

    * :attr:`output_size` describes the spatial shape of the large containing
      tensor of the sliding local blocks. It is useful to resolve the ambiguity
      when multiple input shapes map to same number of sliding blocks, e.g.,
      with ``stride > 0``.

    The :attr:`padding`, :attr:`stride` and :attr:`dilation` arguments specify
    how the sliding blocks are retrieved.

    * :attr:`stride` controls the stride for the sliding blocks.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for :attr:`padding` number of points for each dimension before
      reshaping.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    Args:
        output_size (int or tuple): the shape of the spatial dimensions of the
                                    output (i.e., ``output.sizes()[2:]``)
        kernel_size (int or tuple): the size of the sliding blocks
        stride (int or tuple): the stride of the sliding blocks in the input
                               spatial dimensions. Default: 1
        padding (int or tuple, optional): implicit zero padding to be added on
                                          both sides of input. Default: 0
        dilation (int or tuple, optional): a parameter that controls the
                                           stride of elements within the
                                           neighborhood. Default: 1

    * If :attr:`output_size`, :attr:`kernel_size`, :attr:`dilation`,
      :attr:`padding` or :attr:`stride` is an int or a tuple of length 1 then
      their values will be replicated across all spatial dimensions.

    * For the case of two output spatial dimensions this operation is sometimes
      called ``col2im``.

    .. note::
        :class:`~torch.nn.Fold` calculates each combined value in the resulting
        large tensor by summing all values from all containing blocks.
        :class:`~torch.nn.Unfold` extracts the values in the local blocks by
        copying from the large tensor. So, if the blocks overlap, they are not
        inverses of each other.

        In general, folding and unfolding operations are related as
        follows. Consider :class:`~torch.nn.Fold` and
        :class:`~torch.nn.Unfold` instances created with the same
        parameters:

        >>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
        >>> fold = nn.Fold(output_size=..., **fold_params)
        >>> unfold = nn.Unfold(**fold_params)

        Then for any (supported) ``input`` tensor the following
        equality holds:

        ::

            fold(unfold(input)) == divisor * input

        where ``divisor`` is a tensor that depends only on the shape
        and dtype of the ``input``:

        >>> input_ones = torch.ones(input.shape, dtype=input.dtype)
        >>> divisor = fold(unfold(input_ones))

        When the ``divisor`` tensor contains no zero elements, then
        ``fold`` and ``unfold`` operations are inverses of each
        other (up to constant divisor).

    .. warning::
        Currently, only 4-D output tensors (batched image-like tensors) are
        supported.

    Shape:
        - Input: :math:`(N, C \times \prod(\text{kernel\_size}), L)`
        - Output: :math:`(N, C, \text{output\_size}[0], \text{output\_size}[1], \dots)` as described above

    Examples::

        >>> fold = nn.Fold(output_size=(4, 5), kernel_size=(2, 2))
        >>> input = torch.randn(1, 3 * 2 * 2, 12)
        >>> output = fold(input)
        >>> output.size()
        torch.Size([1, 3, 4, 5])

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md

    """
    __constants__ = ...
    output_size: _size_any_t
    kernel_size: _size_any_t
    dilation: _size_any_t
    padding: _size_any_t
    stride: _size_any_t
    def __init__(self, output_size: _size_any_t, kernel_size: _size_any_t, dilation: _size_any_t = ..., padding: _size_any_t = ..., stride: _size_any_t = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


class Unfold(Module):
    r"""Extracts sliding local blocks from a batched input tensor.

    Consider a batched :attr:`input` tensor of shape :math:`(N, C, *)`,
    where :math:`N` is the batch dimension, :math:`C` is the channel dimension,
    and :math:`*` represent arbitrary spatial dimensions. This operation flattens
    each sliding :attr:`kernel_size`-sized block within the spatial dimensions
    of :attr:`input` into a column (i.e., last dimension) of a 3-D :attr:`output`
    tensor of shape :math:`(N, C \times \prod(\text{kernel\_size}), L)`, where
    :math:`C \times \prod(\text{kernel\_size})` is the total number of values
    within each block (a block has :math:`\prod(\text{kernel\_size})` spatial
    locations each containing a :math:`C`-channeled vector), and :math:`L` is
    the total number of such blocks:

    .. math::
        L = \prod_d \left\lfloor\frac{\text{spatial\_size}[d] + 2 \times \text{padding}[d] %
            - \text{dilation}[d] \times (\text{kernel\_size}[d] - 1) - 1}{\text{stride}[d]} + 1\right\rfloor,

    where :math:`\text{spatial\_size}` is formed by the spatial dimensions
    of :attr:`input` (:math:`*` above), and :math:`d` is over all spatial
    dimensions.

    Therefore, indexing :attr:`output` at the last dimension (column dimension)
    gives all values within a certain block.

    The :attr:`padding`, :attr:`stride` and :attr:`dilation` arguments specify
    how the sliding blocks are retrieved.

    * :attr:`stride` controls the stride for the sliding blocks.

    * :attr:`padding` controls the amount of implicit zero-paddings on both
      sides for :attr:`padding` number of points for each dimension before
      reshaping.

    * :attr:`dilation` controls the spacing between the kernel points; also known as the à trous algorithm.
      It is harder to describe, but this `link`_ has a nice visualization of what :attr:`dilation` does.

    Args:
        kernel_size (int or tuple): the size of the sliding blocks
        stride (int or tuple, optional): the stride of the sliding blocks in the input
                                         spatial dimensions. Default: 1
        padding (int or tuple, optional): implicit zero padding to be added on
                                          both sides of input. Default: 0
        dilation (int or tuple, optional): a parameter that controls the
                                           stride of elements within the
                                           neighborhood. Default: 1

    * If :attr:`kernel_size`, :attr:`dilation`, :attr:`padding` or
      :attr:`stride` is an int or a tuple of length 1, their values will be
      replicated across all spatial dimensions.

    * For the case of two input spatial dimensions this operation is sometimes
      called ``im2col``.

    .. note::
        :class:`~torch.nn.Fold` calculates each combined value in the resulting
        large tensor by summing all values from all containing blocks.
        :class:`~torch.nn.Unfold` extracts the values in the local blocks by
        copying from the large tensor. So, if the blocks overlap, they are not
        inverses of each other.

        In general, folding and unfolding operations are related as
        follows. Consider :class:`~torch.nn.Fold` and
        :class:`~torch.nn.Unfold` instances created with the same
        parameters:

        >>> fold_params = dict(kernel_size=..., dilation=..., padding=..., stride=...)
        >>> fold = nn.Fold(output_size=..., **fold_params)
        >>> unfold = nn.Unfold(**fold_params)

        Then for any (supported) ``input`` tensor the following
        equality holds:

        ::

            fold(unfold(input)) == divisor * input

        where ``divisor`` is a tensor that depends only on the shape
        and dtype of the ``input``:

        >>> input_ones = torch.ones(input.shape, dtype=input.dtype)
        >>> divisor = fold(unfold(input_ones))

        When the ``divisor`` tensor contains no zero elements, then
        ``fold`` and ``unfold`` operations are inverses of each
        other (up to constant divisor).

    .. warning::
        Currently, only 4-D input tensors (batched image-like tensors) are
        supported.

    Shape:
        - Input: :math:`(N, C, *)`
        - Output: :math:`(N, C \times \prod(\text{kernel\_size}), L)` as described above

    Examples::

        >>> unfold = nn.Unfold(kernel_size=(2, 3))
        >>> input = torch.randn(2, 5, 3, 4)
        >>> output = unfold(input)
        >>> # each patch contains 30 values (2x3=6 vectors, each of 5 channels)
        >>> # 4 blocks (2x3 kernels) in total in the 3x4 input
        >>> output.size()
        torch.Size([2, 30, 4])

        >>> # Convolution is equivalent with Unfold + Matrix Multiplication + Fold (or view to output shape)
        >>> inp = torch.randn(1, 3, 10, 12)
        >>> w = torch.randn(2, 3, 4, 5)
        >>> inp_unf = torch.nn.functional.unfold(inp, (4, 5))
        >>> out_unf = inp_unf.transpose(1, 2).matmul(w.view(w.size(0), -1).t()).transpose(1, 2)
        >>> out = torch.nn.functional.fold(out_unf, (7, 8), (1, 1))
        >>> # or equivalently (and avoiding a copy),
        >>> # out = out_unf.view(1, 2, 7, 8)
        >>> (torch.nn.functional.conv2d(inp, w) - out).abs().max()
        tensor(1.9073e-06)

    .. _link:
        https://github.com/vdumoulin/conv_arithmetic/blob/master/README.md

    """
    __constants__ = ...
    kernel_size: _size_any_t
    dilation: _size_any_t
    padding: _size_any_t
    stride: _size_any_t
    def __init__(self, kernel_size: _size_any_t, dilation: _size_any_t = ..., padding: _size_any_t = ..., stride: _size_any_t = ...) -> None:
        ...
    
    def forward(self, input: Tensor) -> Tensor:
        ...
    
    def extra_repr(self) -> str:
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.autograd.function import Function

class SyncBatchNorm(Function):
    @staticmethod
    def forward(self, input, weight, bias, running_mean, running_var, eps, momentum, process_group, world_size):
        ...
    
    @staticmethod
    def backward(self, grad_output):
        ...
    


class CrossMapLRN2d(Function):
    @staticmethod
    def forward(ctx, input, size, alpha=..., beta=..., k=...):
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Any, Optional
from torch import Tensor
from .module import Module

class Transformer(Module):
    r"""A transformer model. User is able to modify the attributes as needed. The architecture
    is based on the paper "Attention Is All You Need". Ashish Vaswani, Noam Shazeer,
    Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and
    Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information
    Processing Systems, pages 6000-6010. Users can build the BERT(https://arxiv.org/abs/1810.04805)
    model with corresponding parameters.

    Args:
        d_model: the number of expected features in the encoder/decoder inputs (default=512).
        nhead: the number of heads in the multiheadattention models (default=8).
        num_encoder_layers: the number of sub-encoder-layers in the encoder (default=6).
        num_decoder_layers: the number of sub-decoder-layers in the decoder (default=6).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of encoder/decoder intermediate layer, relu or gelu (default=relu).
        custom_encoder: custom encoder (default=None).
        custom_decoder: custom decoder (default=None).

    Examples::
        >>> transformer_model = nn.Transformer(nhead=16, num_encoder_layers=12)
        >>> src = torch.rand((10, 32, 512))
        >>> tgt = torch.rand((20, 32, 512))
        >>> out = transformer_model(src, tgt)

    Note: A full example to apply nn.Transformer module for the word language model is available in
    https://github.com/pytorch/examples/tree/master/word_language_model
    """
    def __init__(self, d_model: int = ..., nhead: int = ..., num_encoder_layers: int = ..., num_decoder_layers: int = ..., dim_feedforward: int = ..., dropout: float = ..., activation: str = ..., custom_encoder: Optional[Any] = ..., custom_decoder: Optional[Any] = ...) -> None:
        ...
    
    def forward(self, src: Tensor, tgt: Tensor, src_mask: Optional[Tensor] = ..., tgt_mask: Optional[Tensor] = ..., memory_mask: Optional[Tensor] = ..., src_key_padding_mask: Optional[Tensor] = ..., tgt_key_padding_mask: Optional[Tensor] = ..., memory_key_padding_mask: Optional[Tensor] = ...) -> Tensor:
        r"""Take in and process masked source/target sequences.

        Args:
            src: the sequence to the encoder (required).
            tgt: the sequence to the decoder (required).
            src_mask: the additive mask for the src sequence (optional).
            tgt_mask: the additive mask for the tgt sequence (optional).
            memory_mask: the additive mask for the encoder output (optional).
            src_key_padding_mask: the ByteTensor mask for src keys per batch (optional).
            tgt_key_padding_mask: the ByteTensor mask for tgt keys per batch (optional).
            memory_key_padding_mask: the ByteTensor mask for memory keys per batch (optional).

        Shape:
            - src: :math:`(S, N, E)`.
            - tgt: :math:`(T, N, E)`.
            - src_mask: :math:`(S, S)`.
            - tgt_mask: :math:`(T, T)`.
            - memory_mask: :math:`(T, S)`.
            - src_key_padding_mask: :math:`(N, S)`.
            - tgt_key_padding_mask: :math:`(N, T)`.
            - memory_key_padding_mask: :math:`(N, S)`.

            Note: [src/tgt/memory]_mask ensures that position i is allowed to attend the unmasked
            positions. If a ByteTensor is provided, the non-zero positions are not allowed to attend
            while the zero positions will be unchanged. If a BoolTensor is provided, positions with ``True``
            are not allowed to attend while ``False`` values will be unchanged. If a FloatTensor
            is provided, it will be added to the attention weight. 
            [src/tgt/memory]_key_padding_mask provides specified elements in the key to be ignored by
            the attention. If a ByteTensor is provided, the non-zero positions will be ignored while the zero
            positions will be unchanged. If a BoolTensor is provided, the positions with the
            value of ``True`` will be ignored while the position with the value of ``False`` will be unchanged.

            - output: :math:`(T, N, E)`.

            Note: Due to the multi-head attention architecture in the transformer model,
            the output sequence length of a transformer is same as the input sequence
            (i.e. target) length of the decode.

            where S is the source sequence length, T is the target sequence length, N is the
            batch size, E is the feature number

        Examples:
            >>> output = transformer_model(src, tgt, src_mask=src_mask, tgt_mask=tgt_mask)
        """
        ...
    
    def generate_square_subsequent_mask(self, sz: int) -> Tensor:
        r"""Generate a square mask for the sequence. The masked positions are filled with float('-inf').
            Unmasked positions are filled with float(0.0).
        """
        ...
    


class TransformerEncoder(Module):
    r"""TransformerEncoder is a stack of N encoder layers

    Args:
        encoder_layer: an instance of the TransformerEncoderLayer() class (required).
        num_layers: the number of sub-encoder-layers in the encoder (required).
        norm: the layer normalization component (optional).

    Examples::
        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
        >>> transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=6)
        >>> src = torch.rand(10, 32, 512)
        >>> out = transformer_encoder(src)
    """
    __constants__ = ...
    def __init__(self, encoder_layer, num_layers, norm=...) -> None:
        ...
    
    def forward(self, src: Tensor, mask: Optional[Tensor] = ..., src_key_padding_mask: Optional[Tensor] = ...) -> Tensor:
        r"""Pass the input through the encoder layers in turn.

        Args:
            src: the sequence to the encoder (required).
            mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        ...
    


class TransformerDecoder(Module):
    r"""TransformerDecoder is a stack of N decoder layers

    Args:
        decoder_layer: an instance of the TransformerDecoderLayer() class (required).
        num_layers: the number of sub-decoder-layers in the decoder (required).
        norm: the layer normalization component (optional).

    Examples::
        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
        >>> transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=6)
        >>> memory = torch.rand(10, 32, 512)
        >>> tgt = torch.rand(20, 32, 512)
        >>> out = transformer_decoder(tgt, memory)
    """
    __constants__ = ...
    def __init__(self, decoder_layer, num_layers, norm=...) -> None:
        ...
    
    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = ..., memory_mask: Optional[Tensor] = ..., tgt_key_padding_mask: Optional[Tensor] = ..., memory_key_padding_mask: Optional[Tensor] = ...) -> Tensor:
        r"""Pass the inputs (and mask) through the decoder layer in turn.

        Args:
            tgt: the sequence to the decoder (required).
            memory: the sequence from the last layer of the encoder (required).
            tgt_mask: the mask for the tgt sequence (optional).
            memory_mask: the mask for the memory sequence (optional).
            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
            memory_key_padding_mask: the mask for the memory keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        ...
    


class TransformerEncoderLayer(Module):
    r"""TransformerEncoderLayer is made up of self-attn and feedforward network.
    This standard encoder layer is based on the paper "Attention Is All You Need".
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
    in a different way during application.

    Args:
        d_model: the number of expected features in the input (required).
        nhead: the number of heads in the multiheadattention models (required).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of intermediate layer, relu or gelu (default=relu).

    Examples::
        >>> encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8)
        >>> src = torch.rand(10, 32, 512)
        >>> out = encoder_layer(src)
    """
    def __init__(self, d_model, nhead, dim_feedforward=..., dropout=..., activation=...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, src: Tensor, src_mask: Optional[Tensor] = ..., src_key_padding_mask: Optional[Tensor] = ...) -> Tensor:
        r"""Pass the input through the encoder layer.

        Args:
            src: the sequence to the encoder layer (required).
            src_mask: the mask for the src sequence (optional).
            src_key_padding_mask: the mask for the src keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        ...
    


class TransformerDecoderLayer(Module):
    r"""TransformerDecoderLayer is made up of self-attn, multi-head-attn and feedforward network.
    This standard decoder layer is based on the paper "Attention Is All You Need".
    Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
    Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in
    Neural Information Processing Systems, pages 6000-6010. Users may modify or implement
    in a different way during application.

    Args:
        d_model: the number of expected features in the input (required).
        nhead: the number of heads in the multiheadattention models (required).
        dim_feedforward: the dimension of the feedforward network model (default=2048).
        dropout: the dropout value (default=0.1).
        activation: the activation function of intermediate layer, relu or gelu (default=relu).

    Examples::
        >>> decoder_layer = nn.TransformerDecoderLayer(d_model=512, nhead=8)
        >>> memory = torch.rand(10, 32, 512)
        >>> tgt = torch.rand(20, 32, 512)
        >>> out = decoder_layer(tgt, memory)
    """
    def __init__(self, d_model, nhead, dim_feedforward=..., dropout=..., activation=...) -> None:
        ...
    
    def __setstate__(self, state):
        ...
    
    def forward(self, tgt: Tensor, memory: Tensor, tgt_mask: Optional[Tensor] = ..., memory_mask: Optional[Tensor] = ..., tgt_key_padding_mask: Optional[Tensor] = ..., memory_key_padding_mask: Optional[Tensor] = ...) -> Tensor:
        r"""Pass the inputs (and mask) through the decoder layer.

        Args:
            tgt: the sequence to the decoder layer (required).
            memory: the sequence from the last layer of the encoder (required).
            tgt_mask: the mask for the tgt sequence (optional).
            memory_mask: the mask for the memory sequence (optional).
            tgt_key_padding_mask: the mask for the tgt keys per batch (optional).
            memory_key_padding_mask: the mask for the memory keys per batch (optional).

        Shape:
            see the docs in Transformer class.
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
from typing import List, Optional, Tuple, overload
from torch import Tensor
from .module import Module
from ..parameter import Parameter
from ..utils.rnn import PackedSequence
from ... import _VF

_rnn_impls = { 'RNN_TANH': _VF.rnn_tanh,'RNN_RELU': _VF.rnn_relu }
def apply_permutation(tensor: Tensor, permutation: Tensor, dim: int = ...) -> Tensor:
    ...

class RNNBase(Module):
    __constants__ = ...
    mode: str
    input_size: int
    hidden_size: int
    num_layers: int
    bias: bool
    batch_first: bool
    dropout: float
    bidirectional: bool
    def __init__(self, mode: str, input_size: int, hidden_size: int, num_layers: int = ..., bias: bool = ..., batch_first: bool = ..., dropout: float = ..., bidirectional: bool = ...) -> None:
        ...
    
    def __setattr__(self, attr, value):
        ...
    
    def flatten_parameters(self) -> None:
        """Resets parameter data pointer so that they can use faster code paths.

        Right now, this works only if the module is on the GPU and cuDNN is enabled.
        Otherwise, it's a no-op.
        """
        ...
    
    def reset_parameters(self) -> None:
        ...
    
    def check_input(self, input: Tensor, batch_sizes: Optional[Tensor]) -> None:
        ...
    
    def get_expected_hidden_size(self, input: Tensor, batch_sizes: Optional[Tensor]) -> Tuple[int, int, int]:
        ...
    
    def check_hidden_size(self, hx: Tensor, expected_hidden_size: Tuple[int, int, int], msg: str = ...) -> None:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tensor, batch_sizes: Optional[Tensor]):
        ...
    
    def permute_hidden(self, hx: Tensor, permutation: Optional[Tensor]):
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
        ...
    
    def extra_repr(self) -> str:
        ...
    
    def __setstate__(self, d):
        ...
    
    @property
    def all_weights(self) -> List[Parameter]:
        ...
    


class RNN(RNNBase):
    r"""Applies a multi-layer Elman RNN with :math:`\tanh` or :math:`\text{ReLU}` non-linearity to an
    input sequence.


    For each element in the input sequence, each layer computes the following
    function:

    .. math::
        h_t = \tanh(W_{ih} x_t + b_{ih} + W_{hh} h_{(t-1)} + b_{hh})

    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is
    the input at time `t`, and :math:`h_{(t-1)}` is the hidden state of the
    previous layer at time `t-1` or the initial hidden state at time `0`.
    If :attr:`nonlinearity` is ``'relu'``, then :math:`\text{ReLU}` is used instead of :math:`\tanh`.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``
            would mean stacking two RNNs together to form a `stacked RNN`,
            with the second RNN taking in outputs of the first RNN and
            computing the final results. Default: 1
        nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``
        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.
            Default: ``True``
        batch_first: If ``True``, then the input and output tensors are provided
            as `(batch, seq, feature)`. Default: ``False``
        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each
            RNN layer except the last layer, with dropout probability equal to
            :attr:`dropout`. Default: 0
        bidirectional: If ``True``, becomes a bidirectional RNN. Default: ``False``

    Inputs: input, h_0
        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features
          of the input sequence. The input can also be a packed variable length
          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`
          or :func:`torch.nn.utils.rnn.pack_sequence`
          for details.
        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the initial hidden state for each element in the batch.
          Defaults to zero if not provided. If the RNN is bidirectional,
          num_directions should be 2, else it should be 1.

    Outputs: output, h_n
        - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor
          containing the output features (`h_t`) from the last layer of the RNN,
          for each `t`.  If a :class:`torch.nn.utils.rnn.PackedSequence` has
          been given as the input, the output will also be a packed sequence.

          For the unpacked case, the directions can be separated
          using ``output.view(seq_len, batch, num_directions, hidden_size)``,
          with forward and backward being direction `0` and `1` respectively.
          Similarly, the directions can be separated in the packed case.
        - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the hidden state for `t = seq_len`.

          Like *output*, the layers can be separated using
          ``h_n.view(num_layers, num_directions, batch, hidden_size)``.

    Shape:
        - Input1: :math:`(L, N, H_{in})` tensor containing input features where
          :math:`H_{in}=\text{input\_size}` and `L` represents a sequence length.
        - Input2: :math:`(S, N, H_{out})` tensor
          containing the initial hidden state for each element in the batch.
          :math:`H_{out}=\text{hidden\_size}`
          Defaults to zero if not provided. where :math:`S=\text{num\_layers} * \text{num\_directions}`
          If the RNN is bidirectional, num_directions should be 2, else it should be 1.
        - Output1: :math:`(L, N, H_{all})` where :math:`H_{all}=\text{num\_directions} * \text{hidden\_size}`
        - Output2: :math:`(S, N, H_{out})` tensor containing the next hidden state
          for each element in the batch

    Attributes:
        weight_ih_l[k]: the learnable input-hidden weights of the k-th layer,
            of shape `(hidden_size, input_size)` for `k = 0`. Otherwise, the shape is
            `(hidden_size, num_directions * hidden_size)`
        weight_hh_l[k]: the learnable hidden-hidden weights of the k-th layer,
            of shape `(hidden_size, hidden_size)`
        bias_ih_l[k]: the learnable input-hidden bias of the k-th layer,
            of shape `(hidden_size)`
        bias_hh_l[k]: the learnable hidden-hidden bias of the k-th layer,
            of shape `(hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    .. include:: ../cudnn_persistent_rnn.rst

    Examples::

        >>> rnn = nn.RNN(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> output, hn = rnn(input, h0)
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    


class LSTM(RNNBase):
    r"""Applies a multi-layer long short-term memory (LSTM) RNN to an input
    sequence.


    For each element in the input sequence, each layer computes the following
    function:

    .. math::
        \begin{array}{ll} \\
            i_t = \sigma(W_{ii} x_t + b_{ii} + W_{hi} h_{t-1} + b_{hi}) \\
            f_t = \sigma(W_{if} x_t + b_{if} + W_{hf} h_{t-1} + b_{hf}) \\
            g_t = \tanh(W_{ig} x_t + b_{ig} + W_{hg} h_{t-1} + b_{hg}) \\
            o_t = \sigma(W_{io} x_t + b_{io} + W_{ho} h_{t-1} + b_{ho}) \\
            c_t = f_t \odot c_{t-1} + i_t \odot g_t \\
            h_t = o_t \odot \tanh(c_t) \\
        \end{array}

    where :math:`h_t` is the hidden state at time `t`, :math:`c_t` is the cell
    state at time `t`, :math:`x_t` is the input at time `t`, :math:`h_{t-1}`
    is the hidden state of the layer at time `t-1` or the initial hidden
    state at time `0`, and :math:`i_t`, :math:`f_t`, :math:`g_t`,
    :math:`o_t` are the input, forget, cell, and output gates, respectively.
    :math:`\sigma` is the sigmoid function, and :math:`\odot` is the Hadamard product.

    In a multilayer LSTM, the input :math:`x^{(l)}_t` of the :math:`l` -th layer
    (:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by
    dropout :math:`\delta^{(l-1)}_t` where each :math:`\delta^{(l-1)}_t` is a Bernoulli random
    variable which is :math:`0` with probability :attr:`dropout`.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``
            would mean stacking two LSTMs together to form a `stacked LSTM`,
            with the second LSTM taking in outputs of the first LSTM and
            computing the final results. Default: 1
        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.
            Default: ``True``
        batch_first: If ``True``, then the input and output tensors are provided
            as (batch, seq, feature). Default: ``False``
        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each
            LSTM layer except the last layer, with dropout probability equal to
            :attr:`dropout`. Default: 0
        bidirectional: If ``True``, becomes a bidirectional LSTM. Default: ``False``

    Inputs: input, (h_0, c_0)
        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features
          of the input sequence.
          The input can also be a packed variable length sequence.
          See :func:`torch.nn.utils.rnn.pack_padded_sequence` or
          :func:`torch.nn.utils.rnn.pack_sequence` for details.
        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the initial hidden state for each element in the batch.
          If the LSTM is bidirectional, num_directions should be 2, else it should be 1.
        - **c_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the initial cell state for each element in the batch.

          If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.


    Outputs: output, (h_n, c_n)
        - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor
          containing the output features `(h_t)` from the last layer of the LSTM,
          for each `t`. If a :class:`torch.nn.utils.rnn.PackedSequence` has been
          given as the input, the output will also be a packed sequence.

          For the unpacked case, the directions can be separated
          using ``output.view(seq_len, batch, num_directions, hidden_size)``,
          with forward and backward being direction `0` and `1` respectively.
          Similarly, the directions can be separated in the packed case.
        - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the hidden state for `t = seq_len`.

          Like *output*, the layers can be separated using
          ``h_n.view(num_layers, num_directions, batch, hidden_size)`` and similarly for *c_n*.
        - **c_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the cell state for `t = seq_len`.

    Attributes:
        weight_ih_l[k] : the learnable input-hidden weights of the :math:`\text{k}^{th}` layer
            `(W_ii|W_if|W_ig|W_io)`, of shape `(4*hidden_size, input_size)` for `k = 0`.
            Otherwise, the shape is `(4*hidden_size, num_directions * hidden_size)`
        weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\text{k}^{th}` layer
            `(W_hi|W_hf|W_hg|W_ho)`, of shape `(4*hidden_size, hidden_size)`
        bias_ih_l[k] : the learnable input-hidden bias of the :math:`\text{k}^{th}` layer
            `(b_ii|b_if|b_ig|b_io)`, of shape `(4*hidden_size)`
        bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\text{k}^{th}` layer
            `(b_hi|b_hf|b_hg|b_ho)`, of shape `(4*hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    .. include:: ../cudnn_persistent_rnn.rst

    Examples::

        >>> rnn = nn.LSTM(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> c0 = torch.randn(2, 3, 20)
        >>> output, (hn, cn) = rnn(input, (h0, c0))
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def check_forward_args(self, input: Tensor, hidden: Tuple[Tensor, Tensor], batch_sizes: Optional[Tensor]):
        ...
    
    def permute_hidden(self, hx: Tuple[Tensor, Tensor], permutation: Optional[Tensor]) -> Tuple[Tensor, Tensor]:
        ...
    
    @overload
    @torch._jit_internal._overload_method
    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tuple[Tensor, Tensor]]:
        ...
    
    @overload
    @torch._jit_internal._overload_method
    def forward(self, input: PackedSequence, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[PackedSequence, Tuple[Tensor, Tensor]]:
        ...
    
    def forward(self, input, hx=...):
        ...
    


class GRU(RNNBase):
    r"""Applies a multi-layer gated recurrent unit (GRU) RNN to an input sequence.


    For each element in the input sequence, each layer computes the following
    function:

    .. math::
        \begin{array}{ll}
            r_t = \sigma(W_{ir} x_t + b_{ir} + W_{hr} h_{(t-1)} + b_{hr}) \\
            z_t = \sigma(W_{iz} x_t + b_{iz} + W_{hz} h_{(t-1)} + b_{hz}) \\
            n_t = \tanh(W_{in} x_t + b_{in} + r_t * (W_{hn} h_{(t-1)}+ b_{hn})) \\
            h_t = (1 - z_t) * n_t + z_t * h_{(t-1)}
        \end{array}

    where :math:`h_t` is the hidden state at time `t`, :math:`x_t` is the input
    at time `t`, :math:`h_{(t-1)}` is the hidden state of the layer
    at time `t-1` or the initial hidden state at time `0`, and :math:`r_t`,
    :math:`z_t`, :math:`n_t` are the reset, update, and new gates, respectively.
    :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.

    In a multilayer GRU, the input :math:`x^{(l)}_t` of the :math:`l` -th layer
    (:math:`l >= 2`) is the hidden state :math:`h^{(l-1)}_t` of the previous layer multiplied by
    dropout :math:`\delta^{(l-1)}_t` where each :math:`\delta^{(l-1)}_t` is a Bernoulli random
    variable which is :math:`0` with probability :attr:`dropout`.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        num_layers: Number of recurrent layers. E.g., setting ``num_layers=2``
            would mean stacking two GRUs together to form a `stacked GRU`,
            with the second GRU taking in outputs of the first GRU and
            computing the final results. Default: 1
        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.
            Default: ``True``
        batch_first: If ``True``, then the input and output tensors are provided
            as (batch, seq, feature). Default: ``False``
        dropout: If non-zero, introduces a `Dropout` layer on the outputs of each
            GRU layer except the last layer, with dropout probability equal to
            :attr:`dropout`. Default: 0
        bidirectional: If ``True``, becomes a bidirectional GRU. Default: ``False``

    Inputs: input, h_0
        - **input** of shape `(seq_len, batch, input_size)`: tensor containing the features
          of the input sequence. The input can also be a packed variable length
          sequence. See :func:`torch.nn.utils.rnn.pack_padded_sequence`
          for details.
        - **h_0** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the initial hidden state for each element in the batch.
          Defaults to zero if not provided. If the RNN is bidirectional,
          num_directions should be 2, else it should be 1.

    Outputs: output, h_n
        - **output** of shape `(seq_len, batch, num_directions * hidden_size)`: tensor
          containing the output features h_t from the last layer of the GRU,
          for each `t`. If a :class:`torch.nn.utils.rnn.PackedSequence` has been
          given as the input, the output will also be a packed sequence.
          For the unpacked case, the directions can be separated
          using ``output.view(seq_len, batch, num_directions, hidden_size)``,
          with forward and backward being direction `0` and `1` respectively.

          Similarly, the directions can be separated in the packed case.
        - **h_n** of shape `(num_layers * num_directions, batch, hidden_size)`: tensor
          containing the hidden state for `t = seq_len`

          Like *output*, the layers can be separated using
          ``h_n.view(num_layers, num_directions, batch, hidden_size)``.

    Shape:
        - Input1: :math:`(L, N, H_{in})` tensor containing input features where
          :math:`H_{in}=\text{input\_size}` and `L` represents a sequence length.
        - Input2: :math:`(S, N, H_{out})` tensor
          containing the initial hidden state for each element in the batch.
          :math:`H_{out}=\text{hidden\_size}`
          Defaults to zero if not provided. where :math:`S=\text{num\_layers} * \text{num\_directions}`
          If the RNN is bidirectional, num_directions should be 2, else it should be 1.
        - Output1: :math:`(L, N, H_{all})` where :math:`H_{all}=\text{num\_directions} * \text{hidden\_size}`
        - Output2: :math:`(S, N, H_{out})` tensor containing the next hidden state
          for each element in the batch

    Attributes:
        weight_ih_l[k] : the learnable input-hidden weights of the :math:`\text{k}^{th}` layer
            (W_ir|W_iz|W_in), of shape `(3*hidden_size, input_size)` for `k = 0`.
            Otherwise, the shape is `(3*hidden_size, num_directions * hidden_size)`
        weight_hh_l[k] : the learnable hidden-hidden weights of the :math:`\text{k}^{th}` layer
            (W_hr|W_hz|W_hn), of shape `(3*hidden_size, hidden_size)`
        bias_ih_l[k] : the learnable input-hidden bias of the :math:`\text{k}^{th}` layer
            (b_ir|b_iz|b_in), of shape `(3*hidden_size)`
        bias_hh_l[k] : the learnable hidden-hidden bias of the :math:`\text{k}^{th}` layer
            (b_hr|b_hz|b_hn), of shape `(3*hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    .. include:: ../cudnn_persistent_rnn.rst

    Examples::

        >>> rnn = nn.GRU(10, 20, 2)
        >>> input = torch.randn(5, 3, 10)
        >>> h0 = torch.randn(2, 3, 20)
        >>> output, hn = rnn(input, h0)
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    @overload
    @torch._jit_internal._overload_method
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
        ...
    
    @overload
    @torch._jit_internal._overload_method
    def forward(self, input: PackedSequence, hx: Optional[Tensor] = ...) -> Tuple[PackedSequence, Tensor]:
        ...
    
    def forward(self, input, hx=...):
        ...
    


class RNNCellBase(Module):
    __constants__ = ...
    input_size: int
    hidden_size: int
    bias: bool
    weight_ih: Tensor
    weight_hh: Tensor
    def __init__(self, input_size: int, hidden_size: int, bias: bool, num_chunks: int) -> None:
        ...
    
    def extra_repr(self) -> str:
        ...
    
    def check_forward_input(self, input: Tensor) -> None:
        ...
    
    def check_forward_hidden(self, input: Tensor, hx: Tensor, hidden_label: str = ...) -> None:
        ...
    
    def reset_parameters(self) -> None:
        ...
    


class RNNCell(RNNCellBase):
    r"""An Elman RNN cell with tanh or ReLU non-linearity.

    .. math::

        h' = \tanh(W_{ih} x + b_{ih}  +  W_{hh} h + b_{hh})

    If :attr:`nonlinearity` is `'relu'`, then ReLU is used in place of tanh.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        bias: If ``False``, then the layer does not use bias weights `b_ih` and `b_hh`.
            Default: ``True``
        nonlinearity: The non-linearity to use. Can be either ``'tanh'`` or ``'relu'``. Default: ``'tanh'``

    Inputs: input, hidden
        - **input** of shape `(batch, input_size)`: tensor containing input features
        - **hidden** of shape `(batch, hidden_size)`: tensor containing the initial hidden
          state for each element in the batch.
          Defaults to zero if not provided.

    Outputs: h'
        - **h'** of shape `(batch, hidden_size)`: tensor containing the next hidden state
          for each element in the batch

    Shape:
        - Input1: :math:`(N, H_{in})` tensor containing input features where
          :math:`H_{in}` = `input_size`
        - Input2: :math:`(N, H_{out})` tensor containing the initial hidden
          state for each element in the batch where :math:`H_{out}` = `hidden_size`
          Defaults to zero if not provided.
        - Output: :math:`(N, H_{out})` tensor containing the next hidden state
          for each element in the batch

    Attributes:
        weight_ih: the learnable input-hidden weights, of shape
            `(hidden_size, input_size)`
        weight_hh: the learnable hidden-hidden weights, of shape
            `(hidden_size, hidden_size)`
        bias_ih: the learnable input-hidden bias, of shape `(hidden_size)`
        bias_hh: the learnable hidden-hidden bias, of shape `(hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    Examples::

        >>> rnn = nn.RNNCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    __constants__ = ...
    nonlinearity: str
    def __init__(self, input_size: int, hidden_size: int, bias: bool = ..., nonlinearity: str = ...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    


class LSTMCell(RNNCellBase):
    r"""A long short-term memory (LSTM) cell.

    .. math::

        \begin{array}{ll}
        i = \sigma(W_{ii} x + b_{ii} + W_{hi} h + b_{hi}) \\
        f = \sigma(W_{if} x + b_{if} + W_{hf} h + b_{hf}) \\
        g = \tanh(W_{ig} x + b_{ig} + W_{hg} h + b_{hg}) \\
        o = \sigma(W_{io} x + b_{io} + W_{ho} h + b_{ho}) \\
        c' = f * c + i * g \\
        h' = o * \tanh(c') \\
        \end{array}

    where :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        bias: If ``False``, then the layer does not use bias weights `b_ih` and
            `b_hh`. Default: ``True``

    Inputs: input, (h_0, c_0)
        - **input** of shape `(batch, input_size)`: tensor containing input features
        - **h_0** of shape `(batch, hidden_size)`: tensor containing the initial hidden
          state for each element in the batch.
        - **c_0** of shape `(batch, hidden_size)`: tensor containing the initial cell state
          for each element in the batch.

          If `(h_0, c_0)` is not provided, both **h_0** and **c_0** default to zero.

    Outputs: (h_1, c_1)
        - **h_1** of shape `(batch, hidden_size)`: tensor containing the next hidden state
          for each element in the batch
        - **c_1** of shape `(batch, hidden_size)`: tensor containing the next cell state
          for each element in the batch

    Attributes:
        weight_ih: the learnable input-hidden weights, of shape
            `(4*hidden_size, input_size)`
        weight_hh: the learnable hidden-hidden weights, of shape
            `(4*hidden_size, hidden_size)`
        bias_ih: the learnable input-hidden bias, of shape `(4*hidden_size)`
        bias_hh: the learnable hidden-hidden bias, of shape `(4*hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    Examples::

        >>> rnn = nn.LSTMCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> cx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx, cx = rnn(input[i], (hx, cx))
                output.append(hx)
    """
    def __init__(self, input_size: int, hidden_size: int, bias: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tuple[Tensor, Tensor]] = ...) -> Tuple[Tensor, Tensor]:
        ...
    


class GRUCell(RNNCellBase):
    r"""A gated recurrent unit (GRU) cell

    .. math::

        \begin{array}{ll}
        r = \sigma(W_{ir} x + b_{ir} + W_{hr} h + b_{hr}) \\
        z = \sigma(W_{iz} x + b_{iz} + W_{hz} h + b_{hz}) \\
        n = \tanh(W_{in} x + b_{in} + r * (W_{hn} h + b_{hn})) \\
        h' = (1 - z) * n + z * h
        \end{array}

    where :math:`\sigma` is the sigmoid function, and :math:`*` is the Hadamard product.

    Args:
        input_size: The number of expected features in the input `x`
        hidden_size: The number of features in the hidden state `h`
        bias: If ``False``, then the layer does not use bias weights `b_ih` and
            `b_hh`. Default: ``True``

    Inputs: input, hidden
        - **input** of shape `(batch, input_size)`: tensor containing input features
        - **hidden** of shape `(batch, hidden_size)`: tensor containing the initial hidden
          state for each element in the batch.
          Defaults to zero if not provided.

    Outputs: h'
        - **h'** of shape `(batch, hidden_size)`: tensor containing the next hidden state
          for each element in the batch

    Shape:
        - Input1: :math:`(N, H_{in})` tensor containing input features where
          :math:`H_{in}` = `input_size`
        - Input2: :math:`(N, H_{out})` tensor containing the initial hidden
          state for each element in the batch where :math:`H_{out}` = `hidden_size`
          Defaults to zero if not provided.
        - Output: :math:`(N, H_{out})` tensor containing the next hidden state
          for each element in the batch

    Attributes:
        weight_ih: the learnable input-hidden weights, of shape
            `(3*hidden_size, input_size)`
        weight_hh: the learnable hidden-hidden weights, of shape
            `(3*hidden_size, hidden_size)`
        bias_ih: the learnable input-hidden bias, of shape `(3*hidden_size)`
        bias_hh: the learnable hidden-hidden bias, of shape `(3*hidden_size)`

    .. note::
        All the weights and biases are initialized from :math:`\mathcal{U}(-\sqrt{k}, \sqrt{k})`
        where :math:`k = \frac{1}{\text{hidden\_size}}`

    Examples::

        >>> rnn = nn.GRUCell(10, 20)
        >>> input = torch.randn(6, 3, 10)
        >>> hx = torch.randn(3, 20)
        >>> output = []
        >>> for i in range(6):
                hx = rnn(input[i], hx)
                output.append(hx)
    """
    def __init__(self, input_size: int, hidden_size: int, bias: bool = ...) -> None:
        ...
    
    def forward(self, input: Tensor, hx: Optional[Tensor] = ...) -> Tensor:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Optional, Tuple
from torch import Tensor

"""Implement various linear algebra algorithms for low rank matrices.
"""
def get_approximate_basis(A: Tensor, q: int, niter: Optional[int] = ..., M: Optional[Tensor] = ...) -> Tensor:
    """Return tensor :math:`Q` with :math:`q` orthonormal columns such
    that :math:`Q Q^H A` approximates :math:`A`. If :math:`M` is
    specified, then :math:`Q` is such that :math:`Q Q^H (A - M)`
    approximates :math:`A - M`.

    .. note:: The implementation is based on the Algorithm 4.4 from
              Halko et al, 2009.

    .. note:: For an adequate approximation of a k-rank matrix
              :math:`A`, where k is not known in advance but could be
              estimated, the number of :math:`Q` columns, q, can be
              choosen according to the following criteria: in general,
              :math:`k <= q <= min(2*k, m, n)`. For large low-rank
              matrices, take :math:`q = k + 5..10`.  If k is
              relatively small compared to :math:`min(m, n)`, choosing
              :math:`q = k + 0..2` may be sufficient.

    .. note:: To obtain repeatable results, reset the seed for the
              pseudorandom number generator

    Arguments::
        A (Tensor): the input tensor of size :math:`(*, m, n)`

        q (int): the dimension of subspace spanned by :math:`Q`
                 columns.

        niter (int, optional): the number of subspace iterations to
                               conduct; ``niter`` must be a
                               nonnegative integer. In most cases, the
                               default value 2 is more than enough.

        M (Tensor, optional): the input tensor's mean of size
                              :math:`(*, 1, n)`.

    References::
        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding
          structure with randomness: probabilistic algorithms for
          constructing approximate matrix decompositions,
          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at
          `arXiv <http://arxiv.org/abs/0909.4061>`_).
    """
    ...

def svd_lowrank(A: Tensor, q: Optional[int] = ..., niter: Optional[int] = ..., M: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor, Tensor]:
    r"""Return the singular value decomposition ``(U, S, V)`` of a matrix,
    batches of matrices, or a sparse matrix :math:`A` such that
    :math:`A \approx U diag(S) V^T`. In case :math:`M` is given, then
    SVD is computed for the matrix :math:`A - M`.

    .. note:: The implementation is based on the Algorithm 5.1 from
              Halko et al, 2009.

    .. note:: To obtain repeatable results, reset the seed for the
              pseudorandom number generator

    .. note:: The input is assumed to be a low-rank matrix.

    .. note:: In general, use the full-rank SVD implementation
              ``torch.svd`` for dense matrices due to its 10-fold
              higher performance characteristics. The low-rank SVD
              will be useful for huge sparse matrices that
              ``torch.svd`` cannot handle.

    Arguments::
        A (Tensor): the input tensor of size :math:`(*, m, n)`

        q (int, optional): a slightly overestimated rank of A.

        niter (int, optional): the number of subspace iterations to
                               conduct; niter must be a nonnegative
                               integer, and defaults to 2

        M (Tensor, optional): the input tensor's mean of size
                              :math:`(*, 1, n)`.

    References::
        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding
          structure with randomness: probabilistic algorithms for
          constructing approximate matrix decompositions,
          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at
          `arXiv <http://arxiv.org/abs/0909.4061>`_).

    """
    ...

def pca_lowrank(A: Tensor, q: Optional[int] = ..., center: bool = ..., niter: int = ...) -> Tuple[Tensor, Tensor, Tensor]:
    r"""Performs linear Principal Component Analysis (PCA) on a low-rank
    matrix, batches of such matrices, or sparse matrix.

    This function returns a namedtuple ``(U, S, V)`` which is the
    nearly optimal approximation of a singular value decomposition of
    a centered matrix :math:`A` such that :math:`A = U diag(S) V^T`.

    .. note:: The relation of ``(U, S, V)`` to PCA is as follows:

                - :math:`A` is a data matrix with ``m`` samples and
                  ``n`` features

                - the :math:`V` columns represent the principal directions

                - :math:`S ** 2 / (m - 1)` contains the eigenvalues of
                  :math:`A^T A / (m - 1)` which is the covariance of
                  ``A`` when ``center=True`` is provided.

                - ``matmul(A, V[:, :k])`` projects data to the first k
                  principal components

    .. note:: Different from the standard SVD, the size of returned
              matrices depend on the specified rank and q
              values as follows:

                - :math:`U` is m x q matrix

                - :math:`S` is q-vector

                - :math:`V` is n x q matrix

    .. note:: To obtain repeatable results, reset the seed for the
              pseudorandom number generator

    Arguments:

        A (Tensor): the input tensor of size :math:`(*, m, n)`

        q (int, optional): a slightly overestimated rank of
                           :math:`A`. By default, ``q = min(6, m,
                           n)``.

        center (bool, optional): if True, center the input tensor,
                                 otherwise, assume that the input is
                                 centered.

        niter (int, optional): the number of subspace iterations to
                               conduct; niter must be a nonnegative
                               integer, and defaults to 2.

    References::

        - Nathan Halko, Per-Gunnar Martinsson, and Joel Tropp, Finding
          structure with randomness: probabilistic algorithms for
          constructing approximate matrix decompositions,
          arXiv:0909.4061 [math.NA; math.PR], 2009 (available at
          `arXiv <http://arxiv.org/abs/0909.4061>`_).

    """
    ...

"""
This type stub file was generated by pyright.
"""

_registry = {  }
_symbolic_versions = {  }
def register_version(domain, version):
    ...

def register_ops_helper(domain, version, iter_version):
    ...

def register_ops_in_version(domain, version):
    ...

def get_ops_in_version(version):
    ...

def is_registered_version(domain, version):
    ...

def register_op(opname, op, domain, version):
    ...

def is_registered_op(opname, domain, version):
    ...

def get_op_supported_version(opname, domain, version):
    ...

def get_registered_op(opname, domain, version):
    ...

"""
This type stub file was generated by pyright.
"""

import torch._C as _C

TensorProtoDataType = _C._onnx.TensorProtoDataType
OperatorExportTypes = _C._onnx.OperatorExportTypes
TrainingMode = _C._onnx.TrainingMode
PYTORCH_ONNX_CAFFE2_BUNDLE = _C._onnx.PYTORCH_ONNX_CAFFE2_BUNDLE
ONNX_ARCHIVE_MODEL_PROTO_NAME = "__MODEL_PROTO"
ir_version = _C._onnx.IR_VERSION
producer_name = "pytorch"
producer_version = _C._onnx.PRODUCER_VERSION
constant_folding_opset_versions = [9, 10, 11, 12]
class ExportTypes:
    PROTOBUF_FILE = ...
    ZIP_ARCHIVE = ...
    COMPRESSED_ZIP_ARCHIVE = ...
    DIRECTORY = ...


def export(model, args, f, export_params=..., verbose=..., training=..., input_names=..., output_names=..., aten=..., export_raw_ir=..., operator_export_type=..., opset_version=..., _retain_param_name=..., do_constant_folding=..., example_outputs=..., strip_doc_string=..., dynamic_axes=..., keep_initializers_as_inputs=..., custom_opsets=..., enable_onnx_checker=..., use_external_data_format=...):
    r"""
    Export a model into ONNX format.  This exporter runs your model
    once in order to get a trace of its execution to be exported;
    at the moment, it supports a limited set of dynamic models (e.g., RNNs.)

    Arguments:
        model (torch.nn.Module): the model to be exported.
        args (tuple of arguments): the inputs to
            the model, e.g., such that ``model(*args)`` is a valid
            invocation of the model.  Any non-Tensor arguments will
            be hard-coded into the exported model; any Tensor arguments
            will become inputs of the exported model, in the order they
            occur in args.  If args is a Tensor, this is equivalent
            to having called it with a 1-ary tuple of that Tensor.
            (Note: passing keyword arguments to the model is not currently
            supported.  Give us a shout if you need it.)
        f: a file-like object (has to implement fileno that returns a file descriptor)
            or a string containing a file name.  A binary Protobuf will be written
            to this file.
        export_params (bool, default True): if specified, all parameters will
            be exported.  Set this to False if you want to export an untrained model.
            In this case, the exported model will first take all of its parameters
            as arguments, the ordering as specified by ``model.state_dict().values()``
        verbose (bool, default False): if specified, we will print out a debug
            description of the trace being exported.
        training (enum, default TrainingMode.EVAL):
            TrainingMode.EVAL: export the model in inference mode.
            TrainingMode.PRESERVE: export the model in inference mode if model.training is
            False and to a training friendly mode if model.training is True.
            TrainingMode.TRAINING: export the model in a training friendly mode.
        input_names(list of strings, default empty list): names to assign to the
            input nodes of the graph, in order
        output_names(list of strings, default empty list): names to assign to the
            output nodes of the graph, in order
        aten (bool, default False): [DEPRECATED. use operator_export_type] export the
            model in aten mode. If using aten mode, all the ops original exported
            by the functions in symbolic_opset<version>.py are exported as ATen ops.
        export_raw_ir (bool, default False): [DEPRECATED. use operator_export_type]
            export the internal IR directly instead of converting it to ONNX ops.
        operator_export_type (enum, default OperatorExportTypes.ONNX):
            OperatorExportTypes.ONNX: All ops are exported as regular ONNX ops
            (with ONNX namespace).
            OperatorExportTypes.ONNX_ATEN: All ops are exported as ATen ops
            (with aten namespace).
            OperatorExportTypes.ONNX_ATEN_FALLBACK: If an ATen op is not supported
            in ONNX or its symbolic is missing, fall back on ATen op. Registered ops
            are exported to ONNX regularly.
            Example graph:
                graph(%0 : Float):
                  %3 : int = prim::Constant[value=0]()
                  %4 : Float = aten::triu(%0, %3) # missing op
                  %5 : Float = aten::mul(%4, %0) # registered op
                  return (%5)
            is exported as:
                graph(%0 : Float):
                  %1 : Long() = onnx::Constant[value={0}]()
                  %2 : Float = aten::ATen[operator="triu"](%0, %1)  # missing op
                  %3 : Float = onnx::Mul(%2, %0) # registered op
                  return (%3)
            In the above example, aten::triu is not supported in ONNX, hence
            exporter falls back on this op.
            OperatorExportTypes.RAW: Export raw ir.
            OperatorExportTypes.ONNX_FALLTHROUGH: If an op is not supported
            in ONNX, fall through and export the operator as is, as a custom 
            ONNX op. Using this mode, the op can be exported and implemented by
            the user for their runtime backend.
            Example graph:
                graph(%x.1 : Long(1:1)):
                  %1 : None = prim::Constant()
                  %2 : Tensor = aten::sum(%x.1, %1)
                  %y.1 : Tensor[] = prim::ListConstruct(%2)
                  return (%y.1)
            is exported as:
                graph(%x.1 : Long(1:1)):
                  %1 : Tensor = onnx::ReduceSum[keepdims=0](%x.1)
                  %y.1 : Long() = prim::ListConstruct(%1)
                  return (%y.1)
            In the above example, prim::ListConstruct is not supported, hence
            exporter falls through.

        opset_version (int, default is 9): by default we export the model to the
            opset version of the onnx submodule. Since ONNX's latest opset may
            evolve before next stable release, by default we export to one stable
            opset version. Right now, supported stable opset version is 9.
            The opset_version must be _onnx_master_opset or in _onnx_stable_opsets
            which are defined in torch/onnx/symbolic_helper.py
        do_constant_folding (bool, default False): If True, the constant-folding
            optimization is applied to the model during export. Constant-folding
            optimization will replace some of the ops that have all constant
            inputs, with pre-computed constant nodes.
        example_outputs (tuple of Tensors, default None): Model's example outputs being exported.
            example_outputs must be provided when exporting a ScriptModule or TorchScript Function.
        strip_doc_string (bool, default True): if True, strips the field
            "doc_string" from the exported model, which information about the stack
            trace.
        dynamic_axes (dict<string, dict<int, string>> or dict<string, list(int)>, default empty dict):
            a dictionary to specify dynamic axes of input/output, such that:
            - KEY:  input and/or output names
            - VALUE: index of dynamic axes for given key and potentially the name to be used for
            exported dynamic axes. In general the value is defined according to one of the following
            ways or a combination of both:
            (1). A list of integers specifying the dynamic axes of provided input. In this scenario
            automated names will be generated and applied to dynamic axes of provided input/output
            during export.
            OR (2). An inner dictionary that specifies a mapping FROM the index of dynamic axis in
            corresponding input/output TO the name that is desired to be applied on such axis of
            such input/output during export.

            Example. if we have the following shape for inputs and outputs:

            .. code-block:: none

                shape(input_1) = ('b', 3, 'w', 'h')
                and shape(input_2) = ('b', 4)
                and shape(output)  = ('b', 'd', 5)

            Then dynamic axes can be defined either as:
                (a). ONLY INDICES:
                    dynamic_axes = {'input_1':[0, 2, 3], 'input_2':[0], 'output':[0, 1]}

                    where automatic names will be generated for exported dynamic axes

                (b). INDICES WITH CORRESPONDING NAMES:
                    dynamic_axes = {'input_1':{0:'batch', 1:'width', 2:'height'},
                    'input_2':{0:'batch'},
                    'output':{0:'batch', 1:'detections'}

                    where provided names will be applied to exported dynamic axes

                (c). MIXED MODE OF (a) and (b)
                    dynamic_axes = {'input_1':[0, 2, 3], 'input_2':{0:'batch'}, 'output':[0,1]}
        keep_initializers_as_inputs (bool, default None): If True, all the initializers
            (typically corresponding to parameters) in the exported graph will also be
            added as inputs to the graph. If False, then initializers are not added as
            inputs to the graph, and only the non-parameter inputs are added as inputs.
            This may allow for better optimizations (such as constant folding etc.) by
            backends/runtimes that execute these graphs. If unspecified (default None),
            then the behavior is chosen automatically as follows. If operator_export_type
            is OperatorExportTypes.ONNX, the behavior is equivalent to setting this
            argument to False. For other values of operator_export_type, the behavior is
            equivalent to setting this argument to True. Note that for ONNX opset version < 9,
            initializers MUST be part of graph inputs. Therefore, if opset_version argument is
            set to a 8 or lower, this argument will be ignored.
        custom_opsets (dict<string, int>, default empty dict): A dictionary to indicate
            custom opset domain and version at export. If model contains a custom opset,
            it is optional to specify the domain and opset version in the dictionary:
            - KEY: opset domain name
            - VALUE: opset version
            If the custom opset is not provided in this dictionary, opset version is set
            to 1 by default.
        enable_onnx_checker (bool, default True): If True the onnx model checker will be run
            as part of the export, to ensure the exported model is a valid ONNX model.
        external_data_format (bool, default False): If True, then the model is exported
            in ONNX external data format, in which case some of the model parameters are stored
            in external binary files and not in the ONNX model file itself. See link for format
            details: 
            https://github.com/onnx/onnx/blob/8b3f7e2e7a0f2aba0e629e23d89f07c7fc0e6a5e/onnx/onnx.proto#L423
            Also, in this case,  argument 'f' must be a string specifying the location of the model.
            The external binary files will be stored in the same location specified by the model 
            location 'f'. If False, then the model is stored in regular format, i.e. model and
            parameters are all in one file. This argument is ignored for all export types other
            than ONNX. 
    """
    ...

def export_to_pretty_string(*args, **kwargs):
    ...

def select_model_mode_for_export(model, mode):
    r"""
    A context manager to temporarily set the training mode of 'model'
    to 'mode', resetting it when we exit the with-block.  A no-op if
    mode is None.

    In version 1.6 changed to this from set_training
    """
    ...

def is_in_onnx_export():
    r"""
    Check whether it's in the middle of the ONNX export.
    This function returns True in the middle of torch.onnx.export().
    torch.onnx.export should be executed with single thread.
    """
    ...

def register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version):
    ...

"""
This type stub file was generated by pyright.
"""

import re
import contextlib

__IN_ONNX_EXPORT = False
def is_in_onnx_export():
    ...

@contextlib.contextmanager
def select_model_mode_for_export(model, mode):
    ...

def export(model, args, f, export_params=..., verbose=..., training=..., input_names=..., output_names=..., aten=..., export_raw_ir=..., operator_export_type=..., opset_version=..., _retain_param_name=..., do_constant_folding=..., example_outputs=..., strip_doc_string=..., dynamic_axes=..., keep_initializers_as_inputs=..., custom_opsets=..., enable_onnx_checker=..., use_external_data_format=...):
    ...

def warn_on_static_input_change(input_states):
    ...

def export_to_pretty_string(model, args, f, export_params=..., verbose=..., training=..., input_names=..., output_names=..., aten=..., export_raw_ir=..., operator_export_type=..., export_type=..., example_outputs=..., propagate=..., google_printer=..., opset_version=..., _retain_param_name=..., keep_initializers_as_inputs=..., custom_opsets=..., add_node_names=..., do_constant_folding=...):
    ...

attr_pattern = re.compile("^(.+)_([ifstgz])$")
def register_custom_op_symbolic(symbolic_name, symbolic_fn, opset_version):
    ...

"""
This type stub file was generated by pyright.
"""

from torch.nn.modules.utils import _pair, _single, _triple
from torch.onnx.symbolic_helper import parse_args

def unused(g):
    ...

def reshape(g, self, shape):
    ...

def reshape_as(g, self, other):
    ...

def add(g, self, other, alpha=...):
    ...

def sub(g, self, other, alpha=...):
    ...

def rsub(g, self, other, alpha=...):
    ...

def mul(g, self, other):
    ...

def div(g, self, other):
    ...

def floor_divide(g, self, other):
    ...

def true_divide(g, self, other):
    ...

def reciprocal(g, self):
    ...

@parse_args('v', 'i')
def cat(g, tensor_list, dim):
    ...

@parse_args('v', 'i')
def stack(g, tensor_list, dim):
    ...

def mm(g, self, other):
    ...

def bmm(g, self, other):
    ...

def matmul(g, self, other):
    ...

@parse_args('v', 'v', 'v', 't', 't')
def addmm(g, self, mat1, mat2, beta, alpha):
    ...

def neg(g, self):
    ...

def sqrt(g, self):
    ...

def rsqrt(g, self):
    ...

def tanh(g, self):
    ...

def sin(g, self):
    ...

def cos(g, self):
    ...

def tan(g, self):
    ...

def asin(g, self):
    ...

def acos(g, self):
    ...

def atan(g, self):
    ...

def sigmoid(g, self):
    ...

def sign(g, self):
    ...

def overload_by_arg_count(fn):
    ...

sum = _reduce_with_dtype('ReduceSum', 'sum')
mean = _reduce_with_dtype('ReduceMean', 'mean')
prod = _reduce_with_dtype('ReduceProd', 'prod', allow_multi_dim_support=False)
@parse_args('v', 'i', 'none')
def cumsum(g, input, dim, dtype):
    ...

def t(g, self):
    ...

def expand(g, self, size, implicit):
    ...

def expand_as(g, self, other):
    ...

def embedding(g, weight, indices, padding_idx, scale_grad_by_freq, sparse):
    ...

@parse_args('v', 'v', 'v', 'i', 'i', 'i', 'v', 'i')
def embedding_bag(g, embedding_matrix, indices, offsets, scale_grad_by_freq, mode, sparse, per_sample_weights, include_last_offset):
    ...

def size(g, self, dim=...):
    ...

@parse_args('v', 'i', 'i')
def transpose(g, self, dim0, dim1):
    ...

@parse_args('v', 'is')
def permute(g, self, dims):
    ...

def view(g, self, size):
    ...

def prim_ConstantSplit(g, self, split_size, dim):
    ...

def prim_ConstantChunk(g, self, chunks, dim):
    ...

def split(g, self, split_size_or_sizes, dim):
    ...

@parse_args('v', 'is', 'i')
def split_with_sizes(g, self, split_sizes, dim):
    ...

@parse_args('v', 'i')
def unbind(g, self, dim=...):
    ...

@parse_args('v', 'i', 'v')
def select(g, self, dim, index):
    ...

def squeeze(g, self, dim=...):
    ...

def prelu(g, self, weight):
    ...

def relu(g, input):
    ...

def ceil(g, input):
    ...

def floor(g, input):
    ...

@parse_args('v', 't', 't')
def threshold(g, self, threshold, value):
    ...

def leaky_relu(g, input, negative_slope, inplace=...):
    ...

@parse_args('v', 'i')
def glu(g, input, dim):
    ...

@parse_args('v', 'i', 'none')
def softmax(g, input, dim, dtype=...):
    ...

@parse_args('v', 't', 'v')
def softplus(g, self, beta, threshold):
    ...

def get_pool_ceil_padding(input, kernel_size, stride, padding):
    ...

max_pool1d = _max_pool("max_pool1d", _single, 1, return_indices=False)
max_pool2d = _max_pool("max_pool2d", _pair, 2, return_indices=False)
max_pool3d = _max_pool("max_pool3d", _triple, 3, return_indices=False)
max_pool1d_with_indices = _max_pool("max_pool1d_with_indices", _single, 1, return_indices=True)
max_pool2d_with_indices = _max_pool("max_pool2d_with_indices", _pair, 2, return_indices=True)
max_pool3d_with_indices = _max_pool("max_pool3d_with_indices", _triple, 3, return_indices=True)
avg_pool1d = _avg_pool('avg_pool1d', _single)
avg_pool2d = _avg_pool('avg_pool2d', _pair)
avg_pool3d = _avg_pool('avg_pool3d', _triple)
adaptive_avg_pool1d = _adaptive_pool('adaptive_avg_pool1d', "AveragePool", _single)
adaptive_avg_pool2d = _adaptive_pool('adaptive_avg_pool2d', "AveragePool", _pair)
adaptive_avg_pool3d = _adaptive_pool('adaptive_avg_pool3d', "AveragePool", _triple)
adaptive_max_pool1d = _adaptive_pool('adaptive_max_pool1d', "MaxPool", _single, max_pool1d_with_indices)
adaptive_max_pool2d = _adaptive_pool('adaptive_max_pool2d', "MaxPool", _pair, max_pool2d_with_indices)
adaptive_max_pool3d = _adaptive_pool('adaptive_max_pool3d', "MaxPool", _triple, max_pool3d_with_indices)
@parse_args('v', 'is', 'f')
def constant_pad_nd(g, input, padding, value):
    ...

@parse_args('v', 'is')
def reflection_pad(g, input, padding):
    ...

@parse_args('v', 'is')
def replication_pad(g, input, padding):
    ...

reflection_pad1d = reflection_pad
reflection_pad2d = reflection_pad
reflection_pad3d = reflection_pad
replication_pad1d = replication_pad
replication_pad2d = replication_pad
replication_pad3d = replication_pad
upsample_nearest1d = _interpolate('upsample_nearest1d', 3, "nearest")
upsample_nearest2d = _interpolate('upsample_nearest2d', 4, "nearest")
upsample_nearest3d = _interpolate('upsample_nearest3d', 5, "nearest")
upsample_linear1d = _interpolate('upsample_linear1d', 3, "linear")
upsample_bilinear2d = _interpolate('upsample_bilinear2d', 4, "linear")
upsample_trilinear3d = _interpolate('upsample_trilinear3d', 5, "linear")
@parse_args('v')
def bitwise_not(g, inp):
    ...

def wrap_logical_op_with_cast_to(to_type):
    ...

def wrap_logical_op_with_cast_to_and_from(to_type):
    ...

def wrap_logical_op_with_negation(func):
    ...

def eq(g, self, other):
    ...

@wrap_logical_op_with_negation
def ne(g, self, other):
    ...

def gt(g, input, other):
    ...

def gt_impl(g, input, other):
    ...

def lt(g, input, other):
    ...

def lt_impl(g, input, other):
    ...

@wrap_logical_op_with_negation
def ge(g, input, other):
    ...

@wrap_logical_op_with_negation
def le(g, input, other):
    ...

def where(g, condition, self, other):
    ...

@parse_args('v', 'i', 'none')
def log_softmax(g, input, dim, dtype=...):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv1d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv2d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i')
def conv3d(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose1d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose2d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'is')
def conv_transpose3d(g, input, weight, bias, stride, padding, output_padding, groups, dilation):
    ...

@parse_args('v', 'v', 'v', 'v', 'v', 'i', 'f', 'f', 'i')
def batch_norm(g, input, weight, bias, running_mean, running_var, training, momentum, eps, cudnn_enabled):
    ...

@parse_args('v', 'is', 'v', 'v', 'f', 'i')
def layer_norm(g, input, normalized_shape, weight, bias, eps, cudnn_enable):
    ...

@parse_args('v', 'v', 'v', 'v', 'v', 'i', 'f', 'f', 'i')
def instance_norm(g, input, weight, bias, running_mean, running_var, use_input_stats, momentum, eps, cudnn_enabled):
    ...

@parse_args('v', 'i', 'i', 'i')
def unfold(g, input, dimension, size, step):
    ...

@parse_args('v', 't', 't', 't')
def elu(g, input, alpha, scale, input_scale):
    ...

def selu(g, input):
    ...

@parse_args('v', 'i', 'v')
def index_select(g, self, dim, index):
    ...

def index_put(g, self, indices_list_value, values, accumulate):
    ...

def index_fill(g, self, dim, index, value):
    ...

def index_copy(g, self, dim, index, source):
    ...

def type_as(g, self, other):
    ...

@parse_args('v', 'v', 'i', 'f')
def cosine_similarity(g, x1, x2, dim, eps):
    ...

def clone(g, input, unused_memory_format):
    ...

def abs(g, self):
    ...

def log(g, self):
    ...

def log1p(g, self):
    ...

def pow(g, self, exponent):
    ...

def clamp(g, self, min, max):
    ...

@parse_args('v', 'f')
def clamp_min(g, self, min):
    ...

@parse_args('v', 'f')
def clamp_max(g, self, max):
    ...

def max(g, self, dim_or_y=..., keepdim=...):
    ...

def min(g, self, dim_or_y=..., keepdim=...):
    ...

def exp(g, self):
    ...

@parse_args('v', 'f', 'i')
def dropout(g, input, p, train):
    ...

feature_dropout = _unsupported_dropout("feature_dropout")
alpha_dropout = _unsupported_dropout("alpha_dropout")
feature_alpha_dropout = _unsupported_dropout("feature_alpha_dropout")
dropout_ = dropout
feature_dropout_ = feature_dropout
alpha_dropout_ = alpha_dropout
feature_alpha_dropout_ = feature_alpha_dropout
@parse_args('v', 't', 'is', 'i')
def norm(g, self, p, dim, keepdim):
    ...

@parse_args('v', 'v', 'v', 'i')
def conv_tbc(g, input, weight, bias, pad):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def empty(g, sizes, dtype, layout, device, pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def empty_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def scalar_tensor(g, scalar, dtype, *options):
    ...

@parse_args('v', 'i', 'v', 'v', 'v')
def zeros(g, sizes, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def zeros_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'v', 'i', 'v', 'v', 'v')
def new_zeros(g, self, sizes, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'i', 'v', 'v', 'v', 'v')
def ones_like(g, input, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def full(g, sizes, value, dtype, layout, device, pin_memory=...):
    ...

@parse_args('v', 'f', 'i', 'v', 'v', 'v', 'v')
def full_like(g, input, fill_value, dtype=..., layout=..., device=..., pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'v', 'v', 'v', 'i')
def slice(g, self, dim, start, end, step):
    ...

@parse_args('v', 'f', 'f')
def hardtanh(g, self, min_val, max_val):
    ...

def alias(g, self):
    ...

@parse_args('v', 'i')
def unsqueeze(g, self, dim):
    ...

@parse_args('v', 'i', 'i', 'none')
def sort(g, self, dim, decending, out=...):
    ...

def numel(g, self):
    ...

@parse_args('v', 'i', 'i', 'i', 'i', 'none')
def topk(g, self, k, dim, largest, sorted, out=...):
    ...

def to(g, self, *args):
    ...

def repeat(g, self, repeats):
    ...

@parse_args('v', 'i')
def pixel_shuffle(g, self, upscale_factor):
    ...

def lstm(g, *args):
    ...

gru = _one_hidden_rnn('GRU')
rnn_tanh = _one_hidden_rnn('RNN_TANH')
rnn_relu = _one_hidden_rnn('RNN_RELU')
def detach(g, input):
    ...

@parse_args('v', 'i')
def contiguous(g, input, memory_format):
    ...

def randn(g, shapes, dtype, *options):
    ...

def rand(g, shapes, dtype, *options):
    ...

def randn_like(g, self, dtype, layout=..., device=..., pin_memory=..., memory_format=...):
    ...

def rand_like(g, self, dtype, layout=..., device=..., pin_memory=..., memory_format=...):
    ...

@parse_args('v', 'f', 'f', 'i', 'none')
def rrelu(g, input, lower, upper, training, generator):
    ...

@parse_args('v')
def log_sigmoid(g, input):
    ...

@parse_args('v')
def erf(g, input):
    ...

@parse_args('v', 'i', 'i')
def flatten(g, input, start_dim, end_dim):
    ...

@parse_args('v')
def nonzero(g, input):
    ...

@parse_args('v')
def isnan(g, input):
    ...

@parse_args('v', 'i', 'i', 'i')
def narrow(g, input, dim, start, length):
    ...

def argmax(g, input, dim, keepdim):
    ...

def argmin(g, input, dim, keepdim):
    ...

@parse_args('v', 'i', 'v', 'v')
def scatter(g, self, dim, index, src):
    ...

@parse_args('v', 'i', 'v', 'v')
def scatter_add(g, self, dim, index, src):
    ...

def log2(g, self):
    ...

def prim_shape(g, self):
    ...

@parse_args('v', 'i')
def one_hot(g, self, num_classes):
    ...

@parse_args('v', 'i', 'v', 'v')
def gather(g, self, dim, index, sparse_grad=...):
    ...

def std(g, input, *args):
    ...

@parse_args('v', 'is', 'i')
def logsumexp(g, input, dim, keepdim):
    ...

def arange(g, *args):
    ...

def masked_fill(g, self, mask, value):
    ...

def index(g, self, index):
    ...

@parse_args('v', 'is', 'i')
def frobenius_norm(g, self, dim=..., keepdim=...):
    ...

@parse_args('v', 'i', 'b', 'v')
def multinomial(g, input, num_samples, replacement=..., generator=...):
    ...

def baddbmm(g, self, batch1, batch2, beta, alpha):
    ...

def meshgrid(g, tensor_list):
    ...

def remainder(g, input, other):
    ...

def gelu(g, self):
    ...

@parse_args('v', 'i', 'v', 'v', 'f', 'i')
def group_norm(g, input, num_groups, weight, bias, eps, cudnn_enabled):
    ...

def dim(g, self):
    '''Implement the dim functionality available for a pytorch tensor in ONNX'''
    ...

def take(g, self, index):
    ...

"""
This type stub file was generated by pyright.
"""

import torch

_sum = sum
def parse_args(*arg_descriptors):
    ...

def assert_training_mode(op_mode, op_name):
    ...

_default_onnx_opset_version = 9
_onnx_master_opset = 10
_onnx_stable_opsets = [7, 8, 9, 10, 11, 12]
_export_onnx_opset_version = _default_onnx_opset_version
_operator_export_type = None
_training_mode = None
cast_pytorch_to_onnx = { 'Byte': torch.onnx.TensorProtoDataType.UINT8,'Char': torch.onnx.TensorProtoDataType.INT8,'Double': torch.onnx.TensorProtoDataType.DOUBLE,'Float': torch.onnx.TensorProtoDataType.FLOAT,'Half': torch.onnx.TensorProtoDataType.FLOAT16,'Int': torch.onnx.TensorProtoDataType.INT32,'Long': torch.onnx.TensorProtoDataType.INT64,'Short': torch.onnx.TensorProtoDataType.INT16,'Bool': torch.onnx.TensorProtoDataType.BOOL,'ComplexFloat': torch.onnx.TensorProtoDataType.COMPLEX64,'ComplexDouble': torch.onnx.TensorProtoDataType.COMPLEX128,'Undefined': torch.onnx.TensorProtoDataType.UNDEFINED }
scalar_name_to_pytorch = { 'uint8_t': 'Byte','int8_t': 'Char','double': 'Double','float': 'Float','half': 'Half','int': 'Int','int64_t': 'Long','int16_t': 'Short','bool': 'Bool','complex64': 'ComplexFloat','complex128': 'ComplexDouble' }
scalar_type_to_pytorch_type = [torch.uint8, torch.int8, torch.short, torch.int, torch.int64, torch.half, torch.float, torch.double, torch.complex32, torch.complex64, torch.complex128, torch.bool]
scalar_type_to_onnx = [cast_pytorch_to_onnx["Byte"], cast_pytorch_to_onnx["Char"], cast_pytorch_to_onnx["Short"], cast_pytorch_to_onnx["Int"], cast_pytorch_to_onnx["Long"], cast_pytorch_to_onnx["Half"], cast_pytorch_to_onnx["Float"], cast_pytorch_to_onnx["Double"], cast_pytorch_to_onnx["Undefined"], cast_pytorch_to_onnx["ComplexFloat"], cast_pytorch_to_onnx["ComplexDouble"], cast_pytorch_to_onnx["Bool"]]
_quantized_ops = set()
"""
This type stub file was generated by pyright.
"""

from torch.onnx.symbolic_helper import parse_args

def register_quantized_ops(domain, version):
    ...

def nchw2nhwc(g, input):
    ...

def nhwc2nchw(g, input):
    ...

def linear_prepack(g, weight, bias):
    ...

@parse_args('v', 'v', 'v', 'f', 'i')
def linear(g, input, weight, bias, scale, zero_point):
    ...

def conv_prepack(g, input, weight, bias, stride, padding, dilation, groups):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'f', 'i')
def conv2d(g, input, weight, bias, stride, padding, dilation, groups, scale, zero_point):
    ...

@parse_args('v', 'v', 'v', 'is', 'is', 'is', 'i', 'f', 'i')
def conv2d_relu(g, input, weight, bias, stride, padding, dilation, groups, scale, zero_point):
    ...

@parse_args('v', 'v', 'f', 'i')
def add(g, input_a, input_b, scale, zero_point):
    ...

@parse_args('v')
def relu(g, input):
    ...

@parse_args('v', 'f', 'i', 't')
def quantize_per_tensor(g, input, scale, zero_point, dtype):
    ...

@parse_args('v')
def dequantize(g, input):
    ...

def upsample_nearest2d(g, input, output_size, align_corners=..., scales_h=..., scales_w=...):
    ...

@parse_args('v', 'is', 'is', 'is', 'is', 'i')
def max_pool2d(g, input, kernel_size, stride, padding, dilation, ceil_mode):
    ...

@parse_args('v', 'is', 'is', 'is', 'i', 'i', 'none')
def avg_pool2d(g, input, kernel_size, stride, padding, ceil_mode, count_include_pad, divisor_override=...):
    ...

def reshape(g, input, shape):
    ...

@parse_args('v', 'v', 'v', 'v', 'i')
def slice(g, input, dim, start, end, step):
    ...

def cat(g, tensor_list, dim, scale=..., zero_point=...):
    ...

@parse_args('v')
def sigmoid(g, input):
    ...

"""
This type stub file was generated by pyright.
"""

from torch.onnx.symbolic_helper import parse_args
from torch.nn.modules.utils import _pair, _single, _triple

@parse_args('v', 'f', 'f')
def hardtanh(g, self, min_val, max_val):
    ...

def clamp(g, self, min, max):
    ...

def clamp_min(g, self, min):
    ...

def clamp_max(g, self, max):
    ...

@parse_args('v', 'i', 'v')
def select(g, self, dim, index):
    ...

def index_put(g, self, indices_list_value, values, accumulate=...):
    ...

@parse_args('v', 'i')
def pixel_shuffle(g, self, upscale_factor):
    ...

upsample_nearest1d = _interpolate('upsample_nearest1d', 3, "nearest")
upsample_nearest2d = _interpolate('upsample_nearest2d', 4, "nearest")
upsample_nearest3d = _interpolate('upsample_nearest3d', 5, "nearest")
upsample_linear1d = _interpolate('upsample_linear1d', 3, "linear")
upsample_bilinear2d = _interpolate('upsample_bilinear2d', 4, "linear")
upsample_trilinear3d = _interpolate('upsample_trilinear3d', 5, "linear")
upsample_bicubic2d = _interpolate('upsample_bicubic2d', 4, "cubic")
@parse_args('v', 'i', 'v', 'v')
def gather(g, self, dim, index, sparse_grad=...):
    ...

@parse_args('v', 'i', 'v', 'v')
def scatter(g, self, dim, index, src):
    ...

@parse_args('v', 'i', 'none')
def cumsum(g, self, dim, dtype=...):
    ...

def masked_select(g, self, mask):
    ...

def masked_scatter(g, self, mask, source):
    ...

def append(g, self, tensor):
    ...

def insert(g, self, pos, tensor):
    ...

def pop(g, tensor_list, dim):
    ...

def cat(g, tensor_list, dim):
    ...

def stack(g, tensor_list, dim):
    ...

avg_pool1d = _avg_pool('avg_pool1d', _single)
avg_pool2d = _avg_pool('avg_pool2d', _pair)
avg_pool3d = _avg_pool('avg_pool3d', _triple)
@parse_args('v', 'i', 'i', 'i', 'i')
def unique_dim(g, self, dim, sorted, return_inverse, return_counts):
    ...

@parse_args('v', 'v', 'i', 'i', 'i', 'none')
def topk(g, self, k, dim, largest, sorted, out=...):
    ...

@parse_args('v', 'i', 'i', 'none')
def sort(g, self, dim, decending, out=...):
    ...

def round(g, self):
    ...

@parse_args('v', 'v', 'i')
def split_with_sizes(g, self, split_sizes, dim):
    ...

def constant_pad_nd(g, input, padding, value=...):
    ...

def reflection_pad(g, input, padding):
    ...

def replication_pad(g, input, padding):
    ...

reflection_pad1d = reflection_pad
reflection_pad2d = reflection_pad
reflection_pad3d = reflection_pad
replication_pad1d = replication_pad
replication_pad2d = replication_pad
replication_pad3d = replication_pad
def det(g, self):
    ...

def logdet(g, input):
    ...

def arange(g, *args):
    ...

def size(g, self, dim=...):
    ...

def squeeze(g, self, dim=...):
    ...

@parse_args('v', 'i')
def unsqueeze(g, self, dim):
    ...

def mm(g, self, other):
    ...

def index_fill(g, self, dim, index, value):
    ...

def index_copy(g, self, dim, index, source):
    ...

@parse_args('v', 'is', 'is', 'is', 'is')
def im2col(g, input, kernel_size, dilation, padding, stride):
    ...

@parse_args('v', 'i', 'i')
def flatten(g, input, start_dim, end_dim):
    ...

"""
This type stub file was generated by pyright.
"""

from torch.nn.modules.utils import _pair, _single, _triple
from torch.onnx.symbolic_helper import parse_args

@parse_args('v', 'i', 'i', 'none')
def sort(g, self, dim, decending, out=...):
    ...

@parse_args('v', 'v', 'i', 'i', 'i', 'none')
def topk(g, self, k, dim, largest, sorted, out=...):
    ...

max_pool1d = _max_pool("max_pool1d", _single, 1, return_indices=False)
max_pool2d = _max_pool("max_pool2d", _pair, 2, return_indices=False)
max_pool3d = _max_pool("max_pool3d", _triple, 3, return_indices=False)
max_pool1d_with_indices = _max_pool("max_pool1d_with_indices", _single, 1, return_indices=True)
max_pool2d_with_indices = _max_pool("max_pool2d_with_indices", _pair, 2, return_indices=True)
max_pool3d_with_indices = _max_pool("max_pool3d_with_indices", _triple, 3, return_indices=True)
avg_pool1d = _avg_pool('avg_pool1d', _single)
avg_pool2d = _avg_pool('avg_pool2d', _pair)
avg_pool3d = _avg_pool('avg_pool3d', _triple)
upsample_nearest1d = _interpolate('upsample_nearest1d', 3, "nearest")
upsample_nearest2d = _interpolate('upsample_nearest2d', 4, "nearest")
upsample_nearest3d = _interpolate('upsample_nearest3d', 5, "nearest")
upsample_linear1d = _interpolate('upsample_linear1d', 3, "linear")
upsample_bilinear2d = _interpolate('upsample_bilinear2d', 4, "linear")
upsample_trilinear3d = _interpolate('upsample_trilinear3d', 5, "linear")
@parse_args('v', 'v', 'v', 'v', 'i')
def slice(g, self, dim, start, end, step):
    ...

@parse_args('v', 'is')
def flip(g, input, dims):
    ...

def fmod(g, input, other):
    ...

"""
This type stub file was generated by pyright.
"""

import os
import sys
import platform
import ctypes
import torch.cuda
import torch.autograd
import torch.futures
import torch.nn
import torch.nn.intrinsic
import torch.nn.quantized
import torch.optim
import torch.multiprocessing
import torch.sparse
import torch.utils.backcompat
import torch.onnx
import torch.jit
import torch.hub
import torch.random
import torch.distributions
import torch.testing
import torch.backends.cuda
import torch.backends.mkl
import torch.backends.mkldnn
import torch.backends.openmp
import torch.backends.quantized
import torch.quantization
import torch.utils.data
import torch.__config__
import torch.__future__
import torch.quasirandom
from ._utils import _import_dotted_name
from ._utils_internal import USE_GLOBAL_DEPS, USE_RTLD_GLOBAL_WITH_LIBTORCH, get_file_path, prepare_multiprocessing_environment
from .version import __version__
from ._six import string_classes as _string_classes
from typing import Set, Type
from torch._C import *
from .random import get_rng_state, initial_seed, manual_seed, seed, set_rng_state
from .serialization import load, save
from ._tensor_str import set_printoptions
from .tensor import Tensor
from .storage import _StorageBase
from .functional import *
from torch.autograd import enable_grad, no_grad, set_grad_enabled
from . import _storage_docs, _tensor_docs, _torch_docs
from torch._ops import ops
from torch._classes import classes
from torch.multiprocessing._atfork import register_after_fork
from ._lobpcg import lobpcg

r"""
The torch package contains data structures for multi-dimensional
tensors and mathematical operations over these are defined.
Additionally, it provides many utilities for efficient serializing of
Tensors and arbitrary types, and other useful utilities.

It has a CUDA counterpart, that enables you to run your tensor computations
on an NVIDIA GPU with compute capability >= 3.0.
"""
if sys.version_info < (3, ):
    ...
if sys.platform == 'win32':
    ...
if USE_RTLD_GLOBAL_WITH_LIBTORCH or os.getenv('TORCH_USE_RTLD_GLOBAL') and platform.system() != 'Windows':
    old_flags = sys.getdlopenflags()
else:
    ...
if False:
    ...
def typename(o):
    ...

def is_tensor(obj):
    r"""Returns True if `obj` is a PyTorch tensor.

    Note that this function is simply doing ``isinstance(obj, Tensor)``.
    Using that ``isinstance`` check is better for typechecking with mypy,
    and more explicit - so it's recommended to use that instead of
    ``is_tensor``.

    Args:
        obj (Object): Object to test
    """
    ...

def is_storage(obj):
    r"""Returns True if `obj` is a PyTorch storage object.

    Args:
        obj (Object): Object to test
    """
    ...

def set_default_tensor_type(t):
    r"""Sets the default ``torch.Tensor`` type to floating point tensor type
    ``t``. This type will also be used as default floating point type for
    type inference in :func:`torch.tensor`.

    The default floating point tensor type is initially ``torch.FloatTensor``.

    Args:
        t (type or string): the floating point tensor type or its name

    Example::

        >>> torch.tensor([1.2, 3]).dtype    # initial default for floating point is torch.float32
        torch.float32
        >>> torch.set_default_tensor_type(torch.DoubleTensor)
        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
        torch.float64

    """
    ...

def set_default_dtype(d):
    r"""Sets the default floating point dtype to :attr:`d`.
    This dtype is:
    1. The inferred dtype for python floats in :func:`torch.tensor`.
    2. Used to infer dtype for python complex numbers. The default complex dtype is set to
       ``torch.complex128`` if default floating point dtype is ``torch.float64``,
       otherwise it's set to ``torch.complex64``

    The default floating point dtype is initially ``torch.float32``.

    Args:
        d (:class:`torch.dtype`): the floating point dtype to make the default

    Example::
        >>> # initial default for floating point is torch.float32
        >>> torch.tensor([1.2, 3]).dtype
        torch.float32
        >>> # initial default for floating point is torch.complex64
        >>> torch.tensor([1.2, 3j]).dtype
        torch.complex64
        >>> torch.set_default_dtype(torch.float64)
        >>> torch.tensor([1.2, 3]).dtype    # a new floating point tensor
        torch.float64
        >>> torch.tensor([1.2, 3j]).dtype   # a new complex tensor
        torch.complex128

    """
    ...

class DoubleStorage(_C.DoubleStorageBase, _StorageBase):
    ...


class FloatStorage(_C.FloatStorageBase, _StorageBase):
    ...


class HalfStorage(_C.HalfStorageBase, _StorageBase):
    ...


class LongStorage(_C.LongStorageBase, _StorageBase):
    ...


class IntStorage(_C.IntStorageBase, _StorageBase):
    ...


class ShortStorage(_C.ShortStorageBase, _StorageBase):
    ...


class CharStorage(_C.CharStorageBase, _StorageBase):
    ...


class ByteStorage(_C.ByteStorageBase, _StorageBase):
    ...


class BoolStorage(_C.BoolStorageBase, _StorageBase):
    ...


class BFloat16Storage(_C.BFloat16StorageBase, _StorageBase):
    ...


class ComplexDoubleStorage(_C.ComplexDoubleStorageBase, _StorageBase):
    ...


class ComplexFloatStorage(_C.ComplexFloatStorageBase, _StorageBase):
    ...


class QUInt8Storage(_C.QUInt8StorageBase, _StorageBase):
    ...


class QInt8Storage(_C.QInt8StorageBase, _StorageBase):
    ...


class QInt32Storage(_C.QInt32StorageBase, _StorageBase):
    ...


_storage_classes = DoubleStorage, FloatStorage, LongStorage, IntStorage, ShortStorage, CharStorage, ByteStorage, HalfStorage, BoolStorage, QUInt8Storage, QInt8Storage, QInt32Storage, BFloat16Storage, ComplexFloatStorage, ComplexDoubleStorage
def manager_path():
    ...

if False:
    ...
def compiled_with_cxx11_abi():
    r"""Returns whether PyTorch was built with _GLIBCXX_USE_CXX11_ABI=1"""
    ...

legacy_contiguous_format = contiguous_format
quantized_lstm = torch.ops.aten.quantized_lstm
quantized_gru = torch.ops.aten.quantized_gru
"""
This type stub file was generated by pyright.
"""

import builtins
import io
import math
import sys

inf = math.inf
nan = math.nan
string_classes = (str, bytes)
int_classes = int
FileNotFoundError = builtins.FileNotFoundError
StringIO = io.StringIO
container_abcs = collections.abc
PY3 = sys.version_info[0] == 3
PY37 = sys.version_info[0] == 3 and sys.version_info[1] >= 7
def with_metaclass(meta, *bases):
    """Create a base class with a metaclass."""
    class metaclass(meta):
        ...
    
    

def raise_from(value, from_value):
    ...

def get_function_from_type(cls, name):
    ...

def istuple(obj):
    ...

def bind_method(fn, obj, obj_type):
    ...

"""
This type stub file was generated by pyright.
"""

_MPI_AVAILABLE = True
_NCCL_AVAILABLE = True
_GLOO_AVAILABLE = True
class Backend(object):
    """
    An enum-like class of available backends: GLOO, NCCL, MPI, and other registered
    backends.

    The values of this class are lowercase strings, e.g., ``"gloo"``. They can
    be accessed as attributes, e.g., ``Backend.NCCL``.

    This class can be directly called to parse the string, e.g.,
    ``Backend(backend_str)`` will check if ``backend_str`` is valid, and
    return the parsed lowercase string if so. It also accepts uppercase strings,
    e.g., ``Backend("GLOO")`` returns ``"gloo"``.

    .. note:: The entry ``Backend.UNDEFINED`` is present but only used as
              initial value of some fields. Users should neither use it directly
              nor assume its existence.
    """
    UNDEFINED = ...
    GLOO = ...
    NCCL = ...
    MPI = ...
    TCP = ...
    def __new__(cls, name):
        ...
    
    @classmethod
    def register_backend(cls, name, func):
        """
        Registers a new backend.

        This class method is used by 3rd party cpp extension to register new backend.

        Arguments:
            name (str): Backend name matching with the one in `init_process_group()`.
            func (function): Function handler that instantiates the backend.
                             The function should be implemented in the backend cpp extension
                             and takes four arguments, including prefix_store, rank,
                             world_size, and timeout.

        .. note:: This support of 3rd party backend is experimental and subject to change.

        """
        ...
    


_backend = Backend.UNDEFINED
dist_backend = Backend
class reduce_op(object):
    r"""
    Deprecated enum-like class for reduction operations: ``SUM``, ``PRODUCT``,
    ``MIN``, and ``MAX``.

    :class:`~torch.distributed.ReduceOp` is recommended to use instead.
    """
    def __init__(self) -> None:
        ...
    
    def __getattribute__(self, key):
        ...
    


reduce_op = reduce_op()
class group(object):
    WORLD = ...


class GroupMember(object):
    WORLD = ...
    NON_GROUP_MEMBER = ...


_pg_map = {  }
_pg_names = {  }
_pg_group_ranks = {  }
_default_pg = None
_default_pg_init_method = None
_group_count = 0
def is_mpi_available():
    """
    Checks if the MPI backend is available.

    """
    ...

def is_nccl_available():
    """
    Checks if the NCCL backend is available.

    """
    ...

def is_gloo_available():
    """
    Checks if the Gloo backend is available.

    """
    ...

def is_initialized():
    """
    Checking if the default process group has been initialized

    """
    ...

def get_backend(group=...):
    """
    Returns the backend of the given process group.

    Arguments:
        group (ProcessGroup, optional): The process group to work on. The
            default is the general main process group. If another specific group
            is specified, the calling process must be part of :attr:`group`.

    Returns:
        The backend of the given process group as a lower case string.

    """
    ...

def init_process_group(backend, init_method=..., timeout=..., world_size=..., rank=..., store=..., group_name=...):
    """
    Initializes the default distributed process group, and this will also
    initialize the distributed package.

    There are 2 main ways to initialize a process group:
        1. Specify ``store``, ``rank``, and ``world_size`` explicitly.
        2. Specify ``init_method`` (a URL string) which indicates where/how
           to discover peers. Optionally specify ``rank`` and ``world_size``,
           or encode all required parameters in the URL and omit them.

    If neither is specified, ``init_method`` is assumed to be "env://".


    Arguments:
        backend (str or Backend): The backend to use. Depending on
            build-time configurations, valid values include ``mpi``, ``gloo``,
            and ``nccl``. This field should be given as a lowercase string
            (e.g., ``"gloo"``), which can also be accessed via
            :class:`Backend` attributes (e.g., ``Backend.GLOO``). If using
            multiple processes per machine with ``nccl`` backend, each process
            must have exclusive access to every GPU it uses, as sharing GPUs
            between processes can result in deadlocks.
        init_method (str, optional): URL specifying how to initialize the
                                     process group. Default is "env://" if no
                                     ``init_method`` or ``store`` is specified.
                                     Mutually exclusive with ``store``.
        world_size (int, optional): Number of processes participating in
                                    the job. Required if ``store`` is specified.
        rank (int, optional): Rank of the current process.
                              Required if ``store`` is specified.
        store(Store, optional): Key/value store accessible to all workers, used
                                to exchange connection/address information.
                                Mutually exclusive with ``init_method``.
        timeout (timedelta, optional): Timeout for operations executed against
            the process group. Default value equals 30 minutes.
            This is applicable for the ``gloo`` backend. For ``nccl``, this is
            applicable only if the environment variable ``NCCL_BLOCKING_WAIT``
            is set to 1.
        group_name (str, optional, deprecated): Group name.

    To enable ``backend == Backend.MPI``, PyTorch needs to be built from source
    on a system that supports MPI.

    """
    ...

def destroy_process_group(group=...):
    """
    Destroy a given process group, and deinitialize the distributed package

    Arguments:
        group (ProcessGroup, optional): The process group to be destroyed, if
                                        group.WORLD is given, all process
                                        groups including the default one will
                                        be destroyed.
    """
    ...

def get_rank(group=...):
    """
    Returns the rank of current process group

    Rank is a unique identifier assigned to each process within a distributed
    process group. They are always consecutive integers ranging from 0 to
    ``world_size``.

    Arguments:
        group (ProcessGroup, optional): The process group to work on

    Returns:
        The rank of the process group
        -1, if not part of the group

    """
    ...

def get_world_size(group=...):
    """
    Returns the number of processes in the current process group

    Arguments:
        group (ProcessGroup, optional): The process group to work on

    Returns:
        The world size of the process group
        -1, if not part of the group

    """
    ...

def isend(tensor, dst, group=..., tag=...):
    """
    Sends a tensor asynchronously.

    Arguments:
        tensor (Tensor): Tensor to send.
        dst (int): Destination rank.
        group (ProcessGroup, optional): The process group to work on
        tag (int, optional): Tag to match send with remote recv

    Returns:
        A distributed request object.
        None, if not part of the group

    """
    ...

def irecv(tensor, src, group=..., tag=...):
    """
    Receives a tensor asynchronously.

    Arguments:
        tensor (Tensor): Tensor to fill with received data.
        src (int): Source rank.
        group (ProcessGroup, optional): The process group to work on
        tag (int, optional): Tag to match recv with remote send

    Returns:
        A distributed request object.
        None, if not part of the group

    """
    ...

def send(tensor, dst, group=..., tag=...):
    """
    Sends a tensor synchronously.

    Arguments:
        tensor (Tensor): Tensor to send.
        dst (int): Destination rank.
        group (ProcessGroup, optional): The process group to work on
        tag (int, optional): Tag to match send with remote recv

    """
    ...

def recv(tensor, src=..., group=..., tag=...):
    """
    Receives a tensor synchronously.

    Arguments:
        tensor (Tensor): Tensor to fill with received data.
        src (int, optional): Source rank. Will receive from any
            process if unspecified.
        group (ProcessGroup, optional): The process group to work on
        tag (int, optional): Tag to match recv with remote send

    Returns:
        Sender rank
        -1, if not part of the group

    """
    ...

def broadcast_multigpu(tensor_list, src, group=..., async_op=..., src_tensor=...):
    """
    Broadcasts the tensor to the whole group with multiple GPU tensors
    per node.

    ``tensor`` must have the same number of elements in all the GPUs from
    all processes participating in the collective. each tensor in the list must
    be on a different GPU

    Only nccl and gloo backend are currently supported
    tensors should only be GPU tensors

    Arguments:
        tensor_list (List[Tensor]): Tensors that participate in the collective
            operation. If ``src`` is the rank, then the specified ``src_tensor``
            element of ``tensor_list`` (``tensor_list[src_tensor]``) will be
            broadcast to all other tensors (on different GPUs) in the src process
            and all tensors in ``tensor_list`` of other non-src processes.
            You also need to make sure that ``len(tensor_list)`` is the same
            for all the distributed processes calling this function.

        src (int): Source rank.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op
        src_tensor (int, optional): Source tensor rank within ``tensor_list``

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def broadcast(tensor, src, group=..., async_op=...):
    """
    Broadcasts the tensor to the whole group.

    ``tensor`` must have the same number of elements in all processes
    participating in the collective.

    Arguments:
        tensor (Tensor): Data to be sent if ``src`` is the rank of current
            process, and tensor to be used to save received data otherwise.
        src (int): Source rank.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_reduce_multigpu(tensor_list, op=..., group=..., async_op=...):
    r"""
    Reduces the tensor data across all machines in such a way that all get
    the final result. This function reduces a number of tensors on every node,
    while each tensor resides on different GPUs.
    Therefore, the input tensor in the tensor list needs to be GPU tensors.
    Also, each tensor in the tensor list needs to reside on a different GPU.

    After the call, all ``tensor`` in ``tensor_list`` is going to be bitwise
    identical in all processes.

    Only nccl and gloo backend is currently supported
    tensors should only be GPU tensors

    Arguments:
        tensor list (List[Tensor]): List of input and output tensors of
            the collective. The function operates in-place and requires that
            each tensor to be a GPU tensor on different GPUs.
            You also need to make sure that ``len(tensor_list)`` is the same for
            all the distributed processes calling this function.
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_reduce(tensor, op=..., group=..., async_op=...):
    """
    Reduces the tensor data across all machines in such a way that all get
    the final result.

    After the call ``tensor`` is going to be bitwise identical in all processes.

    Arguments:
        tensor (Tensor): Input and output of the collective. The function
            operates in-place.
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_reduce_coalesced(tensors, op=..., group=..., async_op=...):
    """
    WARNING: at this time individual shape checking is not implemented across nodes.
    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the
    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the allreduce
    operation will proceed without complaint and return erroneous outputs. This lack
    of shape checking results in significant performance improvements but users of this
    function should take extra care to ensure that each node passes in tensors whose
    shapes match across nodes.

    Reduces each tensor in tensors (residing on the same device) across all machines
    in such a way that all get the final result.

    After the call each tensor in tensors is going to bitwise identical
    in all processes.

    Arguments:
        tensors (List[Tensor]): Input and output of the collective. The function
            operates in-place.
        op (Optional[ReduceOp]): One of the values from
            ``torch.distributed.ReduceOp`` enum. Specifies an operation used for
            element-wise reductions.
        group (Optional[ProcessGroup]): The process group to work on.
        async_op (Optional[bool]): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group.

    """
    ...

def reduce_multigpu(tensor_list, dst, op=..., group=..., async_op=..., dst_tensor=...):
    """
    Reduces the tensor data on multiple GPUs across all machines. Each tensor
    in ``tensor_list`` should reside on a separate GPU

    Only the GPU of ``tensor_list[dst_tensor]`` on the process with rank ``dst``
    is going to receive the final result.

    Only nccl backend is currently supported
    tensors should only be GPU tensors

    Arguments:
        tensor_list (List[Tensor]): Input and output GPU tensors of the
            collective. The function operates in-place.
            You also need to make sure that ``len(tensor_list)`` is the same for
            all the distributed processes calling this function.
        dst (int): Destination rank
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op
        dst_tensor (int, optional): Destination tensor rank within
                                    ``tensor_list``

    Returns:
        Async work handle, if async_op is set to True.
        None, otherwise

    """
    ...

def reduce(tensor, dst, op=..., group=..., async_op=...):
    """
    Reduces the tensor data across all machines.

    Only the process with rank ``dst`` is going to receive the final result.

    Arguments:
        tensor (Tensor): Input and output of the collective. The function
            operates in-place.
        dst (int): Destination rank
        op (optional): One of the values from
            ``torch.distributed.ReduceOp``
            enum.  Specifies an operation used for element-wise reductions.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_gather_multigpu(output_tensor_lists, input_tensor_list, group=..., async_op=...):
    """
    Gathers tensors from the whole group in a list.
    Each tensor in ``tensor_list`` should reside on a separate GPU

    Only nccl backend is currently supported
    tensors should only be GPU tensors

    Arguments:
        output_tensor_lists (List[List[Tensor]]): Output lists. It should
            contain correctly-sized tensors on each GPU to be used for output
            of the collective, e.g. ``output_tensor_lists[i]`` contains the
            all_gather result that resides on the GPU of
            ``input_tensor_list[i]``.

            Note that each element of ``output_tensor_lists`` has the size of
            ``world_size * len(input_tensor_list)``, since the function all
            gathers the result from every single GPU in the group. To interpret
            each element of ``output_tensor_lists[i]``, note that
            ``input_tensor_list[j]`` of rank k will be appear in
            ``output_tensor_lists[i][k * world_size + j]``

            Also note that ``len(output_tensor_lists)``, and the size of each
            element in ``output_tensor_lists`` (each element is a list,
            therefore ``len(output_tensor_lists[i])``) need to be the same
            for all the distributed processes calling this function.

        input_tensor_list (List[Tensor]): List of tensors(on different GPUs) to
            be broadcast from current process.
            Note that ``len(input_tensor_list)`` needs to be the same for
            all the distributed processes calling this function.

        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_gather(tensor_list, tensor, group=..., async_op=...):
    """
    Gathers tensors from the whole group in a list.

    Arguments:
        tensor_list (list[Tensor]): Output list. It should contain
            correctly-sized tensors to be used for output of the collective.
        tensor (Tensor): Tensor to be broadcast from current process.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def all_gather_coalesced(output_tensor_lists, input_tensor_list, group=..., async_op=...):
    """
    Gathers input tensors from the whole group in a list in a coalesced manner.

    Arguments:
        output_tensor_lists (list[list[Tensor]]): Output list. It should contain
            correctly-sized tensors to be used for output of the collective.
        input_tensor_list (list[Tensor]): Tensors to be broadcast from
            current process. At least one tensor has to be non empty.
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    Example:
        we have 2 process groups, 2 ranks.
        rank 0 passes:
            input_tensor_list = [[[1, 1], [1, 1]], [2], [3, 3]]
            output_tensor_lists =
               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],
                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]
        rank 1 passes:
            input_tensor_list = [[[3, 3], [3, 3]], [5], [1, 1]]
            output_tensor_lists =
               [[[[-1, -1], [-1, -1]], [-1], [-1, -1]],
                [[[-1, -1], [-1, -1]], [-1], [-1, -1]]]
        both rank 0 and 1 get:
            output_tensor_lists =
               [[[1, 1], [1, 1]], [2], [3, 3]],
                [[3, 3], [3, 3]], [5], [1, 1]]].

    WARNING: at this time individual shape checking is not implemented across nodes.
    For example, if the rank 0 node passes [torch.rand(4), torch.rand(2)] and the
    rank 1 node passes [torch.rand(2), torch.rand(2), torch.rand(2)], the
    all_gather_coalesced operation will proceed without complaint and return
    erroneous outputs. This lack of shape checking results in significant
    performance improvements but users of this function should take extra care
    to ensure that each node passes in tensors whose shapes match across nodes.
    """
    ...

def gather(tensor, gather_list=..., dst=..., group=..., async_op=...):
    """
    Gathers a list of tensors in a single process.

    Arguments:
        tensor (Tensor): Input tensor.
        gather_list (list[Tensor], optional): List of appropriately-sized
            tensors to use for gathered data (default is None, must be specified
            on the destination rank)
        dst (int, optional): Destination rank (default is 0)
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def scatter(tensor, scatter_list=..., src=..., group=..., async_op=...):
    """
    Scatters a list of tensors to all processes in a group.

    Each process will receive exactly one tensor and store its data in the
    ``tensor`` argument.

    Arguments:
        tensor (Tensor): Output tensor.
        scatter_list (list[Tensor]): List of tensors to scatter (default is
            None, must be specified on the source rank)
        src (int): Source rank (default is 0)
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group

    """
    ...

def reduce_scatter_multigpu(output_tensor_list, input_tensor_lists, op=..., group=..., async_op=...):
    """
    Reduce and scatter a list of tensors to the whole group.  Only nccl backend
    is currently supported.

    Each tensor in ``output_tensor_list`` should reside on a separate GPU, as
    should each list of tensors in ``input_tensor_lists``.

    Arguments:
        output_tensor_list (List[Tensor]): Output tensors (on different GPUs)
            to receive the result of the operation.

            Note that ``len(output_tensor_list)`` needs to be the same for all
            the distributed processes calling this function.

        input_tensor_lists (List[List[Tensor]]): Input lists.  It should
            contain correctly-sized tensors on each GPU to be used for input of
            the collective, e.g. ``input_tensor_lists[i]`` contains the
            reduce_scatter input that resides on the GPU of
            ``output_tensor_list[i]``.

            Note that each element of ``input_tensor_lists`` has the size of
            ``world_size * len(output_tensor_list)``, since the function
            scatters the result from every single GPU in the group.  To
            interpret each element of ``input_tensor_lists[i]``, note that
            ``output_tensor_list[j]`` of rank k receives the reduce-scattered
            result from ``input_tensor_lists[i][k * world_size + j]``

            Also note that ``len(input_tensor_lists)``, and the size of each
            element in ``input_tensor_lists`` (each element is a list,
            therefore ``len(input_tensor_lists[i])``) need to be the same for
            all the distributed processes calling this function.

        group (ProcessGroup, optional): The process group to work on.
        async_op (bool, optional): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group.

    """
    ...

def reduce_scatter(output, input_list, op=..., group=..., async_op=...):
    """
    Reduces, then scatters a list of tensors to all processes in a group.

    Arguments:
        output (Tensor): Output tensor.
        input_list (list[Tensor]): List of tensors to reduce and scatter.
        group (ProcessGroup, optional): The process group to work on.
        async_op (bool, optional): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group.

    """
    ...

def all_to_all_single(output, input, output_split_sizes=..., input_split_sizes=..., group=..., async_op=...):
    """
    Each process splits input tensor and then scatters the split list
    to all processes in a group. Then concatenate the received tensors from all
    the processes in the group and return single output tensor.

    Arguments:
        output (Tensor): Gathered cancatenated output tensor.
        input (Tensor): Input tensor to scatter.
        output_split_sizes: (list[Int], optional): Output split sizes for dim 0
            if specified None or empty, dim 0 of ``output`` tensor must divide
            equally by ``world_size``.
        input_split_sizes: (list[Int], optional): Input split sizes for dim 0
            if specified None or empty, dim 0 of ``input`` tensor must divide
            equally by ``world_size``.
        group (ProcessGroup, optional): The process group to work on.
        async_op (bool, optional): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group.

    .. warning::
        `all_to_all_single` is experimental and subject to change.

    Examples:
        >>> input = torch.arange(4) + rank * 4
        >>> input
        tensor([0, 1, 2, 3])     # Rank 0
        tensor([4, 5, 6, 7])     # Rank 1
        tensor([8, 9, 10, 11])   # Rank 2
        tensor([12, 13, 14, 15]) # Rank 3
        >>> output = torch.empty([4], dtype=torch.int64)
        >>> dist.all_to_all_single(output, input)
        >>> output
        tensor([0, 4, 8, 12])    # Rank 0
        tensor([1, 5, 9, 13])    # Rank 1
        tensor([2, 6, 10, 14])   # Rank 2
        tensor([3, 7, 11, 15])   # Rank 3

        >>> # Essentially, it is similar to following operation:
        >>> scatter_list = list(input.chunk(world_size))
        >>> gather_list  = list(output.chunk(world_size))
        >>> for i in range(world_size):
        >>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)

        >>> # Another example with uneven split
        >>> input
        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
        tensor([20, 21, 22, 23, 24])                                     # Rank 2
        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
        >>> input_splits
        [2, 2, 1, 1]                                                     # Rank 0
        [3, 2, 2, 2]                                                     # Rank 1
        [2, 1, 1, 1]                                                     # Rank 2
        [2, 2, 2, 1]                                                     # Rank 3
        >>> output_splits
        [2, 3, 2, 2]                                                     # Rank 0
        [2, 2, 1, 2]                                                     # Rank 1
        [1, 2, 1, 2]                                                     # Rank 2
        [1, 2, 1, 1]                                                     # Rank 3
        >>> output = ...
        >>> dist.all_to_all_single(output, input, output_splits, input_splits)
        >>> output
        tensor([ 0,  1, 10, 11, 12, 20, 21, 30, 31])                     # Rank 0
        tensor([ 2,  3, 13, 14, 22, 32, 33])                             # Rank 1
        tensor([ 4, 15, 16, 23, 34, 35])                                 # Rank 2
        tensor([ 5, 17, 18, 24, 36])                                     # Rank 3
    """
    ...

def all_to_all(output_tensor_list, input_tensor_list, group=..., async_op=...):
    """
    Each process scatters list of input tensors to all processes in a group and
    return gathered list of tensors in output list.

    Arguments:
        output_tensor_list (list[Tensor]): List of tensors to be gathered one 
            per rank.
        input_tensor_list (list[Tensor]): List of tensors to scatter one per rank.
        group (ProcessGroup, optional): The process group to work on.
        async_op (bool, optional): Whether this op should be an async op.

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group.

    .. warning::
        `all_to_all` is experimental and subject to change.

    Examples:
        >>> input = torch.arange(4) + rank * 4
        >>> input = list(input.chunk(4))
        >>> input
        [tensor([0]), tensor([1]), tensor([2]), tensor([3])]     # Rank 0
        [tensor([4]), tensor([5]), tensor([6]), tensor([7])]     # Rank 1
        [tensor([8]), tensor([9]), tensor([10]), tensor([11])]   # Rank 2
        [tensor([12]), tensor([13]), tensor([14]), tensor([15])] # Rank 3
        >>> output = list(torch.empty([4], dtype=torch.int64).chunk(4))
        >>> dist.all_to_all(output, input)
        >>> output
        [tensor([0]), tensor([4]), tensor([8]), tensor([12])]    # Rank 0
        [tensor([1]), tensor([5]), tensor([9]), tensor([13])]    # Rank 1
        [tensor([2]), tensor([6]), tensor([10]), tensor([14])]   # Rank 2
        [tensor([3]), tensor([7]), tensor([11]), tensor([15])]   # Rank 3

        >>> # Essentially, it is similar to following operation:
        >>> scatter_list = input
        >>> gather_list  = output
        >>> for i in range(world_size):
        >>>   dist.scatter(gather_list[i], scatter_list if i == rank else [], src = i)

        >>> input
        tensor([0, 1, 2, 3, 4, 5])                                       # Rank 0
        tensor([10, 11, 12, 13, 14, 15, 16, 17, 18])                     # Rank 1
        tensor([20, 21, 22, 23, 24])                                     # Rank 2
        tensor([30, 31, 32, 33, 34, 35, 36])                             # Rank 3
        >>> input_splits
        [2, 2, 1, 1]                                                     # Rank 0
        [3, 2, 2, 2]                                                     # Rank 1
        [2, 1, 1, 1]                                                     # Rank 2
        [2, 2, 2, 1]                                                     # Rank 3
        >>> output_splits
        [2, 3, 2, 2]                                                     # Rank 0
        [2, 2, 1, 2]                                                     # Rank 1
        [1, 2, 1, 2]                                                     # Rank 2
        [1, 2, 1, 1]                                                     # Rank 3
        >>> input = list(input.split(input_splits))
        >>> input
        [tensor([0, 1]), tensor([2, 3]), tensor([4]), tensor([5])]                   # Rank 0
        [tensor([10, 11, 12]), tensor([13, 14]), tensor([15, 16]), tensor([17, 18])] # Rank 1
        [tensor([20, 21]), tensor([22]), tensor([23]), tensor([24])]                 # Rank 2
        [tensor([30, 31]), tensor([32, 33]), tensor([34, 35]), tensor([36])]         # Rank 3
        >>> output = ...
        >>> dist.all_to_all(output, input)
        >>> output
        [tensor([0, 1]), tensor([10, 11, 12]), tensor([20, 21]), tensor([30, 31])]   # Rank 0
        [tensor([2, 3]), tensor([13, 14]), tensor([22]), tensor([32, 33])]           # Rank 1
        [tensor([4]), tensor([15, 16]), tensor([23]), tensor([34, 35])]              # Rank 2
        [tensor([5]), tensor([17, 18]), tensor([24]), tensor([36])]                  # Rank 3
    """
    ...

def barrier(group=..., async_op=...):
    """
    Synchronizes all processes.

    This collective blocks processes until the whole group enters this function,
    if async_op is False, or if async work handle is called on wait().

    Arguments:
        group (ProcessGroup, optional): The process group to work on
        async_op (bool, optional): Whether this op should be an async op

    Returns:
        Async work handle, if async_op is set to True.
        None, if not async_op or if not part of the group
    """
    ...

def new_group(ranks=..., timeout=..., backend=...):
    """
    Creates a new distributed group.

    This function requires that all processes in the main group (i.e. all
    processes that are part of the distributed job) enter this function, even
    if they are not going to be members of the group. Additionally, groups
    should be created in the same order in all processes.

    Arguments:
        ranks (list[int]): List of ranks of group members.
        timeout (timedelta, optional): Timeout for operations executed against
            the process group. Default value equals 30 minutes.
            This is only applicable for the ``gloo`` backend.
        backend (str or Backend, optional): The backend to use. Depending on
            build-time configurations, valid values are ``gloo`` and ``nccl``.
            By default uses the same backend as the global group. This field
            should be given as a lowercase string (e.g., ``"gloo"``), which can
            also be accessed via :class:`Backend` attributes (e.g.,
            ``Backend.GLOO``).

    Returns:
        A handle of distributed group that can be given to collective calls.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from __future__ import absolute_import, division, print_function, unicode_literals
from .distributed_c10d import *
from .distributed_c10d import _backend

def is_available():
    """
    Returns ``True`` if the distributed package is available. Otherwise,
    ``torch.distributed`` does not expose any other APIs. Currently,
    ``torch.distributed`` is available on Linux and MacOS. Set
    ``USE_DISTRIBUTED=1`` to enable it when building PyTorch from source.
    Currently, the default value is ``USE_DISTRIBUTED=1`` for Linux and
    ``USE_DISTRIBUTED=0`` for MacOS.
    """
    ...

if is_available() and not torch._C._c10d_init():
    ...
if is_available():
    ...
"""
This type stub file was generated by pyright.
"""

from datetime import timedelta

default_pg_timeout = timedelta(minutes=30)
"""
This type stub file was generated by pyright.
"""

import sys
import torch
from __future__ import absolute_import, division, print_function, unicode_literals

def is_available():
    ...

if is_available() and not torch._C._dist_autograd_init():
    ...
class context(object):
    '''
    Context object to wrap forward and backward passes when using
    distributed autograd. The ``context_id`` generated in the ``with``
    statement  is required to uniquely identify a distributed backward pass
    on all workers. Each worker stores metadata associated with this
    ``context_id``, which is required to correctly execute a distributed
    autograd pass.

    Example::
        >>> import torch.distributed.autograd as dist_autograd
        >>> with dist_autograd.context() as context_id:
        >>>   t1 = torch.rand((3, 3), requires_grad=True)
        >>>   t2 = torch.rand((3, 3), requires_grad=True)
        >>>   loss = rpc.rpc_sync("worker1", torch.add, args=(t1, t2)).sum()
        >>>   dist_autograd.backward(context_id, [loss])
    '''
    def __enter__(self):
        ...
    
    def __exit__(self, type, value, traceback):
        ...
    


"""
This type stub file was generated by pyright.
"""

_rendezvous_handlers = {  }
def register_rendezvous_handler(scheme, handler):
    """Registers a new rendezvous handler.

    Before we can run collective algorithms, participating processes
    need to find each other and exchange information to be able to
    communicate. We call this process rendezvous.

    The outcome of the rendezvous process is a triplet containing a
    shared key/value store, the rank of the process, and the total
    number of participating processes.

    If none of the bundled rendezvous methods apply to your execution
    environment you can opt to register your own rendezvous handler.
    Pick a unique name and use the URL scheme to identify it when
    calling the `rendezvous()` function.

    Arguments:
        scheme (str): URL scheme to identify your rendezvous handler.
        handler (function): Handler that is invoked when the
            `rendezvous()` function is called with a URL that uses
            the corresponding scheme. It must be a generator function
            that yields the triplet.
    """
    ...

def rendezvous(url, rank=..., world_size=..., **kwargs):
    ...

"""
This type stub file was generated by pyright.
"""

import collections
import enum

BackendValue = collections.namedtuple("BackendValue", ["construct_rpc_backend_options_handler", "init_backend_handler"])
_backend_type_doc = """
    An enum class of available backends.

    PyTorch ships with two builtin backends: ``BackendType.PROCESS_GROUP`` and
    ``BackendType.TENSORPIPE``. Additional ones can be registered using the
    :func:`~torch.distributed.rpc.backend_registry.register_backend` function.
"""
BackendType = enum.Enum(value="BackendType", names={  })
def backend_registered(backend_name):
    """
    Checks if backend_name is registered as an RPC backend.

    Arguments:
        backend_name (str): string to identify the RPC backend.
    Returns:
        True if the backend has been registered with ``register_backend``, else
        False.
    """
    ...

def register_backend(backend_name, construct_rpc_backend_options_handler, init_backend_handler):
    """Registers a new RPC backend.

    Arguments:
        backend_name (str): backend string to identify the handler.
        construct_rpc_backend_options_handler (function):
            Handler that is invoked when
            rpc_backend.construct_rpc_backend_options(**dict) is called.
        init_backend_handler (function): Handler that is invoked when the
            `_init_rpc_backend()` function is called with a backend.
             This returns the agent.
    """
    ...

def construct_rpc_backend_options(backend, rpc_timeout=..., init_method=..., **kwargs):
    ...

def init_backend(backend, *args, **kwargs):
    ...

"""
This type stub file was generated by pyright.
"""

import numbers
import sys
import torch
import torch.distributed as dist
import torch.distributed.autograd as dist_autograd
from __future__ import absolute_import, division, print_function, unicode_literals
from . import _set_profiler_node_id, api, backend_registry, functions
from .api import *
from .backend_registry import BackendType
from .server_process_global_profiler import _server_process_global_profile

def is_available():
    ...

if is_available() and not torch._C._rpc_init():
    ...
if is_available():
    def init_rpc(name, backend=..., rank=..., world_size=..., rpc_backend_options=...):
        r"""
        Initializes RPC primitives such as the local RPC agent
        and distributed autograd, which immediately makes the current
        process ready to send and receive RPCs.

        Arguments:
            backend (BackendType, optional): The type of RPC backend
                implementation. Supported values include
                ``BackendType.PROCESS_GROUP`` (the default) and
                ``BackendType.TENSORPIPE``. See :ref:`rpc-backends` for more
                information.
            name (str): a globally unique name of this node. (e.g.,
                ``Trainer3``, ``ParameterServer2``, ``Master``, ``Worker1``)
                Name can only contain number, alphabet, underscore, colon,
                and/or dash, and must be shorter than 128 characters.
            rank (int): a globally unique id/rank of this node.
            world_size (int): The number of workers in the group.
            rpc_backend_options (RpcBackendOptions, optional): The options
                passed to the RpcAgent constructor. It must be an agent-specific
                subclass of :class:`~torch.distributed.rpc.RpcBackendOptions`
                and contains agent-specific initialization configurations. By
                default, for all agents, it sets the default timeout to 60
                seconds and performs the rendezvous with an underlying process
                group initialized using ``init_method = "env://"``,
                meaning that environment variables ``MASTER_ADDR`` and
                ``MASTER_PORT`` need to be set properly. See
                :ref:`rpc-backends` for more information and find which options
                are available.
        """
        ...
    
"""
This type stub file was generated by pyright.
"""

from datetime import timedelta
from . import _DEFAULT_INIT_METHOD, _DEFAULT_NUM_SEND_RECV_THREADS, _DEFAULT_NUM_WORKER_THREADS, _DEFAULT_RPC_TIMEOUT_SEC, _UNSET_RPC_TIMEOUT

DEFAULT_RPC_TIMEOUT_SEC = _DEFAULT_RPC_TIMEOUT_SEC
DEFAULT_INIT_METHOD = _DEFAULT_INIT_METHOD
DEFAULT_NUM_SEND_RECV_THREADS = _DEFAULT_NUM_SEND_RECV_THREADS
DEFAULT_NUM_WORKER_THREADS = _DEFAULT_NUM_WORKER_THREADS
DEFAULT_PROCESS_GROUP_TIMEOUT = timedelta(milliseconds=2 ** 31 - 1)
UNSET_RPC_TIMEOUT = _UNSET_RPC_TIMEOUT
"""
This type stub file was generated by pyright.
"""

import collections
import logging
import threading
from typing import Generic, TypeVar
from .internal import _internal_rpc_pickler

logger = logging.getLogger(__name__)
_ignore_rref_leak = True
_default_pickler = _internal_rpc_pickler
class WaitAllWorkersStates(object):
    def __init__(self) -> None:
        ...
    


_ALL_WORKER_NAMES = None
_wait_all_workers_dict_lock = threading.RLock()
_wait_all_workers_sequence_id = 0
_wait_all_workers_sequence_id_to_states = collections.defaultdict(WaitAllWorkersStates)
@_require_initialized
def shutdown(graceful=...):
    r"""
    Perform a shutdown of the RPC agent, and then destroy the RPC agent. This
    stops the local agent from accepting outstanding requests, and shuts
    down the RPC framework by terminating all RPC threads. If ``graceful=True``,
    this will block until all local and remote RPC processes reach this method
    and wait for all outstanding work to complete. Otherwise, if
    ``graceful=False``, this is a local shutdown, and it does not wait for other
    RPC processes to reach this method.

    .. warning::
        For :class:`~torch.futures.Future` objects returned by
        :meth:`~torch.distributed.rpc.rpc_async`, ``future.wait()`` should not
        be called after ``shutdown()``.

    Arguments:
        graceful (bool): Whether to do a graceful shutdown or not. If True,
                         this will 1) wait until there is no pending system
                         messages for ``UserRRefs`` and delete them; 2) block
                         until all local and remote RPC processes have reached
                         this method and wait for all outstanding work to
                         complete.

    Example::
        Make sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly
        on both workers. Refer to :meth:`~torch.distributed.init_process_group`
        API for more details. For example,

        >>> export MASTER_ADDR=localhost
        >>> export MASTER_PORT=5678

        Then run the following code in two different processes:

        >>> # On worker 0:
        >>> import torch
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> # do some work
        >>> result = rpc.rpc_sync("worker1", torch.add, args=(torch.ones(1), 1))
        >>> # ready to shutdown
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> # wait for worker 0 to finish work, and then shutdown.
        >>> rpc.shutdown()
    """
    ...

@_require_initialized
def get_worker_info(worker_name=...):
    r"""
    Get :class:`~torch.distributed.rpc.WorkerInfo` of a given worker name.
    Use this :class:`~torch.distributed.rpc.WorkerInfo` to avoid passing an
    expensive string on every invocation.

    Arguments:
        worker_name (str): the string name of a worker. If ``None``, return the
                           the id of the current worker. (default ``None``)

    Returns:
        :class:`~torch.distributed.rpc.WorkerInfo` instance for the given
        ``worker_name`` or :class:`~torch.distributed.rpc.WorkerInfo` of the
        current worker if ``worker_name`` is ``None``.
    """
    ...

T = TypeVar("T")
GenericWithOneTypeVar = Generic[T]
def method_factory(method_name, docstring):
    ...

@_require_initialized
def remote(to, func, args=..., kwargs=..., timeout=...):
    r"""
    Make a remote call to run ``func`` on worker ``to`` and return an
    :class:`~torch.distributed.rpc.RRef` to the result value immediately.
    Worker ``to`` will be the owner of the returned
    :class:`~torch.distributed.rpc.RRef`, and the worker calling ``remote`` is
    a user. The owner manages the global reference count of its
    :class:`~torch.distributed.rpc.RRef`, and the owner
    :class:`~torch.distributed.rpc.RRef` is only destructed when globally there
    are no living references to it.

    Arguments:
        to (str or WorkerInfo): id or name of the destination worker.
        func (callable): a callable function, such as Python callables, builtin
                         operators (e.g. :meth:`~torch.add`) and annotated
                         TorchScript functions.
        args (tuple): the argument tuple for the ``func`` invocation.
        kwargs (dict): is a dictionary of keyword arguments for the ``func``
                       invocation.

        timeout (float, optional): timeout in seconds for this remote call. If the
                                   creation of this
                                   :class:`~torch.distributed.rpc.RRef` on worker
                                   ``to`` is not successfully processed on this
                                   worker within this timeout, then the next time
                                   there is an attempt to use the RRef (such as
                                   ``to_here()``), a timeout will be raised
                                   indicating this failure. A value of 0 indicates
                                   an infinite timeout, i.e. a timeout error will
                                   never be raised. If not provided, the default
                                   value set during initialization or with
                                   ``_set_rpc_timeout`` is used.

    Returns:
        A user :class:`~torch.distributed.rpc.RRef` instance to the result
        value. Use the blocking API :meth:`torch.distributed.rpc.RRef.to_here`
        to retrieve the result value locally.

    .. warning ::
        Using GPU tensors as arguments or return values of ``func`` is not
        supported since we don't support sending GPU tensors over the wire. You
        need to explicitly copy GPU tensors to CPU before using them as
        arguments or return values of ``func``.

    .. warning ::
        The ``remote`` API does not copy storages of argument tensors until
        sending them over the wire, which could be done by a different thread
        depending on the RPC backend type. The caller should make sure that the
        contents of those tensors stay intact until the returned RRef is
        confirmed by the owner, which can be checked using the
        :meth:`torch.distributed.rpc.RRef.confirmed_by_owner` API.

    .. warning ::
        Errors such as timeouts for the ``remote`` API are handled on a
        best-effort basis. This means that when remote calls initiated by
        ``remote`` fail, such as with a timeout error, we take a best-effort
        approach to error handling. This means that errors are handled and set
        on the resulting RRef on an asynchronous basis. If the RRef has not been
        used by the application before this handling (such as ``to_here`` or
        fork call), then future uses of the ``RRef`` will appropriately raise
        errors. However, it is possible that the user application will use the
        ``RRef`` before the errors are handled. In this case, errors may not be
        raised as they have not yet been handled.

    Example::
        Make sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly
        on both workers. Refer to :meth:`~torch.distributed.init_process_group`
        API for more details. For example,

        >>> export MASTER_ADDR=localhost
        >>> export MASTER_PORT=5678

        Then run the following code in two different processes:

        >>> # On worker 0:
        >>> import torch
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> rref1 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 3))
        >>> rref2 = rpc.remote("worker1", torch.add, args=(torch.ones(2), 1))
        >>> x = rref1.to_here() + rref2.to_here()
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()

        Below is an example of running a TorchScript function using RPC.

        >>> # On both workers:
        >>> @torch.jit.script
        >>> def my_script_add(t1, t2):
        >>>    return torch.add(t1, t2)

        >>> # On worker 0:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> rref = rpc.remote("worker1", my_script_add, args=(torch.ones(2), 3))
        >>> rref.to_here()
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()
    """
    ...

@_require_initialized
def rpc_sync(to, func, args=..., kwargs=..., timeout=...):
    r"""
    Make a blocking RPC call to run function ``func`` on worker ``to``. RPC
    messages are sent and received in parallel to execution of Python code. This
    method is thread-safe.

    Arguments:
        to (str or WorkerInfo): id or name of the destination worker.
        func (callable): a callable function, such as Python callables, builtin
                         operators (e.g. :meth:`~torch.add`) and annotated
                         TorchScript functions.
        args (tuple): the argument tuple for the ``func`` invocation.
        kwargs (dict): is a dictionary of keyword arguments for the ``func``
                       invocation.
        timeout (float, optional): timeout in seconds to use for this RPC. If
                                   the RPC does not complete in this amount of
                                   time, an exception indicating it has
                                   timed out will be raised. A value of 0
                                   indicates an infinite timeout, i.e. a timeout
                                   error will never be raised. If not provided,
                                   the default value set during initialization
                                   or with ``_set_rpc_timeout`` is used.

    Returns:
        Returns the result of running ``func`` with ``args`` and ``kwargs``.

    .. warning ::
        Using GPU tensors as arguments or return values of ``func`` is not
        supported since we don't support sending GPU tensors over the wire. You
        need to explicitly copy GPU tensors to CPU before using them as
        arguments or return values of ``func``.

    Example::
        Make sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly
        on both workers. Refer to :meth:`~torch.distributed.init_process_group`
        API for more details. For example,

        >>> export MASTER_ADDR=localhost
        >>> export MASTER_PORT=5678

        Then run the following code in two different processes:

        >>> # On worker 0:
        >>> import torch
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> ret = rpc.rpc_sync("worker1", torch.add, args=(torch.ones(2), 3))
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()

        Below is an example of running a TorchScript function using RPC.

        >>> # On both workers:
        >>> @torch.jit.script
        >>> def my_script_add(t1, t2):
        >>>    return torch.add(t1, t2)

        >>> # On worker 0:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> ret = rpc.rpc_sync("worker1", my_script_add, args=(torch.ones(2), 3))
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()

    """
    ...

@_require_initialized
def rpc_async(to, func, args=..., kwargs=..., timeout=...):
    r"""
    Make a non-blocking RPC call to run function ``func`` on worker ``to``. RPC
    messages are sent and received in parallel to execution of Python code. This
    method is thread-safe. This method will immediately return a
    :class:`~torch.futures.Future` that can be awaited on.

    Arguments:
        to (str or WorkerInfo): id or name of the destination worker.
        func (callable): a callable function, such as Python callables, builtin
                         operators (e.g. :meth:`~torch.add`) and annotated
                         TorchScript functions.
        args (tuple): the argument tuple for the ``func`` invocation.
        kwargs (dict): is a dictionary of keyword arguments for the ``func``
                       invocation.
        timeout (float, optional): timeout in seconds to use for this RPC. If
                                   the RPC does not complete in this amount of
                                   time, an exception indicating it has
                                   timed out will be raised. A value of 0
                                   indicates an infinite timeout, i.e. a timeout
                                   error will never be raised. If not provided,
                                   the default value set during initialization
                                   or with ``_set_rpc_timeout`` is used.


    Returns:
        Returns a :class:`~torch.futures.Future` object that can be waited
        on. When completed, the return value of ``func`` on ``args`` and
        ``kwargs`` can be retrieved from the :class:`~torch.futures.Future`
        object.

    .. warning ::
        Using GPU tensors as arguments or return values of ``func`` is not
        supported since we don't support sending GPU tensors over the wire. You
        need to explicitly copy GPU tensors to CPU before using them as
        arguments or return values of ``func``.

    .. warning ::
        The ``rpc_async`` API does not copy storages of argument tensors until
        sending them over the wire, which could be done by a different thread
        depending on the RPC backend type. The caller should make sure that the
        contents of those tensors stay intact until the returned
        :class:`~torch.futures.Future` completes.

    Example::
        Make sure that ``MASTER_ADDR`` and ``MASTER_PORT`` are set properly
        on both workers. Refer to :meth:`~torch.distributed.init_process_group`
        API for more details. For example,

        >>> export MASTER_ADDR=localhost
        >>> export MASTER_PORT=5678

        Then run the following code in two different processes:

        >>> # On worker 0:
        >>> import torch
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> fut1 = rpc.rpc_async("worker1", torch.add, args=(torch.ones(2), 3))
        >>> fut2 = rpc.rpc_async("worker1", min, args=(1, 2))
        >>> result = fut1.wait() + fut2.wait()
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()

        Below is an example of running a TorchScript function using RPC.

        >>> # On both workers:
        >>> @torch.jit.script
        >>> def my_script_add(t1, t2):
        >>>    return torch.add(t1, t2)

        >>> # On worker 0:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> fut = rpc.rpc_async("worker1", my_script_add, args=(torch.ones(2), 3))
        >>> ret = fut.wait()
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> rpc.shutdown()
    """
    ...

"""
This type stub file was generated by pyright.
"""

def async_execution(fn):
    r"""
    A decorator for a function indicating that the return value of the function
    is guaranteed to be a :class:`~torch.futures.Future` object and this
    function can run asynchronously on the RPC callee. More specifically, the
    callee extracts the :class:`~torch.futures.Future` returned by the wrapped
    function and installs subsequent processing steps as a callback to that
    :class:`~torch.futures.Future`. The installed callback will read the value
    from the :class:`~torch.futures.Future` when completed and send the
    value back as the RPC response. That also means the returned
    :class:`~torch.futures.Future` only exists on the callee side and is never
    sent through RPC. This decorator is useful when the wrapped function's
    (``fn``) execution needs to pause and resume due to, e.g., containing
    :meth:`~torch.distributed.rpc.rpc_async` or waiting for other signals.

    .. note:: To enable asynchronous execution, applications must pass the
        function object returned by this decorator to RPC APIs. Otherwise, RPC
        will not be able to detect the attributes installed by this decorator.
        However, this does not mean this decorator has to be outmost one when
        defining a function. For example, when combined with ``@staticmethod``
        or ``@classmethod``, ``@rpc.functions.async_execution`` needs to be the
        inner decorator to allow the target function be recognized as a static
        or class function. This target function can still execute asynchronously
        because, when accessed, the static or class method preserves attributes
        installed by ``@rpc.functions.async_execution``.

    .. warning:: `autograd profiler <https://pytorch.org/docs/stable/autograd.html#profiler>`_
        does not work with ``async_execution`` functions.

    Example::
        The returned :class:`~torch.futures.Future` object can come from
        ``rpc.rpc_async``, ``Future.then(cb)``, or :class:`~torch.futures.Future`
        constructor. The example below shows directly using the
        :class:`~torch.futures.Future` returned by ``Future.then(cb)``.

        >>> from torch.distributed import rpc
        >>>
        >>> # omitting setup and shutdown RPC
        >>>
        >>> # On all workers
        >>> @rpc.functions.async_execution
        >>> def async_add_chained(to, x, y, z):
        >>>     # This function runs on "worker1" and returns immediately when
        >>>     # the callback is installed through the `then(cb)` API. In the
        >>>     # mean time, the `rpc_async` to "worker2" can run concurrently.
        >>>     # When the return value of that `rpc_async` arrives at
        >>>     # "worker1", "worker1" will run the lambda function accordinly
        >>>     # and set the value for the previously returned `Future`, which
        >>>     # will then trigger RPC to send the result back to "worker0".
        >>>     return rpc.rpc_async(to, torch.add, args=(x, y)).then(
        >>>         lambda fut: fut.wait() + z
        >>>     )
        >>>
        >>> # On worker0
        >>> ret = rpc.rpc_sync(
        >>>     "worker1",
        >>>     async_add_chained,
        >>>     args=("worker2", torch.ones(2), 1, 1)
        >>> )
        >>> print(ret)  # prints tensor([3., 3.])

        When combined with TorchScript decorators, this decorator must be the
        outmost one.

        >>> from torch.distributed import rpc
        >>>
        >>> # omitting setup and shutdown RPC
        >>>
        >>> # On all workers
        >>> @torch.jit.script
        >>> def script_add(x, y):
        >>>     # type: (Tensor, Tensor) -> Tensor
        >>>     return x + y
        >>>
        >>> @rpc.functions.async_execution
        >>> @torch.jit.script
        >>> def async_add(to, x, y):
        >>>     # type: (str, Tensor, Tensor) -> Future[Tensor]
        >>>     return rpc.rpc_async(to, script_add, (x, y))
        >>>
        >>> # On worker0
        >>> ret = rpc.rpc_sync(
        >>>     "worker1",
        >>>     async_add,
        >>>     args=("worker2", torch.ones(2), 1)
        >>> )
        >>> print(ret)  # prints tensor([2., 2.])

        When combined with static or class method, this decorator must be the
        inner one.

        >>> from torch.distributed import rpc
        >>>
        >>> # omitting setup and shutdown RPC
        >>>
        >>> # On all workers
        >>> class AsyncExecutionClass:
        >>>
        >>>     @staticmethod
        >>>     @rpc.functions.async_execution
        >>>     def static_async_add(to, x, y, z):
        >>>         return rpc.rpc_async(to, torch.add, args=(x, y)).then(
        >>>             lambda fut: fut.wait() + z
        >>>         )
        >>>
        >>>     @classmethod
        >>>     @rpc.functions.async_execution
        >>>     def class_async_add(cls, to, x, y, z):
        >>>         ret_fut = torch.futures.Future()
        >>>         rpc.rpc_async(to, torch.add, args=(x, y)).then(
        >>>             lambda fut: ret_fut.set_result(fut.wait() + z)
        >>>         )
        >>>         return ret_fut
        >>>
        >>> # On worker0
        >>> ret = rpc.rpc_sync(
        >>>     "worker1",
        >>>     AsyncExecutionClass.static_async_add,
        >>>     args=("worker2", torch.ones(2), 1, 2)
        >>> )
        >>> print(ret)  # prints tensor([4., 4.])
        >>>
        >>> ret = rpc.rpc_sync(
        >>>     "worker1",
        >>>     AsyncExecutionClass.class_async_add,
        >>>     args=("worker2", torch.ones(2), 1, 2)
        >>> )
        >>> print(ret)  # prints tensor([4., 4.])
    """
    ...

"""
This type stub file was generated by pyright.
"""

from torch.autograd.profiler import profile

class _server_process_global_profile(profile):
    """
    It has the same API as ``torch.autograd.profiler.profile`` class,
    except that it enables profiling on all threads running RPC server request callbacks.

    Context manager that manages autograd profiler state and holds a summary of results.
    Under the hood it just records events of functions being executed in C++ and
    exposes those events to Python. You can wrap any code into it and it will
    only report runtime of PyTorch functions.
    Note: profiler is thread local and is automatically propagated into the async tasks

    Arguments:
        enabled (bool, optional): Setting this to False makes this context manager a no-op.
            Default: ``True``.

        use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.
            Adds approximately 4us of overhead to each tensor operation.
            Default: ``False``

        record_shapes (bool, optional): If shapes recording is set, information
            about input dimensions will be collected. This allows one to see which
            dimensions have been used under the hood and further group by them
            using prof.key_averages(group_by_input_shape=True). Please note that
            shape recording might skew your profiling data. It is recommended to
            use separate runs with and without shape recording to validate the timing.
            Most likely the skew will be negligible for bottom most events (in a case
            of nested function calls). But for higher level functions the total
            self cpu time might be artificially increased because of the shape
            collection.

        profile_memory (bool, optional): Whether to report memory usage, default: ``False``

    .. warning:
        Enabling memory profiling incurs additional profiler overhead

    .. warning:
        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),
        one cannot use the profiler with ``use_cuda = True`` to benchmark
        DataLoaders with ``num_workers > 0``. If you wish to benchmark data loading,
        please use ``use_cuda = False`` or ``num_workers = 0``.

    Example:
        >>> # On worker 0:
        >>> import torch
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker0", rank=0, world_size=2)
        >>> x, y = torch.tensor(1), torch.tensor(2)
        >>> outer_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
        >>> outer_profile_rref.rpc_sync().__enter__()
        >>> rpc.rpc_sync(dst_worker_name, torch.add, (x, y))
        >>> inner_profile_rref = rpc.remote(dst_worker_name, rpc._server_process_global_profile)
        >>> inner_profile_rref.rpc_sync().__enter__()
        >>> rpc.rpc_sync(dst_worker_name, torch.sub, (x, y))
        >>> inner_profile_rref.rpc_sync().__exit__(None, None, None)
        >>> outer_profile_rref.rpc_sync().__exit__(None, None, None
        >>> print(inner_profile_rref.rpc_sync().key_averages())
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        Name       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        sub        85.06%           76.275us         100.00%          89.667us         89.667us         1
        empty      14.94%           13.392us         14.94%           13.392us         13.392us         1
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        Self CPU time total: 89.667us
        >>> print(outer_profile_rref.rpc_sync().key_averages())
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        Name       Self CPU total %  Self CPU total   CPU total %      CPU total        CPU time avg     Number of Calls
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        sub        35.65%           76.275us         41.91%           89.667us         89.667us         1
        empty      12.67%           27.101us         12.67%           27.101us         13.551us         2
        add        51.68%           110.550us        58.09%           124.259us        124.259us        1
        ---------  ---------------  ---------------  ---------------  ---------------  ---------------  ---------------
        Self CPU time total: 213.926us
        >>> rpc.shutdown()

        >>> # On worker 1:
        >>> import torch.distributed.rpc as rpc
        >>> rpc.init_rpc("worker1", rank=1, world_size=2)
        >>> # wait for worker 0 to finish work, and then shutdown.
        >>> rpc.shutdown()
    """
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __enter__(self):
        """
        Turn on server-side process-global profiling.
        This enables thread-local profiler on all RPC threads running server-side request callbacks.
        """
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        """
        Turn off server-side process-global profiling.
        Aggregrate all profiling events recorded by RPC threads.

        These attribuets are assigned on exiting context.

        Attributes:
            function_events (torch.autograd.profiler.EventList).  It's a list that has helper
            methods, like 1) show record items in a pretty-print table.
            2) do averaging by grouping on keys. 3) and more.

            process_global_function_events (List[torch.autograd.profiler.FunctionEvent]).
            It's a list of ``FunctionEvent`` elements. Every element is a profiling result
            of an RPC request handling within the profiling range.
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

import collections
import threading
from enum import Enum

_thread_local_tensor_tables = threading.local()
class RPCExecMode(Enum):
    SYNC = ...
    ASYNC = ...
    REMOTE = ...


class _InternalRPCPickler:
    r"""
    This class provides serialize() and deserialize() interfaces to serialize
    data to be "binary string + tensor table" format
    So for RPC python UDF function and args, non tensor data will be serialized
    into regular binary string, tensor data will be put into thread local tensor
    tables, this serialization format is consistent with builtin operator and args
    using JIT pickler. This format will make tensor handling in C++ much easier,
    e.g. attach tensor to distributed autograd graph in C++
    """
    def __init__(self) -> None:
        ...
    
    def serialize(self, obj):
        r"""
        Serialize non tensor data into binary string, tensor data into
        tensor table
        """
        ...
    
    def deserialize(self, binary_data, tensor_table):
        r"""
        Deserilize binary string + tensor table to original obj
        """
        ...
    


_internal_rpc_pickler = _InternalRPCPickler()
def serialize(obj):
    ...

def deserialize(binary_data, tensor_table):
    ...

PythonUDF = collections.namedtuple("PythonUDF", ["func", "args", "kwargs"])
RemoteException = collections.namedtuple("RemoteException", ["msg", "exception_type"])
"""
This type stub file was generated by pyright.
"""

from typing import Any

class detect_anomaly(object):
    r"""Context-manager that enable anomaly detection for the autograd engine.

    This does two things:
    - Running the forward pass with detection enabled will allow the backward
    pass to print the traceback of the forward operation that created the failing
    backward function.
    - Any backward computation that generate "nan" value will raise an error.

    .. warning::
        This mode should be enabled only for debugging as the different tests
        will slow down your program execution.

    Example:

        >>> import torch
        >>> from torch import autograd
        >>> class MyFunc(autograd.Function):
        ...     @staticmethod
        ...     def forward(ctx, inp):
        ...         return inp.clone()
        ...     @staticmethod
        ...     def backward(ctx, gO):
        ...         # Error during the backward pass
        ...         raise RuntimeError("Some error in backward")
        ...         return gO.clone()
        >>> def run_fn(a):
        ...     out = MyFunc.apply(a)
        ...     return out.sum()
        >>> inp = torch.rand(10, 10, requires_grad=True)
        >>> out = run_fn(inp)
        >>> out.backward()
            Traceback (most recent call last):
              File "<stdin>", line 1, in <module>
              File "/your/pytorch/install/torch/tensor.py", line 93, in backward
                torch.autograd.backward(self, gradient, retain_graph, create_graph)
              File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
                allow_unreachable=True)  # allow_unreachable flag
              File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
                return self._forward_cls.backward(self, *args)
              File "<stdin>", line 8, in backward
            RuntimeError: Some error in backward
        >>> with autograd.detect_anomaly():
        ...     inp = torch.rand(10, 10, requires_grad=True)
        ...     out = run_fn(inp)
        ...     out.backward()
            Traceback of forward call that caused the error:
              File "tmp.py", line 53, in <module>
                out = run_fn(inp)
              File "tmp.py", line 44, in run_fn
                out = MyFunc.apply(a)
            Traceback (most recent call last):
              File "<stdin>", line 4, in <module>
              File "/your/pytorch/install/torch/tensor.py", line 93, in backward
                torch.autograd.backward(self, gradient, retain_graph, create_graph)
              File "/your/pytorch/install/torch/autograd/__init__.py", line 90, in backward
                allow_unreachable=True)  # allow_unreachable flag
              File "/your/pytorch/install/torch/autograd/function.py", line 76, in apply
                return self._forward_cls.backward(self, *args)
              File "<stdin>", line 8, in backward
            RuntimeError: Some error in backward

    """
    def __init__(self) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    


class set_detect_anomaly(object):
    r"""Context-manager that sets the anomaly detection for the autograd engine on or off.

    ``set_detect_anomaly`` will enable or disable the autograd anomaly detection
    based on its argument :attr:`mode`.
    It can be used as a context-manager or as a function.

    See ``detect_anomaly`` above for details of the anomaly detection behaviour.

    Arguments:
        mode (bool): Flag whether to enable anomaly detection (``True``),
                     or disable (``False``).

    """
    def __init__(self, mode: bool) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Any, Callable, TypeVar

FuncType = Callable[..., Any]
T = TypeVar('T', bound=FuncType)
class no_grad:
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    
    def __call__(self, func: T) -> T:
        ...
    


class enable_grad:
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    
    def __call__(self, func: T) -> T:
        ...
    


class set_grad_enabled:
    def __init__(self, mode: bool) -> None:
        ...
    
    def __enter__(self) -> None:
        ...
    
    def __exit__(self, *args: Any) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
import warnings
from typing import Any, Callable, Optional, Sequence, Tuple, Union
from torch.types import _TensorOrTensors
from .variable import Variable
from .function import Function, NestedIOFunction
from .gradcheck import gradcheck, gradgradcheck
from .grad_mode import enable_grad, no_grad, set_grad_enabled
from .anomaly_mode import detect_anomaly, set_detect_anomaly
from . import functional, profiler

"""
``torch.autograd`` provides classes and functions implementing automatic
differentiation of arbitrary scalar valued functions. It requires minimal
changes to the existing code - you only need to declare :class:`Tensor` s
for which gradients should be computed with the ``requires_grad=True`` keyword.
As of now, we only support autograd for floating point :class:`Tensor` types (
half, float, double and bfloat16) and complex :class:`Tensor` types (cfloat, cdouble).
"""
def backward(tensors: _TensorOrTensors, grad_tensors: Optional[_TensorOrTensors] = ..., retain_graph: Optional[bool] = ..., create_graph: bool = ..., grad_variables: Optional[_TensorOrTensors] = ...) -> None:
    r"""Computes the sum of gradients of given tensors w.r.t. graph leaves.

    The graph is differentiated using the chain rule. If any of ``tensors``
    are non-scalar (i.e. their data has more than one element) and require
    gradient, then the Jacobian-vector product would be computed, in this
    case the function additionally requires specifying ``grad_tensors``.
    It should be a sequence of matching length, that contains the "vector"
    in the Jacobian-vector product, usually the gradient of the differentiated
    function w.r.t. corresponding tensors (``None`` is an acceptable value for
    all tensors that don't need gradient tensors).

    This function accumulates gradients in the leaves - you might need to zero
    ``.grad`` attributes or set them to ``None`` before calling it.
    See :ref:`Default gradient layouts<default-grad-layouts>`
    for details on the memory layout of accumulated gradients.

    .. note::
        Using this method with ``create_graph=True`` will create a reference cycle
        between the parameter and its gradient which can cause a memory leak.
        We recommend using ``autograd.grad`` when creating the graph to avoid this.
        If you have to use this function, make sure to reset the ``.grad`` fields of your
        parameters to ``None`` after use to break the cycle and avoid the leak.

    Arguments:
        tensors (sequence of Tensor): Tensors of which the derivative will be
            computed.
        grad_tensors (sequence of (Tensor or None)): The "vector" in the Jacobian-vector
            product, usually gradients w.r.t. each element of corresponding tensors.
            None values can be specified for scalar Tensors or ones that don't require
            grad. If a None value would be acceptable for all grad_tensors, then this
            argument is optional.
        retain_graph (bool, optional): If ``False``, the graph used to compute the grad
            will be freed. Note that in nearly all cases setting this option to ``True``
            is not needed and often can be worked around in a much more efficient
            way. Defaults to the value of ``create_graph``.
        create_graph (bool, optional): If ``True``, graph of the derivative will
            be constructed, allowing to compute higher order derivative products.
            Defaults to ``False``.
    """
    ...

def grad(outputs: _TensorOrTensors, inputs: _TensorOrTensors, grad_outputs: Optional[_TensorOrTensors] = ..., retain_graph: Optional[bool] = ..., create_graph: bool = ..., only_inputs: bool = ..., allow_unused: bool = ...) -> Tuple[torch.Tensor, ...]:
    r"""Computes and returns the sum of gradients of outputs w.r.t. the inputs.

    ``grad_outputs`` should be a sequence of length matching ``output``
    containing the "vector" in Jacobian-vector product, usually the pre-computed
    gradients w.r.t. each of the outputs. If an output doesn't require_grad,
    then the gradient can be ``None``).

    If ``only_inputs`` is ``True``, the function will only return a list of gradients
    w.r.t the specified inputs. If it's ``False``, then gradient w.r.t. all remaining
    leaves will still be computed, and will be accumulated into their ``.grad``
    attribute.

    Arguments:
        outputs (sequence of Tensor): outputs of the differentiated function.
        inputs (sequence of Tensor): Inputs w.r.t. which the gradient will be
            returned (and not accumulated into ``.grad``).
        grad_outputs (sequence of Tensor): The "vector" in the Jacobian-vector product.
            Usually gradients w.r.t. each output. None values can be specified for scalar
            Tensors or ones that don't require grad. If a None value would be acceptable
            for all grad_tensors, then this argument is optional. Default: None.
        retain_graph (bool, optional): If ``False``, the graph used to compute the grad
            will be freed. Note that in nearly all cases setting this option to ``True``
            is not needed and often can be worked around in a much more efficient
            way. Defaults to the value of ``create_graph``.
        create_graph (bool, optional): If ``True``, graph of the derivative will
            be constructed, allowing to compute higher order derivative products.
            Default: ``False``.
        allow_unused (bool, optional): If ``False``, specifying inputs that were not
            used when computing outputs (and therefore their grad is always zero)
            is an error. Defaults to ``False``.
    """
    ...

def variable(*args, **kwargs):
    ...

if not torch._C._autograd_init():
    ...
"""
This type stub file was generated by pyright.
"""

from .tensor import *

"""
This type stub file was generated by pyright.
"""

from ..function import Function

class Type(Function):
    @staticmethod
    def forward(ctx, i, dest_type):
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


class Resize(Function):
    @staticmethod
    def forward(ctx, tensor, sizes):
        ...
    
    @staticmethod
    def backward(ctx, grad_output):
        ...
    


"""
This type stub file was generated by pyright.
"""

from collections import defaultdict, namedtuple

class EventList(list):
    """A list of Events (for pretty printing)"""
    def __init__(self, *args, **kwargs) -> None:
        ...
    
    def __str__(self) -> str:
        ...
    
    def populate_cpu_children(self):
        """Populates child events into each underlying FunctionEvent object.
        One event is a child of another if [s1, e1) is inside [s2, e2). Where
        s1 and e1 would be start and end of the child event's interval. And
        s2 and e2 start and end of the parent event's interval

        Example: In event list [[0, 10], [1, 3], [3, 4]] would have make [0, 10]
        be a parent of two other intervals.

        If for any reason two intervals intersect only partially, this function
        will not record a parent child relationship between then.
        """
        ...
    
    @property
    def self_cpu_time_total(self):
        ...
    
    @property
    def cpu_children_populated(self):
        ...
    
    def table(self, sort_by=..., row_limit=..., header=...):
        """Prints an EventList as a nicely formatted table.

        Arguments:
            sort_by (str, optional): Attribute used to sort entries. By default
                they are printed in the same order as they were registered.
                Valid keys include: ``cpu_time``, ``cuda_time``, ``cpu_time_total``,
                ``cuda_time_total``, ``cpu_memory_usage``, ``cuda_memory_usage``,
                ``self_cpu_memory_usage``, ``self_cuda_memory_usage``, ``count``.

        Returns:
            A string containing the table.
        """
        ...
    
    def export_chrome_trace(self, path):
        """Exports an EventList as a Chrome tracing tools file.

        The checkpoint can be later loaded and inspected under ``chrome://tracing`` URL.

        Arguments:
            path (str): Path where the trace will be written.
        """
        ...
    
    def key_averages(self, group_by_input_shapes=...):
        """Averages all function events over their keys.

        @param group_by_input_shapes The key would become
        (event name, input dimensions) rather than just event name.
        This is useful to see which dimensionality contributes to the runtime
        the most and may help with dimension specific optimizations or
        choosing best candidates for quantization (aka fitting a roof line)

        Returns:
            An EventList containing FunctionEventAvg objects.
        """
        ...
    
    def total_average(self):
        """Averages all events.

        Returns:
            A FunctionEventAvg object.
        """
        ...
    


class profile(object):
    """Context manager that manages autograd profiler state and holds a summary of results.
    Under the hood it just records events of functions being executed in C++ and
    exposes those events to Python. You can wrap any code into it and it will
    only report runtime of PyTorch functions.
    Note: profiler is thread local and is automatically propagated into the async tasks

    Arguments:
        enabled (bool, optional): Setting this to False makes this context manager a no-op.
            Default: ``True``.

        use_cuda (bool, optional): Enables timing of CUDA events as well using the cudaEvent API.
            Adds approximately 4us of overhead to each tensor operation.
            Default: ``False``

        record_shapes (bool, optional): If shapes recording is set, information
            about input dimensions will be collected. This allows one to see which
            dimensions have been used under the hood and further group by them
            using prof.key_averages(group_by_input_shape=True). Please note that
            shape recording might skew your profiling data. It is recommended to
            use separate runs with and without shape recording to validate the timing.
            Most likely the skew will be negligible for bottom most events (in a case
            of nested function calls). But for higher level functions the total
            self cpu time might be artificially increased because of the shape
            collection.

        profile_memory (bool, optional): Whether to report memory usage, default: ``False``

    .. warning:
        Enabling memory profiling incurs additional profiler overhead

    .. warning:
        This context managers should not be called recursively, i.e. no nested
        instances are allowed

    .. warning:
        Due to some CUDA multiprocessing limitations (multiprocessing-cuda-note_),
        one cannot use the profiler with ``use_cuda = True`` to benchmark
        DataLoaders with ``num_workers > 0``. If you wish to benchmark data loading,
        please use ``use_cuda = False`` or ``num_workers = 0``.

    Example:
        >>> x = torch.randn((1, 1), requires_grad=True)
        >>> with torch.autograd.profiler.profile() as prof:
        >>>     for _ in range(100):  # any normal python code, really!
        >>>         y = x ** 2
        >>          y.backward()
        >>> # NOTE: some columns were removed for brevity
        >>> print(prof.key_averages().table(sort_by="self_cpu_time_total"))
        -----------------------------------  ---------------  ---------------  ---------------
        Name                                 Self CPU total   CPU time avg     Number of Calls
        -----------------------------------  ---------------  ---------------  ---------------
        mul                                  32.048ms         32.048ms         200
        pow                                  27.041ms         27.041ms         200
        PowBackward0                         9.727ms          55.483ms         100
        torch::autograd::AccumulateGrad      9.148ms          9.148ms          100
        torch::autograd::GraphRoot           691.816us        691.816us        100
        -----------------------------------  ---------------  ---------------  ---------------

    """
    def __init__(self, enabled=..., use_cuda=..., record_shapes=..., profile_memory=...) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        ...
    
    def __repr__(self):
        ...
    
    def __str__(self) -> str:
        ...
    
    def table(self, sort_by=..., row_limit=..., header=...):
        ...
    
    def export_chrome_trace(self, path):
        ...
    
    def key_averages(self, group_by_input_shape=...):
        ...
    
    def total_average(self):
        ...
    
    @property
    def self_cpu_time_total(self):
        """ Returns total time spent on CPU obtained as a sum of
        all self times across all the events.
        """
        ...
    


class record_function(ContextDecorator):
    """Context manager/function decorator that adds a label to a block of
    Python code (or function) when running autograd profiler. It is
    useful when tracing the code profile.

    Arguments:
        name (str): Label assigned to the block of code.
        node_id (int): ID of node, for distributed profiling. Unset in
        non-distributed cases.

    Example:
        >>> x = torch.randn((1, 1), requires_grad=True)
        >>> with torch.autograd.profiler.profile() as prof:
        ...     y = x ** 2
        ...     with torch.autograd.profiler.record_function("label-z"): # label the block
        ...         z = y ** 3
        ...     y.backward()
        ...
        >>> # NOTE: some columns were removed for brevity
        >>> print(prof.key_averages().table(sort_by="self_cpu_time_total"))
        -----------------------------------  ---------------  ---------------  ---------------
        Name                                 Self CPU total %  CPU time avg     Number of Calls
        -----------------------------------  ---------------  ---------------  ---------------
        pow                                  60.77%           47.470us         3
        mul                                  21.73%           25.465us         2
        PowBackward0                         12.03%           121.891us        1
        torch::autograd::AccumulateGrad      2.70%            6.324us          1
        label-z                              2.13%            12.421us         1
        torch::autograd::GraphRoot           0.64%            1.503us          1
        -----------------------------------  ---------------  ---------------  ---------------
        Self CPU time total: 234.344us
        CUDA time total: 0.000us

    """
    def __init__(self, name) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    


class emit_nvtx(object):
    """Context manager that makes every autograd operation emit an NVTX range.

    It is useful when running the program under nvprof::

        nvprof --profile-from-start off -o trace_name.prof -- <regular command here>

    Unfortunately, there's no way to force nvprof to flush the data it collected
    to disk, so for CUDA profiling one has to use this context manager to annotate
    nvprof traces and wait for the process to exit before inspecting them.
    Then, either NVIDIA Visual Profiler (nvvp) can be used to visualize the timeline, or
    :func:`torch.autograd.profiler.load_nvprof` can load the results for inspection
    e.g. in Python REPL.

    .. warning:
        This context manager should not be called recursively, i.e. at most one
        instance should be enabled at any given time.

    Arguments:
        enabled (bool, optional, default=True): Setting ``enabled=False`` makes this context manager a no-op.
            Default: ``True``.
        record_shapes (bool, optional, default=False): If ``record_shapes=True``, the nvtx range wrapping
            each autograd op will append information about the sizes of Tensor arguments received
            by that op, in the following format:
            ``[[arg0.size(0), arg0.size(1), ...], [arg1.size(0), arg1.size(1), ...], ...]``
            Non-tensor arguments will be represented by ``[]``.
            Arguments will be listed in the order they are received by the backend op.
            Please note that this order may not match the order in which those arguments were passed
            on the Python side.  Also note that shape recording may increase the overhead of nvtx range creation.

    Example:
        >>> with torch.cuda.profiler.profile():
        ...     model(x) # Warmup CUDA memory allocator and profiler
        ...     with torch.autograd.profiler.emit_nvtx():
        ...         model(x)

    **Forward-backward correlation**

    When viewing a profile created using :class:`emit_nvtx` in the Nvidia Visual Profiler,
    correlating each backward-pass op with the corresponding forward-pass op can be difficult.
    To ease this task, :class:`emit_nvtx` appends sequence number information to the ranges it
    generates.

    During the forward pass, each function range is decorated with ``seq=<N>``.  ``seq`` is a running
    counter, incremented each time a new backward Function object is created and stashed for backward.
    Thus, the ``seq=<N>`` annotation associated with each forward function range tells you that
    if a backward Function object is created by this forward function,
    the backward object will receive sequence number N.
    During the backward pass, the top-level range wrapping each C++ backward Function's
    ``apply()`` call is decorated with ``stashed seq=<M>``.  ``M`` is the sequence number that
    the backward object was created with.  By comparing ``stashed seq`` numbers in backward with ``seq``
    numbers in forward, you can track down which forward op created each backward Function.

    Any functions executed during the backward pass are also decorated with ``seq=<N>``.  During
    default backward (with ``create_graph=False``) this information is irrelevant, and in fact,
    ``N`` may simply be 0 for all such functions.  Only the top-level ranges associated with
    backward Function objects' ``apply()`` methods are useful, as a way to correlate these Function
    objects with the earlier forward pass.

    **Double-backward**

    If, on the other hand, a backward pass with ``create_graph=True`` is underway (in other words,
    if you are setting up for a double-backward), each function's execution during backward
    is given a nonzero, useful ``seq=<N>``.  Those functions may themselves create Function objects
    to be executed later during double-backward, just as the original functions in the forward pass did.
    The relationship between backward and double-backward is conceptually the same as the relationship
    between forward and backward: The functions still emit current-sequence-number-tagged ranges,
    the Function objects they create still stash those sequence numbers, and during the eventual
    double-backward, the Function objects' ``apply()`` ranges are still tagged with ``stashed seq``
    numbers, which can be compared to `seq` numbers from the backward pass.

    .. warning:
        The sequence number is thread-local, and some forward functions don't create an associated
        backward Function object (instead delegating that to sub-functions further down the call chain).
        For these reasons, the correspondence of stashed sequence numbers in
        backward Function ``apply()`` ranges with `seq` numbers in forward-pass ranges is
        not guaranteed to be 1 to 1.  The sequence numbers alone may not be enough to fully
        disambiguate which forward function created which
        backward Function object.  You may need to make a judgment based on analytic knowledge of what
        the expected correspondence should be.
    """
    def __init__(self, enabled=..., record_shapes=...) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, exc_type, exc_val, exc_tb):
        ...
    


def load_nvprof(path):
    """Opens an nvprof trace file and parses autograd annotations.

    Arguments:
        path (str): path to nvprof trace
    """
    ...

def format_time(time_us):
    """Defines how to format time in FunctionEvent"""
    ...

def format_time_share(time_us, total_time_us):
    """Defines how to format time in FunctionEvent"""
    ...

def format_memory(nbytes):
    """Returns a formatted memory size string"""
    ...

def attr_formatter(name):
    ...

class FormattedTimesMixin(object):
    """Helpers for FunctionEvent and FunctionEventAvg.

    The subclass should define `*_time_total` and `count` attributes.
    """
    cpu_time_str = ...
    cuda_time_str = ...
    cpu_time_total_str = ...
    cuda_time_total_str = ...
    self_cpu_time_total_str = ...
    @property
    def cpu_time(self):
        ...
    
    @property
    def cuda_time(self):
        ...
    


class Interval(object):
    def __init__(self, start, end) -> None:
        ...
    
    def elapsed_us(self):
        ...
    


Kernel = namedtuple('Kernel', ['name', 'device', 'interval'])
class FunctionEvent(FormattedTimesMixin):
    """Profiling information about a single function."""
    def __init__(self, id, node_id, name, thread, cpu_start, cpu_end, input_shapes=..., cpu_memory_usage=..., cuda_memory_usage=..., is_async=..., is_remote=...) -> None:
        ...
    
    def append_kernel(self, name, device, start, end):
        ...
    
    def append_cpu_child(self, child):
        """Append a CPU child of type FunctionEvent.

        One is supposed to append only dirrect children to the event to have
        correct self cpu time being reported.
        """
        ...
    
    @property
    def self_cpu_memory_usage(self):
        ...
    
    @property
    def self_cuda_memory_usage(self):
        ...
    
    @property
    def self_cpu_time_total(self):
        ...
    
    @property
    def cuda_time_total(self):
        ...
    
    @property
    def cpu_time_total(self):
        ...
    
    @property
    def key(self):
        ...
    
    def __repr__(self):
        ...
    


class FunctionEventAvg(FormattedTimesMixin):
    """Used to average stats over multiple FunctionEvent objects."""
    def __init__(self) -> None:
        ...
    
    def add(self, other, group_by_input_shapes=...):
        ...
    
    def __iadd__(self, other):
        ...
    
    def __repr__(self):
        ...
    


class StringTable(defaultdict):
    def __missing__(self, key):
        ...
    


def parse_cpu_trace(thread_records):
    ...

class EnforceUnique(object):
    """Raises an error if a key is seen more than once."""
    def __init__(self) -> None:
        ...
    
    def see(self, *key):
        ...
    


def parse_nvprof_trace(path):
    ...

def build_table(events, sort_by=..., header=..., row_limit=..., use_cuda=..., profile_memory=...):
    """Prints a summary of events (which can be a list of FunctionEvent or FunctionEventAvg)."""
    ...

"""
This type stub file was generated by pyright.
"""

from torch.types import _TensorOrTensors
from typing import Callable, Optional, Union

def zero_gradients(x):
    ...

def make_jacobian(input, num_out):
    ...

def iter_tensors(x, only_requiring_grad=...):
    ...

def get_numerical_jacobian(fn, input, target=..., eps=...):
    """
    input: input to `fn`
    target: the Tensors wrt whom Jacobians are calculated (default=`input`)

    Note that `target` may not even be part of `input` to `fn`, so please be
    **very careful** in this to not clone `target`.
    """
    ...

def get_analytical_jacobian(input, output, nondet_tol=...):
    ...

def gradcheck(func: Callable[..., Union[_TensorOrTensors]], inputs: _TensorOrTensors, eps: float = ..., atol: float = ..., rtol: float = ..., raise_exception: bool = ..., check_sparse_nnz: bool = ..., nondet_tol: float = ..., check_undefined_grad: bool = ...) -> bool:
    r"""Check gradients computed via small finite differences against analytical
    gradients w.r.t. tensors in :attr:`inputs` that are of floating point or complex type
    and with ``requires_grad=True``.

    The check between numerical and analytical gradients uses :func:`~torch.allclose`.

    .. note::
        The default values are designed for :attr:`input` of double precision.
        This check will likely fail if :attr:`input` is of less precision, e.g.,
        ``FloatTensor``.

    .. warning::
       If any checked tensor in :attr:`input` has overlapping memory, i.e.,
       different indices pointing to the same memory address (e.g., from
       :func:`torch.expand`), this check will likely fail because the numerical
       gradients computed by point perturbation at such indices will change
       values at all other indices that share the same memory address.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a Tensor or a tuple of Tensors
        inputs (tuple of Tensor or Tensor): inputs to the function
        eps (float, optional): perturbation for finite differences
        atol (float, optional): absolute tolerance
        rtol (float, optional): relative tolerance
        raise_exception (bool, optional): indicating whether to raise an exception if
            the check fails. The exception gives more information about the
            exact nature of the failure. This is helpful when debugging gradchecks.
        check_sparse_nnz (bool, optional): if True, gradcheck allows for SparseTensor input,
            and for any SparseTensor at input, gradcheck will perform check at nnz positions only.
        nondet_tol (float, optional): tolerance for non-determinism. When running
            identical inputs through the differentiation, the results must either match
            exactly (default, 0.0) or be within this tolerance.
        check_undefined_grad (bool, options): if True, check if undefined output grads
            are supported and treated as zeros

    Returns:
        True if all differences satisfy allclose condition
    """
    ...

def gradgradcheck(func: Callable[..., _TensorOrTensors], inputs: _TensorOrTensors, grad_outputs: Optional[_TensorOrTensors] = ..., eps: float = ..., atol: float = ..., rtol: float = ..., gen_non_contig_grad_outputs: bool = ..., raise_exception: bool = ..., nondet_tol: float = ..., check_undefined_grad: bool = ...) -> bool:
    r"""Check gradients of gradients computed via small finite differences
    against analytical gradients w.r.t. tensors in :attr:`inputs` and
    :attr:`grad_outputs` that are of floating point or complex type and with
    ``requires_grad=True``.

    This function checks that backpropagating through the gradients computed
    to the given :attr:`grad_outputs` are correct.

    The check between numerical and analytical gradients uses :func:`~torch.allclose`.

    .. note::
        The default values are designed for :attr:`input` and
        :attr:`grad_outputs` of double precision. This check will likely fail if
        they are of less precision, e.g., ``FloatTensor``.

    .. warning::
       If any checked tensor in :attr:`input` and :attr:`grad_outputs` has
       overlapping memory, i.e., different indices pointing to the same memory
       address (e.g., from :func:`torch.expand`), this check will likely fail
       because the numerical gradients computed by point perturbation at such
       indices will change values at all other indices that share the same
       memory address.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a Tensor or a tuple of Tensors
        inputs (tuple of Tensor or Tensor): inputs to the function
        grad_outputs (tuple of Tensor or Tensor, optional): The gradients with
            respect to the function's outputs.
        eps (float, optional): perturbation for finite differences
        atol (float, optional): absolute tolerance
        rtol (float, optional): relative tolerance
        gen_non_contig_grad_outputs (bool, optional): if :attr:`grad_outputs` is
            ``None`` and :attr:`gen_non_contig_grad_outputs` is ``True``, the
            randomly generated gradient outputs are made to be noncontiguous
        raise_exception (bool, optional): indicating whether to raise an exception if
            the check fails. The exception gives more information about the
            exact nature of the failure. This is helpful when debugging gradchecks.
        nondet_tol (float, optional): tolerance for non-determinism. When running
            identical inputs through the differentiation, the results must either match
            exactly (default, 0.0) or be within this tolerance. Note that a small amount
            of nondeterminism in the gradient will lead to larger inaccuracies in
            the second derivative.
        check_undefined_grad (bool, options): if True, check if undefined output grads
            are supported and treated as zeros

    Returns:
        True if all differences satisfy allclose condition
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from torch._six import with_metaclass

class VariableMeta(type):
    def __instancecheck__(cls, other):
        ...
    


class Variable(with_metaclass(VariableMeta, torch._C._LegacyVariableBase)):
    ...


"""
This type stub file was generated by pyright.
"""

def vjp(func, inputs, v=..., create_graph=..., strict=...):
    r"""Function that computes the dot product between a vector ``v`` and the
    Jacobian of the given function at the point given by the inputs.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a tuple of Tensors or a Tensor.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        v (tuple of Tensors or Tensor): The vector for which the vector
            Jacobian product is computed.  Must be the same size as the output
            of ``func``. This argument is optional when the output of ``func``
            contains a single element and (if it is not provided) will be set
            as a Tensor containing a single ``1``.
        create_graph (bool, optional): If ``True``, both the output and result
            will be computed in a differentiable way. Note that when ``strict``
            is ``False``, the result can not require gradients or be
            disconnected from the inputs.  Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we
            detect that there exists an input such that all the outputs are
            independent of it. If ``False``, we return a Tensor of zeros as the
            vjp for said inputs, which is the expected mathematical value.
            Defaults to ``False``.

    Returns:
        vjp (tuple of Tensors or Tensor): result of the dot product with
        the same shape as the inputs.

    Example:

        >>> def exp_reducer(x):
        ...   return x.exp().sum(dim=1)
        >>> inputs = torch.rand(4, 4)
        >>> v = torch.ones(4)
        >>> vjp(exp_reducer, inputs, v)
        (tensor([5.7817, 7.2458, 5.7830, 6.7782]),
         tensor([[1.4458, 1.3962, 1.3042, 1.6354],
                [2.1288, 1.0652, 1.5483, 2.5035],
                [2.2046, 1.1292, 1.1432, 1.3059],
                [1.3225, 1.6652, 1.7753, 2.0152]]))

        >>> vjp(exp_reducer, inputs, v, create_graph=True)
        (tensor([5.7817, 7.2458, 5.7830, 6.7782], grad_fn=<SumBackward1>),
         tensor([[1.4458, 1.3962, 1.3042, 1.6354],
                [2.1288, 1.0652, 1.5483, 2.5035],
                [2.2046, 1.1292, 1.1432, 1.3059],
                [1.3225, 1.6652, 1.7753, 2.0152]], grad_fn=<MulBackward0>))

        >>> def adder(x, y):
        ...   return 2 * x + 3 * y
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> v = torch.ones(2)
        >>> vjp(adder, inputs, v)
        (tensor([2.4225, 2.3340]),
         (tensor([2., 2.]), tensor([3., 3.])))
    """
    ...

def jvp(func, inputs, v=..., create_graph=..., strict=...):
    r"""Function that computes the dot product between  the Jacobian of
    the given function at the point given by the inputs and a vector ``v``.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a tuple of Tensors or a Tensor.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        v (tuple of Tensors or Tensor): The vector for which the Jacobian
            vector product is computed. Must be the same size as the input of
            ``func``. This argument is optional when the input to ``func``
            contains a single element and (if it is not provided) will be set
            as a Tensor containing a single ``1``.
        create_graph (bool, optional): If ``True``, both the output and result
            will be computed in a differentiable way. Note that when ``strict``
            is ``False``, the result can not require gradients or be
            disconnected from the inputs.  Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we
            detect that there exists an input such that all the outputs are
            independent of it. If ``False``, we return a Tensor of zeros as the
            jvp for said inputs, which is the expected mathematical value.
            Defaults to ``False``.

    Returns:
        jvp (tuple of Tensors or Tensor): result of the dot product with
        the same shape as the output.

    Example:

        >>> def exp_reducer(x):
        ...   return x.exp().sum(dim=1)
        >>> inputs = torch.rand(4, 4)
        >>> v = torch.ones(4, 4)
        >>> jvp(exp_reducer, inputs, v)
        (tensor([6.3090, 4.6742, 7.9114, 8.2106]),
         tensor([6.3090, 4.6742, 7.9114, 8.2106]))

        >>> jvp(exp_reducer, inputs, v, create_graph=True)
        (tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SumBackward1>),
         tensor([6.3090, 4.6742, 7.9114, 8.2106], grad_fn=<SqueezeBackward1>))

        >>> def adder(x, y):
        ...   return 2 * x + 3 * y
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> v = (torch.ones(2), torch.ones(2))
        >>> jvp(adder, inputs, v)
        (tensor([2.2399, 2.5005]),
         tensor([5., 5.]))

    Note:
        The jvp is currently computed by using the backward of the backward
        (sometimes called the double backwards trick) as we don't have support
        for forward mode AD in PyTorch at the moment.
    """
    ...

def jacobian(func, inputs, create_graph=..., strict=...):
    r"""Function that computes the Jacobian of a given function.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a tuple of Tensors or a Tensor.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        create_graph (bool, optional): If ``True``, the Jacobian will be
            computed in a differentiable manner. Note that when ``strict`` is
            ``False``, the result can not require gradients or be disconnected
            from the inputs.  Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we
            detect that there exists an input such that all the outputs are
            independent of it. If ``False``, we return a Tensor of zeros as the
            jacobian for said inputs, which is the expected mathematical value.
            Defaults to ``False``.

    Returns:
        Jacobian (Tensor or nested tuple of Tensors): if there are a single
            input and output, this will be a single Tensor containing the
            Jacobian for the linearized inputs and output. If one of the two is
            a tuple, then the Jacobian will be a tuple of Tensors. If both of
            them are tuples, then the Jacobian will be a tuple of tuple of
            Tensors where ``Jacobian[i][j]`` will contain the Jacobian of the
            ``i``\th output and ``j``\th input and will have as size the
            concatenation of the sizes of the corresponding output and the
            corresponding input.

    Example:

        >>> def exp_reducer(x):
        ...   return x.exp().sum(dim=1)
        >>> inputs = torch.rand(2, 2)
        >>> jacobian(exp_reducer, inputs)
        tensor([[[1.4917, 2.4352],
                 [0.0000, 0.0000]],

                [[0.0000, 0.0000],
                 [2.4369, 2.3799]]])

        >>> jacobian(exp_reducer, inputs, create_graph=True)
        tensor([[[1.4917, 2.4352],
                 [0.0000, 0.0000]],

                [[0.0000, 0.0000],
                 [2.4369, 2.3799]]], grad_fn=<ViewBackward>)

        >>> def exp_adder(x, y):
        ...   return 2 * x.exp() + 3 * y
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> jacobian(exp_adder, inputs)
        (tensor([[2.8052, 0.0000],
                [0.0000, 3.3963]]),
         tensor([[3., 0.],
                 [0., 3.]]))
    """
    ...

def hessian(func, inputs, create_graph=..., strict=...):
    r"""Function that computes the Hessian of a given scalar function.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a Tensor with a single element.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        create_graph (bool, optional): If ``True``, the Hessian will be computed in
            a differentiable manner. Note that when ``strict`` is ``False``, the result can not
            require gradients or be disconnected from the inputs.
            Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input
            such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the
            hessian for said inputs, which is the expected mathematical value.
            Defaults to ``False``.

    Returns:
        Hessian (Tensor or a tuple of tuple of Tensors) if there are a single input,
            this will be a single Tensor containing the Hessian for the input.
            If it is a tuple, then the Hessian will be a tuple of tuples where
            ``Hessian[i][j]`` will contain the Hessian of the ``i``\th input
            and ``j``\th input with size the sum of the size of the ``i``\th input plus
            the size of the ``j``\th input.

    Example:

        >>> def pow_reducer(x):
        ...   return x.pow(3).sum()
        >>> inputs = torch.rand(2, 2)
        >>> hessian(pow_reducer, inputs)
        tensor([[[[5.2265, 0.0000],
                  [0.0000, 0.0000]],

                 [[0.0000, 4.8221],
                  [0.0000, 0.0000]]],


                [[[0.0000, 0.0000],
                  [1.9456, 0.0000]],

                 [[0.0000, 0.0000],
                  [0.0000, 3.2550]]]])

        >>> hessian(pow_reducer, inputs, create_graph=True)
        tensor([[[[5.2265, 0.0000],
                  [0.0000, 0.0000]],

                 [[0.0000, 4.8221],
                  [0.0000, 0.0000]]],


                [[[0.0000, 0.0000],
                  [1.9456, 0.0000]],

                 [[0.0000, 0.0000],
                  [0.0000, 3.2550]]]], grad_fn=<ViewBackward>)


        >>> def pow_adder_reducer(x, y):
        ...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> hessian(pow_adder_reducer, inputs)
        ((tensor([[4., 0.],
                  [0., 4.]]),
          tensor([[0., 0.],
                  [0., 0.]])),
         (tensor([[0., 0.],
                  [0., 0.]]),
          tensor([[6., 0.],
                  [0., 6.]])))
    """
    ...

def vhp(func, inputs, v=..., create_graph=..., strict=...):
    r"""Function that computes the dot product between a vector ``v`` and the
    Hessian of a given scalar function at the point given by the inputs.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a Tensor with a single element.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        v (tuple of Tensors or Tensor): The vector for which the vector Hessian product is computed. Must be the
            same size as the input of ``func``. This argument is optional when
            ``func``'s input contains a single element and (if it is not provided) will be set as a Tensor
            containing a single ``1``.
        create_graph (bool, optional): If ``True``, both the output and result will be
            computed in a differentiable way. Note that when ``strict`` is ``False``, the result can not
            require gradients or be disconnected from the inputs.
            Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we detect that there exists an input
            such that all the outputs are independent of it. If ``False``, we return a Tensor of zeros as the
            vhp for said inputs, which is the expected mathematical value.
            Defaults to ``False``.

    Returns:
        func_output (tuple of Tensors or Tensor): output of ``func(inputs)``
        vhp (tuple of Tensors or Tensor): result of the dot product with the same shape
            as the inputs.
    Example::
        >>> def pow_reducer(x):
        ...   return x.pow(3).sum()
        >>> inputs = torch.rand(2, 2)
        >>> v = torch.ones(2, 2)
        >>> vhp(pow_reducer, inputs, v)
       (tensor(0.5591),
        tensor([[1.0689, 1.2431],
                [3.0989, 4.4456]]))
        >>> vhp(pow_reducer, inputs, v, create_graph=True)
        (tensor(0.5591, grad_fn=<SumBackward0>),
         tensor([[1.0689, 1.2431],
                 [3.0989, 4.4456]], grad_fn=<MulBackward0>))
        >>> def pow_adder_reducer(x, y):
        ...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> v = (torch.zeros(2), torch.ones(2))
        >>> vhp(pow_adder_reducer, inputs, v)
        (tensor(4.8053),
         (tensor([0., 0.]),
          tensor([6., 6.])))
    """
    ...

def hvp(func, inputs, v=..., create_graph=..., strict=...):
    r"""Function that computes the dot product between the Hessian of a given scalar
    function and a vector ``v`` at the point given by the inputs.

    Args:
        func (function): a Python function that takes Tensor inputs and returns
            a Tensor with a single element.
        inputs (tuple of Tensors or Tensor): inputs to the function ``func``.
        v (tuple of Tensors or Tensor): The vector for which the Hessian vector
            product is computed. Must be the same size as the input of
            ``func``. This argument is optional when ``func``'s input contains
            a single element and (if it is not provided) will be set as a
            Tensor containing a single ``1``.
        create_graph (bool, optional): If ``True``, both the output and result will be
            computed in a differentiable way. Note that when ``strict`` is
            ``False``, the result can not require gradients or be disconnected
            from the inputs.  Defaults to ``False``.
        strict (bool, optional): If ``True``, an error will be raised when we
            detect that there exists an input such that all the outputs are
            independent of it. If ``False``, we return a Tensor of zeros as the
            hvp for said inputs, which is the expected mathematical value.
            Defaults to ``False``.
    Returns:
        func_output (tuple of Tensors or Tensor): output of ``func(inputs)``
            hvp (tuple of Tensors or Tensor): result of the dot product with
            the same shape as the inputs.

    Example:

        >>> def pow_reducer(x):
        ...   return x.pow(3).sum()
        >>> inputs = torch.rand(2, 2)
        >>> v = torch.ones(2, 2)
        >>> hvp(pow_reducer, inputs, v)
        (tensor(0.1448),
         tensor([[2.0239, 1.6456],
                 [2.4988, 1.4310]]))

        >>> hvp(pow_reducer, inputs, v, create_graph=True)
        (tensor(0.1448, grad_fn=<SumBackward0>),
         tensor([[2.0239, 1.6456],
                 [2.4988, 1.4310]], grad_fn=<MulBackward0>))


        >>> def pow_adder_reducer(x, y):
        ...   return (2 * x.pow(2) + 3 * y.pow(2)).sum()
        >>> inputs = (torch.rand(2), torch.rand(2))
        >>> v = (torch.zeros(2), torch.ones(2))
        >>> hvp(pow_adder_reducer, inputs, v)
        (tensor(2.3030),
         (tensor([0., 0.]),
          tensor([6., 6.])))

    Note:

        This function is significantly slower than `vhp` due to backward mode AD constraints.
        If your functions is twice continuously differentiable, then hvp = vhp.t(). So if you
        know that your function satisfies this condition, you should use vhp instead that is
        much faster with the current implementation.

    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
import torch._C as _C
from torch._six import with_metaclass
from typing import Any

class _ContextMethodMixin(object):
    def save_for_backward(self, *tensors):
        r"""Saves given tensors for a future call to :func:`~Function.backward`.

        **This should be called at most once, and only from inside the**
        :func:`forward` **method.**

        Later, saved tensors can be accessed through the :attr:`saved_tensors`
        attribute. Before returning them to the user, a check is made to ensure
        they weren't used in any in-place operation that modified their content.

        Arguments can also be ``None``.
        """
        ...
    
    def mark_dirty(self, *args):
        r"""Marks given tensors as modified in an in-place operation.

        **This should be called at most once, only from inside the**
        :func:`forward` **method, and all arguments should be inputs.**

        Every tensor that's been modified in-place in a call to :func:`forward`
        should be given to this function, to ensure correctness of our checks.
        It doesn't matter whether the function is called before or after
        modification.
        """
        ...
    
    def mark_shared_storage(self, *pairs):
        ...
    
    def mark_non_differentiable(self, *args):
        r"""Marks outputs as non-differentiable.

        **This should be called at most once, only from inside the**
        :func:`forward` **method, and all arguments should be outputs.**

        This will mark outputs as not requiring gradients, increasing the
        efficiency of backward computation. You still need to accept a gradient
        for each output in :meth:`~Function.backward`, but it's always going to
        be a zero tensor with the same shape as the shape of a corresponding
        output.

        This is used e.g. for indices returned from a max :class:`Function`.
        """
        ...
    


class _HookMixin(object):
    ...


class BackwardCFunction(_C._FunctionBase, _ContextMethodMixin, _HookMixin):
    _is_legacy = ...
    def apply(self, *args):
        ...
    


class FunctionMeta(type):
    """Function metaclass.

    This metaclass sets up the following properties:
        _is_legacy: True if forward is not defined as a static method.
        _backward_cls: The Function class corresponding to the differentiated
            version of this function (which is generated on the fly by this
            metaclass).
    """
    def __init__(cls, name, bases, attrs) -> None:
        ...
    


class Function(with_metaclass(FunctionMeta, _C._FunctionBase, _ContextMethodMixin, _HookMixin)):
    r"""Records operation history and defines formulas for differentiating ops.

    See the Note on extending the autograd engine for more details on how to use
    this class: https://pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd

    Every operation performed on :class:`Tensor` s creates a new function
    object, that performs the computation, and records that it happened.
    The history is retained in the form of a DAG of functions, with edges
    denoting data dependencies (``input <- output``). Then, when backward is
    called, the graph is processed in the topological ordering, by calling
    :func:`backward` methods of each :class:`Function` object, and passing
    returned gradients on to next :class:`Function` s.

    Normally, the only way users interact with functions is by creating
    subclasses and defining new operations. This is a recommended way of
    extending torch.autograd.

    Examples::

        >>> class Exp(Function):
        >>>
        >>>     @staticmethod
        >>>     def forward(ctx, i):
        >>>         result = i.exp()
        >>>         ctx.save_for_backward(result)
        >>>         return result
        >>>
        >>>     @staticmethod
        >>>     def backward(ctx, grad_output):
        >>>         result, = ctx.saved_tensors
        >>>         return grad_output * result
        >>>
        >>> #Use it by calling the apply method:
        >>> output = Exp.apply(input)
    """
    def __call__(self, *args, **kwargs):
        ...
    
    is_traceable = ...
    @staticmethod
    def forward(ctx: Any, *args: Any, **kwargs: Any) -> Any:
        r"""Performs the operation.

        This function is to be overridden by all subclasses.

        It must accept a context ctx as the first argument, followed by any
        number of arguments (tensors or other types).

        The context can be used to store tensors that can be then retrieved
        during the backward pass.
        """
        ...
    
    @staticmethod
    def backward(ctx: Any, *grad_outputs: Any) -> Any:
        r"""Defines a formula for differentiating the operation.

        This function is to be overridden by all subclasses.

        It must accept a context :attr:`ctx` as the first argument, followed by
        as many outputs did :func:`forward` return, and it should return as many
        tensors, as there were inputs to :func:`forward`. Each argument is the
        gradient w.r.t the given output, and each returned value should be the
        gradient w.r.t. the corresponding input.

        The context can be used to retrieve tensors saved during the forward
        pass. It also has an attribute :attr:`ctx.needs_input_grad` as a tuple
        of booleans representing whether each input needs gradient. E.g.,
        :func:`backward` will have ``ctx.needs_input_grad[0] = True`` if the
        first input to :func:`forward` needs gradient computated w.r.t. the
        output.
        """
        ...
    


def once_differentiable(fn):
    ...

def traceable(fn_cls):
    r"""Marks Function as traceable for the JIT.

    Traceable functions have additional restrictions - they can't pass any
    data-dependent values to backward (e.g. Prod passes the output, which makes
    it non-traceable), and their backward should be implemented entirely in terms
    of operations on autograd Tensors in all cases.

    DON'T USE THIS DECORATOR. IT IS FOR INTERNAL USE ONLY AND SHOULD BE HANDLED WITH
    CARE (or can give incorrect results otherwise).
    """
    ...

class InplaceFunction(Function):
    def __init__(self, inplace=...) -> None:
        ...
    


_iter_jit_values = _iter_filter(lambda o: o is None or isinstance(o, torch._C.Value), condition_msg="jit's Values or None")
_iter_tensors = _iter_filter(lambda x: isinstance(x, torch.Tensor), condition_msg="Tensors", conversion=_jit_unwrap_structured)
_iter_tensors_permissive = _iter_filter(lambda x: isinstance(x, torch.Tensor), allow_unknown=True, condition_msg="Tensors (permissive)")
_iter_None_tensors = _iter_filter(lambda o: o is None or isinstance(o, torch.Tensor), condition_msg="Tensors or None")
_map_tensor_data = _nested_map(lambda x: isinstance(x, torch.Tensor), lambda o: o.data, condition_msg="Tensors")
class NestedIOFunction(Function):
    def backward(self, *gradients: Any) -> Any:
        ...
    
    __call__ = ...
    def forward(self, *args: Any) -> Any:
        ...
    
    def save_for_backward(self, *args: Any) -> None:
        ...
    
    @property
    def saved_tensors(self):
        ...
    
    def mark_dirty(self, *args: Any, **kwargs: Any) -> None:
        ...
    
    def mark_non_differentiable(self, *args: Any, **kwargs: Any) -> None:
        ...
    
    def forward_extended(self, *input: Any) -> None:
        ...
    
    def backward_extended(self, *grad_output: Any) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

import sys

if sys.platform == 'win32' or sys.version_info < (3, 7):
    ...
else:
    ...
def register_after_fork(func):
    """Register a callable to be executed in the child process after a fork.

    Note:
        In python < 3.7 this will only work with processes created using the
        ``multiprocessing`` module. In python >= 3.7 it also works with
        ``os.fork()``.

    Arguments:
        func (function): Function taking no arguments to be called in the child after fork

    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
import sys
import multiprocessing
from .reductions import init_reductions
from multiprocessing import *
from .spawn import ProcessContext, SpawnContext, _supports_context, spawn, start_processes

"""
torch.multiprocessing is a wrapper around the native :mod:`multiprocessing`
module. It registers custom reducers, that use shared memory to provide shared
views on the same data in different processes. Once the tensor/storage is moved
to shared_memory (see :func:`~torch.Tensor.share_memory_`), it will be possible
to send it to other processes without making any copies.

The API is 100% compatible with the original module - it's enough to change
``import multiprocessing`` to ``import torch.multiprocessing`` to have all the
tensors sent through the queues or shared via other mechanisms, moved to shared
memory.

Because of the similarity of APIs we do not document most of this package
contents, and we recommend referring to very good docs of the original module.
"""
if sys.version_info < (3, 3):
    ...
if sys.platform == 'darwin' or sys.platform == 'win32':
    _sharing_strategy = 'file_system'
    _all_sharing_strategies = 'file_system'
else:
    ...
def set_sharing_strategy(new_strategy):
    """Sets the strategy for sharing CPU tensors.

    Arguments:
        new_strategy (str): Name of the selected strategy. Should be one of
            the values returned by :func:`get_all_sharing_strategies()`.
    """
    ...

def get_sharing_strategy():
    """Returns the current strategy for sharing CPU tensors."""
    ...

def get_all_sharing_strategies():
    """Returns a set of sharing strategies supported on a current system."""
    ...

"""
This type stub file was generated by pyright.
"""

import sys

_supports_context = sys.version_info >= (3, 4)
class ProcessContext:
    def __init__(self, processes, error_queues) -> None:
        ...
    
    def pids(self):
        ...
    
    def join(self, timeout=...):
        r"""
        Tries to join one or more processes in this spawn context.
        If one of them exited with a non-zero exit status, this function
        kills the remaining processes and raises an exception with the cause
        of the first process exiting.

        Returns ``True`` if all processes have been joined successfully,
        ``False`` if there are more processes that need to be joined.

        Arguments:
            timeout (float): Wait this long before giving up on waiting.
        """
        ...
    


class SpawnContext(ProcessContext):
    def __init__(self, processes, error_queues) -> None:
        ...
    


def start_processes(fn, args=..., nprocs=..., join=..., daemon=..., start_method=...):
    ...

def spawn(fn, args=..., nprocs=..., join=..., daemon=..., start_method=...):
    r"""Spawns ``nprocs`` processes that run ``fn`` with ``args``.

    If one of the processes exits with a non-zero exit status, the
    remaining processes are killed and an exception is raised with the
    cause of termination. In the case an exception was caught in the
    child process, it is forwarded and its traceback is included in
    the exception raised in the parent process.

    Arguments:
        fn (function): Function is called as the entrypoint of the
            spawned process. This function must be defined at the top
            level of a module so it can be pickled and spawned. This
            is a requirement imposed by multiprocessing.

            The function is called as ``fn(i, *args)``, where ``i`` is
            the process index and ``args`` is the passed through tuple
            of arguments.

        args (tuple): Arguments passed to ``fn``.
        nprocs (int): Number of processes to spawn.
        join (bool): Perform a blocking join on all processes.
        daemon (bool): The spawned processes' daemon flag. If set to True,
                       daemonic processes will be created.
        start_method (string): (deprecated) this method will always use ``spawn``
                               as the start method. To use a different start method
                               use ``start_processes()``.

    Returns:
        None if ``join`` is ``True``,
        :class:`~ProcessContext` if ``join`` is ``False``

    """
    ...

"""
This type stub file was generated by pyright.
"""

import multiprocessing

class ConnectionWrapper(object):
    """Proxy class for _multiprocessing.Connection which uses ForkingPickler to
    serialize objects"""
    def __init__(self, conn) -> None:
        ...
    
    def send(self, obj):
        ...
    
    def recv(self):
        ...
    
    def __getattr__(self, name):
        ...
    


class Queue(multiprocessing.queues.Queue):
    def __init__(self, *args, **kwargs) -> None:
        ...
    


class SimpleQueue(multiprocessing.queues.SimpleQueue):
    ...


"""
This type stub file was generated by pyright.
"""

import multiprocessing

def clean_worker(*args, **kwargs):
    ...

class Pool(multiprocessing.pool.Pool):
    """Pool implementation which uses our version of SimpleQueue.
    This lets us pass tensors in shared memory across processes instead of
    serializing the underlying data."""
    ...


"""
This type stub file was generated by pyright.
"""

class StorageWeakRef(object):
    r"""A weak reference to a Storage.

    The cdata member is a Python number containing the integer representation of
    the Storage pointer."""
    def __init__(self, storage) -> None:
        ...
    
    def expired(self):
        ...
    
    def __del__(self):
        ...
    


class SharedCache(dict):
    """dictionary from multiprocessing handles to StorageWeakRef"""
    def __init__(self) -> None:
        ...
    
    def __setitem__(self, key, storage_ref):
        ...
    
    def free_dead_references(self):
        ...
    


shared_cache = SharedCache()
def rebuild_event(device, handle):
    ...

def reduce_event(event):
    ...

def rebuild_tensor(cls, storage, metadata):
    ...

def rebuild_cuda_tensor(tensor_cls, tensor_size, tensor_stride, tensor_offset, storage_cls, storage_device, storage_handle, storage_size_bytes, storage_offset_bytes, requires_grad, ref_counter_handle, ref_counter_offset, event_handle, event_sync_required):
    ...

def reduce_tensor(tensor):
    ...

def fd_id(fd):
    ...

def storage_from_cache(cls, key):
    ...

def rebuild_storage_fd(cls, df, size):
    ...

def rebuild_storage_filename(cls, manager, handle, size):
    ...

def rebuild_storage_empty(cls):
    ...

def reduce_storage(storage):
    ...

def init_reductions():
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from typing import Iterable, List, Union
from .. import Tensor

def get_rng_state(device: Union[int, str, torch.device] = ...) -> Tensor:
    r"""Returns the random number generator state of the specified GPU as a ByteTensor.

    Args:
        device (torch.device or int, optional): The device to return the RNG state of.
            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).

    .. warning::
        This function eagerly initializes CUDA.
    """
    ...

def get_rng_state_all() -> List[Tensor]:
    r"""Returns a list of ByteTensor representing the random number states of all devices."""
    ...

def set_rng_state(new_state: Tensor, device: Union[int, str, torch.device] = ...) -> None:
    r"""Sets the random number generator state of the specified GPU.

    Args:
        new_state (torch.ByteTensor): The desired state
        device (torch.device or int, optional): The device to set the RNG state.
            Default: ``'cuda'`` (i.e., ``torch.device('cuda')``, the current CUDA device).
    """
    ...

def set_rng_state_all(new_states: Iterable[Tensor]) -> None:
    r"""Sets the random number generator state of all devices.

    Args:
        new_state (Iterable of torch.ByteTensor): The desired state for each device"""
    ...

def manual_seed(seed: int) -> None:
    r"""Sets the seed for generating random numbers for the current GPU.
    It's safe to call this function if CUDA is not available; in that
    case, it is silently ignored.

    Args:
        seed (int): The desired seed.

    .. warning::
        If you are working with a multi-GPU model, this function is insufficient
        to get determinism.  To seed all GPUs, use :func:`manual_seed_all`.
    """
    ...

def manual_seed_all(seed: int) -> None:
    r"""Sets the seed for generating random numbers on all GPUs.
    It's safe to call this function if CUDA is not available; in that
    case, it is silently ignored.

    Args:
        seed (int): The desired seed.
    """
    ...

def seed() -> None:
    r"""Sets the seed for generating random numbers to a random number for the current GPU.
    It's safe to call this function if CUDA is not available; in that
    case, it is silently ignored.

    .. warning::
        If you are working with a multi-GPU model, this function will only initialize
        the seed on one GPU.  To initialize all GPUs, use :func:`seed_all`.
    """
    ...

def seed_all() -> None:
    r"""Sets the seed for generating random numbers to a random number on all GPUs.
    It's safe to call this function if CUDA is not available; in that
    case, it is silently ignored.
    """
    ...

def initial_seed() -> int:
    r"""Returns the current random seed of the current GPU.

    .. warning::
        This function eagerly initializes CUDA.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import contextlib
import os
import torch
import traceback
import warnings
import threading
import torch._C
from typing import List, Optional, Tuple, Union
from torch._six import raise_from
from ._utils import _dummy_type, _get_device_index
from .streams import Event, Stream
from .. import device as _device
from .memory import *
from .random import *
from ..storage import _StorageBase
from . import amp, nvtx, profiler, sparse

r"""
This package adds support for CUDA tensor types, that implement the same
function as CPU tensors, but they utilize GPUs for computation.

It is lazily initialized, so you can always import it, and use
:func:`is_available()` to determine if your system supports CUDA.

:ref:`cuda-semantics` has more details about working with CUDA.
"""
_initialized = False
_tls = threading.local()
_initialization_lock = threading.Lock()
_queued_calls = []
_is_in_bad_fork = getattr(torch._C, "_cuda_isInBadFork", lambda : False)
_device_t = Union[_device, str, int]
if hasattr(torch._C, '_CudaDeviceProperties'):
    _CudaDeviceProperties = torch._C._CudaDeviceProperties
else:
    _CudaDeviceProperties = _dummy_type('_CudaDeviceProperties')
def is_available() -> bool:
    r"""Returns a bool indicating if CUDA is currently available."""
    ...

def is_initialized():
    r"""Returns whether PyTorch's CUDA state has been initialized."""
    ...

class DeferredCudaCallError(Exception):
    ...


def init():
    r"""Initialize PyTorch's CUDA state.  You may need to call
    this explicitly if you are interacting with PyTorch via
    its C API, as Python bindings for CUDA functionality will not
    be until this initialization takes place.  Ordinary users
    should not need this, as all of PyTorch's CUDA methods
    automatically initialize CUDA state on-demand.

    Does nothing if the CUDA state is already initialized.
    """
    ...

def cudart():
    ...

class cudaStatus(object):
    ...


class CudaError(RuntimeError):
    def __init__(self, code: int) -> None:
        ...
    


def check_error(res: int) -> None:
    ...

class device(object):
    r"""Context-manager that changes the selected device.

    Arguments:
        device (torch.device or int): device index to select. It's a no-op if
            this argument is a negative integer or ``None``.
    """
    def __init__(self, device) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    


class device_of(device):
    r"""Context-manager that changes the current device to that of given object.

    You can use both tensors and storages as arguments. If a given object is
    not allocated on a GPU, this is a no-op.

    Arguments:
        obj (Tensor or Storage): object allocated on the selected device.
    """
    def __init__(self, obj) -> None:
        ...
    


def set_device(device: _device_t) -> None:
    r"""Sets the current device.

    Usage of this function is discouraged in favor of :any:`device`. In most
    cases it's better to use ``CUDA_VISIBLE_DEVICES`` environmental variable.

    Arguments:
        device (torch.device or int): selected device. This function is a no-op
            if this argument is negative.
    """
    ...

def get_device_name(device: Optional[_device_t] = ...) -> str:
    r"""Gets the name of a device.

    Arguments:
        device (torch.device or int, optional): device for which to return the
            name. This function is a no-op if this argument is a negative
            integer. It uses the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).
    """
    ...

def get_device_capability(device: Optional[_device_t] = ...) -> Tuple[int, int]:
    r"""Gets the cuda capability of a device.

    Arguments:
        device (torch.device or int, optional): device for which to return the
            device capability. This function is a no-op if this argument is
            a negative integer. It uses the current device, given by
            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).

    Returns:
        tuple(int, int): the major and minor cuda capability of the device
    """
    ...

def get_device_properties(device: _device_t) -> _CudaDeviceProperties:
    ...

@contextlib.contextmanager
def stream(stream):
    r"""Context-manager that selects a given stream.

    All CUDA kernels queued within its context will be enqueued on a selected
    stream.

    Arguments:
        stream (Stream): selected stream. This manager is a no-op if it's
            ``None``.

    .. note:: Streams are per-device. If the selected stream is not on the
        current device, this function will also change the current device to
        match the stream.
    """
    ...

def device_count() -> int:
    r"""Returns the number of GPUs available."""
    ...

def get_arch_list() -> List[str]:
    r"""Returns list CUDA architecutres this library was compiled for."""
    ...

def get_gencode_flags() -> str:
    r"""Returns NVCC gencode flags this library were compiled with."""
    ...

def current_device() -> int:
    r"""Returns the index of a currently selected device."""
    ...

def synchronize(device: _device_t = ...) -> None:
    r"""Waits for all kernels in all streams on a CUDA device to complete.

    Arguments:
        device (torch.device or int, optional): device for which to synchronize.
            It uses the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).
    """
    ...

def ipc_collect():
    r"""Force collects GPU memory after it has been released by CUDA IPC.

    .. note::
        Checks if any sent CUDA tensors could be cleaned from the memory. Force
        closes shared memory file used for reference counting if there is no
        active counters. Useful when the producer process stopped actively sending
        tensors and want to release unused memory.
    """
    ...

def current_stream(device: Optional[_device_t] = ...) -> Stream:
    r"""Returns the currently selected :class:`Stream` for a given device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            the currently selected :class:`Stream` for the current device, given
            by :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).
    """
    ...

def default_stream(device: Optional[_device_t] = ...) -> Stream:
    r"""Returns the default :class:`Stream` for a given device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            the default :class:`Stream` for the current device, given by
            :func:`~torch.cuda.current_device`, if :attr:`device` is ``None``
            (default).
    """
    ...

def current_blas_handle():
    r"""Returns cublasHandle_t pointer to current cuBLAS handle"""
    ...

if not hasattr(torch._C, 'CudaDoubleStorageBase'):
    ...
class _CudaBase(object):
    is_cuda = ...
    is_sparse = ...
    def type(self, *args, **kwargs):
        ...
    
    __new__ = ...


class DoubleStorage(_CudaBase, torch._C.CudaDoubleStorageBase, _StorageBase):
    ...


class FloatStorage(_CudaBase, torch._C.CudaFloatStorageBase, _StorageBase):
    ...


class LongStorage(_CudaBase, torch._C.CudaLongStorageBase, _StorageBase):
    ...


class IntStorage(_CudaBase, torch._C.CudaIntStorageBase, _StorageBase):
    ...


class ShortStorage(_CudaBase, torch._C.CudaShortStorageBase, _StorageBase):
    ...


class CharStorage(_CudaBase, torch._C.CudaCharStorageBase, _StorageBase):
    ...


class ByteStorage(_CudaBase, torch._C.CudaByteStorageBase, _StorageBase):
    ...


class HalfStorage(_CudaBase, torch._C.CudaHalfStorageBase, _StorageBase):
    ...


class BoolStorage(_CudaBase, torch._C.CudaBoolStorageBase, _StorageBase):
    ...


class BFloat16Storage(_CudaBase, torch._C.CudaBFloat16StorageBase, _StorageBase):
    ...


class ComplexDoubleStorage(_CudaBase, torch._C.CudaComplexDoubleStorageBase, _StorageBase):
    ...


class ComplexFloatStorage(_CudaBase, torch._C.CudaComplexFloatStorageBase, _StorageBase):
    ...


"""
This type stub file was generated by pyright.
"""

def range_push(msg):
    """
    Pushes a range onto a stack of nested range span.  Returns zero-based
    depth of the range that is started.

    Arguments:
        msg (string): ASCII message to associate with range
    """
    ...

def range_pop():
    """
    Pops a range off of a stack of nested range spans.  Returns the
    zero-based depth of the range that is ended.
    """
    ...

def mark(msg):
    """
    Describe an instantaneous event that occurred at some point.

    Arguments:
        msg (string): ASCII message to associate with the event.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import contextlib

DEFAULT_FLAGS = ["gpustarttimestamp", "gpuendtimestamp", "gridsize3d", "threadblocksize", "streamid", "enableonstart 0", "conckerneltrace"]
def init(output_file, flags=..., output_mode=...):
    ...

def start():
    ...

def stop():
    ...

@contextlib.contextmanager
def profile():
    ...

"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, Union
from torch.types import Device

def caching_allocator_alloc(size, device: Union[Device, int] = ..., stream=...):
    r"""Performs a memory allocation using the CUDA memory allocator.

    Memory is allocated for a given device and a stream, this
    function is intended to be used for interoperability with other
    frameworks. Allocated memory is released through
    :func:`~torch.cuda.caching_allocator_delete`.

    Arguments:
        size (int): number of bytes to be allocated.
        device (torch.device or int, optional): selected device. If it is 
            ``None`` the default CUDA device is used.
        stream (torch.cuda.Stream or int, optional): selected stream. If is ``None`` then
            the default stream for the selected device is used.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def caching_allocator_delete(mem_ptr):
    r"""Deletes memory allocated using the CUDA memory allocator.

    Memory allocated with :func:`~torch.cuda.caching_allocator_alloc`.
    is freed here. The associated device and stream are tracked inside
    the allocator.

    Arguments:
        mem_ptr (int): memory address to be freed by the allocator.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def empty_cache() -> None:
    r"""Releases all unoccupied cached memory currently held by the caching
    allocator so that those can be used in other GPU application and visible in
    `nvidia-smi`.

    .. note::
        :func:`~torch.cuda.empty_cache` doesn't increase the amount of GPU
        memory available for PyTorch. However, it may help reduce fragmentation
        of GPU memory in certain cases. See :ref:`cuda-memory-management` for
        more details about GPU memory management.
    """
    ...

def memory_stats(device: Union[Device, int] = ...) -> Dict[str, Any]:
    r"""Returns a dictionary of CUDA memory allocator statistics for a
    given device.

    The return value of this function is a dictionary of statistics, each of
    which is a non-negative integer.

    Core statistics:

    - ``"allocated.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      number of allocation requests received by the memory allocator.
    - ``"allocated_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      amount of allocated memory.
    - ``"segment.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      number of reserved segments from ``cudaMalloc()``.
    - ``"reserved_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      amount of reserved memory.
    - ``"active.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      number of active memory blocks.
    - ``"active_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      amount of active memory.
    - ``"inactive_split.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      number of inactive, non-releasable memory blocks.
    - ``"inactive_split_bytes.{all,large_pool,small_pool}.{current,peak,allocated,freed}"``:
      amount of inactive, non-releasable memory.

    For these core statistics, values are broken down as follows.

    Pool type:

    - ``all``: combined statistics across all memory pools.
    - ``large_pool``: statistics for the large allocation pool
      (as of October 2019, for size >= 1MB allocations).
    - ``small_pool``: statistics for the small allocation pool
      (as of October 2019, for size < 1MB allocations).

    Metric type:

    - ``current``: current value of this metric.
    - ``peak``: maximum value of this metric.
    - ``allocated``: historical total increase in this metric.
    - ``freed``: historical total decrease in this metric.

    In addition to the core statistics, we also provide some simple event
    counters:

    - ``"num_alloc_retries"``: number of failed ``cudaMalloc`` calls that
      result in a cache flush and retry.
    - ``"num_ooms"``: number of out-of-memory errors thrown.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistics for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def memory_stats_as_nested_dict(device: Union[Device, int] = ...) -> Dict[str, Any]:
    r"""Returns the result of :func:`~torch.cuda.memory_stats` as a nested dictionary."""
    ...

def reset_accumulated_memory_stats(device: Union[Device, int] = ...) -> None:
    r"""Resets the "accumulated" (historical) stats tracked by the CUDA memory allocator.

    See :func:`~torch.cuda.memory_stats` for details. Accumulated stats correspond to
    the `"allocated"` and `"freed"` keys in each individual stat dict, as well as
    `"num_alloc_retries"` and `"num_ooms"`.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def reset_peak_memory_stats(device: Union[Device, int] = ...) -> None:
    r"""Resets the "peak" stats tracked by the CUDA memory allocator.

    See :func:`~torch.cuda.memory_stats` for details. Peak stats correspond to the
    `"peak"` key in each individual stat dict.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def reset_max_memory_allocated(device: Union[Device, int] = ...) -> None:
    r"""Resets the starting point in tracking maximum GPU memory occupied by
    tensors for a given device.

    See :func:`~torch.cuda.max_memory_allocated` for details.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. warning::
        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets
        /all/ peak memory stats.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def reset_max_memory_cached(device: Union[Device, int] = ...) -> None:
    r"""Resets the starting point in tracking maximum GPU memory managed by the
    caching allocator for a given device.

    See :func:`~torch.cuda.max_memory_cached` for details.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. warning::
        This function now calls :func:`~torch.cuda.reset_peak_memory_stats`, which resets
        /all/ peak memory stats.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def memory_allocated(device: Union[Device, int] = ...) -> int:
    r"""Returns the current GPU memory occupied by tensors in bytes for a given
    device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        This is likely less than the amount shown in `nvidia-smi` since some
        unused memory can be held by the caching allocator and some context
        needs to be created on GPU. See :ref:`cuda-memory-management` for more
        details about GPU memory management.
    """
    ...

def max_memory_allocated(device: Union[Device, int] = ...) -> int:
    r"""Returns the maximum GPU memory occupied by tensors in bytes for a given
    device.

    By default, this returns the peak allocated memory since the beginning of
    this program. :func:`~torch.cuda.reset_peak_stats` can be used to
    reset the starting point in tracking this metric. For example, these two
    functions can measure the peak allocated memory usage of each iteration in a
    training loop.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def memory_reserved(device: Union[Device, int] = ...) -> int:
    r"""Returns the current GPU memory managed by the caching allocator in bytes
    for a given device.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def max_memory_reserved(device: Union[Device, int] = ...) -> int:
    r"""Returns the maximum GPU memory managed by the caching allocator in bytes
    for a given device.

    By default, this returns the peak cached memory since the beginning of this
    program. :func:`~torch.cuda.reset_peak_stats` can be used to reset
    the starting point in tracking this metric. For example, these two functions
    can measure the peak cached memory amount of each iteration in a training
    loop.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            statistic for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def memory_cached(device: Union[Device, int] = ...) -> int:
    r"""Deprecated; see :func:`~torch.cuda.memory_reserved`."""
    ...

def max_memory_cached(device: Union[Device, int] = ...) -> int:
    r"""Deprecated; see :func:`~torch.cuda.max_memory_reserved`."""
    ...

def memory_snapshot():
    r"""Returns a snapshot of the CUDA memory allocator state across all devices.

    Interpreting the output of this function requires familiarity with the
    memory allocator internals.

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

def memory_summary(device: Union[Device, int] = ..., abbreviated: bool = ...) -> str:
    r"""Returns a human-readable printout of the current memory allocator
    statistics for a given device.

    This can be useful to display periodically during training, or when
    handling out-of-memory exceptions.

    Arguments:
        device (torch.device or int, optional): selected device. Returns
            printout for the current device, given by :func:`~torch.cuda.current_device`,
            if :attr:`device` is ``None`` (default).
        abbreviated (bool, optional): whether to return an abbreviated summary
            (default: False).

    .. note::
        See :ref:`cuda-memory-management` for more details about GPU memory
        management.
    """
    ...

"""
This type stub file was generated by pyright.
"""

"""
This type stub file was generated by pyright.
"""

"""
This type stub file was generated by pyright.
"""

from .autocast_mode import autocast, custom_bwd, custom_fwd
from .grad_scaler import GradScaler

"""
This type stub file was generated by pyright.
"""

class autocast(object):
    r"""
    Instances of :class:`autocast` serve as context managers or decorators that
    allow regions of your script to run in mixed precision.

    In these regions, CUDA ops run in an op-specific dtype chosen by autocast
    to improve performance while maintaining accuracy.
    See the :ref:`Autocast Op Reference<autocast-op-reference>` for details.

    When entering an autocast-enabled region, Tensors may be any type.
    You should not call ``.half()`` on your model(s) or inputs when using autocasting.

    :class:`autocast` should wrap only the forward pass(es) of your network, including the loss
    computation(s).  Backward passes under autocast are not recommended.
    Backward ops run in the same type that autocast used for corresponding forward ops.

    Example::

        # Creates model and optimizer in default precision
        model = Net().cuda()
        optimizer = optim.SGD(model.parameters(), ...)

        for input, target in data:
            optimizer.zero_grad()

            # Enables autocasting for the forward pass (model + loss)
            with autocast():
                output = model(input)
                loss = loss_fn(output, target)

            # Exits the context manager before backward()
            loss.backward()
            optimizer.step()

    See the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage (along with gradient scaling)
    in more complex scenarios (e.g., gradient penalty, multiple models/losses, custom autograd functions).

    :class:`autocast` can also be used as a decorator, e.g., on the ``forward`` method of your model::

        class AutocastModel(nn.Module):
            ...
            @autocast()
            def forward(self, input):
                ...

    Floating-point Tensors produced in an autocast-enabled region may be ``float16``.
    After returning to an autocast-disabled region, using them with floating-point
    Tensors of different dtypes may cause type mismatch errors.  If so, cast the Tensor(s)
    produced in the autocast region back to ``float32`` (or other dtype if desired).
    If a Tensor from the autocast region is already ``float32``, the cast is a no-op,
    and incurs no additional overhead.  Example::

        # Creates some tensors in default dtype (here assumed to be float32)
        a_float32 = torch.rand((8, 8), device="cuda")
        b_float32 = torch.rand((8, 8), device="cuda")
        c_float32 = torch.rand((8, 8), device="cuda")
        d_float32 = torch.rand((8, 8), device="cuda")

        with autocast():
            # torch.mm is on autocast's list of ops that should run in float16.
            # Inputs are float32, but the op runs in float16 and produces float16 output.
            # No manual casts are required.
            e_float16 = torch.mm(a_float32, b_float32)
            # Also handles mixed input types
            f_float16 = torch.mm(d_float32, e_float16)

        # After exiting autocast, calls f_float16.float() to use with d_float32
        g_float32 = torch.mm(d_float32, f_float16.float())

    Type mismatch errors *in* an autocast-enabled region are a bug; if this is what you observe,
    please file an issue.

    ``autocast(enabled=False)`` subregions can be nested in autocast-enabled regions.
    Locally disabling autocast can be useful, for example, if you want to force a subregion
    to run in a particular ``dtype``.  Disabling autocast gives you explicit control over
    the execution type.  In the subregion, inputs from the surrounding region
    should be cast to ``dtype`` before use::

        # Creates some tensors in default dtype (here assumed to be float32)
        a_float32 = torch.rand((8, 8), device="cuda")
        b_float32 = torch.rand((8, 8), device="cuda")
        c_float32 = torch.rand((8, 8), device="cuda")
        d_float32 = torch.rand((8, 8), device="cuda")

        with autocast():
            e_float16 = torch.mm(a_float32, b_float32)

            with autocast(enabled=False):
                # Calls e_float16.float() to ensure float32 execution
                # (necessary because e_float16 was created in an autocasted region)
                f_float32 = torch.mm(c_float32, e_float16.float())

            # No manual casts are required when re-entering the autocast-enabled region.
            # torch.mm again runs in float16 and produces float16 output, regardless of input types.
            g_float16 = torch.mm(d_float32, f_float32)

    The autocast state is thread-local.  If you want it enabled in a new thread, the context manager or decorator
    must be invoked in that thread.  This affects :class:`torch.nn.DataParallel` and
    :class:`torch.nn.parallel.DistributedDataParallel` when used with more than one GPU per process
    (see :ref:`Working with Multiple GPUs<amp-multigpu>`).

    Arguments:
        enabled(bool, optional, default=True):  Whether autocasting should be enabled in the region.
    """
    def __init__(self, enabled=...) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    
    def __call__(self, func):
        ...
    


def custom_fwd(fwd=..., **kwargs):
    """
    Helper decorator for ``forward`` methods of custom autograd functions (subclasses of
    :class:`torch.autograd.Function`).  See the :ref:`example page<amp-custom-examples>` for more detail.

    Arguments:
        cast_inputs (:class:`torch.dtype` or None, optional, default=None):  If not ``None``,
            when ``forward`` runs in an autocast-enabled region, casts incoming
            floating-point CUDA Tensors to the target dtype (non-floating-point Tensors are not affected),
            then executes ``forward`` with autocast disabled.
            If ``None``, ``forward``'s internal ops execute with the current autocast state.

    .. note::
        If the decorated ``forward`` is called outside an autocast-enabled region,
        :func:`custom_fwd<custom_fwd>` is a no-op and ``cast_inputs`` has no effect.
    """
    ...

def custom_bwd(bwd):
    """
    Helper decorator for backward methods of custom autograd functions (subclasses of
    :class:`torch.autograd.Function`).
    Ensures that ``backward`` executes with the same autocast state as ``forward``.
    See the :ref:`example page<amp-custom-examples>` for more detail.
    """
    ...

"""
This type stub file was generated by pyright.
"""

from enum import Enum

class _MultiDeviceReplicator(object):
    """
    Lazily serves copies of a tensor to requested devices.  Copies are cached per-device.
    """
    def __init__(self, master_tensor) -> None:
        ...
    
    def get(self, device):
        ...
    


class OptState(Enum):
    READY = ...
    UNSCALED = ...
    STEPPED = ...


class GradScaler(object):
    """
    An instance ``scaler`` of :class:`GradScaler` helps perform the steps of gradient scaling
    conveniently.

    * ``scaler.scale(loss)`` multiplies a given loss by ``scaler``'s current scale factor.
    * ``scaler.step(optimizer)`` safely unscales gradients and calls ``optimizer.step()``.
    * ``scaler.update()`` updates ``scaler``'s scale factor.

    Example::

        # Creates a GradScaler once at the beginning of training.
        scaler = GradScaler()

        for epoch in epochs:
            for input, target in data:
                optimizer.zero_grad()
                output = model(input)
                loss = loss_fn(output, target)

                # Scales loss.  Calls backward() on scaled loss to create scaled gradients.
                scaler.scale(loss).backward()

                # scaler.step() first unscales gradients of the optimizer's params.
                # If gradients don't contain infs/NaNs, optimizer.step() is then called,
                # otherwise, optimizer.step() is skipped.
                scaler.step(optimizer)

                # Updates the scale for next iteration.
                scaler.update()

    See the :ref:`Automatic Mixed Precision examples<amp-examples>` for usage
    (along with autocasting) in more complex cases like gradient clipping, gradient accumulation, gradient penalty,
    and multiple losses/optimizers.

    ``scaler`` dynamically estimates the scale factor each iteration.  To minimize gradient underflow,
    a large scale factor should be used.  However, ``float16`` values can "overflow" (become inf or NaN) if
    the scale factor is too large.  Therefore, the optimal scale factor is the largest factor that can be used
    without incurring inf or NaN gradient values.
    ``scaler`` approximates the optimal scale factor over time by checking the gradients for infs and NaNs during every
    ``scaler.step(optimizer)`` (or optional separate ``scaler.unscale_(optimizer)``, see :meth:`unscale_`).

    * If infs/NaNs are found, ``scaler.step(optimizer)`` skips the underlying ``optimizer.step()`` (so the params
      themselves remain uncorrupted) and ``update()`` multiplies the scale by ``backoff_factor``.

    * If no infs/NaNs are found, ``scaler.step(optimizer)`` runs the underlying ``optimizer.step()`` as usual.
      If ``growth_interval`` unskipped iterations occur consecutively, ``update()`` multiplies the scale by
      ``growth_factor``.

    The scale factor often causes infs/NaNs to appear in gradients for the first few iterations as its
    value calibrates.  ``scaler.step`` will skip the underlying ``optimizer.step()`` for these
    iterations.  After that, step skipping should occur rarely (once every few hundred or thousand iterations).

    Arguments:
        init_scale (float, optional, default=2.**16):  Initial scale factor.
        growth_factor (float, optional, default=2.0):  Factor by which the scale is multiplied during
            :meth:`update` if no inf/NaN gradients occur for ``growth_factor`` consecutive iterations.
        backoff_factor (float, optional, default=0.5):  Factor by which the scale is multiplied during
            :meth:`update` if inf/NaN gradients occur in an iteration.
        growth_interval (int, optional, default=2000):  Number of consecutive iterations without inf/NaN gradients
            that must occur for the scale to be multiplied by ``growth_factor``.
        enabled (bool, optional, default=True):  If ``False``, disables gradient scaling. :meth:`step` simply
            invokes the underlying ``optimizer.step()``, and other methods become no-ops.
    """
    def __init__(self, init_scale=..., growth_factor=..., backoff_factor=..., growth_interval=..., enabled=...) -> None:
        ...
    
    def scale(self, outputs):
        """
        Multiplies ('scales') a tensor or list of tensors by the scale factor.

        Returns scaled outputs.  If this instance of :class:`GradScaler` is not enabled, outputs are returned
        unmodified.

        Arguments:
            outputs (Tensor or iterable of Tensors):  Outputs to scale.
        """
        ...
    
    def unscale_(self, optimizer):
        """
        Divides ("unscales") the optimizer's gradient tensors by the scale factor.

        :meth:`unscale_` is optional, serving cases where you need to
        :ref:`modify or inspect gradients<working-with-unscaled-gradients>`
        between the backward pass(es) and :meth:`step`.
        If :meth:`unscale_` is not called explicitly,  gradients will be unscaled  automatically during :meth:`step`.

        Simple example, using :meth:`unscale_` to enable clipping of unscaled gradients::

            ...
            scaler.scale(loss).backward()
            scaler.unscale_(optimizer)
            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm)
            scaler.step(optimizer)
            scaler.update()

        Arguments:
            optimizer (torch.optim.Optimizer):  Optimizer that owns the gradients to be unscaled.

        .. note::
            :meth:`unscale_` does not incur a CPU-GPU sync.

        .. warning::
            :meth:`unscale_` should only be called once per optimizer per :meth:`step` call,
            and only after all gradients for that optimizer's assigned parameters have been accumulated.
            Calling :meth:`unscale_` twice for a given optimizer between each :meth:`step` triggers a RuntimeError.
        """
        ...
    
    def step(self, optimizer, *args, **kwargs):
        """
        :meth:`step` carries out the following two operations:

        1.  Internally invokes ``unscale_(optimizer)`` (unless :meth:`unscale_` was explicitly called for ``optimizer``
            earlier in the iteration).  As part of the :meth:`unscale_`, gradients are checked for infs/NaNs.
        2.  If no inf/NaN gradients are found, invokes ``optimizer.step()`` using the unscaled
            gradients.  Otherwise, ``optimizer.step()`` is skipped to avoid corrupting the params.

        ``*args`` and ``**kwargs`` are forwarded to ``optimizer.step()``.

        Returns the return value of ``optimizer.step(*args, **kwargs)``.

        Arguments:
            optimizer (torch.optim.Optimizer):  Optimizer that applies the gradients.
            args:  Any arguments.
            kwargs:  Any keyword arguments.

        .. warning::
            Closure use is not currently supported.
        """
        ...
    
    def update(self, new_scale=...):
        """
        Updates the scale factor.

        If any optimizer steps were skipped the scale is multiplied by ``backoff_factor``
        to reduce it. If ``growth_interval`` unskipped iterations occurred consecutively,
        the scale is multiplied by ``growth_factor`` to increase it.

        Passing ``new_scale`` sets the scale directly.

        Arguments:
            new_scale (float or :class:`torch.cuda.FloatTensor`, optional, default=None):  New scale factor.

        .. warning::
            :meth:`update` should only be called at the end of the iteration, after ``scaler.step(optimizer)`` has
            been invoked for all optimizers used this iteration.
        """
        ...
    
    def get_scale(self):
        """
        Returns a Python float containing the current scale, or 1.0 if scaling is disabled.

        .. warning::
            :meth:`get_scale` incurs a CPU-GPU sync.
        """
        ...
    
    def get_growth_factor(self):
        r"""
        Returns a Python float containing the scale growth factor.
        """
        ...
    
    def set_growth_factor(self, new_factor):
        r"""
        Arguments:
            new_scale (float):  Value to use as the new scale growth factor.
        """
        ...
    
    def get_backoff_factor(self):
        r"""
        Returns a Python float containing the scale backoff factor.
        """
        ...
    
    def set_backoff_factor(self, new_factor):
        r"""
        Arguments:
            new_scale (float):  Value to use as the new scale backoff factor.
        """
        ...
    
    def get_growth_interval(self):
        r"""
        Returns a Python int containing the growth interval.
        """
        ...
    
    def set_growth_interval(self, new_interval):
        r"""
        Arguments:
            new_interval (int):  Value to use as the new growth interval.
        """
        ...
    
    def is_enabled(self):
        r"""
        Returns a bool indicating whether this instance is enabled.
        """
        ...
    
    def state_dict(self):
        r"""
        Returns the state of the scaler as a :class:`dict`.  It contains five entries:

        * ``"scale"`` - a Python float containing the current scale
        * ``"growth_factor"`` - a Python float containing the current growth factor
        * ``"backoff_factor"`` - a Python float containing the current backoff factor
        * ``"growth_interval"`` - a Python int containing the current growth interval
        * ``"_growth_tracker"`` - a Python int containing the number of recent consecutive unskipped steps.

        If this instance is not enabled, returns an empty dict.

        .. note::
           If you wish to checkpoint the scaler's state after a particular iteration, :meth:`state_dict`
           should be called after :meth:`update`.
        """
        ...
    
    def load_state_dict(self, state_dict):
        r"""
        Loads the scaler state.  If this instance is disabled, :meth:`load_state_dict` is a no-op.

        Arguments:
           state_dict(dict): scaler state.  Should be an object returned from a call to :meth:`state_dict`.
        """
        ...
    
    def __getstate__(self):
        ...
    
    def __setstate__(self, state):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch

if not hasattr(torch._C, '_CudaStreamBase'):
    ...
class Stream(torch._C._CudaStreamBase):
    r"""Wrapper around a CUDA stream.

    A CUDA stream is a linear sequence of execution that belongs to a specific
    device, independent from other streams.  See :ref:`cuda-semantics` for
    details.

    Arguments:
        device(torch.device or int, optional): a device on which to allocate
            the stream. If :attr:`device` is ``None`` (default) or a negative
            integer, this will use the current device.
        priority(int, optional): priority of the stream. Lower numbers
                                 represent higher priorities.
    """
    def __new__(cls, device=..., priority=..., **kwargs):
        ...
    
    def wait_event(self, event):
        r"""Makes all future work submitted to the stream wait for an event.

        Arguments:
            event (Event): an event to wait for.

        .. note:: This is a wrapper around ``cudaStreamWaitEvent()``: see
           `CUDA Stream documentation`_ for more info.

           This function returns without waiting for :attr:`event`: only future
           operations are affected.

        .. _CUDA Stream documentation:
           http://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__STREAM.html
        """
        ...
    
    def wait_stream(self, stream):
        r"""Synchronizes with another stream.

        All future work submitted to this stream will wait until all kernels
        submitted to a given stream at the time of call complete.

        Arguments:
            stream (Stream): a stream to synchronize.

        .. note:: This function returns without waiting for currently enqueued
           kernels in :attr:`stream`: only future operations are affected.
        """
        ...
    
    def record_event(self, event=...):
        r"""Records an event.

        Arguments:
            event (Event, optional): event to record. If not given, a new one
                will be allocated.

        Returns:
            Recorded event.
        """
        ...
    
    def query(self):
        r"""Checks if all the work submitted has been completed.

        Returns:
            A boolean indicating if all kernels in this stream are completed."""
        ...
    
    def synchronize(self):
        r"""Wait for all the kernels in this stream to complete.

        .. note:: This is a wrapper around ``cudaStreamSynchronize()``: see
           `CUDA Stream documentation`_ for more info.
        """
        ...
    
    def __eq__(self, o) -> bool:
        ...
    
    def __hash__(self) -> int:
        ...
    
    def __repr__(self):
        ...
    


class Event(torch._C._CudaEventBase):
    r"""Wrapper around a CUDA event.

    CUDA events are synchronization markers that can be used to monitor the
    device's progress, to accurately measure timing, and to synchronize CUDA
    streams.

    The underlying CUDA events are lazily initialized when the event is first
    recorded or exported to another process. After creation, only streams on the
    same device may record the event. However, streams on any device can wait on
    the event.

    Arguments:
        enable_timing (bool, optional): indicates if the event should measure time
            (default: ``False``)
        blocking (bool, optional): if ``True``, :meth:`wait` will be blocking (default: ``False``)
        interprocess (bool): if ``True``, the event can be shared between processes
            (default: ``False``)

    .. _CUDA Event Documentation:
       https://docs.nvidia.com/cuda/cuda-runtime-api/group__CUDART__EVENT.html
    """
    def __new__(cls, enable_timing=..., blocking=..., interprocess=...):
        ...
    
    @classmethod
    def from_ipc_handle(cls, device, handle):
        r"""Reconstruct an event from an IPC handle on the given device."""
        ...
    
    def record(self, stream=...):
        r"""Records the event in a given stream.

        Uses ``torch.cuda.current_stream()`` if no stream is specified. The
        stream's device must match the event's device."""
        ...
    
    def wait(self, stream=...):
        r"""Makes all future work submitted to the given stream wait for this
        event.

        Use ``torch.cuda.current_stream()`` if no stream is specified."""
        ...
    
    def query(self):
        r"""Checks if all work currently captured by event has completed.

        Returns:
            A boolean indicating if all work currently captured by event has
            completed.
        """
        ...
    
    def elapsed_time(self, end_event):
        r"""Returns the time elapsed in milliseconds after the event was
        recorded and before the end_event was recorded.
        """
        ...
    
    def synchronize(self):
        r"""Waits for the event to complete.

        Waits until the completion of all work currently captured in this event.
        This prevents the CPU thread from proceeding until the event completes.

         .. note:: This is a wrapper around ``cudaEventSynchronize()``: see
            `CUDA Event documentation`_ for more info.
        """
        ...
    
    def ipc_handle(self):
        r"""Returns an IPC handle of this event. If not recorded yet, the event
        will use the current device. """
        ...
    
    def __repr__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

import types
from contextlib import contextmanager

__allow_nonbracketed_mutation_flag = True
def disable_global_flags():
    ...

def flags_frozen():
    ...

class ContextProp(object):
    def __init__(self, getter, setter) -> None:
        ...
    
    def __get__(self, obj, objtype):
        ...
    
    def __set__(self, obj, val):
        ...
    


class PropModule(types.ModuleType):
    def __init__(self, m, name) -> None:
        ...
    
    def __getattr__(self, attr):
        ...
    


"""
This type stub file was generated by pyright.
"""

import sys
import torch

def is_built():
    r"""Returns whether PyTorch is built with CUDA support.  Note that this
    doesn't necessarily mean CUDA is available; just that if this PyTorch
    binary were run a machine with working CUDA drivers and devices, we
    would be able to use it."""
    ...

class cuFFTPlanCacheAttrContextProp(object):
    def __init__(self, getter, setter) -> None:
        ...
    
    def __get__(self, obj, objtype):
        ...
    
    def __set__(self, obj, val):
        ...
    


class cuFFTPlanCache(object):
    r"""
    Represents a specific plan cache for a specific `device_index`. The
    attributes `size` and `max_size`, and method `clear`, can fetch and/ or
    change properties of the C++ cuFFT plan cache.
    """
    def __init__(self, device_index) -> None:
        ...
    
    size = ...
    max_size = ...
    def clear(self):
        ...
    


class cuFFTPlanCacheManager(object):
    r"""
    Represents all cuFFT plan caches. When indexed with a device object/index,
    this object returns the `cuFFTPlanCache` corresponding to that device.

    Finally, this object, when used directly as a `cuFFTPlanCache` object (e.g.,
    setting the `.max_size`) attribute, the current device's cuFFT plan cache is
    used.
    """
    __initialized = ...
    def __init__(self) -> None:
        ...
    
    def __getitem__(self, device):
        ...
    
    def __getattr__(self, name):
        ...
    
    def __setattr__(self, name, value):
        ...
    


class CUDAModule(object):
    def __init__(self, m) -> None:
        ...
    
    cufft_plan_cache = ...


"""
This type stub file was generated by pyright.
"""

import sys
import torch
import types
from __future__ import absolute_import, division, print_function, unicode_literals

class _QEngineProp(object):
    def __get__(self, obj, objtype):
        ...
    
    def __set__(self, obj, val):
        ...
    


class _SupportedQEnginesProp(object):
    def __get__(self, obj, objtype):
        ...
    
    def __set__(self, obj, val):
        ...
    


class QuantizedEngine(types.ModuleType):
    def __init__(self, m, name) -> None:
        ...
    
    def __getattr__(self, attr):
        ...
    
    engine = ...
    supported_engines = ...


"""
This type stub file was generated by pyright.
"""

import torch

def is_available():
    r"""Returns whether PyTorch is built with MKL support."""
    ...

"""
This type stub file was generated by pyright.
"""

import sys
import torch
from contextlib import contextmanager
from torch.backends import ContextProp, PropModule, __allow_nonbracketed_mutation

def is_available():
    r"""Returns whether PyTorch is built with MKL-DNN support."""
    ...

def set_flags(_enabled):
    ...

@contextmanager
def flags(enabled=...):
    ...

class MkldnnModule(PropModule):
    def __init__(self, m, name) -> None:
        ...
    
    enabled = ...


"""
This type stub file was generated by pyright.
"""

import torch

def is_available():
    r"""Returns whether PyTorch is built with OpenMP support."""
    ...

"""
This type stub file was generated by pyright.
"""

import sys
import torch
import warnings
from contextlib import contextmanager
from torch.backends import ContextProp, PropModule, __allow_nonbracketed_mutation

__cudnn_version = None
if _cudnn is not None:
    ...
else:
    ...
def version():
    ...

CUDNN_TENSOR_DTYPES = torch.half, torch.float, torch.double
def is_available():
    r"""Returns a bool indicating if CUDNN is currently available."""
    ...

def is_acceptable(tensor):
    ...

def set_flags(_enabled, _benchmark, _deterministic):
    ...

@contextmanager
def flags(enabled=..., benchmark=..., deterministic=...):
    ...

class CudnnModule(PropModule):
    def __init__(self, m, name) -> None:
        ...
    
    enabled = ...
    deterministic = ...
    benchmark = ...


enabled: bool
deterministic: bool
benchmark: bool
"""
This type stub file was generated by pyright.
"""

def get_cudnn_mode(mode):
    ...

class Unserializable(object):
    def __init__(self, inner) -> None:
        ...
    
    def get(self):
        ...
    
    def __getstate__(self):
        ...
    
    def __setstate__(self, state):
        ...
    


def init_dropout_state(dropout, train, dropout_seed, dropout_state):
    ...

"""
This type stub file was generated by pyright.
"""

from .optimizer import Optimizer, _params_t

class Adagrad(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., lr_decay: float = ..., weight_decay: float = ..., initial_accumulator_value: float = ..., eps: float = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .optimizer import Optimizer, _params_t

class ASGD(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., lambd: float = ..., alpha: float = ..., t0: float = ..., weight_decay: float = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from . import lr_scheduler as lr_scheduler
from .adadelta import Adadelta as Adadelta
from .adagrad import Adagrad as Adagrad
from .adam import Adam as Adam
from .adamax import Adamax as Adamax
from .adamw import AdamW as AdamW
from .asgd import ASGD as ASGD
from .lbfgs import LBFGS as LBFGS
from .optimizer import Optimizer as Optimizer
from .rmsprop import RMSprop as RMSprop
from .rprop import Rprop as Rprop
from .sgd import SGD as SGD
from .sparse_adam import SparseAdam as SparseAdam

"""
This type stub file was generated by pyright.
"""

from typing import Tuple
from .optimizer import Optimizer, _params_t

class Adamax(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., betas: Tuple[float, float] = ..., eps: float = ..., weight_decay: float = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Optional
from .optimizer import Optimizer, _params_t

class LBFGS(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., max_iter: int = ..., max_eval: Optional[int] = ..., tolerance_grad: float = ..., tolerance_change: float = ..., history_size: int = ..., line_search_fn: Optional[str] = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Callable, Iterable, List, Optional, Union
from .. import Tensor

_params_t = Union[Iterable[Tensor], Iterable[dict]]
class Optimizer:
    defaults: dict
    state: dict
    param_groups: List[dict]
    def __init__(self, params: _params_t, default: dict) -> None:
        ...
    
    def __setstate__(self, statue: dict) -> None:
        ...
    
    def state_dict(self) -> dict:
        ...
    
    def load_state_dict(self, state_dict: dict) -> None:
        ...
    
    def zero_grad(self) -> None:
        ...
    
    def step(self, closure: Optional[Callable[[], float]] = ...) -> Optional[float]:
        ...
    
    def add_param_group(self, param_group: dict) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Tuple
from .optimizer import Optimizer, _params_t

class Rprop(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., etas: Tuple[float, float] = ..., step_sizes: Tuple[float, float] = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Tuple
from .optimizer import Optimizer, _params_t

class Adam(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., betas: Tuple[float, float] = ..., eps: float = ..., weight_decay: float = ..., amsgrad: bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .optimizer import Optimizer, _params_t

class RMSprop(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., alpha: float = ..., eps: float = ..., weight_decay: float = ..., momentum: float = ..., centered: bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .optimizer import Optimizer, _params_t

class Adadelta(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., rho: float = ..., eps: float = ..., weight_decay: float = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .optimizer import Optimizer, _params_t

class SGD(Optimizer):
    def __init__(self, params: _params_t, lr: float, momentum: float = ..., dampening: float = ..., weight_decay: float = ..., nesterov: bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Any, Callable, Iterable, List, Optional, Union
from .optimizer import Optimizer

class _LRScheduler:
    def __init__(self, optimizer: Optimizer, last_epoch: int = ...) -> None:
        ...
    
    def state_dict(self) -> dict:
        ...
    
    def load_state_dict(self, state_dict: dict) -> None:
        ...
    
    def get_lr(self) -> float:
        ...
    
    def step(self, epoch: Optional[int] = ...) -> None:
        ...
    


class LambdaLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, lr_lambda: Union[Callable[[int], float], List[Callable[[int], float]]], last_epoch: int = ...) -> None:
        ...
    


class StepLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, step_size: int, gamma: float = ..., last_epoch: int = ...) -> None:
        ...
    


class MultiStepLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, milestones: Iterable[int], gamma: float = ..., last_epoch: int = ...) -> None:
        ...
    


class ExponentialLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, gamma: float, last_epoch: int = ...) -> None:
        ...
    


class CosineAnnealingLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, T_max: int, eta_min: float, last_epoch: int = ...) -> None:
        ...
    


class ReduceLROnPlateau:
    in_cooldown: bool
    def __init__(self, optimizer: Optimizer, mode: str = ..., factor: float = ..., patience: int = ..., verbose: bool = ..., threshold: float = ..., threshold_mode: str = ..., cooldown: int = ..., min_lr: float = ..., eps: float = ...) -> None:
        ...
    
    def step(self, metrics: Any, epoch: Optional[int] = ...) -> None:
        ...
    
    def state_dict(self) -> dict:
        ...
    
    def load_state_dict(self, state_dict: dict):
        ...
    


class CyclicLR(_LRScheduler):
    def __init__(self, optimizer: Optimizer, base_lr: float = ..., max_lr: float = ..., step_size_up: int = ..., step_size_down: int = ..., mode: str = ..., gamma: float = ..., scale_fn: Optional[Callable[[float], float]] = ..., scale_mode: str = ..., cycle_momentum: bool = ..., base_momentum: float = ..., max_momentum: float = ..., last_epoch: int = ...) -> None:
        ...
    


class CosineAnnealingWarmRestarts(_LRScheduler):
    def __init__(self, optimizer: Optimizer, T_0: int = ..., T_mult: int = ..., eta_min: int = ..., last_epoch: int = ...) -> None:
        ...
    
    def step(self, epoch: Optional[int] = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Tuple
from .optimizer import Optimizer, _params_t

class AdamW(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., betas: Tuple[float, float] = ..., eps: float = ..., weight_decay: float = ..., amsgrad: bool = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Tuple
from .optimizer import Optimizer, _params_t

class SparseAdam(Optimizer):
    def __init__(self, params: _params_t, lr: float = ..., betas: Tuple[float, float] = ..., eps: float = ...) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

import os.path as _osp
from __future__ import absolute_import, division, print_function, unicode_literals
from .throughput_benchmark import ThroughputBenchmark

def set_module(obj, mod):
    ...

cmake_prefix_path = _osp.join(_osp.dirname(_osp.dirname(__file__)), 'share', 'cmake')
"""
This type stub file was generated by pyright.
"""

from torch._C import _get_backcompat_broadcast_warn, _get_backcompat_keepdim_warn, _set_backcompat_broadcast_warn, _set_backcompat_keepdim_warn

class Warning(object):
    def __init__(self, setter, getter) -> None:
        ...
    
    def set_enabled(self, value):
        ...
    
    def get_enabled(self):
        ...
    
    enabled = ...


broadcast_warning = Warning(_set_backcompat_broadcast_warn, _get_backcompat_broadcast_warn)
keepdim_warning = Warning(_set_backcompat_keepdim_warn, _get_backcompat_keepdim_warn)
"""
This type stub file was generated by pyright.
"""

class RemovableHandle(object):
    """A handle which provides the capability to remove a hook."""
    next_id = ...
    def __init__(self, hooks_dict) -> None:
        ...
    
    def remove(self):
        ...
    
    def __getstate__(self):
        ...
    
    def __setstate__(self, state):
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, type, value, tb):
        ...
    


def unserializable_hook(f):
    """
    Decorator which marks a function as an unserializable hook.
    This suppresses warnings that would otherwise arise if you attempt
    to serialize a tensor that has a hook.
    """
    ...

def warn_if_has_hooks(tensor):
    ...

"""
This type stub file was generated by pyright.
"""

def format_time(time_us=..., time_ms=..., time_s=...):
    '''Defines how to format time'''
    ...

class ExecutionStats(object):
    def __init__(self, c_stats, benchmark_config) -> None:
        ...
    
    @property
    def latency_avg_ms(self):
        ...
    
    @property
    def num_iters(self):
        ...
    
    @property
    def iters_per_second(self):
        '''
        Returns total number of iterations per second across all calling threads
        '''
        ...
    
    @property
    def total_time_seconds(self):
        ...
    
    def __str__(self) -> str:
        ...
    


class ThroughputBenchmark(object):
    '''
    This class is a wrapper around a c++ component throughput_benchmark::ThroughputBenchmark
    responsible for executing a PyTorch module (nn.Module or ScriptModule)
    under an inference server like load. It can emulate multiple calling threads
    to a single module provided. In the future we plan to enhance this component
    to support inter and intra-op parallelism as well as multiple models
    running in a single process.

    Please note that even though nn.Module is supported, it might incur an overhead
    from the need to hold GIL every time we execute Python code or pass around
    inputs as Python objects. As soon as you have a ScriptModule version of your
    model for inference deployment it is better to switch to using it in this
    benchmark.

    Example::

        >>> from torch.utils import ThroughputBenchmark
        >>> bench = ThroughputBenchmark(my_module)
        >>> # Pre-populate benchmark's data set with the inputs
        >>> for input in inputs:
                # Both args and kwargs work, same as any PyTorch Module / ScriptModule
                bench.add_input(input[0], x2=input[1])
        >>> Inputs supplied above are randomly used during the execution
        >>> stats = bench.benchmark(
                num_calling_threads=4,
                num_warmup_iters = 100,
                num_iters = 1000,
            )
        >>> print("Avg latency (ms): {}".format(stats.latency_avg_ms))
        >>> print("Number of iterations: {}".format(stats.num_iters))

    '''
    def __init__(self, module) -> None:
        ...
    
    def run_once(self, *args, **kwargs):
        '''
        Given input id (input_idx) run benchmark once and return prediction.
        This is useful for testing that benchmark actually runs the module you
        want it to run. input_idx here is an index into inputs array populated
        by calling add_input() method.
        '''
        ...
    
    def add_input(self, *args, **kwargs):
        '''
        Store a single input to a module into the benchmark memory and keep it
        there. During the benchmark execution every thread is going to pick up a
        random input from the all the inputs ever supplied to the benchmark via
        this function.
        '''
        ...
    
    def benchmark(self, num_calling_threads=..., num_warmup_iters=..., num_iters=..., profiler_output_path=...):
        '''
        Args:
            num_warmup_iters (int): Warmup iters are used to make sure we run a module
                a few times before actually measuring things. This way we avoid cold
                caches and any other similar problems. This is the number of warmup
                iterations for each of the thread in separate

            num_iters (int): Number of iterations the benchmark should run with.
                This number is separate from the warmup iterations. Also the number is
                shared across all the threads. Once the num_iters iterations across all
                the threads is reached, we will stop execution. Though total number of
                iterations might be slightly larger. Which is reported as
                stats.num_iters where stats is the result of this function

            profiler_output_path (string): Location to save Autograd Profiler trace.
                If not empty, Autograd Profiler will be enabled for the main benchmark
                execution (but not the warmup phase). The full trace will be saved
                into the file path provided by this argument


        This function returns BenchmarkExecutionStats object which is defined via pybind11.
        It currently has two fields:
            - num_iters - number of actual iterations the benchmark have made
            - avg_latency_ms - average time it took to infer on one input example in milliseconds
        '''
        ...
    


"""
This type stub file was generated by pyright.
"""

r""""Contains definitions of the methods used by the _BaseDataLoaderIter to fetch
data from an iterable-style or map-style dataset. This logic is shared in both
single- and multi-processing data loading.
"""
class _BaseDatasetFetcher(object):
    def __init__(self, dataset, auto_collation, collate_fn, drop_last) -> None:
        ...
    
    def fetch(self, possibly_batched_index):
        ...
    


class _IterableDatasetFetcher(_BaseDatasetFetcher):
    def __init__(self, dataset, auto_collation, collate_fn, drop_last) -> None:
        ...
    
    def fetch(self, possibly_batched_index):
        ...
    


class _MapDatasetFetcher(_BaseDatasetFetcher):
    def __init__(self, dataset, auto_collation, collate_fn, drop_last) -> None:
        ...
    
    def fetch(self, possibly_batched_index):
        ...
    


"""
This type stub file was generated by pyright.
"""

from collections import namedtuple
from . import IS_WINDOWS

r""""Contains definitions of the methods used by the _BaseDataLoaderIter workers.

These **needs** to be in global scope since Py2 doesn't support serializing
static methods.
"""
if IS_WINDOWS:
    class ManagerWatchdog(object):
        def __init__(self) -> None:
            ...
        
        def is_alive(self):
            ...
        
    
    
else:
    class ManagerWatchdog(object):
        def __init__(self) -> None:
            ...
        
        def is_alive(self):
            ...
        
    
    
_worker_info = None
class WorkerInfo(object):
    __initialized = ...
    def __init__(self, **kwargs) -> None:
        ...
    
    def __setattr__(self, key, val):
        ...
    
    def __repr__(self):
        ...
    


def get_worker_info():
    r"""Returns the information about the current
    :class:`~torch.utils.data.DataLoader` iterator worker process.

    When called in a worker, this returns an object guaranteed to have the
    following attributes:

    * :attr:`id`: the current worker id.
    * :attr:`num_workers`: the total number of workers.
    * :attr:`seed`: the random seed set for the current worker. This value is
      determined by main process RNG and the worker id. See
      :class:`~torch.utils.data.DataLoader`'s documentation for more details.
    * :attr:`dataset`: the copy of the dataset object in **this** process. Note
      that this will be a different object in a different process than the one
      in the main process.

    When called in the main process, this returns ``None``.

    .. note::
       When used in a :attr:`worker_init_fn` passed over to
       :class:`~torch.utils.data.DataLoader`, this method can be useful to
       set up each worker process differently, for instance, using ``worker_id``
       to configure the ``dataset`` object to only read a specific fraction of a
       sharded dataset, or use ``seed`` to seed other libraries used in dataset
       code (e.g., NumPy).
    """
    ...

_IterableDatasetStopIteration = namedtuple('_IterableDatasetStopIteration', ['worker_id'])
"""
This type stub file was generated by pyright.
"""

import sys
import atexit
from torch._utils import ExceptionWrapper
from . import collate, fetch, pin_memory, signal_handling, worker

r"""Utility classes & functions for data loading. Code in this folder is mostly
used by ../dataloder.py.

A lot of multiprocessing is used in data loading, which only supports running
functions defined in global environment (py2 can't serialize static methods).
Therefore, for code tidiness we put these functions into different files in this
folder.
"""
IS_WINDOWS = sys.platform == "win32"
MP_STATUS_CHECK_INTERVAL = 5
python_exit_status = False
"""
This type stub file was generated by pyright.
"""

import re

r""""Contains definitions of the methods used by the _BaseDataLoaderIter workers to
collate samples fetched from dataset into Tensor(s).

These **needs** to be in global scope since Py2 doesn't support serializing
static methods.
"""
np_str_obj_array_pattern = re.compile(r'[SaUO]')
def default_convert(data):
    r"""Converts each NumPy array data field into a tensor"""
    ...

default_collate_err_msg_format = "default_collate: batch must contain tensors, numpy arrays, numbers, " "dicts or lists; found {}"
def default_collate(batch):
    r"""Puts each data field into a tensor with outer dimension batch size"""
    ...

"""
This type stub file was generated by pyright.
"""

r""""Signal handling for multiprocessing data loading.

NOTE [ Signal handling in multiprocessing data loading ]

In cases like DataLoader, if a worker process dies due to bus error/segfault
or just hang, the main process will hang waiting for data. This is difficult
to avoid on PyTorch side as it can be caused by limited shm, or other
libraries users call in the workers. In this file and `DataLoader.cpp`, we make
our best effort to provide some error message to users when such unfortunate
events happen.

When a _BaseDataLoaderIter starts worker processes, their pids are registered in a
defined in `DataLoader.cpp`: id(_BaseDataLoaderIter) => Collection[ Worker pids ]
via `_set_worker_pids`.

When an error happens in a worker process, the main process received a SIGCHLD,
and Python will eventually call the handler registered below
(in `_set_SIGCHLD_handler`). In the handler, the `_error_if_any_worker_fails`
call checks all registered worker pids and raise proper error message to
prevent main process from hanging waiting for data from worker.

Additionally, at the beginning of each worker's `_utils.worker._worker_loop`,
`_set_worker_signal_handlers` is called to register critical signal handlers
(e.g., for SIGSEGV, SIGBUS, SIGFPE, SIGTERM) in C, which just prints an error
message to stderr before triggering the default handler. So a message will also
be printed from the worker process when it is killed by such signals.

See NOTE [ Data Loader Multiprocessing Shutdown Logic ] for the reasoning of
this signal handling design and other mechanism we implement to make our
multiprocessing data loading robust to errors.
"""
_SIGCHLD_handler_set = False
"""
This type stub file was generated by pyright.
"""

r""""Contains definitions of the methods used by the _BaseDataLoaderIter to put
fetched tensors into pinned memory.

These **needs** to be in global scope since Py2 doesn't support serializing
static methods.
"""
def pin_memory(data):
    ...

"""
This type stub file was generated by pyright.
"""

from typing import Generic, Iterator, List, Optional, Sequence, Sized, TypeVar
from ... import Tensor

T_co = TypeVar('T_co', covariant=True)
class Sampler(Generic[T_co]):
    def __init__(self, data_source: Sized) -> None:
        ...
    
    def __iter__(self) -> Iterator[T_co]:
        ...
    
    def __len__(self) -> int:
        ...
    


class SequentialSampler(Sampler[int]):
    data_source: Sized
    ...


class RandomSampler(Sampler[int]):
    data_source: Sized
    replacement: bool
    num_samples: int
    def __init__(self, data_source: Sized, replacement: bool = ..., num_samples: Optional[int] = ...) -> None:
        ...
    


class SubsetRandomSampler(Sampler[int]):
    indices: Sequence[int]
    def __init__(self, indices: Sequence[int]) -> None:
        ...
    


class WeightedRandomSampler(Sampler[int]):
    weights: Tensor
    num_samples: int
    replacement: bool
    def __init__(self, weights: Sequence[float], num_samples: int, replacement: bool = ...) -> None:
        ...
    


class BatchSampler(Sampler[List[int]]):
    sampler: Sampler[int]
    batch_size: int
    drop_last: bool
    def __init__(self, sampler: Sampler[int], batch_size: int, drop_last: bool) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from .sampler import BatchSampler as BatchSampler, RandomSampler as RandomSampler, Sampler as Sampler, SequentialSampler as SequentialSampler, SubsetRandomSampler as SubsetRandomSampler, WeightedRandomSampler as WeightedRandomSampler
from .distributed import DistributedSampler as DistributedSampler
from .dataset import ChainDataset as ChainDataset, ConcatDataset as ConcatDataset, Dataset as Dataset, IterableDataset as IterableDataset, Subset as Subset, TensorDataset as TensorDataset, random_split as random_split
from .dataloader import DataLoader as DataLoader, get_worker_info as get_worker_info

"""
This type stub file was generated by pyright.
"""

from typing import Iterator, Optional, TypeVar
from . import Dataset, Sampler

T_co = TypeVar('T_co', covariant=True)
class DistributedSampler(Sampler[T_co]):
    def __init__(self, dataset: Dataset, num_replicas: Optional[int] = ..., rank: Optional[int] = ..., shuffle: bool = ...) -> None:
        ...
    
    def __iter__(self) -> Iterator[T_co]:
        ...
    
    def __len__(self) -> int:
        ...
    
    def set_epoch(self, epoch: int) -> None:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Generic, Iterable, List, Optional, Sequence, Tuple, TypeVar
from ... import Generator, Tensor

T_co = TypeVar('T_co', covariant=True)
T = TypeVar('T')
class Dataset(Generic[T_co]):
    def __getitem__(self, index: int) -> T_co:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __add__(self, other: T_co) -> ConcatDataset[T_co]:
        ...
    


class IterableDataset(Dataset[T_co]):
    def __iter__(self) -> Iterable[T_co]:
        ...
    


class TensorDataset(Dataset[Tuple[Tensor, ...]]):
    tensors: List[Tensor]
    def __init__(self, *tensors: Tensor) -> None:
        ...
    


class ConcatDataset(Dataset[T_co]):
    datasets: List[Dataset[T_co]]
    cumulative_sizes: List[int]
    def __init__(self, datasets: Iterable[Dataset]) -> None:
        ...
    


class ChainDataset(Dataset[T_co]):
    def __init__(self, datasets: Iterable[Dataset]) -> None:
        ...
    


class Subset(Dataset[T_co]):
    dataset: Dataset[T_co]
    indices: Sequence[int]
    def __init__(self, dataset: Dataset[T_co], indices: Sequence[int]) -> None:
        ...
    


def random_split(dataset: Dataset[T], lengths: Sequence[int], generator: Optional[Generator]) -> List[Subset[T]]:
    ...

"""
This type stub file was generated by pyright.
"""

from typing import Any, Callable, Generic, List, Optional, Sequence, TypeVar, overload
from . import Dataset, Sampler

T_co = TypeVar('T_co', covariant=True)
T = TypeVar('T')
_worker_init_fn_t = Callable[[int], None]
_collate_fn_t = Callable[[List[T]], Any]
def default_collate(batch: List[T]) -> Any:
    ...

class DataLoader(Generic[T_co]):
    dataset: Dataset[T_co]
    batch_size: int
    num_workers: int
    pin_memory: bool
    drop_last: bool
    timeout: float
    @overload
    def __init__(self, dataset: Dataset[T_co], batch_size: int = ..., shuffle: bool = ..., sampler: Optional[Sampler[int]] = ..., num_workers: int = ..., collate_fn: _collate_fn_t = ..., pin_memory: bool = ..., drop_last: bool = ..., timeout: float = ..., worker_init_fn: _worker_init_fn_t = ...) -> None:
        ...
    
    @overload
    def __init__(self, dataset: Dataset[T_co], batch_sampler: Optional[Sampler[Sequence[int]]] = ..., num_workers: int = ..., collate_fn: _collate_fn_t = ..., pin_memory: bool = ..., timeout: float = ..., worker_init_fn: _worker_init_fn_t = ...) -> None:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __iter__(self) -> _BaseDataLoaderIter:
        ...
    


class _BaseDataLoaderIter:
    def __init__(self, loader: DataLoader) -> None:
        ...
    
    def __len__(self) -> int:
        ...
    
    def __iter__(self) -> _BaseDataLoaderIter:
        ...
    
    def __next__(self) -> Any:
        ...
    


"""
This type stub file was generated by pyright.
"""

"""Adds docstrings to functions defined in the torch._C"""
def parse_kwargs(desc):
    """Maps a description of args to a dictionary of {argname: description}.
    Input:
        ('    weight (Tensor): a weight tensor\n' +
         '        Some optional description')
    Output: {
        'weight': \
        'weight (Tensor): a weight tensor\n        Some optional description'
    }
    """
    ...

def merge_dicts(*dicts):
    ...

common_args = parse_kwargs("""
    input (Tensor): the input tensor.
    generator (:class:`torch.Generator`, optional): a pseudorandom number generator for sampling
    out (Tensor, optional): the output tensor.
""")
reduceops_common_args = merge_dicts(common_args, parse_kwargs("""
    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
        If specified, the input tensor is casted to :attr:`dtype` before the operation
        is performed. This is useful for preventing data type overflows. Default: None.
    keepdim (bool): whether the output tensor has :attr:`dim` retained or not.
"""))
multi_dim_common = merge_dicts(reduceops_common_args, parse_kwargs("""
    dim (int or tuple of ints): the dimension or dimensions to reduce.
"""), { 'keepdim_details': """
If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension(s) :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in the
output tensor having 1 (or ``len(dim)``) fewer dimension(s).
""" })
single_dim_common = merge_dicts(reduceops_common_args, parse_kwargs("""
    dim (int): the dimension to reduce.
"""), { 'keepdim_details': """If :attr:`keepdim` is ``True``, the output tensor is of the same size
as :attr:`input` except in the dimension :attr:`dim` where it is of size 1.
Otherwise, :attr:`dim` is squeezed (see :func:`torch.squeeze`), resulting in
the output tensor having 1 fewer dimension than :attr:`input`.""" })
factory_common_args = merge_dicts(common_args, parse_kwargs("""
    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
        Default: if ``None``, uses a global default (see :func:`torch.set_default_tensor_type`).
    layout (:class:`torch.layout`, optional): the desired layout of returned Tensor.
        Default: ``torch.strided``.
    device (:class:`torch.device`, optional): the desired device of returned tensor.
        Default: if ``None``, uses the current device for the default tensor type
        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
        for CPU tensor types and the current CUDA device for CUDA tensor types.
    requires_grad (bool, optional): If autograd should record operations on the
        returned tensor. Default: ``False``.
    pin_memory (bool, optional): If set, returned tensor would be allocated in
        the pinned memory. Works only for CPU tensors. Default: ``False``.
    memory_format (:class:`torch.memory_format`, optional): the desired memory format of
        returned Tensor. Default: ``torch.contiguous_format``.
"""))
factory_like_common_args = parse_kwargs("""
    input (Tensor): the size of :attr:`input` will determine size of the output tensor.
    layout (:class:`torch.layout`, optional): the desired layout of returned tensor.
        Default: if ``None``, defaults to the layout of :attr:`input`.
    dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
        Default: if ``None``, defaults to the dtype of :attr:`input`.
    device (:class:`torch.device`, optional): the desired device of returned tensor.
        Default: if ``None``, defaults to the device of :attr:`input`.
    requires_grad (bool, optional): If autograd should record operations on the
        returned tensor. Default: ``False``.
    pin_memory (bool, optional): If set, returned tensor would be allocated in
        the pinned memory. Works only for CPU tensors. Default: ``False``.
    memory_format (:class:`torch.memory_format`, optional): the desired memory format of
        returned Tensor. Default: ``torch.preserve_format``.
""")
factory_data_common_args = parse_kwargs("""
    data (array_like): Initial data for the tensor. Can be a list, tuple,
        NumPy ``ndarray``, scalar, and other types.
    dtype (:class:`torch.dtype`, optional): the desired data type of returned tensor.
        Default: if ``None``, infers data type from :attr:`data`.
    device (:class:`torch.device`, optional): the desired device of returned tensor.
        Default: if ``None``, uses the current device for the default tensor type
        (see :func:`torch.set_default_tensor_type`). :attr:`device` will be the CPU
        for CPU tensor types and the current CUDA device for CUDA tensor types.
    requires_grad (bool, optional): If autograd should record operations on the
        returned tensor. Default: ``False``.
    pin_memory (bool, optional): If set, returned tensor would be allocated in
        the pinned memory. Works only for CPU tensors. Default: ``False``.
""")
"""
This type stub file was generated by pyright.
"""

"""
This type stub file was generated by pyright.
"""

import torch
import builtins
from typing import List, Sequence, Tuple, Union

_TensorOrTensors = Union[torch.Tensor, Sequence[torch.Tensor]]
_int = builtins.int
_float = builtins.float
_bool = builtins.bool
_dtype = torch.dtype
_device = torch.device
_qscheme = torch.qscheme
_size = Union[torch.Size, List[_int], Tuple[_int, ...]]
_layout = torch.layout
Number = Union[builtins.int, builtins.float, builtins.bool]
Device = Union[_device, str, None]
"""
This type stub file was generated by pyright.
"""

from __future__ import absolute_import, division, print_function, unicode_literals
from .quantize import *
from .observer import *
from .qconfig import *
from .fake_quantize import *
from .fuse_modules import fuse_modules
from .stubs import *
from .quantize_jit import *

def default_eval_fn(model, calib_data):
    r"""
    Default evaluation function takes a torch.utils.data.Dataset or a list of
    input Tensors and run the model on the dataset
    """
    ...

_all__ = ['QuantWrapper', 'QuantStub', 'DeQuantStub', 'quantize', 'quantize_dynamic', 'quantize_qat', 'prepare', 'convert', 'prepare_qat', 'quantize_jit', 'quantize_dynamic_jit', 'propagate_qconfig_', 'add_quant_dequant', 'add_observer_', 'swap_module', 'default_eval_fn', 'get_observer_dict', 'ObserverBase', 'WeightObserver', 'observer', 'default_observer', 'default_weight_observer', 'QConfig', 'default_qconfig', 'default_dynamic_qconfig', 'float16_dynamic_qconfig', 'default_qat_qconfig', 'prepare_qat', 'quantize_qat', 'fuse_modules']
"""
This type stub file was generated by pyright.
"""

import torch
import torch.nn as nn
from abc import ABCMeta, abstractmethod

ABC = ABCMeta(str("ABC"), (object, ), {  })
class ObserverBase(ABC, nn.Module):
    r"""Base observer Module.
    Any observer implementation should derive from this class.

    Concrete observers should follow the same API. In forward, they will update
    the statistics of the observed Tensor. And they should provide a
    `calculate_qparams` function that computes the quantization parameters given
    the collected statistics.

    Args:
        dtype: Quantized data type
    """
    def __init__(self, dtype) -> None:
        ...
    
    @abstractmethod
    def forward(self, x):
        ...
    
    @abstractmethod
    def calculate_qparams(self, **kwargs):
        ...
    
    with_args = ...


class _ObserverBase(ObserverBase):
    r"""Internal common base for all qint/quint8 observers.

    This base is for commonly used paramters used internally.
    Users should use `~torch.quantization.observer.ObserverBase` as a base class
    for custom observers.

    Args:
        dtype: Quantized data type.
        qscheme: Quantization scheme to be used.
        reduce_range: Reduces the range of the quantized data type by 1 bit.
                      This is sometimes required to avoid instruction overflow.

    .. warning::

        :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.

    .. warning::

        :attr:`qscheme` can only take one of the following options:

        - ``torch.per_tensor_affine``
        - ``torch.per_tensor_symmetric``
        - ``torch.per_channel_affine``
        - ``torch.per_channel_symmetric``
    """
    def __init__(self, dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    


class MinMaxObserver(_ObserverBase):
    r"""Observer module for computing the quantization parameters based on the
    running min and max values.

    This observer uses the tensor min/max statistics to compute the quantization
    parameters. The module records the running minimum and maximum of incoming
    tensors, and uses this statistic to compute the quantization parameters.

    Args:
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    Given running min/max as :math:`x_\text{min}` and :math:`x_\text{max}`,
    scale :math:`s` and zero point :math:`z` are computed as:

    The running minimum/maximum :math:`x_\text{min/max}` is computed as:

    .. math::

        \begin{array}{ll}
        x_\text{min} &= \begin{cases}
            \min(X) & \text{if~}x_\text{min} = \text{None} \\
            \min\left(x_\text{min}, \min(X)\right) & \text{otherwise}
        \end{cases}\\
        x_\text{max} &= \begin{cases}
            \max(X) & \text{if~}x_\text{max} = \text{None} \\
            \max\left(x_\text{max}, \max(X)\right) & \text{otherwise}
        \end{cases}\\
        \end{array}

    where :math:`X` is the observed tensor.

    The scale :math:`s` and zero point :math:`z` are then computed as:

    .. math::

        \begin{aligned}
            \text{if Symmetric:}&\\
            &s = 2 \max(|x_\text{min}|, x_\text{max}) /
                \left( Q_\text{max} - Q_\text{min} \right) \\
            &z = \begin{cases}
                0 & \text{if dtype is qint8} \\
                128 & \text{otherwise}
            \end{cases}\\
            \text{Otherwise:}&\\
                &s = \left( x_\text{max} - x_\text{min}  \right ) /
                    \left( Q_\text{max} - Q_\text{min} \right ) \\
                &z = Q_\text{min} - \text{round}(x_\text{min} / s)
        \end{aligned}

    where :math:`Q_\text{min}` and :math:`Q_\text{max}` are the minimum and
    maximum of the quantized data type.

    .. warning:: Only works with ``torch.per_tensor_symmetric`` quantization scheme

    .. warning:: :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.

    .. note:: If the running minimum equals to the running maximum, the scale
              and zero_point are set to 1.0 and 0.
    """
    def __init__(self, dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    
    def forward(self, x_orig):
        r"""Records the running minimum and maximum of ``x``."""
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        r"""Calculates the quantization parameters."""
        ...
    
    @torch.jit.export
    def extra_repr(self):
        ...
    


class MovingAverageMinMaxObserver(MinMaxObserver):
    r"""Observer module for computing the quantization parameters based on the
    moving average of the min and max values.

    This observer computes the quantization parameters based on the moving
    averages of minimums and maximums of the incoming tensors. The module
    records the average minimum and maximum of incoming tensors, and uses this
    statistic to compute the quantization parameters.

    Args:
        averaging_constant: Averaging constant for min/max.
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    The moving average min/max is computed as follows

    .. math::

        \begin{array}{ll}
                x_\text{min} = \begin{cases}
                    \min(X) & \text{if~}x_\text{min} = \text{None} \\
                    (1 - c) x_\text{min} + c \min(X) & \text{otherwise}
                \end{cases}\\
                x_\text{max} = \begin{cases}
                    \max(X) & \text{if~}x_\text{max} = \text{None} \\
                    (1 - c) x_\text{max} + c \max(X) & \text{otherwise}
                \end{cases}\\
        \end{array}

    where :math:`x_\text{min/max}` is the running average min/max, :math:`X` is
    is the incoming tensor, and :math:`c` is the ``averaging_constant``.

    The scale and zero point are then computed as in
    :class:`~torch.quantization.observer.MinMaxObserver`.

    .. note:: Only works with ``torch.per_tensor_affine`` quantization shceme.

    .. note:: If the running minimum equals to the running maximum, the scale
              and zero_point are set to 1.0 and 0.
    """
    def __init__(self, averaging_constant=..., dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    
    def forward(self, x_orig):
        ...
    


class MinMaxDynamicQuantObserver(MinMaxObserver):
    r"""Observer module for computing the quantization parameters based on the
    tensor min and max values in dynamic quantization.

    This observer will mimic the quantization steps followed in the operator
    to compute the activation tensor quantization parameters at run-time.

    Args:
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    .. warning:: Only works with ``torch.per_tensor_symmetric`` quantization scheme

    .. warning:: :attr:`dtype` can only take ``torch.qint8`` or ``torch.quint8``.

    .. note:: If the running minimum equals to the running maximum, the scale
              and zero_point are set to 0.1 and 0.
    """
    @torch.jit.export
    def calculate_qparams(self):
        r"""Calculates the quantization parameters."""
        ...
    


class PerChannelMinMaxObserver(_ObserverBase):
    r"""Observer module for computing the quantization parameters based on the
    running per channel min and max values.

    This observer uses the tensor min/max statistics to compute the per channel
    quantization parameters. The module records the running minimum and maximum
    of incoming tensors, and uses this statistic to compute the quantization
    parameters.

    Args:
        ch_axis: Channel axis
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    The quantization parameters are computed the same way as in
    :class:`~torch.quantization.observer.MinMaxObserver`, with the difference
    that the running min/max values are stored per channel.
    Scales and zero points are thus computed per channel as well.

    .. note:: If the running minimum equals to the running maximum, the scales
              and zero_points are set to 1.0 and 0.
    """
    def __init__(self, ch_axis=..., dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    
    def forward(self, x_orig):
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    
    def extra_repr(self):
        ...
    


class MovingAveragePerChannelMinMaxObserver(PerChannelMinMaxObserver):
    r"""Observer module for computing the quantization parameters based on the
    running per channel min and max values.

    This observer uses the tensor min/max statistics to compute the per channel
    quantization parameters. The module records the running minimum and maximum
    of incoming tensors, and uses this statistic to compute the quantization
    parameters.

    Args:
        averaging_constant: Averaging constant for min/max.
        ch_axis: Channel axis
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    The quantization parameters are computed the same way as in
    :class:`~torch.quantization.observer.MovingAverageMinMaxObserver`, with the
    difference that the running min/max values are stored per channel.
    Scales and zero points are thus computed per channel as well.

    .. note:: If the running minimum equals to the running maximum, the scales
              and zero_points are set to 1.0 and 0.
    """
    def __init__(self, averaging_constant=..., ch_axis=..., dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    
    def forward(self, x_orig):
        ...
    


class HistogramObserver(_ObserverBase):
    r"""
    The module records the running histogram of tensor values along with
    min/max values. ``calculate_qparams`` will calculate scale and zero_point.

    Args:
        bins: Number of bins to use for the histogram
        upsample_rate: Factor by which the histograms are upsampled, this is
                       used to interpolate histograms with varying ranges across observations
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit

    The scale and zero point are computed as follows:

    1. Create the histogram of the incoming inputs.
        The histogram is computed continuously, and the ranges per bin change
        with every new tensor observed.
    2. Search the distribution in the histogram for optimal min/max values.
        The search for the min/max values ensures the minimization of the
        quantization error with respect to the floating point model.
    3. Compute the scale and zero point the same way as in the
        :class:`~torch.quantization.MinMaxObserver`
    """
    def __init__(self, bins=..., upsample_rate=..., dtype=..., qscheme=..., reduce_range=...) -> None:
        ...
    
    def forward(self, x_orig: Tensor) -> Tensor:
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    


class RecordingObserver(_ObserverBase):
    r"""
    The module is mainly for debug and records the tensor values during runtime.

    Args:
        dtype: Quantized data type
        qscheme: Quantization scheme to be used
        reduce_range: Reduces the range of the quantized data type by 1 bit
    """
    __annotations__ = ...
    def __init__(self, **kwargs) -> None:
        ...
    
    def forward(self, x):
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    
    @torch.jit.export
    def get_tensor_value(self):
        ...
    


class NoopObserver(ObserverBase):
    r"""
    Observer that doesn't do anything and just passes its configuration to the
    quantized module's ``.from_float()``.

    Primarily used for quantization to float16 which doesn't require determining
    ranges.

    Args:
        dtype: Quantized data type
    """
    def __init__(self, dtype=...) -> None:
        ...
    
    def forward(self, x):
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    


default_observer = MinMaxObserver.with_args(reduce_range=True)
default_debug_observer = RecordingObserver
default_weight_observer = MinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_tensor_symmetric)
default_histogram_observer = HistogramObserver.with_args(reduce_range=True)
default_per_channel_weight_observer = PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric)
default_dynamic_quant_observer = MinMaxDynamicQuantObserver
"""
This type stub file was generated by pyright.
"""

from torch import nn

class QuantStub(nn.Module):
    r"""Quantize stub module, before calibration, this is same as an observer,
    it will be swapped as `nnq.Quantize` in `convert`.

    Args:
        qconfig: quantization configuration for the tensor,
            if qconfig is not provided, we will get qconfig from parent modules
    """
    def __init__(self, qconfig=...) -> None:
        ...
    
    def forward(self, x):
        ...
    


class DeQuantStub(nn.Module):
    r"""Dequantize stub module, before calibration, this is same as identity,
    this will be swapped as `nnq.DeQuantize` in `convert`.
    """
    def __init__(self) -> None:
        ...
    
    def forward(self, x):
        ...
    


class QuantWrapper(nn.Module):
    r"""A wrapper class that wraps the input module, adds QuantStub and
    DeQuantStub and surround the call to module with call to quant and dequant
    modules.

    This is used by the `quantization` utility functions to add the quant and
    dequant modules, before `convert` function `QuantStub` will just be observer,
    it observes the input tensor, after `convert`, `QuantStub`
    will be swapped to `nnq.Quantize` which does actual quantization. Similarly
    for `DeQuantStub`.
    """
    def __init__(self, module) -> None:
        ...
    
    def forward(self, X):
        ...
    


"""
This type stub file was generated by pyright.
"""

import enum

class QuantType(enum.IntEnum):
    DYNAMIC = ...
    STATIC = ...


def script_qconfig(qconfig):
    r"""Instantiate the activation and weight observer modules and script
    them, these observer module instances will be deepcopied during
    prepare_jit step.
    """
    ...

def script_qconfig_dict(qconfig_dict):
    r"""Helper function used by `prepare_jit`.
    Apply `script_qconfig` for all entries in `qconfig_dict` that is
    not None.
    """
    ...

def fuse_conv_bn_jit(model, inplace=...):
    r""" Fuse conv - bn module
    Works for eval model only.

    Args:
        model: TorchScript model from scripting or tracing
    """
    ...

def prepare_jit(model, qconfig_dict, inplace=...):
    ...

def prepare_dynamic_jit(model, qconfig_dict, inplace=...):
    ...

def convert_jit(model, inplace=..., debug=...):
    ...

def convert_dynamic_jit(model, inplace=..., debug=...):
    ...

def quantize_jit(model, qconfig_dict, run_fn, run_args, inplace=..., debug=...):
    r"""Quantize the input float TorchScript model with
    post training static quantization.

    First it will prepare the model for calibration, then it calls
    `run_fn` which will run the calibration step, after that we will
    convert the model to a quantized model.

    Args:
        `model`: input float TorchScript model
        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and
        qconfig for that module as value, empty key means the qconfig will be applied
        to whole model unless it’s overwritten by more specific configurations, the
        qconfig for each module is either found in the dictionary or fallback to
         the qconfig of parent module.

        Right now qconfig_dict is the only way to configure how the model is quantized,
        and it is done in the granularity of module, that is, we only support one type
        of qconfig for each torch.nn.Module, and the qconfig for sub module will
        override the qconfig for parent module, empty string means global configuration.
        `run_fn`: a calibration function for calibrating the prepared model
        `run_args`: positional arguments for `run_fn`
        `inplace`: carry out model transformations in-place, the original module is
        mutated
        `debug`: flag for producing a debug friendly model (preserve weight attribute)

    Return:
        Quantized TorchSciprt model.

    Example:
    ```python
    import torch
    from torch.quantization import get_default_qconfig
    from torch.quantization import quantize_jit

    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
    qconfig = get_default_qconfig('fbgemm')
    def calibrate(model, data_loader):
        model.eval()
        with torch.no_grad():
            for image, target in data_loader:
                model(image)

    quantized_model = quantize_jit(
        ts_model,
        {'': qconfig},
        calibrate,
        [data_loader_test])
    ```
    """
    ...

def quantize_dynamic_jit(model, qconfig_dict, inplace=..., debug=...):
    r"""Quantize the input float TorchScript model with
    post training dynamic quantization.
    Currently only qint8 quantization of torch.nn.Linear is supported.

    Args:
        `model`: input float TorchScript model
        `qconfig_dict`: qconfig_dict is a dictionary with names of sub modules as key and
        qconfig for that module as value, please see detailed
        descriptions in :func:`~torch.quantization.quantize_jit`
        `inplace`: carry out model transformations in-place, the original module is
        mutated
        `debug`: flag for producing a debug friendly model (preserve weight attribute)

    Return:
        Quantized TorchSciprt model.

    Example:
    ```python
    import torch
    from torch.quantization import per_channel_dynamic_qconfig
    from torch.quantization import quantize_dynmiac_jit

    ts_model = torch.jit.script(float_model.eval())  # or torch.jit.trace(float_model, input)
    qconfig = get_default_qconfig('fbgemm')
    def calibrate(model, data_loader):
        model.eval()
        with torch.no_grad():
            for image, target in data_loader:
                model(image)

    quantized_model = quantize_dynamic_jit(
        ts_model,
        {'': qconfig},
        calibrate,
        [data_loader_test])
    ```
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from torch.nn import Module
from .observer import HistogramObserver, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver

class FakeQuantize(Module):
    r""" Simulate the quantize and dequantize operations in training time.
    The output of this module is given by

    x_out = (clamp(round(x/scale + zero_point), quant_min, quant_max)-zero_point)*scale



    * :attr:`scale` defines the scale factor used for quantization.

    * :attr:`zero_point` specifies the quantized value to which 0 in floating point maps to

    * :attr:`quant_min` specifies the minimum allowable quantized value.

    * :attr:`quant_max` specifies the maximum allowable quantized value.

    * :attr:`fake_quant_enable` controls the application of fake quantization on tensors, note that
      statistics can still be updated.

    * :attr:`observer_enable` controls statistics collection on tensors

    * :attr:`dtype` specifies the quantized dtype that is being emulated with fake-quantization,
                    allowable values are torch.qint8 and torch.quint8. The values of quant_min and
                    quant_max should be chosen to be consistent with the dtype


    Args:
        observer (module): Module for observing statistics on input tensors and calculating scale
                           and zero-point.
        quant_min (int): The minimum allowable quantized value.
        quant_max (int): The maximum allowable quantized value.
        observer_kwargs (optional): Arguments for the observer module

    Attributes:
        observer (Module): User provided module that collects statistics on the input tensor and
                           provides a method to calculate scale and zero-point.

    """
    def __init__(self, observer=..., quant_min=..., quant_max=..., **observer_kwargs) -> None:
        ...
    
    @torch.jit.export
    def enable_fake_quant(self, enabled: bool = ...) -> FakeQuantize:
        ...
    
    @torch.jit.export
    def disable_fake_quant(self):
        ...
    
    @torch.jit.export
    def enable_observer(self, enabled: bool = ...) -> FakeQuantize:
        ...
    
    @torch.jit.export
    def disable_observer(self):
        ...
    
    @torch.jit.export
    def calculate_qparams(self):
        ...
    
    def forward(self, X):
        ...
    
    with_args = ...
    @torch.jit.export
    def extra_repr(self):
        ...
    


default_fake_quant = FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
default_weight_fake_quant = FakeQuantize.with_args(observer=MovingAverageMinMaxObserver, quant_min=- 128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_tensor_symmetric, reduce_range=False)
default_per_channel_weight_fake_quant = FakeQuantize.with_args(observer=MovingAveragePerChannelMinMaxObserver, quant_min=- 128, quant_max=127, dtype=torch.qint8, qscheme=torch.per_channel_symmetric, reduce_range=False, ch_axis=0)
default_histogram_fake_quant = FakeQuantize.with_args(observer=HistogramObserver, quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_affine, reduce_range=True)
def disable_fake_quant(mod):
    ...

def enable_fake_quant(mod):
    ...

def disable_observer(mod):
    ...

def enable_observer(mod):
    ...

"""
This type stub file was generated by pyright.
"""

def fuse_conv_bn(conv, bn):
    r"""Given the conv and bn modules, fuses them and returns the fused module

    Args:
        conv: Module instance of type conv2d/conv3d
        bn: Spatial BN instance that needs to be fused with the conv

    Examples::

        >>> m1 = nn.Conv2d(10, 20, 3)
        >>> b1 = nn.BatchNorm2d(20)
        >>> m2 = fuse_conv_bn(m1, b1)
    """
    ...

def fuse_conv_bn_relu(conv, bn, relu):
    r"""Given the conv and bn modules, fuses them and returns the fused module

    Args:
        conv: Module instance of type conv2d/conv3d
        bn: Spatial BN instance that needs to be fused with the conv

    Examples::

        >>> m1 = nn.Conv2d(10, 20, 3)
        >>> b1 = nn.BatchNorm2d(20)
        >>> m2 = fuse_conv_bn(m1, b1)
    """
    ...

def fuse_known_modules(mod_list):
    r"""Returns a list of modules that fuses the operations specified
     in the input module list.

    Fuses only the following sequence of modules:
    conv, bn
    conv, bn, relu
    conv, relu
    linear, relu
    For these sequences, the first element in the output module list performs
    the fused operation. The rest of the elements are set to nn.Identity()
    """
    ...

def fuse_modules(model, modules_to_fuse, inplace=..., fuser_func=...):
    r"""Fuses a list of modules into a single module

    Fuses only the following sequence of modules:
    conv, bn
    conv, bn, relu
    conv, relu
    linear, relu
    bn, relu
    All other sequences are left unchanged.
    For these sequences, replaces the first item in the list
    with the fused module, replacing the rest of the modules
    with identity.

    Arguments:
        model: Model containing the modules to be fused
        modules_to_fuse: list of list of module names to fuse. Can also be a list
                         of strings if there is only a single list of modules to fuse.
        inplace: bool specifying if fusion happens in place on the model, by default
                 a new model is returned
        fuser_func: Function that takes in a list of modules and outputs a list of fused modules
                    of the same length. For example,
                    fuser_func([convModule, BNModule]) returns the list [ConvBNModule, nn.Identity()]
                    Defaults to torch.quantization.fuse_known_modules
    Returns:
        model with fused modules. A new copy is created if inplace=True.

    Examples::

            >>> m = myModel()
            >>> # m is a module containing  the sub-modules below
            >>> modules_to_fuse = [ ['conv1', 'bn1', 'relu1'], ['submodule.conv', 'submodule.relu']]
            >>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)
            >>> output = fused_m(input)

            >>> m = myModel()
            >>> # Alternately provide a single list of modules to fuse
            >>> modules_to_fuse = ['conv1', 'bn1', 'relu1']
            >>> fused_m = torch.quantization.fuse_modules(m, modules_to_fuse)
            >>> output = fused_m(input)

    """
    ...

"""
This type stub file was generated by pyright.
"""

def propagate_qconfig_(module, qconfig_dict=..., white_list=...):
    r"""Propagate qconfig through the module hierarchy and assign `qconfig`
    attribute on each leaf module

    Args:
        module: input module
        qconfig_dict: dictionary that maps from name or type of submodule to
            quantization configuration, qconfig applies to all submodules of a
            given module unless qconfig for the submodules are specified (when
            the submodule already has qconfig attribute)

    Return:
        None, module is modified inplace with qconfig attached
    """
    ...

def add_observer_(module, non_leaf_module_list=..., device=...):
    r"""Add observer for the leaf child of the module.

    This function insert observer module to all leaf child module that
    has a valid qconfig attribute.

    Args:
        module: input module with qconfig attributes for all the leaf modules that we want to quantize
        device: parent device, if any
        non_leaf_module_list: list of non-leaf modules we want to add observer

    Return:
        None, module is modified inplace with added observer modules and forward_hooks
    """
    ...

def get_unique_devices_(module):
    ...

def add_quant_dequant(module):
    r"""Wrap the leaf child module in QuantWrapper if it has a valid qconfig
    Note that this function will modify the children of module inplace and it
    can return a new module which wraps the input module as well.

    Args:
        module: input module with qconfig attributes for all the leaf modules
        that we want to quantize

    Return:
        Either the inplace modified module with submodules wrapped in
        `QuantWrapper` based on qconfig or a new `QuantWrapper` module which
        wraps the input module, the latter case only happens when the input
        module is a leaf module and we want to quantize it.
    """
    ...

def prepare(model, inplace=..., white_list=..., observer_non_leaf_module_list=...):
    r"""Prepares a copy of the model for quantization calibration or quantization-aware training.

    Quantization configuration should be assigned preemptively
    to individual submodules in `.qconfig` attribute.

    The model will be attached with observer or fake quant modules, and qconfig
    will be propagated.

    Args:
        model: input model to be modified in-place
        inplace: carry out model transformations in-place, the original module is mutated
        white_list: list of quantizable modules
        observer_non_leaf_module_list: list of non-leaf modules we want to add observer
    """
    ...

def quantize(model, run_fn, run_args, mapping=..., inplace=...):
    r"""Quantize the input float model with post training static quantization.

    First it will prepare the model for calibration, then it calls
    `run_fn` which will run the calibration step, after that we will
    convert the model to a quantized model.

    Args:
        model: input float model
        run_fn: a calibration function for calibrating the prepared model
        run_args: positional arguments for `run_fn`
        inplace: carry out model transformations in-place, the original module is mutated
        mapping: correspondence between original module types and quantized counterparts

    Return:
        Quantized model.
    """
    ...

def quantize_dynamic(model, qconfig_spec=..., dtype=..., mapping=..., inplace=...):
    r"""Converts a float model to dynamic (i.e. weights-only) quantized model.

    Replaces specified modules with dynamic weight-only quantized versions and output the quantized model.

    For simplest usage provide `dtype` argument that can be float16 or qint8. Weight-only quantization
    by default is performed for layers with large weights size - i.e. Linear and RNN variants.

    Fine grained control is possible with `qconfig` and `mapping` that act similarly to `quantize()`.
    If `qconfig` is provided, the `dtype` argument is ignored.

    Args:
        module: input model
        qconfig_spec: Either:

            - A dictionary that maps from name or type of submodule to quantization
              configuration, qconfig applies to all submodules of a given
              module unless qconfig for the submodules are specified (when the
              submodule already has qconfig attribute). Entries in the dictionary
              need to be QConfigDynamic instances.

            - A set of types and/or submodule names to apply dynamic quantization to,
              in which case the `dtype` argument is used to specifiy the bit-width

        inplace: carry out model transformations in-place, the original module is mutated
        mapping: maps type of a submodule to a type of corresponding dynamically quantized version
            with which the submodule needs to be replaced

    """
    ...

def prepare_qat(model, mapping=..., inplace=...):
    r"""
    Prepares a copy of the model for quantization calibration or
    quantization-aware training and convers it to quantized version.

    Quantization configuration should be assigned preemptively
    to individual submodules in `.qconfig` attribute.

    Args:
        model: input model to be modified in-place
        mapping: dictionary that maps float modules to quantized modules to be
                 replaced.
        inplace: carry out model transformations in-place, the original module
                 is mutated
    """
    ...

def quantize_qat(model, run_fn, run_args, inplace=...):
    r"""Do quantization aware training and output a quantized model

    Args:
        model: input model
        run_fn: a function for evaluating the prepared model, can be a
                function that simply runs the prepared model or a training
                loop
        run_args: positional arguments for `run_fn`

    Return:
        Quantized model.
    """
    ...

def convert(module, mapping=..., inplace=...):
    r"""Converts the float module with observers (where we can get quantization
    parameters) to a quantized module.

    Args:
        module: calibrated module with observers
        mapping: a dictionary that maps from float module type to quantized
                 module type, can be overwrritten to allow swapping user defined
                 Modules
        inplace: carry out model transformations in-place, the original module
                 is mutated

    """
    ...

def swap_module(mod, mapping):
    r"""Swaps the module if it has a quantized counterpart and it has an
    `observer` attached.

    Args:
        mod: input module
        mapping: a dictionary that maps from nn module to nnq module

    Return:
        The corresponding quantized module of `mod`
    """
    ...

def get_observer_dict(mod, target_dict, prefix=...):
    r"""Traverse the modules and save all observers into dict.
    This is mainly used for quantization accuracy debug
    Args:
        mod: the top module we want to save all observers
        prefix: the prefix for the current module
        target_dict: the dictionary used to save all the observers
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch.nn as nn
from collections import namedtuple
from .observer import *
from .fake_quantize import *

class QConfig(namedtuple('QConfig', ['activation', 'weight'])):
    """
    Describes how to quantize a layer or a part of the network by providing
    settings (observer classes) for activations and weights respectively.


    Note that QConfig needs to contain observer **classes** (like MinMaxObserver) or a callable that returns
    instances on invocation, not the concrete observer instances themselves.
    Quantization preparation function will instantiate observers multiple times for each of the layers.


    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`
    method (that behaves like functools.partial):

      my_qconfig = QConfig(activation=MinMaxObserver.with_args(dtype=torch.qint8),
      weight=default_observer.with_args(dtype=torch.qint8))
    """
    def __new__(cls, activation, weight):
        ...
    


default_qconfig = QConfig(activation=default_observer, weight=default_weight_observer)
default_debug_qconfig = QConfig(weight=default_weight_observer, activation=default_debug_observer)
default_per_channel_qconfig = QConfig(activation=default_observer, weight=default_per_channel_weight_observer)
class QConfigDynamic(namedtuple('QConfigDynamic', ['activation', 'weight'])):
    """
    Describes how to dynamically quantize a layer or a part of the network by providing
    settings (observer classe) for weights.

    It's like QConfig, but for dynamic quantization.

    Note that QConfigDynamic needs to contain observer **classes** (like MinMaxObserver) or a callable that returns
    instances on invocation, not the concrete observer instances themselves.
    Quantization function will instantiate observers multiple times for each of the layers.

    Observer classes have usually reasonable default arguments, but they can be overwritten with `with_args`
    method (that behaves like functools.partial):

      my_qconfig = QConfigDynamic(weight=default_observer.with_args(dtype=torch.qint8))
    """
    def __new__(cls, activation=..., weight=...):
        ...
    


default_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer, weight=default_weight_observer)
float16_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer, weight=NoopObserver.with_args(dtype=torch.float16))
per_channel_dynamic_qconfig = QConfigDynamic(activation=default_dynamic_quant_observer, weight=default_per_channel_weight_observer)
default_qat_qconfig = QConfig(activation=default_fake_quant, weight=default_weight_fake_quant)
default_weight_only_qconfig = QConfig(activation=torch.nn.Identity, weight=default_weight_fake_quant)
default_activation_only_qconfig = QConfig(activation=default_fake_quant, weight=torch.nn.Identity)
def get_default_qconfig(backend=...):
    ...

def get_default_qat_qconfig(backend=...):
    ...

"""
This type stub file was generated by pyright.
"""

import torch.nn.intrinsic as nni
import torch.nn.intrinsic.quantized as nniq
import torch.nn.intrinsic.qat as nniqat
import torch.nn.quantized as nnq
import torch.nn.quantized.dynamic as nnqd
import torch.nn.qat as nnqat
from torch import nn
from .stubs import DeQuantStub, QuantStub

DEFAULT_MODULE_MAPPING = { nn.Linear: nnq.Linear,nn.ReLU: nnq.ReLU,nn.ReLU6: nnq.ReLU6,nn.Hardswish: nnq.Hardswish,nn.ELU: nnq.ELU,nn.Conv1d: nnq.Conv1d,nn.Conv2d: nnq.Conv2d,nn.Conv3d: nnq.Conv3d,nn.BatchNorm2d: nnq.BatchNorm2d,nn.BatchNorm3d: nnq.BatchNorm3d,nn.LayerNorm: nnq.LayerNorm,nn.GroupNorm: nnq.GroupNorm,nn.InstanceNorm1d: nnq.InstanceNorm1d,nn.InstanceNorm2d: nnq.InstanceNorm2d,nn.InstanceNorm3d: nnq.InstanceNorm3d,QuantStub: nnq.Quantize,DeQuantStub: nnq.DeQuantize,nnq.FloatFunctional: nnq.QFunctional,nni.ConvReLU1d: nniq.ConvReLU1d,nni.ConvReLU2d: nniq.ConvReLU2d,nni.ConvReLU3d: nniq.ConvReLU3d,nni.LinearReLU: nniq.LinearReLU,nni.BNReLU2d: nniq.BNReLU2d,nni.BNReLU3d: nniq.BNReLU3d,nniqat.ConvReLU2d: nniq.ConvReLU2d,nniqat.LinearReLU: nniq.LinearReLU,nniqat.ConvBn2d: nnq.Conv2d,nniqat.ConvBnReLU2d: nniq.ConvReLU2d,nnqat.Linear: nnq.Linear,nnqat.Conv2d: nnq.Conv2d }
DEFAULT_QAT_MODULE_MAPPING = { nn.Linear: nnqat.Linear,nn.Conv2d: nnqat.Conv2d,nni.ConvBn2d: nniqat.ConvBn2d,nni.ConvBnReLU2d: nniqat.ConvBnReLU2d,nni.ConvReLU2d: nniqat.ConvReLU2d,nni.LinearReLU: nniqat.LinearReLU }
DEFAULT_DYNAMIC_MODULE_MAPPING = { nn.Linear: nnqd.Linear,nn.LSTM: nnqd.LSTM,nn.LSTMCell: nnqd.LSTMCell,nn.RNNCell: nnqd.RNNCell,nn.GRUCell: nnqd.GRUCell }
_EXCLUDE_QCONFIG_PROPAGATE_LIST = DeQuantStub
_INCLUDE_QCONFIG_PROPAGATE_LIST = nn.Sequential
DEFAULT_QCONFIG_PROPAGATE_WHITE_LIST = set(DEFAULT_MODULE_MAPPING.keys()) | set(DEFAULT_QAT_MODULE_MAPPING.keys()) | set(DEFAULT_DYNAMIC_MODULE_MAPPING.keys()) | _INCLUDE_QCONFIG_PROPAGATE_LIST - _EXCLUDE_QCONFIG_PROPAGATE_LIST
DEFAULT_NUMERIC_SUITE_COMPARE_MODEL_OUTPUT_WHITE_LIST = set(DEFAULT_MODULE_MAPPING.values()) | set(DEFAULT_QAT_MODULE_MAPPING.values()) | set(DEFAULT_DYNAMIC_MODULE_MAPPING.values()) | set(DEFAULT_MODULE_MAPPING.keys()) | set(DEFAULT_QAT_MODULE_MAPPING.keys()) | set(DEFAULT_DYNAMIC_MODULE_MAPPING.keys()) | _INCLUDE_QCONFIG_PROPAGATE_LIST - _EXCLUDE_QCONFIG_PROPAGATE_LIST
"""
This type stub file was generated by pyright.
"""

def annotate(ret, **kwargs):
    ...

class KeyErrorMessage(str):
    r"""str subclass that returns itself in repr"""
    def __repr__(self):
        ...
    


class ExceptionWrapper(object):
    r"""Wraps an exception plus traceback to communicate across threads"""
    def __init__(self, exc_info=..., where=...) -> None:
        ...
    
    def reraise(self):
        r"""Reraises the wrapped exception in the current thread"""
        ...
    


"""
This type stub file was generated by pyright.
"""

from enum import Enum

PYTORCH_ONNX_CAFFE2_BUNDLE: bool
IR_VERSION: int
PRODUCER_VERSION: str
class TensorProtoDataType(Enum):
    UNDEFINED = ...
    FLOAT = ...
    UINT8 = ...
    INT8 = ...
    UINT16 = ...
    INT16 = ...
    INT32 = ...
    INT64 = ...
    STRING = ...
    BOOL = ...
    FLOAT16 = ...
    DOUBLE = ...
    UINT32 = ...
    UINT64 = ...
    COMPLEX64 = ...
    COMPLEX128 = ...


class OperatorExportTypes(Enum):
    ONNX = ...
    ONNX_ATEN = ...
    ONNX_ATEN_FALLBACK = ...
    RAW = ...


class TrainingMode(Enum):
    EVAL = ...
    PRESERVE = ...
    TRAINING = ...


"""
This type stub file was generated by pyright.
"""

import torch
import builtins
from torch import Tensor
from typing import Any, Callable, ContextManager, Iterator, List, NamedTuple, Optional, Sequence, Tuple, Type, TypeVar, Union, overload
from torch._six import inf
from torch.types import Device, Number, _bool, _device, _dtype, _float, _int, _layout, _qscheme, _size
from . import _VariableFunctions as _VariableFunctions, _nn as _nn, _onnx as _onnx

T = TypeVar('T')
class device:
    type: str
    index: _int
    @overload
    def __init__(self, device: Union[_int, str]) -> None:
        ...
    
    @overload
    def __init__(self, type: str, index: _int) -> None:
        ...
    
    def __reduce__(self) -> Tuple[Any, ...]:
        ...
    


class Size(Tuple[_int, ...]):
    ...


class dtype:
    is_floating_point: _bool
    is_complex: _bool
    is_signed: _bool
    ...


class iinfo:
    bits: _int
    min: _int
    max: _int
    def __init__(self, dtype: _dtype) -> None:
        ...
    


class finfo:
    bits: _float
    min: _float
    max: _float
    eps: _float
    tiny: _float
    @overload
    def __init__(self, dtype: _dtype) -> None:
        ...
    
    @overload
    def __init__(self) -> None:
        ...
    


class layout:
    ...


class memory_format:
    ...


class qscheme:
    ...


class Storage:
    ...


class _FunctionBase(object):
    ...


class _LegacyVariableBase(object):
    def __init__(self, data: Optional[Tensor] = ..., requires_grad: Optional[_bool] = ..., volatile: Optional[_bool] = ..., _grad_fn: Optional[_FunctionBase] = ...) -> None:
        ...
    


def get_num_thread() -> _int:
    ...

def set_num_threads(nthreads: _int) -> None:
    ...

def get_num_interop_threads() -> _int:
    ...

def set_num_interop_threads(nthreads: _int) -> None:
    ...

def set_flush_denormal(arg: _bool) -> _bool:
    ...

def get_default_dtype() -> _dtype:
    ...

has_openmp: _bool
has_mkl: _bool
has_lapack: _bool
has_cuda: _bool
has_mkldnn: _bool
has_cudnn: _bool
_GLIBCXX_USE_CXX11_ABI: _bool
default_generator: Generator
class FileCheck(object):
    ...


class Generator(object):
    device: _device
    def __init__(self, device: Union[_device, str, None] = ...) -> None:
        ...
    
    def get_state(self) -> Tensor:
        ...
    
    def set_state(self, _new_state: Tensor) -> Generator:
        ...
    
    def manual_seed(self, seed: _int) -> Generator:
        ...
    
    def seed(self) -> _int:
        ...
    
    def initial_seed(self) -> _int:
        ...
    


class BenchmarkConfig(object):
    num_calling_threads: _int
    num_worker_threads: _int
    num_warmup_iters: _int
    num_iters: _int
    profiler_output_path: str
    ...


class BenchmarkExecutionStats(object):
    latency_avg_ms: _float
    num_iters: _int
    ...


class ThroughputBenchmark(object):
    def __init__(self, module: Any) -> None:
        ...
    
    def add_input(self, *args: Any, **kwargs: Any) -> None:
        ...
    
    def run_once(self, *args: Any, **kwargs: Any) -> Any:
        ...
    
    def benchmark(self, config: BenchmarkConfig) -> BenchmarkExecutionStats:
        ...
    


namedtuple_values_indices = NamedTuple("namedtuple_values_indices", [("values", Tensor), ("indices", Tensor)])
namedtuple_eigenvalues_eigenvectors = NamedTuple("namedtuple_eigenvalues_eigenvectors", [("eigenvalues", Tensor), ("eigenvectors", Tensor)])
namedtuple_a_tau = NamedTuple("namedtuple_a_tau", [("a", Tensor), ("tau", Tensor)])
namedtuple_solution_QR = NamedTuple("namedtuple_solution_QR", [("solution", Tensor), ("QR", Tensor)])
namedtuple_Q_R = NamedTuple("namedtuple_Q_R", [("Q", Tensor), ("R", Tensor)])
namedtuple_sign_logabsdet = NamedTuple("namedtuple_sign_logabsdet", [("sign", Tensor), ("logabsdet", Tensor)])
namedtuple_solution_LU = NamedTuple("namedtuple_solution_LU", [("solution", Tensor), ("LU", Tensor)])
namedtuple_U_S_V = NamedTuple("namedtuple_U_S_V", [("U", Tensor), ("S", Tensor), ("V", Tensor)])
namedtuple_solution_cloned_coefficient = NamedTuple("namedtuple_solution_cloned_coefficient", [("solution", Tensor), ("cloned_coefficient", Tensor)])
class DoubleStorageBase(object):
    ...


class FloatStorageBase(object):
    ...


class LongStorageBase(object):
    ...


class IntStorageBase(object):
    ...


class ShortStorageBase(object):
    ...


class CharStorageBase(object):
    ...


class ByteStorageBase(object):
    ...


class BoolStorageBase(object):
    ...


class HalfStorageBase(object):
    ...


class BFloat16StorageBase(object):
    ...


class ComplexDoubleStorageBase(object):
    ...


class ComplexFloatStorageBase(object):
    ...


class QUInt8StorageBase(object):
    ...


class QInt8StorageBase(object):
    ...


class QInt32StorageBase(object):
    ...


class DoubleTensor(Tensor):
    ...


class FloatTensor(Tensor):
    ...


class LongTensor(Tensor):
    ...


class IntTensor(Tensor):
    ...


class ShortTensor(Tensor):
    ...


class HalfTensor(Tensor):
    ...


class CharTensor(Tensor):
    ...


class ByteTensor(Tensor):
    ...


class BoolTensor(Tensor):
    ...


class _TensorBase(object):
    requires_grad: _bool
    shape: Size
    data: Tensor
    names: List[str]
    device: _device
    dtype: _dtype
    layout: _layout
    real: Tensor
    imag: Tensor
    _version: _bool
    def __abs__(self) -> Tensor:
        ...
    
    def __add__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __and__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __and__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __and__(self, other: Any) -> Tensor:
        ...
    
    def __bool__(self) -> builtins.bool:
        ...
    
    def __div__(self, other: Any) -> Tensor:
        ...
    
    def __eq__(self, other: Any) -> Tensor:
        ...
    
    def __float__(self) -> builtins.float:
        ...
    
    def __floordiv__(self, other: Any) -> Tensor:
        ...
    
    def __ge__(self, other: Any) -> Tensor:
        ...
    
    def __getitem__(self, indices: Union[None, _int, slice, Tensor, List, Tuple]) -> Tensor:
        ...
    
    def __gt__(self, other: Any) -> Tensor:
        ...
    
    def __iadd__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __iand__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __iand__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __iand__(self, other: Any) -> Tensor:
        ...
    
    def __idiv__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __ilshift__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __ilshift__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __ilshift__(self, other: Any) -> Tensor:
        ...
    
    def __imul__(self, other: Any) -> Tensor:
        ...
    
    def __index__(self) -> builtins.int:
        ...
    
    @overload
    def __init__(self, *args: Any, device: Union[_device, str, None] = ...) -> None:
        ...
    
    @overload
    def __init__(self, storage: Storage) -> None:
        ...
    
    @overload
    def __init__(self, other: Tensor) -> None:
        ...
    
    @overload
    def __init__(self, size: _size, *, device: Union[_device, str, None] = ...) -> None:
        ...
    
    def __int__(self) -> builtins.int:
        ...
    
    def __invert__(self) -> Tensor:
        ...
    
    @overload
    def __ior__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __ior__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __ior__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __irshift__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __irshift__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __irshift__(self, other: Any) -> Tensor:
        ...
    
    def __isub__(self, other: Any) -> Tensor:
        ...
    
    def __itruediv__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __ixor__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __ixor__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __ixor__(self, other: Any) -> Tensor:
        ...
    
    def __le__(self, other: Any) -> Tensor:
        ...
    
    def __long__(self) -> builtins.int:
        ...
    
    @overload
    def __lshift__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __lshift__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __lshift__(self, other: Any) -> Tensor:
        ...
    
    def __lt__(self, other: Any) -> Tensor:
        ...
    
    def __matmul__(self, other: Any) -> Tensor:
        ...
    
    def __mod__(self, other: Any) -> Tensor:
        ...
    
    def __mul__(self, other: Any) -> Tensor:
        ...
    
    def __ne__(self, other: Any) -> Tensor:
        ...
    
    def __neg__(self) -> Tensor:
        ...
    
    def __nonzero__(self) -> builtins.bool:
        ...
    
    @overload
    def __or__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __or__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __or__(self, other: Any) -> Tensor:
        ...
    
    def __pow__(self, other: Any) -> Tensor:
        ...
    
    def __radd__(self, other: Any) -> Tensor:
        ...
    
    def __rfloordiv__(self, other: Any) -> Tensor:
        ...
    
    def __rmul__(self, other: Any) -> Tensor:
        ...
    
    def __rpow__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __rshift__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __rshift__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __rshift__(self, other: Any) -> Tensor:
        ...
    
    def __rsub__(self, other: Any) -> Tensor:
        ...
    
    def __rtruediv__(self, other: Any) -> Tensor:
        ...
    
    def __setitem__(self, indices: Union[None, _int, slice, Tensor, List, Tuple], val: Union[Tensor, Number]) -> None:
        ...
    
    def __sub__(self, other: Any) -> Tensor:
        ...
    
    def __truediv__(self, other: Any) -> Tensor:
        ...
    
    @overload
    def __xor__(self, other: Number) -> Tensor:
        ...
    
    @overload
    def __xor__(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def __xor__(self, other: Any) -> Tensor:
        ...
    
    def abs(self) -> Tensor:
        ...
    
    def abs_(self) -> Tensor:
        ...
    
    def absolute(self) -> Tensor:
        ...
    
    def absolute_(self) -> Tensor:
        ...
    
    def acos(self) -> Tensor:
        ...
    
    def acos_(self) -> Tensor:
        ...
    
    def acosh(self) -> Tensor:
        ...
    
    def acosh_(self) -> Tensor:
        ...
    
    def add(self, other: Union[Tensor, Number], *, alpha: Optional[Number] = ..., out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def add_(self, other: Union[Tensor, Number], *, alpha: Optional[Number] = ...) -> Tensor:
        ...
    
    def addbmm(self, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addbmm_(self, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addcdiv(self, tensor1: Tensor, tensor2: Tensor, *, value: Number = ...) -> Tensor:
        ...
    
    def addcdiv_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number = ...) -> Tensor:
        ...
    
    def addcmul(self, tensor1: Tensor, tensor2: Tensor, *, value: Number = ...) -> Tensor:
        ...
    
    def addcmul_(self, tensor1: Tensor, tensor2: Tensor, *, value: Number = ...) -> Tensor:
        ...
    
    def addmm(self, mat1: Tensor, mat2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addmm_(self, mat1: Tensor, mat2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addmv(self, mat: Tensor, vec: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addmv_(self, mat: Tensor, vec: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addr(self, vec1: Tensor, vec2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def addr_(self, vec1: Tensor, vec2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def align_as(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def all(self, dim: _int, keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def all(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def all(self) -> Tensor:
        ...
    
    def allclose(self, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> _bool:
        ...
    
    def angle(self) -> Tensor:
        ...
    
    @overload
    def any(self, dim: _int, keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def any(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def any(self) -> Tensor:
        ...
    
    def apply_(self, callable: Callable) -> Tensor:
        ...
    
    def argmax(self, dim: Optional[_int] = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    def argmin(self, dim: Optional[_int] = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def argsort(self, dim: _int = ..., descending: _bool = ...) -> Tensor:
        ...
    
    @overload
    def argsort(self, dim: Union[str, ellipsis, None], descending: _bool = ...) -> Tensor:
        ...
    
    def as_strided(self, size: _size, stride: _size, storage_offset: Optional[_int] = ...) -> Tensor:
        ...
    
    def as_strided_(self, size: _size, stride: _size, storage_offset: Optional[_int] = ...) -> Tensor:
        ...
    
    def asin(self) -> Tensor:
        ...
    
    def asin_(self) -> Tensor:
        ...
    
    def asinh(self) -> Tensor:
        ...
    
    def asinh_(self) -> Tensor:
        ...
    
    def atan(self) -> Tensor:
        ...
    
    def atan2(self, other: Tensor) -> Tensor:
        ...
    
    def atan2_(self, other: Tensor) -> Tensor:
        ...
    
    def atan_(self) -> Tensor:
        ...
    
    def atanh(self) -> Tensor:
        ...
    
    def atanh_(self) -> Tensor:
        ...
    
    def backward(self, gradient: Optional[Tensor] = ..., retain_graph: Optional[_bool] = ..., create_graph: _bool = ...) -> None:
        ...
    
    def baddbmm(self, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    def baddbmm_(self, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    @overload
    def bernoulli(self, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def bernoulli(self, p: _float, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def bernoulli_(self, p: Tensor, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def bernoulli_(self, p: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def bfloat16(self) -> Tensor:
        ...
    
    def bincount(self, weights: Optional[Tensor] = ..., minlength: _int = ...) -> Tensor:
        ...
    
    @overload
    def bitwise_and(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_and(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def bitwise_and_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_and_(self, other: Tensor) -> Tensor:
        ...
    
    def bitwise_not(self) -> Tensor:
        ...
    
    def bitwise_not_(self) -> Tensor:
        ...
    
    @overload
    def bitwise_or(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_or(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def bitwise_or_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_or_(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def bitwise_xor(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_xor(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def bitwise_xor_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def bitwise_xor_(self, other: Tensor) -> Tensor:
        ...
    
    def bmm(self, mat2: Tensor) -> Tensor:
        ...
    
    def bool(self) -> Tensor:
        ...
    
    def byte(self) -> Tensor:
        ...
    
    def cauchy_(self, median: _float = ..., sigma: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def ceil(self) -> Tensor:
        ...
    
    def ceil_(self) -> Tensor:
        ...
    
    def char(self) -> Tensor:
        ...
    
    def cholesky(self, upper: _bool = ...) -> Tensor:
        ...
    
    def cholesky_inverse(self, upper: _bool = ...) -> Tensor:
        ...
    
    def cholesky_solve(self, input2: Tensor, upper: _bool = ...) -> Tensor:
        ...
    
    def chunk(self, chunks: _int, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
        ...
    
    def clamp(self, min: _float = ..., max: _float = ..., *, out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def clamp_(self, min: _float = ..., max: _float = ...) -> Tensor:
        ...
    
    def clamp_max(self, max: Number) -> Tensor:
        ...
    
    def clamp_max_(self, max: Number) -> Tensor:
        ...
    
    def clamp_min(self, min: Number) -> Tensor:
        ...
    
    def clamp_min_(self, min: Number) -> Tensor:
        ...
    
    def clone(self, *, memory_format: Optional[memory_format] = ...) -> Tensor:
        ...
    
    def coalesce(self) -> Tensor:
        ...
    
    def conj(self) -> Tensor:
        ...
    
    def contiguous(self) -> Tensor:
        ...
    
    def copy_(self, src: Tensor, non_blocking: _bool = ...) -> Tensor:
        ...
    
    def cos(self) -> Tensor:
        ...
    
    def cos_(self) -> Tensor:
        ...
    
    def cosh(self) -> Tensor:
        ...
    
    def cosh_(self) -> Tensor:
        ...
    
    def cpu(self) -> Tensor:
        ...
    
    def cross(self, other: Tensor, dim: Optional[_int] = ...) -> Tensor:
        ...
    
    def cuda(self, device: Optional[Union[_device, _int, str]] = ..., non_blocking: _bool = ...) -> Tensor:
        ...
    
    @overload
    def cummax(self, dim: _int) -> namedtuple_values_indices:
        ...
    
    @overload
    def cummax(self, dim: Union[str, ellipsis, None]) -> namedtuple_values_indices:
        ...
    
    @overload
    def cummin(self, dim: _int) -> namedtuple_values_indices:
        ...
    
    @overload
    def cummin(self, dim: Union[str, ellipsis, None]) -> namedtuple_values_indices:
        ...
    
    @overload
    def cumprod(self, dim: _int, *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def cumprod(self, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def cumsum(self, dim: _int, *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def cumsum(self, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    def deg2rad(self) -> Tensor:
        ...
    
    def deg2rad_(self) -> Tensor:
        ...
    
    def dense_dim(self) -> _int:
        ...
    
    def dequantize(self) -> Tensor:
        ...
    
    def det(self) -> Tensor:
        ...
    
    def detach(self) -> Tensor:
        ...
    
    def detach_(self) -> Tensor:
        ...
    
    def diag(self, diagonal: _int = ...) -> Tensor:
        ...
    
    def diag_embed(self, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor:
        ...
    
    def diagflat(self, offset: _int = ...) -> Tensor:
        ...
    
    @overload
    def diagonal(self, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor:
        ...
    
    @overload
    def diagonal(self, *, outdim: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None], dim2: Union[str, ellipsis, None], offset: _int = ...) -> Tensor:
        ...
    
    def digamma(self) -> Tensor:
        ...
    
    def digamma_(self) -> Tensor:
        ...
    
    def dim(self) -> _int:
        ...
    
    def dist(self, other: Tensor, p: Number = ...) -> Tensor:
        ...
    
    def div(self, other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def div_(self, other: Union[Tensor, Number]) -> Tensor:
        ...
    
    def dot(self, tensor: Tensor) -> Tensor:
        ...
    
    def double(self) -> Tensor:
        ...
    
    def eig(self, eigenvectors: _bool = ...) -> namedtuple_eigenvalues_eigenvectors:
        ...
    
    def element_size(self) -> _int:
        ...
    
    @overload
    def eq(self, other: Number) -> Tensor:
        ...
    
    @overload
    def eq(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def eq_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def eq_(self, other: Tensor) -> Tensor:
        ...
    
    def equal(self, other: Tensor) -> _bool:
        ...
    
    def erf(self) -> Tensor:
        ...
    
    def erf_(self) -> Tensor:
        ...
    
    def erfc(self) -> Tensor:
        ...
    
    def erfc_(self) -> Tensor:
        ...
    
    def erfinv(self) -> Tensor:
        ...
    
    def erfinv_(self) -> Tensor:
        ...
    
    def exp(self) -> Tensor:
        ...
    
    def exp_(self) -> Tensor:
        ...
    
    @overload
    def expand(self, size: _size, *, implicit: _bool = ...) -> Tensor:
        ...
    
    @overload
    def expand(self, *size: _int, implicit: _bool = ...) -> Tensor:
        ...
    
    def expand_as(self, other: Tensor) -> Tensor:
        ...
    
    def expm1(self) -> Tensor:
        ...
    
    def expm1_(self) -> Tensor:
        ...
    
    def exponential_(self, lambd: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def fft(self, signal_ndim: _int, normalized: _bool = ...) -> Tensor:
        ...
    
    @overload
    def fill_(self, value: Number) -> Tensor:
        ...
    
    @overload
    def fill_(self, value: Tensor) -> Tensor:
        ...
    
    def fill_diagonal_(self, fill_value: Number, wrap: _bool = ...) -> Tensor:
        ...
    
    @overload
    def flatten(self, start_dim: _int = ..., end_dim: _int = ...) -> Tensor:
        ...
    
    @overload
    def flatten(self, start_dim: _int, end_dim: _int, out_dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    @overload
    def flatten(self, start_dim: Union[str, ellipsis, None], end_dim: Union[str, ellipsis, None], out_dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    @overload
    def flatten(self, dims: Sequence[Union[str, ellipsis, None]], out_dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    @overload
    def flip(self, dims: _size) -> Tensor:
        ...
    
    @overload
    def flip(self, *dims: _int) -> Tensor:
        ...
    
    def fliplr(self) -> Tensor:
        ...
    
    def flipud(self) -> Tensor:
        ...
    
    def float(self) -> Tensor:
        ...
    
    def floor(self) -> Tensor:
        ...
    
    def floor_(self) -> Tensor:
        ...
    
    def floor_divide(self, other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def floor_divide_(self, other: Union[Tensor, Number]) -> Tensor:
        ...
    
    @overload
    def fmod(self, other: Number) -> Tensor:
        ...
    
    @overload
    def fmod(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def fmod_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def fmod_(self, other: Tensor) -> Tensor:
        ...
    
    def frac(self) -> Tensor:
        ...
    
    def frac_(self) -> Tensor:
        ...
    
    @overload
    def gather(self, dim: _int, index: Tensor, *, sparse_grad: _bool = ...) -> Tensor:
        ...
    
    @overload
    def gather(self, dim: Union[str, ellipsis, None], index: Tensor, *, sparse_grad: _bool = ...) -> Tensor:
        ...
    
    @overload
    def ge(self, other: Number) -> Tensor:
        ...
    
    @overload
    def ge(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def ge_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def ge_(self, other: Tensor) -> Tensor:
        ...
    
    def geometric_(self, p: _float, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def geqrf(self) -> namedtuple_a_tau:
        ...
    
    def ger(self, vec2: Tensor) -> Tensor:
        ...
    
    def get_device(self) -> _int:
        ...
    
    @overload
    def gt(self, other: Number) -> Tensor:
        ...
    
    @overload
    def gt(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def gt_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def gt_(self, other: Tensor) -> Tensor:
        ...
    
    def half(self) -> Tensor:
        ...
    
    def hardshrink(self, lambd: Number = ...) -> Tensor:
        ...
    
    def histc(self, bins: _int = ..., min: Number = ..., max: Number = ...) -> Tensor:
        ...
    
    def ifft(self, signal_ndim: _int, normalized: _bool = ...) -> Tensor:
        ...
    
    @overload
    def index_add(self, dim: _int, index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_add(self, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor:
        ...
    
    def index_add_(self, dim: _int, index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_copy(self, dim: _int, index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_copy(self, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_copy_(self, dim: _int, index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_copy_(self, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor:
        ...
    
    @overload
    def index_fill(self, dim: _int, index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def index_fill(self, dim: _int, index: Tensor, value: Tensor) -> Tensor:
        ...
    
    @overload
    def index_fill(self, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def index_fill(self, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor) -> Tensor:
        ...
    
    @overload
    def index_fill_(self, dim: _int, index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def index_fill_(self, dim: _int, index: Tensor, value: Tensor) -> Tensor:
        ...
    
    @overload
    def index_fill_(self, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def index_fill_(self, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor) -> Tensor:
        ...
    
    def index_put(self, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool = ...) -> Tensor:
        ...
    
    def index_put_(self, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool = ...) -> Tensor:
        ...
    
    @overload
    def index_select(self, dim: _int, index: Tensor) -> Tensor:
        ...
    
    @overload
    def index_select(self, dim: Union[str, ellipsis, None], index: Tensor) -> Tensor:
        ...
    
    def indices(self) -> Tensor:
        ...
    
    def int(self) -> Tensor:
        ...
    
    def int_repr(self) -> Tensor:
        ...
    
    def inverse(self) -> Tensor:
        ...
    
    def irfft(self, signal_ndim: _int, normalized: _bool = ..., onesided: _bool = ..., signal_sizes: _size = ...) -> Tensor:
        ...
    
    def is_coalesced(self) -> _bool:
        ...
    
    def is_complex(self) -> _bool:
        ...
    
    def is_contiguous(self) -> _bool:
        ...
    
    is_cuda: _bool
    def is_distributed(self) -> _bool:
        ...
    
    def is_floating_point(self) -> _bool:
        ...
    
    is_leaf: _bool
    is_meta: _bool
    is_mkldnn: _bool
    def is_nonzero(self) -> _bool:
        ...
    
    def is_pinned(self) -> _bool:
        ...
    
    is_quantized: _bool
    def is_same_size(self, other: Tensor) -> _bool:
        ...
    
    def is_set_to(self, tensor: Tensor) -> _bool:
        ...
    
    def is_signed(self) -> _bool:
        ...
    
    is_sparse: _bool
    def isclose(self, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> Tensor:
        ...
    
    def isfinite(self) -> Tensor:
        ...
    
    def isinf(self) -> Tensor:
        ...
    
    def isnan(self) -> Tensor:
        ...
    
    def item(self) -> Number:
        ...
    
    @overload
    def kthvalue(self, k: _int, dim: _int = ..., keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def kthvalue(self, k: _int, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def le(self, other: Number) -> Tensor:
        ...
    
    @overload
    def le(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def le_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def le_(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def lerp(self, end: Tensor, weight: Number) -> Tensor:
        ...
    
    @overload
    def lerp(self, end: Tensor, weight: Tensor) -> Tensor:
        ...
    
    @overload
    def lerp_(self, end: Tensor, weight: Number) -> Tensor:
        ...
    
    @overload
    def lerp_(self, end: Tensor, weight: Tensor) -> Tensor:
        ...
    
    def lgamma(self) -> Tensor:
        ...
    
    def lgamma_(self) -> Tensor:
        ...
    
    def log(self) -> Tensor:
        ...
    
    def log10(self) -> Tensor:
        ...
    
    def log10_(self) -> Tensor:
        ...
    
    def log1p(self) -> Tensor:
        ...
    
    def log1p_(self) -> Tensor:
        ...
    
    def log2(self) -> Tensor:
        ...
    
    def log2_(self) -> Tensor:
        ...
    
    def log_(self) -> Tensor:
        ...
    
    def log_normal_(self, mean: _float = ..., std: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def log_softmax(self, dim: _int, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def log_softmax(self, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    def logaddexp(self, other: Tensor) -> Tensor:
        ...
    
    def logaddexp2(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def logcumsumexp(self, dim: _int) -> Tensor:
        ...
    
    @overload
    def logcumsumexp(self, dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    def logdet(self) -> Tensor:
        ...
    
    def logical_and(self, other: Tensor) -> Tensor:
        ...
    
    def logical_and_(self, other: Tensor) -> Tensor:
        ...
    
    def logical_not(self) -> Tensor:
        ...
    
    def logical_not_(self) -> Tensor:
        ...
    
    def logical_or(self, other: Tensor) -> Tensor:
        ...
    
    def logical_or_(self, other: Tensor) -> Tensor:
        ...
    
    def logical_xor(self, other: Tensor) -> Tensor:
        ...
    
    def logical_xor_(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def logsumexp(self, dim: Union[_int, _size], keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def logsumexp(self, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool = ...) -> Tensor:
        ...
    
    def long(self) -> Tensor:
        ...
    
    def lstsq(self, A: Tensor) -> namedtuple_solution_QR:
        ...
    
    @overload
    def lt(self, other: Number) -> Tensor:
        ...
    
    @overload
    def lt(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def lt_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def lt_(self, other: Tensor) -> Tensor:
        ...
    
    def lu_solve(self, LU_data: Tensor, LU_pivots: Tensor) -> Tensor:
        ...
    
    def map_(self, tensor: Tensor, callable: Callable) -> Tensor:
        ...
    
    @overload
    def masked_fill(self, mask: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def masked_fill(self, mask: Tensor, value: Tensor) -> Tensor:
        ...
    
    @overload
    def masked_fill_(self, mask: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def masked_fill_(self, mask: Tensor, value: Tensor) -> Tensor:
        ...
    
    def masked_scatter(self, mask: Tensor, source: Tensor) -> Tensor:
        ...
    
    def masked_scatter_(self, mask: Tensor, source: Tensor) -> Tensor:
        ...
    
    def masked_select(self, mask: Tensor) -> Tensor:
        ...
    
    def matmul(self, other: Tensor) -> Tensor:
        ...
    
    def matrix_power(self, n: _int) -> Tensor:
        ...
    
    @overload
    def max(self, dim: _int, keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def max(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def max(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def max(self) -> Tensor:
        ...
    
    @overload
    def mean(self, *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def mean(self, dim: Union[_int, _size], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def mean(self, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def median(self, dim: _int, keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def median(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def median(self) -> Tensor:
        ...
    
    @overload
    def min(self, dim: _int, keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def min(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def min(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def min(self) -> Tensor:
        ...
    
    def mm(self, mat2: Tensor) -> Tensor:
        ...
    
    @overload
    def mode(self, dim: _int = ..., keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def mode(self, dim: Union[str, ellipsis, None], keepdim: _bool = ...) -> namedtuple_values_indices:
        ...
    
    def mul(self, other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def mul_(self, other: Union[Tensor, Number]) -> Tensor:
        ...
    
    def multinomial(self, num_samples: _int, replacement: _bool = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def mv(self, vec: Tensor) -> Tensor:
        ...
    
    def mvlgamma(self, p: _int) -> Tensor:
        ...
    
    def mvlgamma_(self, p: _int) -> Tensor:
        ...
    
    @overload
    def narrow(self, dim: _int, start: _int, length: _int) -> Tensor:
        ...
    
    @overload
    def narrow(self, dim: _int, start: Tensor, length: _int) -> Tensor:
        ...
    
    def narrow_copy(self, dim: _int, start: _int, length: _int) -> Tensor:
        ...
    
    def ndimension(self) -> _int:
        ...
    
    @overload
    def ne(self, other: Number) -> Tensor:
        ...
    
    @overload
    def ne(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def ne_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def ne_(self, other: Tensor) -> Tensor:
        ...
    
    def neg(self) -> Tensor:
        ...
    
    def neg_(self) -> Tensor:
        ...
    
    def nelement(self) -> _int:
        ...
    
    @overload
    def new(self, *args: Any, device: Union[_device, str, None] = ...) -> Tensor:
        ...
    
    @overload
    def new(self, storage: Storage) -> Tensor:
        ...
    
    @overload
    def new(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def new(self, size: _size, *, device: Union[_device, str, None] = ...) -> Tensor:
        ...
    
    @overload
    def new_empty(self, size: _size, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    @overload
    def new_empty(self, *size: _int, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    def new_full(self, size: _size, fill_value: Number, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    def new_ones(self, size: _size, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    def new_tensor(self, data: Any, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    @overload
    def new_zeros(self, size: _size, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    @overload
    def new_zeros(self, *size: _int, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
        ...
    
    def normal_(self, mean: _float = ..., std: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def numel(self) -> _int:
        ...
    
    def numpy(self) -> Any:
        ...
    
    def orgqr(self, input2: Tensor) -> Tensor:
        ...
    
    def ormqr(self, input2: Tensor, input3: Tensor, left: _bool = ..., transpose: _bool = ...) -> Tensor:
        ...
    
    @overload
    def permute(self, dims: _size) -> Tensor:
        ...
    
    @overload
    def permute(self, *dims: _int) -> Tensor:
        ...
    
    def pin_memory(self) -> Tensor:
        ...
    
    def pinverse(self, rcond: _float = ...) -> Tensor:
        ...
    
    def polygamma(self, n: _int) -> Tensor:
        ...
    
    def polygamma_(self, n: _int) -> Tensor:
        ...
    
    @overload
    def pow(self, exponent: Number) -> Tensor:
        ...
    
    @overload
    def pow(self, exponent: Tensor) -> Tensor:
        ...
    
    @overload
    def pow_(self, exponent: Number) -> Tensor:
        ...
    
    @overload
    def pow_(self, exponent: Tensor) -> Tensor:
        ...
    
    def prelu(self, weight: Tensor) -> Tensor:
        ...
    
    @overload
    def prod(self, *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def prod(self, dim: _int, keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def prod(self, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    def put_(self, index: Tensor, source: Tensor, accumulate: _bool = ...) -> Tensor:
        ...
    
    def q_per_channel_axis(self) -> _int:
        ...
    
    def q_per_channel_scales(self) -> Tensor:
        ...
    
    def q_per_channel_zero_points(self) -> Tensor:
        ...
    
    def q_scale(self) -> _float:
        ...
    
    def q_zero_point(self) -> _int:
        ...
    
    def qr(self, some: _bool = ...) -> namedtuple_Q_R:
        ...
    
    def qscheme(self) -> _qscheme:
        ...
    
    def rad2deg(self) -> Tensor:
        ...
    
    def rad2deg_(self) -> Tensor:
        ...
    
    @overload
    def random_(self, from_: _int, to: Optional[_int], *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def random_(self, to: _int, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    @overload
    def random_(self, *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def reciprocal(self) -> Tensor:
        ...
    
    def reciprocal_(self) -> Tensor:
        ...
    
    def relu(self) -> Tensor:
        ...
    
    def relu_(self) -> Tensor:
        ...
    
    @overload
    def remainder(self, other: Number) -> Tensor:
        ...
    
    @overload
    def remainder(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def remainder_(self, other: Number) -> Tensor:
        ...
    
    @overload
    def remainder_(self, other: Tensor) -> Tensor:
        ...
    
    def rename_(self, names: Optional[Sequence[Union[str, ellipsis, None]]]) -> Tensor:
        ...
    
    def renorm(self, p: Number, dim: _int, maxnorm: Number) -> Tensor:
        ...
    
    def renorm_(self, p: Number, dim: _int, maxnorm: Number) -> Tensor:
        ...
    
    @overload
    def repeat(self, repeats: _size) -> Tensor:
        ...
    
    @overload
    def repeat(self, *repeats: _int) -> Tensor:
        ...
    
    @overload
    def repeat_interleave(self, repeats: Tensor, dim: Optional[_int] = ...) -> Tensor:
        ...
    
    @overload
    def repeat_interleave(self, repeats: _int, dim: Optional[_int] = ...) -> Tensor:
        ...
    
    def requires_grad_(self, mode: _bool = ...) -> Tensor:
        ...
    
    @overload
    def reshape(self, shape: _size) -> Tensor:
        ...
    
    @overload
    def reshape(self, *shape: _int) -> Tensor:
        ...
    
    def reshape_as(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def resize_(self, size: _size, *, memory_format: Optional[memory_format] = ...) -> Tensor:
        ...
    
    @overload
    def resize_(self, *size: _int, memory_format: Optional[memory_format] = ...) -> Tensor:
        ...
    
    def resize_as_(self, the_template: Tensor, *, memory_format: Optional[memory_format] = ...) -> Tensor:
        ...
    
    def rfft(self, signal_ndim: _int, normalized: _bool = ..., onesided: _bool = ...) -> Tensor:
        ...
    
    def roll(self, shifts: Union[_int, _size], dims: Union[_int, _size] = ...) -> Tensor:
        ...
    
    def rot90(self, k: _int = ..., dims: _size = ...) -> Tensor:
        ...
    
    def round(self) -> Tensor:
        ...
    
    def round_(self) -> Tensor:
        ...
    
    def rsqrt(self) -> Tensor:
        ...
    
    def rsqrt_(self) -> Tensor:
        ...
    
    @overload
    def scatter(self, dim: _int, index: Tensor, src: Tensor) -> Tensor:
        ...
    
    @overload
    def scatter(self, dim: _int, index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def scatter(self, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor:
        ...
    
    @overload
    def scatter(self, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def scatter_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor:
        ...
    
    @overload
    def scatter_(self, dim: _int, index: Tensor, value: Number) -> Tensor:
        ...
    
    @overload
    def scatter_add(self, dim: _int, index: Tensor, src: Tensor) -> Tensor:
        ...
    
    @overload
    def scatter_add(self, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor:
        ...
    
    def scatter_add_(self, dim: _int, index: Tensor, src: Tensor) -> Tensor:
        ...
    
    @overload
    def select(self, dim: Union[str, ellipsis, None], index: _int) -> Tensor:
        ...
    
    @overload
    def select(self, dim: _int, index: _int) -> Tensor:
        ...
    
    def short(self) -> Tensor:
        ...
    
    def sigmoid(self) -> Tensor:
        ...
    
    def sigmoid_(self) -> Tensor:
        ...
    
    def sign(self) -> Tensor:
        ...
    
    def sign_(self) -> Tensor:
        ...
    
    def sin(self) -> Tensor:
        ...
    
    def sin_(self) -> Tensor:
        ...
    
    def sinh(self) -> Tensor:
        ...
    
    def sinh_(self) -> Tensor:
        ...
    
    @overload
    def size(self) -> Size:
        ...
    
    @overload
    def size(self, _int) -> _int:
        ...
    
    def slogdet(self) -> namedtuple_sign_logabsdet:
        ...
    
    def smm(self, mat2: Tensor) -> Tensor:
        ...
    
    @overload
    def softmax(self, dim: _int, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def softmax(self, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    def solve(self, A: Tensor) -> namedtuple_solution_LU:
        ...
    
    @overload
    def sort(self, dim: _int = ..., descending: _bool = ...) -> namedtuple_values_indices:
        ...
    
    @overload
    def sort(self, dim: Union[str, ellipsis, None], descending: _bool = ...) -> namedtuple_values_indices:
        ...
    
    def sparse_dim(self) -> _int:
        ...
    
    def sparse_mask(self, mask: Tensor) -> Tensor:
        ...
    
    def sparse_resize_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor:
        ...
    
    def sparse_resize_and_clear_(self, size: _size, sparse_dim: _int, dense_dim: _int) -> Tensor:
        ...
    
    def split_with_sizes(self, split_sizes: _size, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
        ...
    
    def sqrt(self) -> Tensor:
        ...
    
    def sqrt_(self) -> Tensor:
        ...
    
    def square(self) -> Tensor:
        ...
    
    def square_(self) -> Tensor:
        ...
    
    @overload
    def squeeze(self) -> Tensor:
        ...
    
    @overload
    def squeeze(self, dim: _int) -> Tensor:
        ...
    
    @overload
    def squeeze(self, dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    @overload
    def squeeze_(self) -> Tensor:
        ...
    
    @overload
    def squeeze_(self, dim: _int) -> Tensor:
        ...
    
    @overload
    def squeeze_(self, dim: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    def sspaddmm(self, mat1: Tensor, mat2: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
        ...
    
    @overload
    def std(self, unbiased: _bool = ...) -> Tensor:
        ...
    
    @overload
    def std(self, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def std(self, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    def storage(self) -> Storage:
        ...
    
    def storage_offset(self) -> _int:
        ...
    
    @overload
    def stride(self) -> Tuple[_int]:
        ...
    
    @overload
    def stride(self, _int) -> _int:
        ...
    
    def sub(self, other: Union[Tensor, Number], *, alpha: Optional[Number] = ..., out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def sub_(self, other: Union[Tensor, Number], *, alpha: Optional[Number] = ...) -> Tensor:
        ...
    
    @overload
    def sum(self, *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def sum(self, dim: Union[_int, _size], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def sum(self, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ...) -> Tensor:
        ...
    
    @overload
    def sum_to_size(self, size: _size) -> Tensor:
        ...
    
    @overload
    def sum_to_size(self, *size: _int) -> Tensor:
        ...
    
    def svd(self, some: _bool = ..., compute_uv: _bool = ...) -> namedtuple_U_S_V:
        ...
    
    def symeig(self, eigenvectors: _bool = ..., upper: _bool = ...) -> namedtuple_eigenvalues_eigenvectors:
        ...
    
    def t(self) -> Tensor:
        ...
    
    def t_(self) -> Tensor:
        ...
    
    def take(self, index: Tensor) -> Tensor:
        ...
    
    def tan(self) -> Tensor:
        ...
    
    def tan_(self) -> Tensor:
        ...
    
    def tanh(self) -> Tensor:
        ...
    
    def tanh_(self) -> Tensor:
        ...
    
    @overload
    def to(self, dtype: _dtype, non_blocking: _bool = ..., copy: _bool = ...) -> Tensor:
        ...
    
    @overload
    def to(self, device: Optional[Union[_device, str]] = ..., dtype: Optional[_dtype] = ..., non_blocking: _bool = ..., copy: _bool = ...) -> Tensor:
        ...
    
    @overload
    def to(self, other: Tensor, non_blocking: _bool = ..., copy: _bool = ...) -> Tensor:
        ...
    
    def to_dense(self) -> Tensor:
        ...
    
    def to_mkldnn(self) -> Tensor:
        ...
    
    @overload
    def to_sparse(self, sparse_dim: _int) -> Tensor:
        ...
    
    @overload
    def to_sparse(self) -> Tensor:
        ...
    
    def tolist(self) -> List:
        ...
    
    def topk(self, k: _int, dim: _int = ..., largest: _bool = ..., sorted: _bool = ...) -> namedtuple_values_indices:
        ...
    
    def trace(self) -> Tensor:
        ...
    
    @overload
    def transpose(self, dim0: _int, dim1: _int) -> Tensor:
        ...
    
    @overload
    def transpose(self, dim0: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None]) -> Tensor:
        ...
    
    def transpose_(self, dim0: _int, dim1: _int) -> Tensor:
        ...
    
    def triangular_solve(self, A: Tensor, upper: _bool = ..., transpose: _bool = ..., unitriangular: _bool = ...) -> namedtuple_solution_cloned_coefficient:
        ...
    
    def tril(self, diagonal: _int = ...) -> Tensor:
        ...
    
    def tril_(self, diagonal: _int = ...) -> Tensor:
        ...
    
    def triu(self, diagonal: _int = ...) -> Tensor:
        ...
    
    def triu_(self, diagonal: _int = ...) -> Tensor:
        ...
    
    def true_divide(self, other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
        ...
    
    def true_divide_(self, other: Union[Tensor, Number]) -> Tensor:
        ...
    
    def trunc(self) -> Tensor:
        ...
    
    def trunc_(self) -> Tensor:
        ...
    
    @overload
    def type(self, dtype: None = ..., non_blocking: _bool = ...) -> str:
        ...
    
    @overload
    def type(self, dtype: Union[str, _dtype], non_blocking: _bool = ...) -> Tensor:
        ...
    
    def type_as(self, other: Tensor) -> Tensor:
        ...
    
    @overload
    def unbind(self, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
        ...
    
    @overload
    def unbind(self, dim: Union[str, ellipsis, None]) -> Union[Tuple[Tensor, ...], List[Tensor]]:
        ...
    
    def unfold(self, dimension: _int, size: _int, step: _int) -> Tensor:
        ...
    
    def uniform_(self, from_: _float = ..., to: _float = ..., *, generator: Optional[Generator] = ...) -> Tensor:
        ...
    
    def unsqueeze(self, dim: _int) -> Tensor:
        ...
    
    def unsqueeze_(self, dim: _int) -> Tensor:
        ...
    
    def values(self) -> Tensor:
        ...
    
    @overload
    def var(self, unbiased: _bool = ...) -> Tensor:
        ...
    
    @overload
    def var(self, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def var(self, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ...) -> Tensor:
        ...
    
    @overload
    def view(self, size: _size) -> Tensor:
        ...
    
    @overload
    def view(self, *size: _int) -> Tensor:
        ...
    
    def view_as(self, other: Tensor) -> Tensor:
        ...
    
    def where(self, condition: Tensor, other: Tensor) -> Tensor:
        ...
    
    def zero_(self) -> Tensor:
        ...
    


class _CudaDeviceProperties:
    name: str
    major: _int
    minor: _int
    multi_processor_count: _int
    total_memory: _int
    is_integrated: _int
    is_multi_gpu_board: _int
    ...


class _CudaStreamBase:
    device: _device
    cuda_stream: _int
    priority: _int
    def __new__(self, priority: _int = ..., _cdata: _int = ...) -> _CudaStreamBase:
        ...
    
    def query(self) -> _bool:
        ...
    
    def synchronize(self) -> None:
        ...
    
    def priority_range(self) -> Tuple[_int, _int]:
        ...
    


class _CudaEventBase:
    device: _device
    cuda_event: _int
    def __new__(cls, enable_timing: _bool = ..., blocking: _bool = ..., interprocess: _bool = ...) -> _CudaEventBase:
        ...
    
    @classmethod
    def from_ipc_handle(cls, device: _device, ipc_handle: bytes) -> _CudaEventBase:
        ...
    
    def record(self, stream: _CudaStreamBase) -> None:
        ...
    
    def wait(self, stream: _CudaStreamBase) -> None:
        ...
    
    def query(self) -> _bool:
        ...
    
    def elapsed_time(self, other: _CudaEventBase) -> _float:
        ...
    
    def synchronize(self) -> None:
        ...
    
    def ipc_handle(self) -> bytes:
        ...
    


"""
This type stub file was generated by pyright.
"""

from typing import Callable

fractional_max_pool2d: Callable
fractional_max_pool3d: Callable
max_pool1d: Callable
max_pool2d: Callable
max_pool3d: Callable
adaptive_max_pool1d: Callable
adaptive_max_pool2d: Callable
adaptive_max_pool3d: Callable
avg_pool2d: Callable
avg_pool3d: Callable
hardtanh_: Callable
elu_: Callable
leaky_relu_: Callable
logsigmoid: Callable
softplus: Callable
softshrink: Callable
one_hot: Callable
"""
This type stub file was generated by pyright.
"""

from torch import Generator, Tensor, memory_format
from typing import Any, List, NamedTuple, Optional, Sequence, Tuple, Union, overload
from torch.types import Number, _bool, _device, _dtype, _float, _int, _layout, _size

namedtuple_values_indices = NamedTuple("namedtuple_values_indices", [("values", Tensor), ("indices", Tensor)])
namedtuple_eigenvalues_eigenvectors = NamedTuple("namedtuple_eigenvalues_eigenvectors", [("eigenvalues", Tensor), ("eigenvectors", Tensor)])
namedtuple_a_tau = NamedTuple("namedtuple_a_tau", [("a", Tensor), ("tau", Tensor)])
namedtuple_solution_QR = NamedTuple("namedtuple_solution_QR", [("solution", Tensor), ("QR", Tensor)])
namedtuple_Q_R = NamedTuple("namedtuple_Q_R", [("Q", Tensor), ("R", Tensor)])
namedtuple_sign_logabsdet = NamedTuple("namedtuple_sign_logabsdet", [("sign", Tensor), ("logabsdet", Tensor)])
namedtuple_solution_LU = NamedTuple("namedtuple_solution_LU", [("solution", Tensor), ("LU", Tensor)])
namedtuple_U_S_V = NamedTuple("namedtuple_U_S_V", [("U", Tensor), ("S", Tensor), ("V", Tensor)])
namedtuple_solution_cloned_coefficient = NamedTuple("namedtuple_solution_cloned_coefficient", [("solution", Tensor), ("cloned_coefficient", Tensor)])
@overload
def __and__(self: Tensor, other: Number) -> Tensor:
    ...

@overload
def __and__(self: Tensor, other: Tensor) -> Tensor:
    ...

@overload
def __lshift__(self: Tensor, other: Number) -> Tensor:
    ...

@overload
def __lshift__(self: Tensor, other: Tensor) -> Tensor:
    ...

@overload
def __or__(self: Tensor, other: Number) -> Tensor:
    ...

@overload
def __or__(self: Tensor, other: Tensor) -> Tensor:
    ...

@overload
def __rshift__(self: Tensor, other: Number) -> Tensor:
    ...

@overload
def __rshift__(self: Tensor, other: Tensor) -> Tensor:
    ...

@overload
def __xor__(self: Tensor, other: Number) -> Tensor:
    ...

@overload
def __xor__(self: Tensor, other: Tensor) -> Tensor:
    ...

def abs(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def abs_(self: Tensor) -> Tensor:
    ...

def absolute(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def absolute_(self: Tensor) -> Tensor:
    ...

def acos(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def acos_(self: Tensor) -> Tensor:
    ...

def acosh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def acosh_(self: Tensor) -> Tensor:
    ...

def adaptive_avg_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tensor:
    ...

def adaptive_max_pool1d(self: Tensor, output_size: Union[_int, _size]) -> Tuple[Tensor, Tensor]:
    ...

@overload
def add(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def add(self: Tensor, alpha: Number, other: Tensor) -> Tensor:
    ...

@overload
def add(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor:
    ...

@overload
def addbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor:
    ...

@overload
def addbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addcdiv(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor:
    ...

@overload
def addcdiv(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addcmul(self: Tensor, tensor1: Tensor, tensor2: Tensor, *, value: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor) -> Tensor:
    ...

@overload
def addcmul(self: Tensor, value: Number, tensor1: Tensor, tensor2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor:
    ...

@overload
def addmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor:
    ...

@overload
def addmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addmv(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor) -> Tensor:
    ...

@overload
def addmv(beta: Number, self: Tensor, alpha: Number, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor) -> Tensor:
    ...

@overload
def addmv(beta: Number, self: Tensor, mat: Tensor, vec: Tensor, *, out: Tensor) -> Tensor:
    ...

def addmv_(self: Tensor, mat: Tensor, vec: Tensor, *, beta: Number = ..., alpha: Number = ...) -> Tensor:
    ...

@overload
def addr(self: Tensor, vec1: Tensor, vec2: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor) -> Tensor:
    ...

@overload
def addr(beta: Number, self: Tensor, alpha: Number, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor) -> Tensor:
    ...

@overload
def addr(beta: Number, self: Tensor, vec1: Tensor, vec2: Tensor, *, out: Tensor) -> Tensor:
    ...

def affine_grid_generator(theta: Tensor, size: _size, align_corners: _bool) -> Tensor:
    ...

@overload
def all(self: Tensor, dim: _int, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def all(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def all(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def allclose(self: Tensor, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> _bool:
    ...

def alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def angle(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def any(self: Tensor, dim: _int, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def any(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def any(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def arange(start: Number, end: Number, step: Number, *, out: Optional[Tensor] = ..., dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def arange(start: Number, end: Number, *, out: Optional[Tensor] = ..., dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def arange(end: Number, *, out: Optional[Tensor] = ..., dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def argmax(self: Tensor, dim: Optional[_int] = ..., keepdim: _bool = ...) -> Tensor:
    ...

def argmin(self: Tensor, dim: Optional[_int] = ..., keepdim: _bool = ...) -> Tensor:
    ...

@overload
def argsort(self: Tensor, dim: _int = ..., descending: _bool = ...) -> Tensor:
    ...

@overload
def argsort(self: Tensor, dim: Union[str, ellipsis, None], descending: _bool = ...) -> Tensor:
    ...

def as_strided(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int] = ...) -> Tensor:
    ...

def as_strided_(self: Tensor, size: _size, stride: _size, storage_offset: Optional[_int] = ...) -> Tensor:
    ...

def as_tensor(data: Any, dtype: _dtype = ..., device: Optional[_device] = ...) -> Tensor:
    ...

def asin(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def asin_(self: Tensor) -> Tensor:
    ...

def asinh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def asinh_(self: Tensor) -> Tensor:
    ...

def atan(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def atan2(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def atan_(self: Tensor) -> Tensor:
    ...

def atanh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def atanh_(self: Tensor) -> Tensor:
    ...

def avg_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., ceil_mode: _bool = ..., count_include_pad: _bool = ...) -> Tensor:
    ...

@overload
def baddbmm(self: Tensor, batch1: Tensor, batch2: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor) -> Tensor:
    ...

@overload
def baddbmm(beta: Number, self: Tensor, alpha: Number, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor) -> Tensor:
    ...

@overload
def baddbmm(beta: Number, self: Tensor, batch1: Tensor, batch2: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def bartlett_window(window_length: _int, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def bartlett_window(window_length: _int, periodic: _bool, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor:
    ...

def batch_norm_backward_elemt(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], mean_dy: Tensor, mean_dy_xmu: Tensor) -> Tensor:
    ...

def batch_norm_backward_reduce(grad_out: Tensor, input: Tensor, mean: Tensor, invstd: Tensor, weight: Optional[Tensor], input_g: _bool, weight_g: _bool, bias_g: _bool) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    ...

def batch_norm_elemt(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, invstd: Tensor, eps: _float, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def batch_norm_gather_stats(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, count: _int) -> Tuple[Tensor, Tensor]:
    ...

def batch_norm_gather_stats_with_counts(input: Tensor, mean: Tensor, invstd: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float, eps: _float, counts: Tensor) -> Tuple[Tensor, Tensor]:
    ...

def batch_norm_stats(input: Tensor, eps: _float) -> Tuple[Tensor, Tensor]:
    ...

def batch_norm_update_stats(input: Tensor, running_mean: Optional[Tensor], running_var: Optional[Tensor], momentum: _float) -> Tuple[Tensor, Tensor]:
    ...

def bernoulli(self: Tensor, *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def bilinear(input1: Tensor, input2: Tensor, weight: Tensor, bias: Optional[Tensor]) -> Tensor:
    ...

def bincount(self: Tensor, weights: Optional[Tensor] = ..., minlength: _int = ...) -> Tensor:
    ...

def binomial(count: Tensor, prob: Tensor, generator: Optional[Generator] = ...) -> Tensor:
    ...

@overload
def bitwise_and(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bitwise_and(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def bitwise_not(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bitwise_or(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bitwise_or(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bitwise_xor(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bitwise_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def blackman_window(window_length: _int, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def blackman_window(window_length: _int, periodic: _bool, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def bmm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bucketize(self: Tensor, boundaries: Tensor, *, out_int32: _bool = ..., right: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def bucketize(self: Number, boundaries: Tensor, *, out_int32: _bool = ..., right: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def can_cast(from_: _dtype, to: _dtype) -> _bool:
    ...

@overload
def cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def cat(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: Union[str, ellipsis, None], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def ceil(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def ceil_(self: Tensor) -> Tensor:
    ...

def celu(self: Tensor, alpha: Number = ...) -> Tensor:
    ...

def celu_(self: Tensor, alpha: Number = ...) -> Tensor:
    ...

def channel_shuffle(self: Tensor, groups: _int) -> Tensor:
    ...

def cholesky(self: Tensor, upper: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def cholesky_inverse(self: Tensor, upper: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def cholesky_solve(self: Tensor, input2: Tensor, upper: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def chunk(self: Tensor, chunks: _int, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def clamp(self, min: _float = ..., max: _float = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def clamp_max(self: Tensor, max: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def clamp_max_(self: Tensor, max: Number) -> Tensor:
    ...

def clamp_min(self: Tensor, min: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def clamp_min_(self: Tensor, min: Number) -> Tensor:
    ...

def clone(self: Tensor, *, memory_format: Optional[memory_format] = ...) -> Tensor:
    ...

def combinations(self: Tensor, r: _int = ..., with_replacement: _bool = ...) -> Tensor:
    ...

def conj(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def constant_pad_nd(self: Tensor, pad: _size, value: Number = ...) -> Tensor:
    ...

def conv1d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., groups: _int = ...) -> Tensor:
    ...

def conv2d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., groups: _int = ...) -> Tensor:
    ...

def conv3d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., groups: _int = ...) -> Tensor:
    ...

def conv_tbc(self: Tensor, weight: Tensor, bias: Tensor, pad: _int = ...) -> Tensor:
    ...

def conv_transpose1d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., output_padding: Union[_int, _size] = ..., groups: _int = ..., dilation: Union[_int, _size] = ...) -> Tensor:
    ...

def conv_transpose2d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., output_padding: Union[_int, _size] = ..., groups: _int = ..., dilation: Union[_int, _size] = ...) -> Tensor:
    ...

def conv_transpose3d(input: Tensor, weight: Tensor, bias: Optional[Tensor] = ..., stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., output_padding: Union[_int, _size] = ..., groups: _int = ..., dilation: Union[_int, _size] = ...) -> Tensor:
    ...

def convolution(input: Tensor, weight: Tensor, bias: Optional[Tensor], stride: _size, padding: _size, dilation: _size, transposed: _bool, output_padding: _size, groups: _int) -> Tensor:
    ...

def cos(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def cos_(self: Tensor) -> Tensor:
    ...

def cosh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def cosh_(self: Tensor) -> Tensor:
    ...

def cosine_similarity(x1: Tensor, x2: Tensor, dim: _int = ..., eps: _float = ...) -> Tensor:
    ...

def cross(self: Tensor, other: Tensor, dim: Optional[_int] = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def cudnn_affine_grid_generator(theta: Tensor, N: _int, C: _int, H: _int, W: _int) -> Tensor:
    ...

def cudnn_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    ...

@overload
def cudnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

@overload
def cudnn_convolution(self: Tensor, weight: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

@overload
def cudnn_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

@overload
def cudnn_convolution_transpose(self: Tensor, weight: Tensor, padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

def cudnn_grid_sampler(self: Tensor, grid: Tensor) -> Tensor:
    ...

def cudnn_is_acceptable(self: Tensor) -> _bool:
    ...

@overload
def cummax(self: Tensor, dim: _int, *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def cummax(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def cummin(self: Tensor, dim: _int, *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def cummin(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def cumprod(self: Tensor, dim: _int, *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def cumprod(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def cumsum(self: Tensor, dim: _int, *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def cumsum(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def deg2rad(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def deg2rad_(self: Tensor) -> Tensor:
    ...

@overload
def dequantize(self: Tensor) -> Tensor:
    ...

@overload
def dequantize(tensors: Union[Tuple[Tensor, ...], List[Tensor]]) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def det(self: Tensor) -> Tensor:
    ...

def detach(self: Tensor) -> Tensor:
    ...

def detach_(self: Tensor) -> Tensor:
    ...

def diag(self: Tensor, diagonal: _int = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def diag_embed(self: Tensor, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor:
    ...

def diagflat(self: Tensor, offset: _int = ...) -> Tensor:
    ...

@overload
def diagonal(self: Tensor, offset: _int = ..., dim1: _int = ..., dim2: _int = ...) -> Tensor:
    ...

@overload
def diagonal(self: Tensor, *, outdim: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None], dim2: Union[str, ellipsis, None], offset: _int = ...) -> Tensor:
    ...

def digamma(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def dist(self: Tensor, other: Tensor, p: Number = ...) -> Tensor:
    ...

def div(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def dot(self: Tensor, tensor: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def dropout(input: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def dropout_(self: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def eig(self: Tensor, eigenvectors: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_eigenvalues_eigenvectors:
    ...

def embedding(weight: Tensor, indices: Tensor, padding_idx: _int = ..., scale_grad_by_freq: _bool = ..., sparse: _bool = ...) -> Tensor:
    ...

def embedding_bag(weight: Tensor, indices: Tensor, offsets: Tensor, scale_grad_by_freq: _bool = ..., mode: _int = ..., sparse: _bool = ..., per_sample_weights: Optional[Tensor] = ..., include_last_offset: _bool = ...) -> Tuple[Tensor, Tensor, Tensor, Tensor]:
    ...

def embedding_renorm_(self: Tensor, indices: Tensor, max_norm: _float, norm_type: _float) -> Tensor:
    ...

@overload
def empty(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], memory_format: Optional[memory_format] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def empty(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], memory_format: Optional[memory_format] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def empty(size: _size, *, memory_format: Optional[memory_format] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def empty(*size: _int, memory_format: Optional[memory_format] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def empty_like(self: Tensor, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def empty_meta(size: _size, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def empty_meta(*size: _int, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def empty_quantized(size: _size, qtensor: Tensor) -> Tensor:
    ...

def empty_strided(size: _size, stride: _size, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def eq(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def eq(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def equal(self: Tensor, other: Tensor) -> _bool:
    ...

def erf(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def erf_(self: Tensor) -> Tensor:
    ...

def erfc(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def erfc_(self: Tensor) -> Tensor:
    ...

def erfinv(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def exp(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def exp_(self: Tensor) -> Tensor:
    ...

def expm1(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def expm1_(self: Tensor) -> Tensor:
    ...

@overload
def eye(n: _int, *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def eye(n: _int, m: _int, *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def fake_quantize_per_channel_affine(self: Tensor, scale: Tensor, zero_point: Tensor, axis: _int, quant_min: _int, quant_max: _int) -> Tensor:
    ...

def fake_quantize_per_tensor_affine(self: Tensor, scale: _float, zero_point: _int, quant_min: _int, quant_max: _int) -> Tensor:
    ...

def fbgemm_linear_fp16_weight(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor:
    ...

def fbgemm_linear_fp16_weight_fp32_activation(input: Tensor, packed_weight: Tensor, bias: Tensor) -> Tensor:
    ...

def fbgemm_linear_int8_weight(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor:
    ...

def fbgemm_linear_int8_weight_fp32_activation(input: Tensor, weight: Tensor, packed: Tensor, col_offsets: Tensor, weight_scale: Number, weight_zero_point: Number, bias: Tensor) -> Tensor:
    ...

def fbgemm_linear_quantize_weight(input: Tensor) -> Tuple[Tensor, Tensor, _float, _int]:
    ...

def fbgemm_pack_gemm_matrix_fp16(input: Tensor) -> Tensor:
    ...

@overload
def fbgemm_pack_quantized_matrix(input: Tensor) -> Tensor:
    ...

@overload
def fbgemm_pack_quantized_matrix(input: Tensor, K: _int, N: _int) -> Tensor:
    ...

def feature_alpha_dropout(input: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def feature_alpha_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def feature_dropout(input: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def feature_dropout_(self: Tensor, p: _float, train: _bool) -> Tensor:
    ...

def fft(self: Tensor, signal_ndim: _int, normalized: _bool = ...) -> Tensor:
    ...

@overload
def fill_(self: Tensor, value: Number) -> Tensor:
    ...

@overload
def fill_(self: Tensor, value: Tensor) -> Tensor:
    ...

@overload
def flatten(self: Tensor, start_dim: _int = ..., end_dim: _int = ...) -> Tensor:
    ...

@overload
def flatten(self: Tensor, start_dim: _int, end_dim: _int, out_dim: Union[str, ellipsis, None]) -> Tensor:
    ...

@overload
def flatten(self: Tensor, start_dim: Union[str, ellipsis, None], end_dim: Union[str, ellipsis, None], out_dim: Union[str, ellipsis, None]) -> Tensor:
    ...

@overload
def flatten(self: Tensor, dims: Sequence[Union[str, ellipsis, None]], out_dim: Union[str, ellipsis, None]) -> Tensor:
    ...

def flip(self: Tensor, dims: _size) -> Tensor:
    ...

def fliplr(self: Tensor) -> Tensor:
    ...

def flipud(self: Tensor) -> Tensor:
    ...

def floor(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def floor_(self: Tensor) -> Tensor:
    ...

def floor_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def fmod(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def fmod(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def frac(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def frac_(self: Tensor) -> Tensor:
    ...

@overload
def frobenius_norm(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def frobenius_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def from_file(filename: str, shared: Optional[_bool] = ..., size: Optional[_int] = ..., *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def from_numpy(ndarray) -> Tensor:
    ...

def full(size: _size, fill_value: Number, *, out: Optional[Tensor] = ..., dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def full_like(self: Tensor, fill_value: Number, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def gather(self: Tensor, dim: _int, index: Tensor, *, sparse_grad: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def gather(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, *, sparse_grad: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def ge(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def ge(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def geqrf(self: Tensor, *, out: Optional[Tensor] = ...) -> namedtuple_a_tau:
    ...

def ger(self: Tensor, vec2: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def get_default_dtype() -> _dtype:
    ...

def get_num_interop_threads() -> _int:
    ...

def get_num_threads() -> _int:
    ...

def grid_sampler(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor:
    ...

def grid_sampler_2d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor:
    ...

def grid_sampler_3d(input: Tensor, grid: Tensor, interpolation_mode: _int, padding_mode: _int, align_corners: _bool) -> Tensor:
    ...

def group_norm(input: Tensor, num_groups: _int, weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: _float = ..., cudnn_enabled: _bool = ...) -> Tensor:
    ...

@overload
def gru(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]:
    ...

@overload
def gru(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]:
    ...

def gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor] = ..., b_hh: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def gt(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def gt(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def hamming_window(window_length: _int, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def hamming_window(window_length: _int, periodic: _bool, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def hamming_window(window_length: _int, periodic: _bool, alpha: _float, beta: _float, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def hann_window(window_length: _int, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def hann_window(window_length: _int, periodic: _bool, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def hardshrink(self: Tensor, lambd: Number = ...) -> Tensor:
    ...

def histc(self: Tensor, bins: _int = ..., min: Number = ..., max: Number = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def hspmm(mat1: Tensor, mat2: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def ifft(self: Tensor, signal_ndim: _int, normalized: _bool = ...) -> Tensor:
    ...

def imag(self: Tensor) -> Tensor:
    ...

@overload
def index_add(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor:
    ...

@overload
def index_add(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor:
    ...

@overload
def index_copy(self: Tensor, dim: _int, index: Tensor, source: Tensor) -> Tensor:
    ...

@overload
def index_copy(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, source: Tensor) -> Tensor:
    ...

@overload
def index_fill(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor:
    ...

@overload
def index_fill(self: Tensor, dim: _int, index: Tensor, value: Tensor) -> Tensor:
    ...

@overload
def index_fill(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor:
    ...

@overload
def index_fill(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Tensor) -> Tensor:
    ...

def index_put(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool = ...) -> Tensor:
    ...

def index_put_(self: Tensor, indices: Optional[Union[Tuple[Tensor, ...], List[Tensor]]], values: Tensor, accumulate: _bool = ...) -> Tensor:
    ...

@overload
def index_select(self: Tensor, dim: _int, index: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def index_select(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def init_num_threads() -> None:
    ...

def instance_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], use_input_stats: _bool, momentum: _float, eps: _float, cudnn_enabled: _bool) -> Tensor:
    ...

def int_repr(self: Tensor) -> Tensor:
    ...

def inverse(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def irfft(self: Tensor, signal_ndim: _int, normalized: _bool = ..., onesided: _bool = ..., signal_sizes: _size = ...) -> Tensor:
    ...

def is_complex(self: Tensor) -> _bool:
    ...

def is_distributed(self: Tensor) -> _bool:
    ...

def is_floating_point(self: Tensor) -> _bool:
    ...

def is_grad_enabled() -> _bool:
    ...

def is_nonzero(self: Tensor) -> _bool:
    ...

def is_same_size(self: Tensor, other: Tensor) -> _bool:
    ...

def is_signed(self: Tensor) -> _bool:
    ...

def is_vulkan_available() -> _bool:
    ...

def isclose(self: Tensor, other: Tensor, rtol: _float = ..., atol: _float = ..., equal_nan: _bool = ...) -> Tensor:
    ...

def isfinite(self: Tensor) -> Tensor:
    ...

def isinf(self: Tensor) -> Tensor:
    ...

def isnan(self: Tensor) -> Tensor:
    ...

@overload
def kthvalue(self: Tensor, k: _int, dim: _int = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def kthvalue(self: Tensor, k: _int, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

def layer_norm(input: Tensor, normalized_shape: _size, weight: Optional[Tensor] = ..., bias: Optional[Tensor] = ..., eps: _float = ..., cudnn_enable: _bool = ...) -> Tensor:
    ...

@overload
def le(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def le(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def lerp(self: Tensor, end: Tensor, weight: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def lerp(self: Tensor, end: Tensor, weight: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def lgamma(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def linspace(start: Number, end: Number, steps: _int = ..., *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def log(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def log10(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def log10_(self: Tensor) -> Tensor:
    ...

def log1p(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def log1p_(self: Tensor) -> Tensor:
    ...

def log2(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def log2_(self: Tensor) -> Tensor:
    ...

def log_(self: Tensor) -> Tensor:
    ...

@overload
def log_softmax(self: Tensor, dim: _int, dtype: Optional[_dtype] = ...) -> Tensor:
    ...

@overload
def log_softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
    ...

def logaddexp(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logaddexp2(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def logcumsumexp(self: Tensor, dim: _int, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def logcumsumexp(self: Tensor, dim: Union[str, ellipsis, None], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logdet(self: Tensor) -> Tensor:
    ...

def logical_and(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logical_not(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logical_or(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logical_xor(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def logspace(start: Number, end: Number, steps: _int = ..., base: _float = ..., *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def logsumexp(self: Tensor, dim: Union[_int, _size], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def logsumexp(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def lstm(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor, Tensor]:
    ...

@overload
def lstm(data: Tensor, batch_sizes: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor] = ..., b_hh: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor]:
    ...

def lstsq(self: Tensor, A: Tensor, *, out: Optional[Tensor] = ...) -> namedtuple_solution_QR:
    ...

@overload
def lt(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def lt(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def lu_solve(self: Tensor, LU_data: Tensor, LU_pivots: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def masked_fill(self: Tensor, mask: Tensor, value: Number) -> Tensor:
    ...

@overload
def masked_fill(self: Tensor, mask: Tensor, value: Tensor) -> Tensor:
    ...

def masked_scatter(self: Tensor, mask: Tensor, source: Tensor) -> Tensor:
    ...

def masked_select(self: Tensor, mask: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def matmul(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def matrix_power(self: Tensor, n: _int) -> Tensor:
    ...

@overload
def matrix_rank(self: Tensor, tol: _float, symmetric: _bool = ...) -> Tensor:
    ...

@overload
def matrix_rank(self: Tensor, symmetric: _bool = ...) -> Tensor:
    ...

@overload
def max(self: Tensor, dim: _int, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def max(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def max(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def max(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def max_pool1d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tensor:
    ...

def max_pool1d_with_indices(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tensor:
    ...

def max_pool3d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tensor:
    ...

def mean(self: Tensor, *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def mean(self: Tensor, dim: Union[_int, _size], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]] = ..., keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def median(self: Tensor, dim: _int, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def median(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def median(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def min(self: Tensor, dim: _int, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def min(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def min(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def min(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def miopen_batch_norm(input: Tensor, weight: Tensor, bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, exponential_average_factor: _float, epsilon: _float) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def miopen_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

def miopen_convolution_transpose(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, output_padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

def miopen_depthwise_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int, benchmark: _bool, deterministic: _bool) -> Tensor:
    ...

def miopen_rnn(input: Tensor, weight: Union[Tuple[Tensor, ...], List[Tensor]], weight_stride0: _int, hx: Tensor, cx: Optional[Tensor], mode: _int, hidden_size: _int, num_layers: _int, batch_first: _bool, dropout: _float, train: _bool, bidirectional: _bool, batch_sizes: _size, dropout_state: Optional[Tensor]) -> Tuple[Tensor, Tensor, Tensor, Tensor, Tensor]:
    ...

def mkldnn_adaptive_avg_pool2d(self: Tensor, output_size: Union[_int, _size]) -> Tensor:
    ...

def mkldnn_convolution(self: Tensor, weight: Tensor, bias: Optional[Tensor], padding: _size, stride: _size, dilation: _size, groups: _int) -> Tensor:
    ...

def mkldnn_convolution_backward_weights(weight_size: _size, grad_output: Tensor, self: Tensor, padding: _size, stride: _size, dilation: _size, groups: _int, bias_defined: _bool) -> Tuple[Tensor, Tensor]:
    ...

def mkldnn_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tensor:
    ...

def mm(self: Tensor, mat2: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def mode(self: Tensor, dim: _int = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def mode(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

def mul(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def multinomial(self: Tensor, num_samples: _int, replacement: _bool = ..., *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def mv(self: Tensor, vec: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def mvlgamma(self: Tensor, p: _int) -> Tensor:
    ...

@overload
def narrow(self: Tensor, dim: _int, start: _int, length: _int) -> Tensor:
    ...

@overload
def narrow(self: Tensor, dim: _int, start: Tensor, length: _int) -> Tensor:
    ...

def native_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], running_mean: Optional[Tensor], running_var: Optional[Tensor], training: _bool, momentum: _float, eps: _float, *, out: Optional[Tensor] = ...) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def native_group_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], N: _int, C: _int, HxW: _int, group: _int, eps: _float) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def native_layer_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], M: _int, N: _int, eps: _float) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def native_norm(self: Tensor, p: Number = ...) -> Tensor:
    ...

def ne(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def ne(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def neg(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def neg_(self: Tensor) -> Tensor:
    ...

@overload
def nonzero(input: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def nonzero(input: Tensor, *, as_tuple: bool = ...) -> Tensor:
    ...

def norm_except_dim(v: Tensor, pow: _int = ..., dim: _int = ...) -> Tensor:
    ...

@overload
def normal(mean: Tensor, std: _float = ..., *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def normal(mean: _float, std: Tensor, *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def normal(mean: Tensor, std: Tensor, *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def normal(mean: _float, std: _float, size: _size, *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def nuclear_norm(self: Tensor, keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def nuclear_norm(self: Tensor, dim: Union[_int, _size], keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def numel(self: Tensor) -> _int:
    ...

def ones(*size: _int, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def ones_like(self: Tensor, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def orgqr(self: Tensor, input2: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def ormqr(self: Tensor, input2: Tensor, input3: Tensor, left: _bool = ..., transpose: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def pairwise_distance(x1: Tensor, x2: Tensor, p: _float = ..., eps: _float = ..., keepdim: _bool = ...) -> Tensor:
    ...

def pdist(self: Tensor, p: _float = ...) -> Tensor:
    ...

def pinverse(self: Tensor, rcond: _float = ...) -> Tensor:
    ...

def pixel_shuffle(self: Tensor, upscale_factor: _int) -> Tensor:
    ...

def poisson(self: Tensor, generator: Optional[Generator] = ...) -> Tensor:
    ...

def poisson_nll_loss(input: Tensor, target: Tensor, log_input: _bool, full: _bool, eps: _float, reduction: _int) -> Tensor:
    ...

def polygamma(n: _int, self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def pow(self: Tensor, exponent: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def pow(self: Tensor, exponent: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def pow(self: Number, exponent: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def prelu(self: Tensor, weight: Tensor) -> Tensor:
    ...

@overload
def prod(self: Tensor, *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def prod(self: Tensor, dim: _int, keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def prod(self: Tensor, dim: Union[str, ellipsis, None], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def promote_types(type1: _dtype, type2: _dtype) -> _dtype:
    ...

def q_per_channel_axis(self: Tensor) -> _int:
    ...

def q_per_channel_scales(self: Tensor) -> Tensor:
    ...

def q_per_channel_zero_points(self: Tensor) -> Tensor:
    ...

def q_scale(self: Tensor) -> _float:
    ...

def q_zero_point(self: Tensor) -> _int:
    ...

def qr(self: Tensor, some: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_Q_R:
    ...

def quantize_per_channel(self: Tensor, scales: Tensor, zero_points: Tensor, axis: _int, dtype: _dtype) -> Tensor:
    ...

@overload
def quantize_per_tensor(self: Tensor, scale: _float, zero_point: _int, dtype: _dtype) -> Tensor:
    ...

@overload
def quantize_per_tensor(tensors: Union[Tuple[Tensor, ...], List[Tensor]], scales: Tensor, zero_points: Tensor, dtype: _dtype) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def quantized_batch_norm(input: Tensor, weight: Optional[Tensor], bias: Optional[Tensor], mean: Tensor, var: Tensor, eps: _float, output_scale: _float, output_zero_point: _int) -> Tensor:
    ...

def quantized_gru_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor:
    ...

def quantized_lstm_cell(input: Tensor, hx: Union[Tuple[Tensor, ...], List[Tensor]], w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tuple[Tensor, Tensor]:
    ...

def quantized_max_pool2d(self: Tensor, kernel_size: Union[_int, _size], stride: Union[_int, _size] = ..., padding: Union[_int, _size] = ..., dilation: Union[_int, _size] = ..., ceil_mode: _bool = ...) -> Tensor:
    ...

def quantized_rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor:
    ...

def quantized_rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Tensor, b_hh: Tensor, packed_ih: Tensor, packed_hh: Tensor, col_offsets_ih: Tensor, col_offsets_hh: Tensor, scale_ih: Number, scale_hh: Number, zero_point_ih: Number, zero_point_hh: Number) -> Tensor:
    ...

def rad2deg(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def rad2deg_(self: Tensor) -> Tensor:
    ...

@overload
def rand(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(size: _size, *, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(*size: _int, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(size: _size, *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(*size: _int, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(size: _size, *, generator: Optional[Generator], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def rand(*size: _int, generator: Optional[Generator], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def rand_like(self: Tensor, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def randint(low: _int, high: _int, size: _size, *, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def randint(high: _int, size: _size, *, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randint_like(self: Tensor, high: _int, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randint_like(self: Tensor, low: _int, high: _int, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(size: _size, *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(*size: _int, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(size: _size, *, generator: Optional[Generator], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(*size: _int, generator: Optional[Generator], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(size: _size, *, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def randn(*size: _int, generator: Optional[Generator], names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def randn_like(self: Tensor, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def randperm(n: _int, *, generator: Optional[Generator] = ..., out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def range(start: Number, end: Number, step: Number = ..., *, out: Optional[Tensor] = ..., dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def real(self: Tensor) -> Tensor:
    ...

def reciprocal(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def reciprocal_(self: Tensor) -> Tensor:
    ...

def relu(self: Tensor) -> Tensor:
    ...

def relu_(self: Tensor) -> Tensor:
    ...

@overload
def remainder(self: Tensor, other: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def remainder(self: Tensor, other: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def renorm(self: Tensor, p: Number, dim: _int, maxnorm: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def repeat_interleave(repeats: Tensor) -> Tensor:
    ...

@overload
def repeat_interleave(self: Tensor, repeats: Tensor, dim: Optional[_int] = ...) -> Tensor:
    ...

@overload
def repeat_interleave(self: Tensor, repeats: _int, dim: Optional[_int] = ...) -> Tensor:
    ...

def reshape(self: Tensor, shape: _size) -> Tensor:
    ...

def resize_as_(self: Tensor, the_template: Tensor, *, memory_format: Optional[memory_format] = ...) -> Tensor:
    ...

@overload
def result_type(tensor: Tensor, other: Tensor) -> _dtype:
    ...

@overload
def result_type(tensor: Tensor, other: Number) -> _dtype:
    ...

@overload
def result_type(scalar: Number, tensor: Tensor) -> _dtype:
    ...

@overload
def result_type(scalar1: Number, scalar2: Number) -> _dtype:
    ...

def rfft(self: Tensor, signal_ndim: _int, normalized: _bool = ..., onesided: _bool = ...) -> Tensor:
    ...

@overload
def rnn_relu(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]:
    ...

@overload
def rnn_relu(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]:
    ...

def rnn_relu_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor] = ..., b_hh: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def rnn_tanh(input: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool, batch_first: _bool) -> Tuple[Tensor, Tensor]:
    ...

@overload
def rnn_tanh(data: Tensor, batch_sizes: Tensor, hx: Tensor, params: Union[Tuple[Tensor, ...], List[Tensor]], has_biases: _bool, num_layers: _int, dropout: _float, train: _bool, bidirectional: _bool) -> Tuple[Tensor, Tensor]:
    ...

def rnn_tanh_cell(input: Tensor, hx: Tensor, w_ih: Tensor, w_hh: Tensor, b_ih: Optional[Tensor] = ..., b_hh: Optional[Tensor] = ...) -> Tensor:
    ...

def roll(self: Tensor, shifts: Union[_int, _size], dims: Union[_int, _size] = ...) -> Tensor:
    ...

def rot90(self: Tensor, k: _int = ..., dims: _size = ...) -> Tensor:
    ...

def round(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def round_(self: Tensor) -> Tensor:
    ...

def rrelu(self: Tensor, lower: Number = ..., upper: Number = ..., training: _bool = ..., generator: Optional[Generator] = ...) -> Tensor:
    ...

def rrelu_(self: Tensor, lower: Number = ..., upper: Number = ..., training: _bool = ..., generator: Optional[Generator] = ...) -> Tensor:
    ...

def rsqrt(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def rsqrt_(self: Tensor) -> Tensor:
    ...

@overload
def rsub(self: Tensor, other: Tensor, *, alpha: Number = ...) -> Tensor:
    ...

@overload
def rsub(self: Tensor, other: Number, alpha: Number = ...) -> Tensor:
    ...

def scalar_tensor(s: Number, *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def scatter(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor:
    ...

@overload
def scatter(self: Tensor, dim: _int, index: Tensor, value: Number) -> Tensor:
    ...

@overload
def scatter(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor:
    ...

@overload
def scatter(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, value: Number) -> Tensor:
    ...

@overload
def scatter_add(self: Tensor, dim: _int, index: Tensor, src: Tensor) -> Tensor:
    ...

@overload
def scatter_add(self: Tensor, dim: Union[str, ellipsis, None], index: Tensor, src: Tensor) -> Tensor:
    ...

@overload
def searchsorted(sorted_sequence: Tensor, self: Tensor, *, out_int32: _bool = ..., right: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def searchsorted(sorted_sequence: Tensor, self: Number, *, out_int32: _bool = ..., right: _bool = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def select(self: Tensor, dim: Union[str, ellipsis, None], index: _int) -> Tensor:
    ...

@overload
def select(self: Tensor, dim: _int, index: _int) -> Tensor:
    ...

def selu(self: Tensor) -> Tensor:
    ...

def selu_(self: Tensor) -> Tensor:
    ...

def set_flush_denormal(mode: _bool) -> _bool:
    ...

def set_num_interop_threads(num: _int) -> None:
    ...

def set_num_threads(num: _int) -> None:
    ...

def sigmoid(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def sigmoid_(self: Tensor) -> Tensor:
    ...

def sign(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def sin(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def sin_(self: Tensor) -> Tensor:
    ...

def sinh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def sinh_(self: Tensor) -> Tensor:
    ...

def slogdet(self: Tensor) -> namedtuple_sign_logabsdet:
    ...

def smm(self: Tensor, mat2: Tensor) -> Tensor:
    ...

@overload
def softmax(self: Tensor, dim: _int, dtype: Optional[_dtype] = ...) -> Tensor:
    ...

@overload
def softmax(self: Tensor, dim: Union[str, ellipsis, None], *, dtype: Optional[_dtype] = ...) -> Tensor:
    ...

def solve(self: Tensor, A: Tensor, *, out: Optional[Tensor] = ...) -> namedtuple_solution_LU:
    ...

@overload
def sort(self: Tensor, dim: _int = ..., descending: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

@overload
def sort(self: Tensor, dim: Union[str, ellipsis, None], descending: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

def sparse_coo_tensor(indices: Tensor, values: Union[Tensor, List], size: Optional[_size] = ..., *, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def split_with_sizes(self: Tensor, split_sizes: _size, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def sqrt(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def sqrt_(self: Tensor) -> Tensor:
    ...

def square(self: Tensor) -> Tensor:
    ...

def square_(self: Tensor) -> Tensor:
    ...

@overload
def squeeze(self: Tensor) -> Tensor:
    ...

@overload
def squeeze(self: Tensor, dim: _int) -> Tensor:
    ...

@overload
def squeeze(self: Tensor, dim: Union[str, ellipsis, None]) -> Tensor:
    ...

@overload
def sspaddmm(self: Tensor, mat1: Tensor, mat2: Tensor, *, beta: Number = ..., alpha: Number = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def sspaddmm(beta: Number, self: Tensor, alpha: Number, mat1: Tensor, mat2: Tensor) -> Tensor:
    ...

@overload
def sspaddmm(beta: Number, self: Tensor, mat1: Tensor, mat2: Tensor) -> Tensor:
    ...

def stack(tensors: Union[Tuple[Tensor, ...], List[Tensor]], dim: _int = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def std(self: Tensor, unbiased: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def std(self: Tensor, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def std(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def std_mean(self: Tensor, unbiased: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

@overload
def std_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

@overload
def std_mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

@overload
def sub(input: Union[Tensor, Number], other: Union[Tensor, Number], *, alpha: Optional[Number] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def sub(self: Tensor, alpha: Number, other: Tensor) -> Tensor:
    ...

@overload
def sub(self: Tensor, alpha: Number, other: Tensor, *, out: Tensor) -> Tensor:
    ...

@overload
def sum(self: Tensor, *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def sum(self: Tensor, dim: Union[_int, _size], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def sum(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], keepdim: _bool = ..., *, dtype: Optional[_dtype] = ..., out: Optional[Tensor] = ...) -> Tensor:
    ...

def svd(self: Tensor, some: _bool = ..., compute_uv: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_U_S_V:
    ...

def symeig(self: Tensor, eigenvectors: _bool = ..., upper: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_eigenvalues_eigenvectors:
    ...

def t(self: Tensor) -> Tensor:
    ...

def take(self: Tensor, index: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def tan(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def tan_(self: Tensor) -> Tensor:
    ...

def tanh(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def tanh_(self: Tensor) -> Tensor:
    ...

def tensor(data: Any, dtype: Optional[_dtype] = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def threshold(self: Tensor, threshold: Number, value: Number, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def threshold_(self: Tensor, threshold: Number, value: Number) -> Tensor:
    ...

def topk(self: Tensor, k: _int, dim: _int = ..., largest: _bool = ..., sorted: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_values_indices:
    ...

def trace(self: Tensor) -> Tensor:
    ...

@overload
def transpose(self: Tensor, dim0: _int, dim1: _int) -> Tensor:
    ...

@overload
def transpose(self: Tensor, dim0: Union[str, ellipsis, None], dim1: Union[str, ellipsis, None]) -> Tensor:
    ...

@overload
def trapz(y: Tensor, x: Tensor, *, dim: _int = ...) -> Tensor:
    ...

@overload
def trapz(y: Tensor, *, dx: _float = ..., dim: _int = ...) -> Tensor:
    ...

def triangular_solve(self: Tensor, A: Tensor, upper: _bool = ..., transpose: _bool = ..., unitriangular: _bool = ..., *, out: Optional[Tensor] = ...) -> namedtuple_solution_cloned_coefficient:
    ...

def tril(self: Tensor, diagonal: _int = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def tril_indices(row: _int, col: _int, offset: _int = ..., *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def triu(self: Tensor, diagonal: _int = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def triu_indices(row: _int, col: _int, offset: _int = ..., *, dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def true_divide(input: Union[Tensor, Number], other: Union[Tensor, Number], *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def trunc(self: Tensor, *, out: Optional[Tensor] = ...) -> Tensor:
    ...

def trunc_(self: Tensor) -> Tensor:
    ...

@overload
def unbind(self: Tensor, dim: _int = ...) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

@overload
def unbind(self: Tensor, dim: Union[str, ellipsis, None]) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def unique_dim(self: Tensor, dim: _int, sorted: _bool = ..., return_inverse: _bool = ..., return_counts: _bool = ...) -> Tuple[Tensor, Tensor, Tensor]:
    ...

def unsqueeze(self: Tensor, dim: _int) -> Tensor:
    ...

def vander(x: Tensor, N: Optional[_int] = ..., increasing: _bool = ...) -> Tensor:
    ...

@overload
def var(self: Tensor, unbiased: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def var(self: Tensor, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def var(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ..., *, out: Optional[Tensor] = ...) -> Tensor:
    ...

@overload
def var_mean(self: Tensor, unbiased: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

@overload
def var_mean(self: Tensor, dim: Union[_int, _size], unbiased: _bool = ..., keepdim: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

@overload
def var_mean(self: Tensor, dim: Sequence[Union[str, ellipsis, None]], unbiased: _bool = ..., keepdim: _bool = ...) -> Tuple[Tensor, Tensor]:
    ...

def view_as_complex(self: Tensor) -> Tensor:
    ...

def view_as_real(self: Tensor) -> Tensor:
    ...

@overload
def where(condition: Tensor, self: Tensor, other: Tensor) -> Tensor:
    ...

@overload
def where(condition: Tensor) -> Union[Tuple[Tensor, ...], List[Tensor]]:
    ...

def zero_(self: Tensor) -> Tensor:
    ...

@overload
def zeros(size: _size, *, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def zeros(*size: _int, names: Optional[Sequence[Union[str, ellipsis, None]]], out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def zeros(size: _size, *, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

@overload
def zeros(*size: _int, out: Optional[Tensor] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

def zeros_like(self: Tensor, *, memory_format: Optional[memory_format] = ..., dtype: _dtype = ..., layout: _layout = ..., device: Union[_device, str, None] = ..., requires_grad: _bool = ...) -> Tensor:
    ...

"""
This type stub file was generated by pyright.
"""

from enum import Enum
from torch.types import Tuple, _bool

is_cuda: _bool
def getRuntimeVersion() -> Tuple[int, int, int]:
    ...

def getCompileVersion() -> Tuple[int, int, int]:
    ...

def getVersionInt() -> int:
    ...

class RNNMode(int, Enum):
    value: int
    rnn_relu = ...
    rnn_tanh = ...
    lstm = ...
    gru = ...


"""
This type stub file was generated by pyright.
"""

import torch
import random
import math
from typing import List, Optional, Tuple, Union, cast

"""
The testing package contains testing-specific utilities.
"""
FileCheck = torch._C.FileCheck
rand_like = torch.rand_like
randn_like = torch.randn_like
def is_integral(dtype: torch.dtype) -> bool:
    ...

_compare_return_type = Tuple[bool, Optional[str]]
def assert_allclose(actual, expected, rtol=..., atol=..., equal_nan=..., msg=...) -> None:
    ...

def make_non_contiguous(tensor: torch.Tensor) -> torch.Tensor:
    ...

def get_all_dtypes(include_half=..., include_bfloat16=..., include_bool=..., include_complex=...) -> List[torch.dtype]:
    ...

def get_all_math_dtypes(device) -> List[torch.dtype]:
    ...

def get_all_complex_dtypes() -> List[torch.dtype]:
    ...

def get_all_int_dtypes() -> List[torch.dtype]:
    ...

def get_all_fp_dtypes(include_half=..., include_bfloat16=...) -> List[torch.dtype]:
    ...

def get_all_device_types() -> List[str]:
    ...

_default_tolerances = { 'float64': (0.00001, 1e-8),'float32': (0.0001, 0.00001),'float16': (0.001, 0.001) }
"""
This type stub file was generated by pyright.
"""

import contextlib
import sys
import types

_SET_GLOBAL_FLAGS = hasattr(sys, 'getdlopenflags') and hasattr(sys, 'setdlopenflags')
@contextlib.contextmanager
def dl_open_guard():
    """
    Context manager to set the RTLD_GLOBAL dynamic linker flag while we open a
    shared library to load custom operators.
    """
    ...

class _OpNamespace(types.ModuleType):
    """
    An op namespace to dynamically bind Operators into Python.

    Say a user has created a custom Operator called "my_namespace::my_op". To
    call this op, the user will write torch.ops.my_namespace.my_op(...).
    At startup, this operation will not yet be bound into Python. Instead, the
    following sequence of magic tricks will occur:
    1. `torch.ops.my_namespace` will invoke the `__getattr__` magic method
       on the `torch.ops` object, which will create a new `_OpNamespace`
       object called `my_namespace` and set it as an attribute on the `ops`
       object.
    2. `torch.ops.my_namespace.my_op` will then invoke `__getattr__` on
       the `my_namespace` object, which will retrieve the operation via
       `torch.get_operation`, a function bound from C++, and then in a similar
       fashion bind this new object onto the `my_namespace` object.
    3. `torch.ops.my_namespace.my_op(...)` then calls this new operation
        and subsequent accesses will incur no further lookup (the namespace and
        operation will already exist).
    """
    def __init__(self, name) -> None:
        ...
    
    def __getattr__(self, op_name):
        ...
    


class _Ops(types.ModuleType):
    __file__ = ...
    def __init__(self) -> None:
        ...
    
    def __getattr__(self, name):
        ...
    
    def load_library(self, path):
        """
        Loads a shared library from the given path into the current process.

        The library being loaded may run global initialization code to register
        custom operators with the PyTorch JIT runtime. This allows dynamically
        loading custom operators. For this, you should compile your operator
        and the static registration code into a shared library object, and then
        call ``torch.ops.load_library('path/to/libcustom.so')`` to load the
        shared object.

        After the library is loaded, it is added to the
        ``torch.ops.loaded_libraries`` attribute, a set that may be inspected
        for the paths of all libraries loaded using this function.

        Arguments:
            path (str): A path to a shared library to load.
        """
        ...
    


ops = _Ops()
"""
This type stub file was generated by pyright.
"""

class __PrinterOptions(object):
    precision = ...
    threshold = ...
    edgeitems = ...
    linewidth = ...
    sci_mode = ...


PRINT_OPTS = __PrinterOptions()
def set_printoptions(precision=..., threshold=..., edgeitems=..., linewidth=..., profile=..., sci_mode=...):
    r"""Set options for printing. Items shamelessly taken from NumPy

    Args:
        precision: Number of digits of precision for floating point output
            (default = 4).
        threshold: Total number of array elements which trigger summarization
            rather than full `repr` (default = 1000).
        edgeitems: Number of array items in summary at beginning and end of
            each dimension (default = 3).
        linewidth: The number of characters per line for the purpose of
            inserting line breaks (default = 80). Thresholded matrices will
            ignore this parameter.
        profile: Sane defaults for pretty printing. Can override with any of
            the above options. (any one of `default`, `short`, `full`)
        sci_mode: Enable (True) or disable (False) scientific notation. If
            None (default) is specified, the value is defined by
            `torch._tensor_str._Formatter`. This value is automatically chosen
            by the framework.
    """
    ...

class _Formatter(object):
    def __init__(self, tensor) -> None:
        ...
    
    def width(self):
        ...
    
    def format(self, value):
        ...
    


def get_summarized_data(self):
    ...

"""
This type stub file was generated by pyright.
"""

class SobolEngine(object):
    r"""
    The :class:`torch.quasirandom.SobolEngine` is an engine for generating
    (scrambled) Sobol sequences. Sobol sequences are an example of low
    discrepancy quasi-random sequences.

    This implementation of an engine for Sobol sequences is capable of
    sampling sequences up to a maximum dimension of 1111. It uses direction
    numbers to generate these sequences, and these numbers have been adapted
    from `here <http://web.maths.unsw.edu.au/~fkuo/sobol/joe-kuo-old.1111>`_.

    References:
      - Art B. Owen. Scrambling Sobol and Niederreiter-Xing points.
        Journal of Complexity, 14(4):466-489, December 1998.

      - I. M. Sobol. The distribution of points in a cube and the accurate
        evaluation of integrals.
        Zh. Vychisl. Mat. i Mat. Phys., 7:784-802, 1967.

    Args:
        dimension (Int): The dimensionality of the sequence to be drawn
        scramble (bool, optional): Setting this to ``True`` will produce
                                   scrambled Sobol sequences. Scrambling is
                                   capable of producing better Sobol
                                   sequences. Default: ``False``.
        seed (Int, optional): This is the seed for the scrambling. The seed
                              of the random number generator is set to this,
                              if specified. Otherwise, it uses a random seed.
                              Default: ``None``

    Examples::

        >>> soboleng = torch.quasirandom.SobolEngine(dimension=5)
        >>> soboleng.draw(3)
        tensor([[0.5000, 0.5000, 0.5000, 0.5000, 0.5000],
                [0.7500, 0.2500, 0.7500, 0.2500, 0.7500],
                [0.2500, 0.7500, 0.2500, 0.7500, 0.2500]])
    """
    MAXBIT = ...
    MAXDIM = ...
    def __init__(self, dimension, scramble=..., seed=...) -> None:
        ...
    
    def draw(self, n=..., out=..., dtype=...):
        r"""
        Function to draw a sequence of :attr:`n` points from a Sobol sequence.
        Note that the samples are dependent on the previous samples. The size
        of the result is :math:`(n, dimension)`.

        Args:
            n (Int, optional): The length of sequence of points to draw.
                               Default: 1
            out (Tensor, optional): The output tensor
            dtype (:class:`torch.dtype`, optional): the desired data type of the
                                                    returned tensor.
                                                    Default: ``torch.float32``
        """
        ...
    
    def reset(self):
        r"""
        Function to reset the ``SobolEngine`` to base state.
        """
        ...
    
    def fast_forward(self, n):
        r"""
        Function to fast-forward the state of the ``SobolEngine`` by
        :attr:`n` steps. This is equivalent to drawing :attr:`n` samples
        without using the samples.

        Args:
            n (Int): The number of steps to fast-forward by.
        """
        ...
    
    def __repr__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

import re

HASH_REGEX = re.compile(r'-([a-f0-9]*)\.')
MASTER_BRANCH = 'master'
ENV_TORCH_HOME = 'TORCH_HOME'
ENV_XDG_CACHE_HOME = 'XDG_CACHE_HOME'
DEFAULT_CACHE_DIR = '~/.cache'
VAR_DEPENDENCY = 'dependencies'
MODULE_HUBCONF = 'hubconf.py'
READ_DATA_CHUNK = 8192
_hub_dir = None
def import_module(name, path):
    ...

def get_dir():
    r"""
    Get the Torch Hub cache directory used for storing downloaded models & weights.

    If :func:`~torch.hub.set_dir` is not called, default path is ``$TORCH_HOME/hub`` where
    environment variable ``$TORCH_HOME`` defaults to ``$XDG_CACHE_HOME/torch``.
    ``$XDG_CACHE_HOME`` follows the X Design Group specification of the Linux
    filesytem layout, with a default value ``~/.cache`` if the environment
    variable is not set.
    """
    ...

def set_dir(d):
    r"""
    Optionally set the Torch Hub directory used to save downloaded models & weights.

    Args:
        d (string): path to a local folder to save downloaded models & weights.
    """
    ...

def list(github, force_reload=...):
    r"""
    List all entrypoints available in `github` hubconf.

    Args:
        github (string): a string with format "repo_owner/repo_name[:tag_name]" with an optional
            tag/branch. The default branch is `master` if not specified.
            Example: 'pytorch/vision[:hub]'
        force_reload (bool, optional): whether to discard the existing cache and force a fresh download.
            Default is `False`.
    Returns:
        entrypoints: a list of available entrypoint names

    Example:
        >>> entrypoints = torch.hub.list('pytorch/vision', force_reload=True)
    """
    ...

def help(github, model, force_reload=...):
    r"""
    Show the docstring of entrypoint `model`.

    Args:
        github (string): a string with format <repo_owner/repo_name[:tag_name]> with an optional
            tag/branch. The default branch is `master` if not specified.
            Example: 'pytorch/vision[:hub]'
        model (string): a string of entrypoint name defined in repo's hubconf.py
        force_reload (bool, optional): whether to discard the existing cache and force a fresh download.
            Default is `False`.
    Example:
        >>> print(torch.hub.help('pytorch/vision', 'resnet18', force_reload=True))
    """
    ...

def load(github, model, *args, **kwargs):
    r"""
    Load a model from a github repo, with pretrained weights.

    Args:
        github (string): a string with format "repo_owner/repo_name[:tag_name]" with an optional
            tag/branch. The default branch is `master` if not specified.
            Example: 'pytorch/vision[:hub]'
        model (string): a string of entrypoint name defined in repo's hubconf.py
        *args (optional): the corresponding args for callable `model`.
        force_reload (bool, optional): whether to force a fresh download of github repo unconditionally.
            Default is `False`.
        verbose (bool, optional): If False, mute messages about hitting local caches. Note that the message
            about first download is cannot be muted.
            Default is `True`.
        **kwargs (optional): the corresponding kwargs for callable `model`.

    Returns:
        a single model with corresponding pretrained weights.

    Example:
        >>> model = torch.hub.load('pytorch/vision', 'resnet50', pretrained=True)
    """
    ...

def download_url_to_file(url, dst, hash_prefix=..., progress=...):
    r"""Download object at the given URL to a local path.

    Args:
        url (string): URL of the object to download
        dst (string): Full path where object will be saved, e.g. `/tmp/temporary_file`
        hash_prefix (string, optional): If not None, the SHA256 downloaded file should start with `hash_prefix`.
            Default: None
        progress (bool, optional): whether or not to display a progress bar to stderr
            Default: True

    Example:
        >>> torch.hub.download_url_to_file('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth', '/tmp/temporary_file')

    """
    ...

def load_state_dict_from_url(url, model_dir=..., map_location=..., progress=..., check_hash=..., file_name=...):
    r"""Loads the Torch serialized object at the given URL.

    If downloaded file is a zip file, it will be automatically
    decompressed.

    If the object is already present in `model_dir`, it's deserialized and
    returned.
    The default value of `model_dir` is ``<hub_dir>/checkpoints`` where
    `hub_dir` is the directory returned by :func:`~torch.hub.get_dir`.

    Args:
        url (string): URL of the object to download
        model_dir (string, optional): directory in which to save the object
        map_location (optional): a function or a dict specifying how to remap storage locations (see torch.load)
        progress (bool, optional): whether or not to display a progress bar to stderr.
            Default: True
        check_hash(bool, optional): If True, the filename part of the URL should follow the naming convention
            ``filename-<sha256>.ext`` where ``<sha256>`` is the first eight or more
            digits of the SHA256 hash of the contents of the file. The hash is used to
            ensure unique names and to verify the contents of the file.
            Default: False
        file_name (string, optional): name for the downloaded file. Filename from `url` will be used if not set.

    Example:
        >>> state_dict = torch.hub.load_state_dict_from_url('https://s3.amazonaws.com/pytorch/models/resnet18-5c106cde.pth')

    """
    ...

"""
This type stub file was generated by pyright.
"""

import math
import warnings
import torch
import torch.backends.cudnn as cudnn
from ..nn.modules.utils import _list_with_default, _pair, _quadruple, _single, _triple
from collections import OrderedDict

_builtin_table = None
_modules_containing_builtins = (torch, torch._C._nn)
_builtin_ops = [(_pair, "aten::_pair"), (_quadruple, "aten::_quadruple"), (_single, "aten::_single"), (_triple, "aten::_triple"), (_list_with_default, "aten::list_with_default"), (OrderedDict, "aten::dict"), (dict, "aten::dict"), (cudnn.is_acceptable, "aten::cudnn_is_acceptable"), (math.ceil, "aten::ceil"), (math.copysign, "aten::copysign"), (math.erf, "aten::erf"), (math.erfc, "aten::erfc"), (math.exp, "aten::exp"), (math.expm1, "aten::expm1"), (math.fabs, "aten::fabs"), (math.floor, "aten::floor"), (math.gamma, "aten::gamma"), (math.lgamma, "aten::lgamma"), (math.log, "aten::log"), (math.log10, "aten::log10"), (math.log1p, "aten::log1p"), (math.pow, "aten::pow"), (math.sqrt, "aten::sqrt"), (math.isnan, "aten::isnan"), (math.asinh, "aten::asinh"), (math.atanh, "aten::atanh"), (math.cosh, "aten::cosh"), (math.sinh, "aten::sinh"), (math.tanh, "aten::tanh"), (math.acos, "aten::acos"), (math.asin, "aten::asin"), (math.atan, "aten::atan"), (math.atan2, "aten::atan2"), (math.cos, "aten::cos"), (math.sin, "aten::sin"), (math.tan, "aten::tan"), (math.asinh, "aten::asinh"), (math.atanh, "aten::atanh"), (math.acosh, "aten::acosh"), (math.sinh, "aten::sinh"), (math.cosh, "aten::cosh"), (math.tanh, "aten::tanh"), (math.fmod, "aten::fmod"), (math.modf, "aten::modf"), (math.factorial, "aten::factorial"), (math.frexp, "aten::frexp"), (math.isnan, "aten::isnan"), (math.isinf, "aten::isinf"), (math.degrees, "aten::degrees"), (math.radians, "aten::radians"), (math.ldexp, "aten::ldexp"), (torch.autograd.grad, "aten::grad"), (torch.autograd.backward, "aten::backward"), (torch._C._infer_size, "aten::_infer_size"), (torch.nn.functional._no_grad_embedding_renorm_, "aten::_no_grad_embedding_renorm_"), (torch.nn.functional.assert_int_or_pair, "aten::_assert_int_or_pair"), (torch.nn.init._no_grad_fill_, "aten::_no_grad_fill_"), (torch.nn.init._no_grad_normal_, "aten::_no_grad_normal_"), (torch.nn.init._no_grad_uniform_, "aten::_no_grad_uniform_"), (torch.nn.init._no_grad_zero_, "aten::_no_grad_zero_"), (torch._C._get_tracing_state, "aten::_get_tracing_state"), (warnings.warn, "aten::warn"), (torch._VF.stft, "aten::stft"), (torch._VF.istft, "aten::istft"), (torch._VF.cdist, "aten::cdist"), (torch._VF.norm, "aten::norm"), (torch._VF.unique_dim, "aten::unique_dim"), (torch._VF.unique_consecutive, "aten::unique_consecutive"), (torch._VF.nuclear_norm, "aten::nuclear_norm"), (torch._VF.frobenius_norm, "aten::frobenius_norm")]
_functional_registered_ops = _gen_torch_functional_registered_ops()
"""
This type stub file was generated by pyright.
"""

import torch._C
import torch._jit_internal as _jit_internal
import torch.jit.annotations
import torch.testing
import torch.jit._recursive
import collections
import contextlib
import copy
import functools
import inspect
import os
import pathlib
import pickle
import re
import sys
import textwrap
import warnings
import weakref
from torch.jit._recursive import ScriptMethodStub, wrap_cpp_module
from torch.jit._builtins import _find_builtin, _get_builtin_table, _register_builtin
from torch._jit_internal import Final, Future, _overload, _overload_method, _qualified_name, export, ignore, unused
from torch.autograd import Variable, function
from torch.jit.frontend import get_default_args, get_jit_class_def, get_jit_def
from torch.nn import Module
from torch.serialization import validate_cuda_device
from torch._six import PY37, get_function_from_type, string_classes, with_metaclass
from torch.utils import set_module
from torch.autograd.grad_mode import _DecoratorContextManager
from typing import List, Optional

_enabled = _parse_env('PYTORCH_JIT', True, "> Using PyTorch JIT", "> PyTorch JIT DISABLED")
_flatten = torch._C._jit_flatten
_unflatten = torch._C._jit_unflatten
_jit_script_class_compile = torch._C._jit_script_class_compile
_python_cu = torch._C.CompilationUnit()
_fork = torch._C.fork
_wait = torch._C.wait
if _enabled:
    Attribute = collections.namedtuple('Attribute', ['value', 'type'])
else:
    def Attribute(value, type):
        ...
    
@contextlib.contextmanager
def optimized_execution(should_optimize):
    """
    A context manager that controls whether the JIT's executor will run
    optimizations before executing a function.
    """
    ...

@contextlib.contextmanager
def fuser(name):
    """
    A context manager that facilitates switching between
    backend fusers.

    Valid names:
    * ``fuser0`` - enables only legacy fuser
    * ``fuser1`` - enables only NNC
    * ``fuser2`` - enables only nvFuser
    """
    ...

DEFAULT_EXTRA_FILES_MAP = torch._C.ExtraFilesMap()
def save(m, f, _extra_files=...):
    r"""
    Save an offline version of this module for use in a separate process. The
    saved module serializes all of the methods, submodules, parameters, and
    attributes of this module. It can be loaded into the C++ API using
    ``torch::jit::load(filename)`` or into the Python API with
    :func:`torch.jit.load <torch.jit.load>`.

    To be able to save a module, it must not make any calls to native Python
    functions.  This means that all submodules must be subclasses of
    :class:`ScriptModule` as well.

    .. DANGER::
        All modules, no matter their device, are always loaded onto the CPU
        during loading.  This is different from :func:`torch.load`'s semantics
        and may change in the future.

    Arguments:
        m: A :class:`ScriptModule` to save.
        f: A file-like object (has to implement write and flush) or a string
           containing a file name.
        _extra_files: Map from filename to contents which will be stored as part of `f`.

    .. note::
        torch.jit.save attempts to preserve the behavior of some operators
        across versions. For example, dividing two integer tensors in
        PyTorch 1.5 performed floor division, and if the module
        containing that code is saved in PyTorch 1.5 and loaded in PyTorch 1.6
        its division behavior will be preserved. The same module saved in
        PyTorch 1.6 will fail to load in PyTorch 1.5, however, since the
        behavior of division changed in 1.6, and 1.5 does not know how to
        replicate the 1.6 behavior.

    Example:

    .. testcode::

        import torch
        import io

        class MyModule(torch.nn.Module):
            def forward(self, x):
                return x + 10

        m = torch.jit.script(MyModule())

        # Save to file
        torch.jit.save(m, 'scriptmodule.pt')
        # This line is equivalent to the previous
        m.save("scriptmodule.pt")

        # Save to io.BytesIO buffer
        buffer = io.BytesIO()
        torch.jit.save(m, buffer)

        # Save with extra files
        extra_files = torch._C.ExtraFilesMap()
        extra_files['foo.txt'] = 'bar'
        torch.jit.save(m, 'scriptmodule.pt', _extra_files=extra_files)
    """
    ...

def load(f, map_location=..., _extra_files=...):
    r"""
    Load a :class:`ScriptModule` or :class:`ScriptFunction` previously
    saved with :func:`torch.jit.save <torch.jit.save>`

    All previously saved modules, no matter their device, are first loaded onto CPU,
    and then are moved to the devices they were saved from. If this fails (e.g.
    because the run time system doesn't have certain devices), an exception is
    raised.

    Arguments:
        f: a file-like object (has to implement read, readline, tell, and seek),
            or a string containing a file name
        map_location (string or torch.device): A simplified version of
            ``map_location`` in `torch.jit.save` used to dynamically remap
            storages to an alternative set of devices.
        _extra_files (dictionary of filename to content): The extra
            filenames given in the map would be loaded and their content
            would be stored in the provided map.

    Returns:
        A :class:`ScriptModule` object.

    Example:

    .. testcode::

        import torch
        import io

        torch.jit.load('scriptmodule.pt')

        # Load ScriptModule from io.BytesIO object
        with open('scriptmodule.pt', 'rb') as f:
            buffer = io.BytesIO(f.read())

        # Load all tensors to the original device
        torch.jit.load(buffer)

        # Load all tensors onto CPU, using a device
        buffer.seek(0)
        torch.jit.load(buffer, map_location=torch.device('cpu'))

        # Load all tensors onto CPU, using a string
        buffer.seek(0)
        torch.jit.load(buffer, map_location='cpu')

        # Load with extra files.
        extra_files = torch._C.ExtraFilesMap()
        extra_files['foo.txt'] = 'bar'
        torch.jit.load('scriptmodule.pt', _extra_files=extra_files)
        print(extra_files['foo.txt'])

    .. testoutput::
        :hide:

        ...

    .. testcleanup::

        import os
        os.remove("scriptmodule.pt")
    """
    ...

def validate_map_location(map_location=...):
    ...

def export_opnames(m):
    r"""
        Returns a list of operator names of a script module and its submodules
    """
    ...

class ConstMap:
    def __init__(self, const_mapping) -> None:
        ...
    
    def __getattr__(self, attr):
        ...
    


class ONNXTracedModule(Module):
    def __init__(self, inner, strict=..., force_outplace=..., return_inputs=..., return_inputs_states=...) -> None:
        ...
    
    def forward(self, *args):
        ...
    


_JIT_TIME = os.environ.get('PYTORCH_JIT_TIME', False)
_JIT_DISABLE = os.environ.get('PYTORCH_JIT_DISABLE', False)
_JIT_STATS = os.environ.get('PYTORCH_JIT_STATS', False)
def verify(model, args, loss_fn=..., devices=...):
    """
    Verify that a JIT compiled model has the same behavior as its uncompiled
    version along with its backwards pass.  If your model returns multiple
    outputs, you must also specify a `loss_fn` to produce a loss for which
    the backwards will be computed.

    This function has side-effects (e.g., it executes your model / saves and loads
    parameters), so don't expect the model to come out exactly the same as what
    you passed in.

    Arguments:
        model (compiled torch.nn.Module or function): the module/function to be
            verified.  The module/function definition MUST have been decorated with
            `@torch.jit.compile`.
        args (tuple or Tensor): the positional arguments to pass to the
            compiled function/module to be verified.  A non-tuple is assumed to
            be a single positional argument to be passed to the model.
        loss_fn (function, optional): the loss function to be applied to
            the output of the model, before backwards is invoked.  By default,
            we assume that a model returns a single result, and we :func:`torch.sum`
            before calling backwards; if this is inappropriate, you can pass your
            own loss function.  Note that if a model returns a tuple of results,
            these are passed as separate positional arguments to `loss_fn`.
        devices (iterable of device IDs, optional): the GPU devices which the
            compiled module will be run on.  This determines the RNG state we
            must save when running both compiled and uncompiled versions of the model.
    """
    ...

def indent(s):
    ...

class TracingCheckError(Exception):
    def __init__(self, graph_diff_error, tensor_compare_error, extra_msg=...) -> None:
        ...
    


class TracerWarning(Warning):
    @staticmethod
    def ignore_lib_warnings():
        ...
    


def make_tuple(example_inputs):
    ...

def make_module(mod, _module_class, _compilation_unit):
    ...

def wrap_check_inputs(check_inputs):
    ...

def trace(func, example_inputs, optimize=..., check_trace=..., check_inputs=..., check_tolerance=..., strict=..., _force_outplace=..., _module_class=..., _compilation_unit=...):
    """
    Trace a function and return an executable  or :class:`ScriptFunction`
    that will be optimized using just-in-time compilation. Tracing is ideal for
    code that operates only on ``Tensor``\\s and lists, dictionaries, and
    tuples of ``Tensor``\\s.

    Using `torch.jit.trace` and `torch.jit.trace_module`, you can turn an
    existing module or Python function into a TorchScript
    :class:`ScriptFunction` or :class:`ScriptModule`. You must provide example
    inputs, and we run the function, recording the operations performed on all
    the tensors.

    * The resulting recording of a standalone function produces `ScriptFunction`.
    * The resulting recording of `nn.Module.forward` or `nn.Module` produces
      `ScriptModule`.

    This module also contains any parameters that the original
    module had as well.

    Warning:
        Tracing only correctly records functions and modules which are not data
        dependent (e.g., do not have conditionals on data in tensors) and do not have
        any untracked external dependencies (e.g., perform input/output or
        access global variables). Tracing only records operations done when the given
        function is run on the given tensors. Therefore, the returned
        `ScriptModule` will always run the same traced graph on any input. This
        has some important implications when your module is expected to run
        different sets of operations, depending on the input and/or the module
        state. For example,

        * Tracing will not record any control-flow like if-statements or loops.
          When this control-flow is constant across your module, this is fine
          and it often inlines the control-flow decisions. But sometimes the
          control-flow is actually part of the model itself. For instance, a
          recurrent network is a loop over the (possibly dynamic) length of an
          input sequence.
        * In the returned :class:`ScriptModule`, operations that have different
          behaviors in ``training`` and ``eval`` modes will always behave as if
          it is in the mode it was in during tracing, no matter which mode the
          `ScriptModule` is in.

        In cases like these, tracing would not be appropriate and
        :func:`scripting <torch.jit.script>` is a better choice. If you trace
        such models, you may silently get incorrect results on subsequent
        invocations of the model. The tracer will try to emit warnings when
        doing something that may cause an incorrect trace to be produced.

    Arguments:
        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`
            that will be run with `example_inputs`. `func` arguments and return
            values  must be tensors or (possibly nested) tuples that contain
            tensors. When a module is passed `torch.jit.trace`, only the
            ``forward`` method is run and traced (see :func:`torch.jit.trace
            <torch.jit.trace_module>` for details).
        example_inputs (tuple):  A tuple of example inputs that will be passed
            to the function while tracing. The resulting trace can be run with
            inputs of different types and shapes assuming the traced operations
            support those types and shapes. `example_inputs` may also be a
            single Tensor in which case it is automatically wrapped in a tuple.

    Keyword arguments:
        check_trace (bool, optional): Check if the same inputs run through
            traced code produce the same outputs. Default: ``True``. You might want
            to disable this if, for example, your network contains non-
            deterministic ops or if you are sure that the network is correct despite
            a checker failure.

        check_inputs (list of tuples, optional): A list of tuples of input
            arguments that should be used to check the trace against what is
            expected. Each tuple is equivalent to a set of input arguments that
            would be specified in ``example_inputs``. For best results, pass in
            a set of checking inputs representative of the space of shapes and
            types of inputs you expect the network to see.  If not specified,
            the original ``example_inputs`` are used for checking
        check_tolerance (float, optional): Floating-point comparison tolerance
            to use in the checker procedure.  This can be used to relax the
            checker strictness in the event that results diverge numerically
            for a known reason, such as operator fusion.
        strict (bool, optional): run the tracer in a strict mode or not
            (default: ``True``). Only turn this off when you want the tracer to
            record your mutable container types (currently ``list``/``dict``)
            and you are sure that the containuer you are using in your
            problem is a ``constant`` structure and does not get used as
            control flow (if, for) conditions.

    Returns:
        If `func` is `nn.Module` or ``forward`` of `nn.Module`, `trace` returns
        a :class:`ScriptModule` object with a single ``forward`` method
        containing the traced code.  The returned `ScriptModule` will
        have the same set of sub-modules and parameters as the original
        ``nn.Module``.  If ``func`` is a standalone function, ``trace``
        returns `ScriptFunction`.

    Example (tracing a function):

    .. testcode::

        import torch

        def foo(x, y):
            return 2 * x + y

        # Run `foo` with the provided inputs and record the tensor operations
        traced_foo = torch.jit.trace(foo, (torch.rand(3), torch.rand(3)))

        # `traced_foo` can now be run with the TorchScript interpreter or saved
        # and loaded in a Python-free environment

    Example (tracing an existing module)::

        import torch
        import torch.nn as nn

        class Net(nn.Module):
            def __init__(self):
                super(Net, self).__init__()
                self.conv = nn.Conv2d(1, 1, 3)

            def forward(self, x):
                return self.conv(x)

        n = Net()
        example_weight = torch.rand(1, 1, 3, 3)
        example_forward_input = torch.rand(1, 1, 3, 3)

        # Trace a specific method and construct `ScriptModule` with
        # a single `forward` method
        module = torch.jit.trace(n.forward, example_forward_input)

        # Trace a module (implicitly traces `forward`) and construct a
        # `ScriptModule` with a single `forward` method
        module = torch.jit.trace(n, example_forward_input)

    """
    ...

_trace_module_map = None
def trace_module(mod, inputs, optimize=..., check_trace=..., check_inputs=..., check_tolerance=..., strict=..., _force_outplace=..., _module_class=..., _compilation_unit=...):
    """
    Trace a module and return an executable :class:`ScriptModule` that will be optimized
    using just-in-time compilation. When a module is passed to :func:`torch.jit.trace <torch.jit.trace>`, only
    the ``forward`` method is run and traced. With ``trace_module``, you can specify a dictionary of
    method names to example inputs to trace (see the ``example_inputs``) argument below.

    See :func:`torch.jit.trace <torch.jit.trace>` for more information on tracing.

    Arguments:
        mod (torch.nn.Module):  A ``torch.nn.Module`` containing methods whose names are
                                specified in ``example_inputs``. The given methods will be compiled
                                as a part of a single `ScriptModule`.
        example_inputs (dict):  A dict containing sample inputs indexed by method names in ``mod``.
                                The inputs will be passed to methods whose names correspond to inputs'
                                keys while tracing.
                                ``{ 'forward' : example_forward_input, 'method2': example_method2_input}``
    Keyword arguments:
        check_trace (``bool``, optional): Check if the same inputs run through
                                      traced code produce the same outputs. Default: ``True``. You might want
                                      to disable this if, for example, your network contains non-
                                      deterministic ops or if you are sure that the network is correct despite
                                      a checker failure.

        check_inputs (list of dicts, optional): A list of dicts of input arguments that should be used
                                                 to check the trace against what is expected. Each tuple
                                                 is equivalent to a set of input arguments that would
                                                 be specified in ``example_inputs``. For best results, pass in a
                                                 set of checking inputs representative of the space of
                                                 shapes and types of inputs you expect the network to see.
                                                 If not specified, the original ``example_inputs`` are used for checking
        check_tolerance (float, optional): Floating-point comparison tolerance to use in the checker procedure.
                                           This can be used to relax the checker strictness in the event that
                                           results diverge numerically for a known reason, such as operator fusion.

    Returns:
        A :class:`ScriptModule` object with a single ``forward`` method containing the traced code.
        When ``func`` is a ``torch.nn.Module``, the returned :class:`ScriptModule` will have the same set of
        sub-modules and parameters as ``func``.

    Example (tracing a module with multiple methods)::

        import torch
        import torch.nn as nn

        class Net(nn.Module):
            def __init__(self):
                super(Net, self).__init__()
                self.conv = nn.Conv2d(1, 1, 3)

            def forward(self, x):
                return self.conv(x)

            def weighted_kernel_sum(self, weight):
                return weight * self.conv.weight


        n = Net()
        example_weight = torch.rand(1, 1, 3, 3)
        example_forward_input = torch.rand(1, 1, 3, 3)

        # Trace a specific method and construct `ScriptModule` with
        # a single `forward` method
        module = torch.jit.trace(n.forward, example_forward_input)

        # Trace a module (implicitly traces `forward`) and construct a
        # `ScriptModule` with a single `forward` method
        module = torch.jit.trace(n, example_forward_input)

        # Trace specific methods on a module (specified in `inputs`), constructs
        # a `ScriptModule` with `forward` and `weighted_kernel_sum` methods
        inputs = {'forward' : example_forward_input, 'weighted_kernel_sum' : example_weight}
        module = torch.jit.trace_module(n, inputs)

    """
    ...

def fork(func, *args, **kwargs):
    """
    Creates an asynchronous task executing `func` and a reference to the value
    of the result of this execution. `fork` will return immediately,
    so the return value of `func` may not have been computed yet. To force completion
    of the task and access the return value invoke `torch.jit.wait` on the Future. `fork` invoked
    with a `func` which returns `T` is typed as `torch.jit.Future[T]`. `fork` calls can be arbitrarily
    nested, and may be invoked with positional and keyword arguments.
    Asynchronous execution will only occur when run in TorchScript. If run in pure python,
    `fork` will not execute in parallel. `fork` will also not execute in parallel when invoked
    while tracing, however the `fork` and `wait` calls will be captured in the exported IR Graph.
    Warning:
        `fork` tasks will execute non-deterministicly. We recommend only spawning
        parallel fork tasks for pure functions that do not modify their inputs,
        module attributes, or global state.
    Arguments:
        func (callable or torch.nn.Module):  A Python function or `torch.nn.Module`
            that will be invoked. If executed in TorchScript, it will execute asynchronously,
            otherwise it will not. Traced invocations of fork will be captured in the IR.
        *args, **kwargs: arguments to invoke `func` with.
    Returns:
        `torch.jit.Future[T]`: a reference to the execution of `func`. The value `T`
        can only be accessed by forcing completion of `func` through `torch.jit.wait`.
    Example (fork a free function):
    .. testcode::
        import torch
        from torch import Tensor
        def foo(a : Tensor, b : int) -> Tensor:
            return a + b
        def bar(a):
            fut : torch.jit.Future[Tensor] = torch.jit.fork(foo, a, b=2)
            return torch.jit.wait(fut)
        script_bar = torch.jit.script(bar)
        input = torch.tensor(2)
        # only the scripted version executes asynchronously
        assert script_bar(input) == bar(input)
        # trace is not run asynchronously, but fork is captured in IR
        graph = torch.jit.trace(bar, (input,)).graph
        assert "fork" in str(graph)
    Example (fork a module method):
    .. testcode::
        import torch
        from torch import Tensor
        class SubMod(torch.nn.Module):
            def forward(self, a: Tensor, b : int):
                return a + b
        class Mod(torch.nn.Module):
            def __init__(self):
                super(self).__init__()
                self.mod = SubMod()
            def forward(self, input):
                fut = torch.jit.fork(self.mod, a, b=2)
                return torch.jit.wait(fut)
        input = torch.tensor(2)
        mod = Mod()
        assert mod(input) == torch.jit.script(mod).forward(input)
    """
    ...

def wait(future):
    """
    Forces completion of a `torch.jit.Future[T]` asynchronous task, returning the
    result of the task. See :func:`~fork` for docs and examples.
    Arguments:
        func (torch.jit.Future[T]): an asynchronous task reference, created through `torch.jit.fork`
    Returns:
        `T`: the return value of the the completed task
    """
    ...

def freeze(mod, preserved_attrs: Optional[List[str]] = ...):
    r"""
    Freezing a :class:`ScriptModule` will clone it and attempt to inline the cloned
    module's submodules, parameters, and attributes as constants in the TorchScript IR Graph.
    By default, `forward` will be preserved, as well as attributes & methods specified in
    `preserved_attrs`. Additionally, any attribute that is modified within a preserved
    method will be preserved.

    Freezing currently only accepts ScriptModules that are in eval mode.

    Arguments:
        mod (:class:`ScriptModule`): a module to be frozen

        preserved_attrs (Optional[List[str]]): a list of attributes to preserve in addition to the forward method.
        Attributes modified in preserved methods will also be preserved.

    Returns:
        Frozen :class:`ScriptModule`.

    Example (Freezing a simple module with a Parameter):
    .. testcode::
        import torch
        class MyModule(torch.nn.Module):
            def __init__(self, N, M):
                super(MyModule, self).__init__()
                self.weight = torch.nn.Parameter(torch.rand(N, M))
                self.linear = torch.nn.Linear(N, M)

            def forward(self, input):
                output = self.weight.mm(input)
                output = self.linear(output)
                return output

        scripted_module = torch.jit.script(MyModule(2, 3).eval())
        frozen_module = torch.jit.freeze(scripted_module)
        # parameters have been removed and inlined into the Graph as constants
        assert len(list(frozen_module.named_parameters())) == 0
        # See the compiled graph as Python code
        print(frozen_module.code)

    Example (Freezing a module with preserved attributes)
    .. testcode::
        import torch
        class MyModule2(torch.nn.Module):
            def __init__(self):
                super(MyModule2, self).__init__()
                self.modified_tensor = torch.tensor(10.)
                self.version = 1

            def forward(self, input):
                self.modified_tensor += 1
                return input + self.modified_tensor

        scripted_module = torch.jit.script(MyModule2().eval())
        frozen_module = torch.jit.freeze(scripted_module, preserved_attrs=["version"])
        # we've manually preserved `version`, so it still exists on the frozen module and can be modified
        assert frozen_module.version == 1
        frozen_module.version = 2
        # `modified_tensor` is detected as being mutated in the forward, so freezing preserves
        # it to retain model semantics
        assert frozen_module(torch.tensor(1)) == torch.tensor(12)
        # now that we've run it once, the next result will be incremented by one
        assert frozen_module(torch.tensor(1)) == torch.tensor(13)

    Note:
        If you're not sure why an attribute is not being inlined as a constant, you can run
        `dump_alias_db` on frozen_module.forward.graph to see if freezing has detected the
        attribute is being modified.
    """
    ...

class CompilationUnit(object):
    def __init__(self, lang=..., _frames_up=...) -> None:
        ...
    
    def define(self, lang, rcb=..., _frames_up=...):
        ...
    
    def __getattr__(self, attr):
        ...
    


class ScriptWarning(Warning):
    ...


def whichmodule(obj):
    """Find the module an object belong to."""
    ...

def script(obj, optimize=..., _frames_up=..., _rcb=...):
    r"""
    Scripting a function or ``nn.Module`` will inspect the source code, compile
    it as TorchScript code using the TorchScript compiler, and return a :class:`ScriptModule` or
    :class:`ScriptFunction`. TorchScript itself is a subset of the Python language, so not all
    features in Python work, but we provide enough functionality to compute on
    tensors and do control-dependent operations. For a complete guide, see the
    :ref:`language-reference`.

    ``torch.jit.script`` can be used as a function for modules and functions, and as a decorator
    ``@torch.jit.script`` for :ref:`torchscript-classes` and functions.

    Arguments:
        obj (callable, class, or ``nn.Module``):  The ``nn.Module``, function, or class type to
                                                  compile.

    Returns:
        If ``obj`` is ``nn.Module``, ``script`` returns
        a :class:`ScriptModule` object. The returned :class:`ScriptModule` will
        have the same set of sub-modules and parameters as the
        original ``nn.Module``. If ``obj`` is a standalone function,
        a :class:`ScriptFunction` will be returned.

    **Scripting a function**
        The ``@torch.jit.script`` decorator will construct a :class:`ScriptFunction`
        by compiling the body of the function.

        Example (scripting a function):

        .. testcode::

            import torch

            @torch.jit.script
            def foo(x, y):
                if x.max() > y.max():
                    r = x
                else:
                    r = y
                return r

            print(type(foo))  # torch.jit.ScriptFuncion

            # See the compiled graph as Python code
            print(foo.code)

            # Call the function using the TorchScript interpreter
            foo(torch.ones(2, 2), torch.ones(2, 2))

        .. testoutput::
            :hide:

            ...

    **Scripting an nn.Module**
        Scripting an ``nn.Module`` by default will compile the ``forward`` method and recursively
        compile any methods, submodules, and functions called by ``forward``. If a ``nn.Module`` only uses
        features supported in TorchScript, no changes to the original module code should be necessary. ``script``
        will construct :class:`ScriptModule` that has copies of the attributes, parameters, and methods of
        the original module.

        Example (scripting a simple module with a Parameter):

        .. testcode::

            import torch

            class MyModule(torch.nn.Module):
                def __init__(self, N, M):
                    super(MyModule, self).__init__()
                    # This parameter will be copied to the new ScriptModule
                    self.weight = torch.nn.Parameter(torch.rand(N, M))

                    # When this submodule is used, it will be compiled
                    self.linear = torch.nn.Linear(N, M)

                def forward(self, input):
                    output = self.weight.mv(input)

                    # This calls the `forward` method of the `nn.Linear` module, which will
                    # cause the `self.linear` submodule to be compiled to a `ScriptModule` here
                    output = self.linear(output)
                    return output

            scripted_module = torch.jit.script(MyModule(2, 3))

        Example (scripting a module with traced submodules):

        .. testcode::

            import torch
            import torch.nn as nn
            import torch.nn.functional as F

            class MyModule(nn.Module):
                def __init__(self):
                    super(MyModule, self).__init__()
                    # torch.jit.trace produces a ScriptModule's conv1 and conv2
                    self.conv1 = torch.jit.trace(nn.Conv2d(1, 20, 5), torch.rand(1, 1, 16, 16))
                    self.conv2 = torch.jit.trace(nn.Conv2d(20, 20, 5), torch.rand(1, 20, 16, 16))

                def forward(self, input):
                  input = F.relu(self.conv1(input))
                  input = F.relu(self.conv2(input))
                  return input

            scripted_module = torch.jit.script(MyModule())

        To compile a method other than ``forward`` (and recursively compile anything it calls), add
        the :func:`@torch.jit.export <torch.jit.export>` decorator to the method. To opt out of compilation
        use :func:`@torch.jit.ignore <torch.jit.ignore>` or :func:`@torch.jit.unused <torch.jit.unused>`.

        Example (an exported and ignored method in a module)::

            import torch
            import torch.nn as nn

            class MyModule(nn.Module):
                def __init__(self):
                    super(MyModule, self).__init__()

                @torch.jit.export
                def some_entry_point(self, input):
                    return input + 10

                @torch.jit.ignore
                def python_only_fn(self, input):
                    # This function won't be compiled, so any
                    # Python APIs can be used
                    import pdb
                    pdb.set_trace()

                def forward(self, input):
                    if self.training:
                        self.python_only_fn(input)
                    return input * 99

            scripted_module = torch.jit.script(MyModule())
            print(scripted_module.some_entry_point(torch.randn(2, 2)))
            print(scripted_module(torch.randn(2, 2)))
    """
    ...

def interface(obj):
    ...

def script_method(fn):
    ...

class OrderedDictWrapper(object):
    def __init__(self, _c) -> None:
        ...
    
    def keys(self):
        ...
    
    def values(self):
        ...
    
    def __len__(self):
        ...
    
    def __delitem__(self, k):
        ...
    
    def items(self):
        ...
    
    def __setitem__(self, k, v):
        ...
    
    def __contains__(self, k):
        ...
    
    def __getitem__(self, k):
        ...
    


class OrderedModuleDict(OrderedDictWrapper):
    def __init__(self, module, python_dict) -> None:
        ...
    
    def items(self):
        ...
    
    def __contains__(self, k):
        ...
    
    def __setitem__(self, k, v):
        ...
    
    def __getitem__(self, k):
        ...
    


class ScriptMeta(type):
    def __init__(cls, name, bases, attrs) -> None:
        ...
    


if _enabled:
    class _CachedForward(object):
        def __get__(self, obj, cls):
            ...
        
    
    
    class ScriptModule(with_metaclass(ScriptMeta, Module)):
        """
        ``ScriptModule``s wrap a C++ ``torch::jit::Module``. ``ScriptModule``s
        contain methods, attributes, parameters, and
        constants. These can be accessed the same as on a normal ``nn.Module``.
        """
        def __init__(self) -> None:
            ...
        
        forward = ...
        def __getattr__(self, attr):
            ...
        
        def __setattr__(self, attr, value):
            ...
        
        def define(self, src):
            ...
        
    
    
    class RecursiveScriptModule(ScriptModule):
        r"""
        The core data structure in TorchScript is the ``ScriptModule``. It is an
        analogue of torch's ``nn.Module`` and represents an entire model as a tree of
        submodules. Like normal modules, each individual module in a ``ScriptModule`` can
        have submodules, parameters, and methods. In ``nn.Module``\s methods are implemented
        as Python functions, but in ``ScriptModule``\s methods are implemented as
        TorchScript functions,  a statically-typed subset of Python that contains all
        of PyTorch's built-in Tensor operations. This difference allows your
        ``ScriptModule``\s code to run without the need for a Python interpreter.

        ``ScriptModule``\s should not be created manually, instead use
        either :func:`tracing <torch.jit.trace>` or :func:`scripting <torch.jit.script>`.
        Tracing and scripting can be applied incrementally and :ref:`composed as necessary <Types>`.

        * Tracing records the tensor operations as executed with a set of example inputs and uses these
          operations to construct a computation graph. You can use the full dynamic behavior of Python with tracing,
          but values other than Tensors and control flow aren't captured in the graph.

        * Scripting inspects the Python code of the model
          and compiles it to TorchScript. Scripting allows the use of many `types`_ of values and supports dynamic control flow.
          Many, but not all features of Python are supported by the compiler, so changes to the source code may be necessary.
        """
        _disable_script_meta = ...
        def __init__(self, cpp_module) -> None:
            ...
        
        @property
        def graph(self):
            r"""
            Returns a string representation of the internal graph for the
            ``forward`` method. See `Interpreting Graphs`_ for details.
            """
            ...
        
        @property
        def inlined_graph(self):
            r"""
            Returns a string representation of the internal graph for the
            ``forward`` method. This graph will be preprocessed to inline all function and method calls.
            See `Interpreting Graphs`_ for details.
            """
            ...
        
        @property
        def code(self):
            r"""
            Returns a pretty-printed representation (as valid Python syntax) of
            the internal graph for the ``forward`` method. See `Inspecting Code`_
            for details.
            """
            ...
        
        @property
        def code_with_constants(self):
            r"""
            Returns a tuple of:

            [0] a pretty-printed representation (as valid Python syntax) of
            the internal graph for the ``forward`` method. See `code`.
            [1] a ConstMap following the CONSTANT.cN format of the output in [0].
            The indices in the [0] output are keys to the underlying constant's values.

            See `Inspecting Code`_ for details.
            """
            ...
        
        def save(self, *args, **kwargs):
            r"""
            save(f, _extra_files=ExtraFilesMap{})

            See :func:`torch.jit.save <torch.jit.save>` for details.
            """
            ...
        
        def save_to_buffer(self, *args, **kwargs):
            ...
        
        def get_debug_state(self, *args, **kwargs):
            ...
        
        def extra_repr(self):
            ...
        
        def graph_for(self, *args, **kwargs):
            ...
        
        @property
        def original_name(self):
            ...
        
        def define(self, src):
            ...
        
        def __getattr__(self, attr):
            ...
        
        def __setattr__(self, attr, value):
            ...
        
        def __getstate__(self):
            ...
        
        def __copy__(self):
            ...
        
        def __deepcopy__(self, memo):
            ...
        
        def forward_magic_method(self, method_name, *args, **kwargs):
            ...
        
        def __iter__(self):
            ...
        
        def __getitem__(self, idx):
            ...
        
        def __len__(self):
            ...
        
        def __contains__(self, key):
            ...
        
        def __dir__(self):
            ...
        
        def __bool__(self):
            ...
        
    
    
    _compiled_methods_whitelist = 'forward', 'register_buffer', 'register_parameter', 'add_module', '_apply', 'apply', 'cuda', 'cpu', 'to', 'type', 'float', 'double', 'half', 'state_dict', '_save_to_state_dict', 'load_state_dict', '_load_from_state_dict', '_named_members', 'parameters', 'named_parameters', 'buffers', 'named_buffers', 'children', 'named_children', 'modules', 'named_modules', 'zero_grad', 'share_memory', '_get_name', 'extra_repr', '_slow_forward', '_tracing_name', 'eval', 'train'
else:
    class ScriptModule(torch.nn.Module):
        def __init__(self) -> None:
            ...
        
    
    
class TracedModule(ScriptModule):
    _disable_script_meta = ...
    def __init__(self, orig, id_set=..., _compilation_unit=...) -> None:
        class QualnameWrapper(torch.nn.Module):
            ...
        
        
    
    def forward(self, *args, **kwargs):
        ...
    
    def __getattr__(self, attr):
        ...
    
    def __setattr__(self, attr, value):
        ...
    
    def extra_repr(self):
        ...
    


if _enabled:
    class TopLevelTracedModule(TracedModule):
        forward = ...
    
    
def is_scripting():
    r"""
    Function that returns True when in compilation and False otherwise. This
    is useful especially with the @unused decorator to leave code in your
    model that is not yet TorchScript compatible.
    .. testcode::

        import torch

        @torch.jit.unused
        def unsupported_linear_op(x):
            return x

        def linear(x):
           if not torch.jit.is_scripting():
              return torch.linear(x)
           else:
              return unsupported_linear_op(x)
    """
    ...

def is_tracing():
    """
    Returns ``True`` in tracing (if a function is called during the tracing of
    code with ``torch.jit.trace``) and ``False`` otherwise.
    """
    ...

_jit_caching_layer = weakref.WeakKeyDictionary()
_jit_function_overload_caching = weakref.WeakKeyDictionary()
_script_classes = {  }
Error = torch._C.JITException
class _disable_tracing(object):
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    


def annotate(the_type, the_value):
    ...

last_executed_optimized_graph = torch._C._last_executed_optimized_graph
ScriptFunction = torch._C.ScriptFunction
if not torch._C._jit_init():
    ...
"""
This type stub file was generated by pyright.
"""

import sys
import ast
import string
from torch._C._jit_tree_views import Assert, Break, ClassDef, Continue, Delete, For, Raise, With

_reserved_prefix = '__jit'
_reserved_names = 'print'
_identifier_chars = set(string.ascii_lowercase + string.ascii_uppercase + string.digits)
def is_reserved_name(name):
    ...

pretty_node_names = { ast.FunctionDef: "function definitions",ast.For: "for loops",ast.Delete: "del statements",ast.ClassDef: "class definitions",ast.With: "with statements",ast.Raise: "raise statements",ast.Assert: "assertions",ast.Import: "import statements",ast.ImportFrom: "import statements",ast.Global: "global variables",ast.Break: "break statements",ast.Continue: "continue statements" }
node_start_tokens = { ast.FunctionDef: "def",ast.For: "for",ast.Delete: "del",ast.ClassDef: "class",ast.With: "with",ast.Raise: "raise",ast.Assert: "assert",ast.Import: "import",ast.ImportFrom: "from",ast.Global: "global",ast.Break: "break",ast.Continue: "continue" }
if sys.version_info >= (3, 6):
    ...
class FrontendError(Exception):
    def __init__(self, source_range, msg) -> None:
        ...
    
    def __str__(self) -> str:
        ...
    


class NotSupportedError(FrontendError):
    ...


class UnsupportedNodeError(NotSupportedError):
    def __init__(self, ctx, offending_node, reason=...) -> None:
        ...
    


class FrontendTypeError(FrontendError):
    ...


def build_withitems(ctx, items):
    ...

def build_stmts(ctx, stmts):
    ...

def get_jit_class_def(cls, self_name):
    ...

def get_jit_def(fn, def_name, self_name=...):
    """
    Build a JIT AST (TreeView) from the given function.

    Arguments:
        fn: A function object to compile
        def_name: The name to give to the resulting AST object. This is not
            always the same as `fn.__name__`, for example:
                def _forward(self):
                    ...
                forward = _forward
            In this case, the `__name__` attribute of the function object is "_forward",
            but we want the result AST to have the name "forward".
        self_name: If this function is a method, what the type name of `self` is.
    """
    ...

class Builder(object):
    def __call__(self, ctx, node):
        ...
    


def build_class_def(ctx, py_def, methods, self_name):
    ...

def build_def(ctx, py_def, type_line, def_name, self_name=...):
    ...

_vararg_kwarg_err = "Compiled functions can't take variable number of arguments " "or use keyword-only arguments with defaults"
def build_param_list(ctx, py_args, self_name):
    ...

def build_param(ctx, py_arg, self_name, kwarg_only):
    ...

def get_default_args(fn):
    ...

class WithItemBuilder(Builder):
    @staticmethod
    def build_withitem(ctx, item):
        ...
    


class StmtBuilder(Builder):
    augassign_map = ...
    @staticmethod
    def build_Expr(ctx, stmt):
        ...
    
    @staticmethod
    def build_Assign(ctx, stmt):
        ...
    
    @staticmethod
    def build_AnnAssign(ctx, stmt):
        ...
    
    @staticmethod
    def build_Delete(ctx, stmt):
        ...
    
    @staticmethod
    def build_Return(ctx, stmt):
        ...
    
    @staticmethod
    def build_Raise(ctx, stmt):
        ...
    
    @staticmethod
    def build_Assert(ctx, stmt):
        ...
    
    @staticmethod
    def build_AugAssign(ctx, stmt):
        ...
    
    @staticmethod
    def build_While(ctx, stmt):
        ...
    
    @staticmethod
    def build_For(ctx, stmt):
        ...
    
    @staticmethod
    def build_If(ctx, stmt):
        ...
    
    @staticmethod
    def build_Print(ctx, stmt):
        ...
    
    @staticmethod
    def build_Pass(ctx, stmt):
        ...
    
    @staticmethod
    def build_Break(ctx, stmt):
        ...
    
    @staticmethod
    def build_Continue(ctx, stmt):
        ...
    
    @staticmethod
    def build_With(ctx, stmt):
        ...
    


class ExprBuilder(Builder):
    binop_map = ...
    unop_map = ...
    boolop_map = ...
    cmpop_map = ...
    @staticmethod
    def build_Attribute(ctx, expr):
        ...
    
    @staticmethod
    def build_Call(ctx, expr):
        ...
    
    @staticmethod
    def build_Ellipsis(ctx, expr):
        ...
    
    @staticmethod
    def build_Name(ctx, expr):
        ...
    
    @staticmethod
    def build_NameConstant(ctx, expr):
        ...
    
    @staticmethod
    def build_BinOp(ctx, expr):
        ...
    
    @staticmethod
    def build_UnaryOp(ctx, expr):
        ...
    
    @staticmethod
    def build_BoolOp(ctx, expr):
        ...
    
    @staticmethod
    def build_IfExp(ctx, expr):
        ...
    
    @staticmethod
    def build_Compare(ctx, expr):
        ...
    
    @staticmethod
    def build_Subscript(ctx, expr):
        ...
    
    @staticmethod
    def build_List(ctx, expr):
        ...
    
    @staticmethod
    def build_Tuple(ctx, expr):
        ...
    
    @staticmethod
    def build_Dict(ctx, expr):
        ...
    
    @staticmethod
    def build_Num(ctx, expr):
        ...
    
    @staticmethod
    def build_Constant(ctx, expr):
        ...
    
    @staticmethod
    def build_Str(ctx, expr):
        ...
    
    @staticmethod
    def build_JoinedStr(ctx, expr):
        ...
    
    @staticmethod
    def build_ListComp(ctx, stmt):
        ...
    
    @staticmethod
    def build_Starred(ctx, expr):
        ...
    


build_expr = ExprBuilder()
build_stmt = StmtBuilder()
build_withitem = WithItemBuilder()
def find_before(ctx, pos, substr, offsets=...):
    ...

"""
This type stub file was generated by pyright.
"""

import torch

if torch.distributed.rpc.is_available():
    ...
class Module(object):
    def __init__(self, name, members) -> None:
        ...
    
    def __getattr__(self, name):
        ...
    


class EvalEnv(object):
    env = ...
    def __init__(self, rcb) -> None:
        ...
    
    def __getitem__(self, name):
        ...
    


def get_signature(fn, rcb, loc, is_method):
    ...

def is_function_or_method(the_callable):
    ...

def is_vararg(the_callable):
    ...

def get_param_names(fn, n_args):
    ...

def check_fn(fn, loc):
    ...

def parse_type_line(type_line, rcb, loc):
    """Parses a type annotation specified as a comment.

    Example inputs:
        # type: (Tensor, torch.Tensor) -> Tuple[Tensor]
        # type: (Tensor, Tuple[Tensor, Tensor]) -> Tensor
    """
    ...

def get_type_line(source):
    """Tries to find the line containing a comment with the type annotation."""
    ...

def split_type_line(type_line):
    """Splits the comment with the type annotation into parts for argument and return types.

    For example, for an input of:
        # type: (Tensor, torch.Tensor) -> Tuple[Tensor, Tensor]

    This function will return:
        ("(Tensor, torch.Tensor)", "Tuple[Tensor, Tensor]")

    """
    ...

def try_real_annotations(fn, loc):
    """Tries to use the Py3.5+ annotation syntax to get the type."""
    ...

def try_ann_to_type(ann, loc):
    ...

def ann_to_type(ann, loc):
    ...

"""
This type stub file was generated by pyright.
"""

import torch
import collections

ScriptMethodStub = collections.namedtuple('ScriptMethodStub', ('resolution_callback', 'def_', 'original_method'))
blacklist = ["_version", "_parameters", "_buffers", "_modules", "_initializing", "_backward_hooks", "_forward_hooks", "_forward_pre_hooks", "_state_dict_hooks", "_load_state_dict_pre_hooks", "dump_patches"]
def make_stub(func, name):
    ...

def make_stub_from_method(nn_module, method_name):
    ...

_constant_types = (bool, float, int, str, type(None), torch.device, torch.layout, torch.dtype)
class SourceContext(torch._C._jit_tree_views.SourceRangeFactory):
    def __init__(self, source, filename, file_lineno, leading_whitespace_len) -> None:
        ...
    


def infer_concrete_type_builder(nn_module):
    """
    Build a ConcreteModuleTypeBuilder from an nn.Module. This
    ConcreteModuleType doesn't have a JIT type associated with it yet, it
    must be filled in by the caller.
    """
    ...

class ConcreteTypeStore(object):
    def __init__(self) -> None:
        ...
    
    def get_or_create_concrete_type(self, nn_module):
        """
        Infer a ConcreteType from this `nn.Module` instance. Underlying JIT
        types are re-used if possible.
        """
        ...
    


concrete_type_store = ConcreteTypeStore()
def create_methods_from_stubs(concrete_type, stubs):
    ...

def create_script_module(nn_module, stubs_fn, share_types=...):
    """
    Creates a new ScriptModule from an nn.Module

    Arguments:
        nn_module:  The original Python nn.Module that we are creating a ScriptModule for.
        stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.
        share_types:  Whether to share underlying JIT types between modules (if possible).
            NOTE: Only set to False this when we cannot guarantee type sharing will work
                correctly. This only happens today for traced modules, where the same
                module can produce different traced methods depending on the inputs.
    """
    ...

def create_script_module_impl(nn_module, concrete_type, stubs_fn):
    """
    Convert an nn.Module to a RecursiveScriptModule.

    Arguments:
        nn_module:  The original Python nn.Module that we are creating a ScriptModule for.
        concrete_type:  The fully initialized ConcreteType of the module.
        stubs_fn:  Lambda that takes an nn.Module and generates a list of ScriptMethodStubs to compile.
    """
    ...

def script_model_defines_attr(script_model, attr):
    ...

def add_python_attr_to_scripted_model(script_model, orig, attr):
    ...

def get_overload_annotations(mod):
    ...

def get_overload_name_mapping(overload_info):
    ...

def make_stubs_for_overloads(overload_info):
    ...

def check_module_initialized(mod):
    ...

def infer_methods_to_compile(nn_module):
    """
    Implements the default rules for which methods should act as starting
    points for compilation (TODO add a link when the rules are published).
    """
    ...

def interface_script(mod_interface, nn_module):
    """
    Makes a ScriptModule from an nn.Module, using the interface methods rule for
    determining which methods to compile.

    Arguments:
        mod_interface: the interface type that the module have
        nn_module:  The original Python nn.Module that we are creating a ScriptModule for.
    """
    ...

def try_compile_fn(fn, loc):
    ...

def wrap_cpp_module(cpp_module):
    """
    Wrap this torch._C.ScriptModule in a Python ScriptModule, recursively for all submodules
    """
    ...

def compile_unbound_method(concrete_type, fn):
    ...

def lazy_bind(concrete_type, unbound_method):
    """
    Returns a function that lazily binds `unbound_method` to a provided
    Module IValue, then invokes the method. We do this so that any Python
    shenanigans that will poison type sharing are impossible at compile
    time.
    """
    ...

"""
This type stub file was generated by pyright.
"""

"""Adds docstrings to Storage functions"""
storage_classes = ['DoubleStorageBase', 'FloatStorageBase', 'LongStorageBase', 'IntStorageBase', 'ShortStorageBase', 'CharStorageBase', 'ByteStorageBase', 'BoolStorageBase', 'BFloat16StorageBase', 'ComplexDoubleStorageBase', 'ComplexFloatStorageBase']
def add_docstr_all(method, docstr):
    ...

"""
This type stub file was generated by pyright.
"""

def show():
    """
    Return a human-readable string with descriptions of the
    configuration of PyTorch.
    """
    ...

def parallel_info():
    r"""Returns detailed string with parallelization settings"""
    ...

"""
This type stub file was generated by pyright.
"""

def check_serializing_named_tensor(tensor):
    ...

def build_dim_map(tensor):
    """Returns a map of { dim: dim_name } where dim is a name if the dim is named
    and the dim index otherwise."""
    ...

def unzip_namedshape(namedshape):
    ...

def namer_api_name(inplace):
    ...

def is_ellipsis(item):
    ...

def single_ellipsis_index(names, fn_name):
    ...

def expand_single_ellipsis(numel_pre_glob, numel_post_glob, names):
    ...

def replace_ellipsis_by_position(ellipsis_idx, names, tensor_names):
    ...

def resolve_ellipsis(names, tensor_names, fn_name):
    """
    Expands ... inside `names` to be equal to a list of names from `tensor_names`.
    """
    ...

def update_names_with_list(tensor, names, inplace):
    ...

def update_names_with_mapping(tensor, rename_map, inplace):
    ...

def update_names(tensor, names, rename_map, inplace):
    """There are two usages:

    tensor.rename(*names) returns a view on tensor with named dims `names`.
    `names` must be of length `tensor.dim()`; otherwise, if '...' is in `names`,
    then it is expanded greedily to be equal to the corresponding names from
    `tensor.names`.

    For example,
    ```
    >>> x = torch.empty(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
    >>> x.rename('...', 'height', 'width').names
    ('N', 'C', 'height', 'width')

    >>> x.rename('batch', '...', 'width').names
    ('batch', 'C', 'H', 'width')
    ```

    tensor.rename(**rename_map) returns a view on tensor that has rename dims
        as specified in the mapping `rename_map`.

    For example,
    ```
    >>> x = torch.empty(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
    >>> x.rename(W='width', H='height').names
    ('N', 'C', 'height', 'width')
    ```

    Finally, tensor.rename has an in-place version called tensor.rename_.
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
import torch._C as _C

class Tensor(torch._C._TensorBase):
    def __deepcopy__(self, memo):
        ...
    
    def __reduce_ex__(self, proto):
        ...
    
    def __setstate__(self, state):
        ...
    
    def __repr__(self):
        ...
    
    def backward(self, gradient=..., retain_graph=..., create_graph=...):
        r"""Computes the gradient of current tensor w.r.t. graph leaves.

        The graph is differentiated using the chain rule. If the tensor is
        non-scalar (i.e. its data has more than one element) and requires
        gradient, the function additionally requires specifying ``gradient``.
        It should be a tensor of matching type and location, that contains
        the gradient of the differentiated function w.r.t. ``self``.

        This function accumulates gradients in the leaves - you might need to zero
        ``.grad`` attributes or set them to ``None`` before calling it.
        See :ref:`Default gradient layouts<default-grad-layouts>`
        for details on the memory layout of accumulated gradients.

        Arguments:
            gradient (Tensor or None): Gradient w.r.t. the
                tensor. If it is a tensor, it will be automatically converted
                to a Tensor that does not require grad unless ``create_graph`` is True.
                None values can be specified for scalar Tensors or ones that
                don't require grad. If a None value would be acceptable then
                this argument is optional.
            retain_graph (bool, optional): If ``False``, the graph used to compute
                the grads will be freed. Note that in nearly all cases setting
                this option to True is not needed and often can be worked around
                in a much more efficient way. Defaults to the value of
                ``create_graph``.
            create_graph (bool, optional): If ``True``, graph of the derivative will
                be constructed, allowing to compute higher order derivative
                products. Defaults to ``False``.
        """
        ...
    
    def register_hook(self, hook):
        r"""Registers a backward hook.

        The hook will be called every time a gradient with respect to the
        Tensor is computed. The hook should have the following signature::

            hook(grad) -> Tensor or None


        The hook should not modify its argument, but it can optionally return
        a new gradient which will be used in place of :attr:`grad`.

        This function returns a handle with a method ``handle.remove()``
        that removes the hook from the module.

        Example::

            >>> v = torch.tensor([0., 0., 0.], requires_grad=True)
            >>> h = v.register_hook(lambda grad: grad * 2)  # double the gradient
            >>> v.backward(torch.tensor([1., 2., 3.]))
            >>> v.grad

             2
             4
             6
            [torch.FloatTensor of size (3,)]

            >>> h.remove()  # removes the hook
        """
        ...
    
    def reinforce(self, reward):
        ...
    
    detach = ...
    detach_ = ...
    def retain_grad(self):
        r"""Enables .grad attribute for non-leaf Tensors."""
        ...
    
    def is_shared(self):
        r"""Checks if tensor is in shared memory.

        This is always ``True`` for CUDA tensors.
        """
        ...
    
    def share_memory_(self):
        r"""Moves the underlying storage to shared memory.

        This is a no-op if the underlying storage is already in shared memory
        and for CUDA tensors. Tensors in shared memory cannot be resized.
        """
        ...
    
    def __reversed__(self):
        r"""Reverses the tensor along dimension 0."""
        ...
    
    def norm(self, p=..., dim=..., keepdim=..., dtype=...):
        r"""See :func:`torch.norm`"""
        ...
    
    def lu(self, pivot=..., get_infos=...):
        r"""See :func:`torch.lu`"""
        ...
    
    def stft(self, n_fft, hop_length=..., win_length=..., window=..., center=..., pad_mode=..., normalized=..., onesided=...):
        r"""See :func:`torch.stft`

        .. warning::
          This function changed signature at version 0.4.1. Calling with
          the previous signature may cause error or return incorrect result.
        """
        ...
    
    def istft(self, n_fft, hop_length=..., win_length=..., window=..., center=..., normalized=..., onesided=..., length=...):
        r"""See :func:`torch.istft`"""
        ...
    
    def resize(self, *sizes):
        ...
    
    def resize_as(self, tensor):
        ...
    
    def split(self, split_size, dim=...):
        r"""See :func:`torch.split`
        """
        ...
    
    def unique(self, sorted=..., return_inverse=..., return_counts=..., dim=...):
        r"""Returns the unique elements of the input tensor.

        See :func:`torch.unique`
        """
        ...
    
    def unique_consecutive(self, return_inverse=..., return_counts=..., dim=...):
        r"""Eliminates all but the first element from every consecutive group of equivalent elements.

        See :func:`torch.unique_consecutive`
        """
        ...
    
    def __rsub__(self, other):
        ...
    
    def __rdiv__(self, other):
        ...
    
    __rtruediv__ = ...
    __itruediv__ = ...
    __pow__ = ...
    def __format__(self, format_spec):
        ...
    
    def __ipow__(self, other):
        ...
    
    @_wrap_type_error_to_not_implemented
    def __rpow__(self, other):
        ...
    
    @_wrap_type_error_to_not_implemented
    def __floordiv__(self, other):
        ...
    
    @_wrap_type_error_to_not_implemented
    def __rfloordiv__(self, other):
        ...
    
    __neg__ = ...
    __eq__ = ...
    __ne__ = ...
    __lt__ = ...
    __le__ = ...
    __gt__ = ...
    __ge__ = ...
    __abs__ = ...
    def __len__(self):
        ...
    
    def __iter__(self):
        ...
    
    def __hash__(self) -> int:
        ...
    
    def __dir__(self):
        ...
    
    __array_priority__ = ...
    def __array__(self, dtype=...):
        ...
    
    def __array_wrap__(self, array):
        ...
    
    def __contains__(self, element):
        r"""Check if `element` is present in tensor

        Arguments:
            element (Tensor or scalar): element to be checked
                for presence in current tensor"
        """
        ...
    
    @property
    def __cuda_array_interface__(self):
        """Array view description for cuda tensors.

        See:
        https://numba.pydata.org/numba-doc/latest/cuda/cuda_array_interface.html
        """
        ...
    
    def refine_names(self, *names):
        r"""Refines the dimension names of :attr:`self` according to :attr:`names`.

        Refining is a special case of renaming that "lifts" unnamed dimensions.
        A ``None`` dim can be refined to have any name; a named dim can only be
        refined to have the same name.

        Because named tensors can coexist with unnamed tensors, refining names
        gives a nice way to write named-tensor-aware code that works with both
        named and unnamed tensors.

        :attr:`names` may contain up to one Ellipsis (``...``).
        The Ellipsis is expanded greedily; it is expanded in-place to fill
        :attr:`names` to the same length as ``self.dim()`` using names from the
        corresponding indices of ``self.names``.

        Python 2 does not support Ellipsis but one may use a string literal
        instead (``'...'``).

        Arguments:
            names (iterable of str): The desired names of the output tensor. May
                contain up to one Ellipsis.

        Examples::

            >>> imgs = torch.randn(32, 3, 128, 128)
            >>> named_imgs = imgs.refine_names('N', 'C', 'H', 'W')
            >>> named_imgs.names
            ('N', 'C', 'H', 'W')

            >>> tensor = torch.randn(2, 3, 5, 7, 11)
            >>> tensor = tensor.refine_names('A', ..., 'B', 'C')
            >>> tensor.names
            ('A', None, None, 'B', 'C')

        .. warning::
            The named tensor API is experimental and subject to change.

        """
        ...
    
    def align_to(self, *names):
        r"""Permutes the dimensions of the :attr:`self` tensor to match the order
        specified in :attr:`names`, adding size-one dims for any new names.

        All of the dims of :attr:`self` must be named in order to use this method.
        The resulting tensor is a view on the original tensor.

        All dimension names of :attr:`self` must be present in :attr:`names`.
        :attr:`names` may contain additional names that are not in ``self.names``;
        the output tensor has a size-one dimension for each of those new names.

        :attr:`names` may contain up to one Ellipsis (``...``).
        The Ellipsis is expanded to be equal to all dimension names of :attr:`self`
        that are not mentioned in :attr:`names`, in the order that they appear
        in :attr:`self`.

        Python 2 does not support Ellipsis but one may use a string literal
        instead (``'...'``).

        Arguments:
            names (iterable of str): The desired dimension ordering of the
                output tensor. May contain up to one Ellipsis that is expanded
                to all unmentioned dim names of :attr:`self`.

        Examples::

            >>> tensor = torch.randn(2, 2, 2, 2, 2, 2)
            >>> named_tensor = tensor.refine_names('A', 'B', 'C', 'D', 'E', 'F')

            # Move the F and E dims to the front while keeping the rest in order
            >>> named_tensor.align_to('F', 'E', ...)

        .. warning::
            The named tensor API is experimental and subject to change.

        """
        ...
    
    def unflatten(self, dim, namedshape):
        r"""Unflattens the named dimension :attr:`dim`, viewing it in the shape
        specified by :attr:`namedshape`.

        Arguments:
            namedshape: (iterable of ``(name, size)`` tuples).

        Examples::

            >>> flat_imgs = torch.rand(32, 3 * 128 * 128, names=('N', 'features'))
            >>> imgs = flat_imgs.unflatten('features', (('C', 3), ('H', 128), ('W', 128)))
            >>> imgs.names, imgs.shape
            (('N', 'C', 'H', 'W'), torch.Size([32, 3, 128, 128]))

        .. warning::
            The named tensor API is experimental and subject to change.

        """
        ...
    
    def rename_(self, *names, **rename_map):
        """In-place version of :meth:`~Tensor.rename`."""
        ...
    
    def rename(self, *names, **rename_map):
        """Renames dimension names of :attr:`self`.

        There are two main usages:

        ``self.rename(**rename_map)`` returns a view on tensor that has dims
        renamed as specified in the mapping :attr:`rename_map`.

        ``self.rename(*names)`` returns a view on tensor, renaming all
        dimensions positionally using :attr:`names`.
        Use ``self.rename(None)`` to drop names on a tensor.

        One cannot specify both positional args :attr:`names` and keyword args
        :attr:`rename_map`.

        Examples::

            >>> imgs = torch.rand(2, 3, 5, 7, names=('N', 'C', 'H', 'W'))
            >>> renamed_imgs = imgs.rename(N='batch', C='channels')
            >>> renamed_imgs.names
            ('batch', 'channels', 'H', 'W')

            >>> renamed_imgs = imgs.rename(None)
            >>> renamed_imgs.names
            (None,)

            >>> renamed_imgs = imgs.rename('batch', 'channel', 'height', 'width')
            >>> renamed_imgs.names
            ('batch', 'channel', 'height', 'width')

        .. warning::
            The named tensor API is experimental and subject to change.

        """
        ...
    
    @property
    def grad(self):
        """
        This attribute is ``None`` by default and becomes a Tensor the first time a call to
        :func:`backward` computes gradients for ``self``.
        The attribute will then contain the gradients computed and future calls to
        :func:`backward` will accumulate (add) gradients into it.
        """
        ...
    
    @grad.setter
    def grad(self, new_grad):
        ...
    
    @grad.deleter
    def grad(self):
        ...
    
    __module__ = ...


"""
This type stub file was generated by pyright.
"""

import types

class VFModule(types.ModuleType):
    vf: types.ModuleType
    def __init__(self, name) -> None:
        ...
    
    def __getattr__(self, attr):
        ...
    


"""
This type stub file was generated by pyright.
"""

from ._torch_docs import parse_kwargs

"""Adds docstrings to Tensor functions"""
def add_docstr_all(method, docstr):
    ...

common_args = parse_kwargs("""
    memory_format (:class:`torch.memory_format`, optional): the desired memory format of
        returned Tensor. Default: ``torch.preserve_format``.
""")
new_common_args = parse_kwargs("""
    size (int...): a list, tuple, or :class:`torch.Size` of integers defining the
        shape of the output tensor.
    dtype (:class:`torch.dtype`, optional): the desired type of returned tensor.
        Default: if None, same :class:`torch.dtype` as this tensor.
    device (:class:`torch.device`, optional): the desired device of returned tensor.
        Default: if None, same :class:`torch.device` as this tensor.
    requires_grad (bool, optional): If autograd should record operations on the
        returned tensor. Default: ``False``.
    pin_memory (bool, optional): If set, returned tensor would be allocated in
        the pinned memory. Works only for CPU tensors. Default: ``False``.
""")
"""
This type stub file was generated by pyright.
"""

"""
Python implementation of __torch_function__

While most of the torch API and handling for __torch_function__ happens
at the C++ level, some of the torch API is written in Python so we need
python-level handling for __torch_function__ overrides as well. The main
developer-facing functionality in this file are handle_torch_function and
has_torch_function. See torch/functional.py and test/test_overrides.py
for usage examples.

NOTE: heavily inspired by NumPy's ``__array_function__`` (see:
https://github.com/pytorch/pytorch/issues/24015 and
https://www.numpy.org/neps/nep-0018-array-function-protocol.html
)

If changing this file in a way that can affect ``__torch_function__`` overhead,
please report the benchmarks in ``benchmarks/overrides_benchmark``. See the
instructions in the ``README.md`` in that directory.
"""
def get_ignored_functions():
    """Return public functions that cannot be overrided by __torch_function__

    Returns
    -------
    A tuple of functions that are publicly available in the torch API but cannot
    be overrided with __torch_function__. Mostly this is because none of the
    arguments of these functions are tensors or tensor-likes.

    """
    ...

def get_testing_overrides():
    """Return a dict containing dummy overrides for all overridable functions

    Returns
    -------
    A dictionary that maps overridable functions in the PyTorch API to
    lambda functions that have the same signature as the real function
    and unconditionally return -1. These lambda functions are useful
    for testing API coverage for a type that defines __torch_function__.

    """
    ...

def handle_torch_function(public_api, relevant_args, *args, **kwargs):
    """Implement a function with checks for __torch_function__ overrides.

    See torch::autograd::handle_torch_function for the equivalent of this
    function in the C++ implementation.

    Arguments
    ---------
    public_api : function
        Function exposed by the public torch API originally called like
        ``public_api(*args, **kwargs)`` on which arguments are now being
        checked.
    relevant_args : iterable
        Iterable of arguments to check for __torch_function__ methods.
    args : tuple
        Arbitrary positional arguments originally passed into ``public_api``.
    kwargs : tuple
        Arbitrary keyword arguments originally passed into ``public_api``.

    Returns
    -------
    Result from calling `implementation()` or an `__torch_function__`
    method, as appropriate.

    Raises
    ------
    TypeError : if no implementation is found.

    """
    ...

def has_torch_function(relevant_args):
    """Check for __torch_function__ implementations in the elements of an iterable

    Arguments
    ---------
    relevant_args : iterable
        Iterable or aguments to check for __torch_function__ methods.

    Returns
    -------
    True if any of the elements of relevant_args have __torch_function__
    implementations, False otherwise.
    """
    ...

def get_overridable_functions():
    """List functions that are overridable via __torch_function__

    Returns
    -------
    A dictionary that maps namespaces that contain overridable functions
    to functions in that namespace that can be overrided.

    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, TYPE_CHECKING, Tuple
from torch import Tensor, dtype as DType

if TYPE_CHECKING:
    ...
else:
    ...
def addmm(mat: Tensor, mat1: Tensor, mat2: Tensor, beta: float = ..., alpha: float = ...) -> Tensor:
    r"""
    This function does exact same thing as :func:`torch.addmm` in the forward,
    except that it supports backward for sparse matrix :attr:`mat1`. :attr:`mat1`
    need to have `sparse_dim = 2`. Note that the gradients of :attr:`mat1` is a
    coalesced sparse tensor.

    Args:
        mat (Tensor): a dense matrix to be added
        mat1 (SparseTensor): a sparse matrix to be multiplied
        mat2 (Tensor): a dense matrix be multiplied
        beta (Number, optional): multiplier for :attr:`mat` (:math:`\beta`)
        alpha (Number, optional): multiplier for :math:`mat1 @ mat2` (:math:`\alpha`)
    """
    ...

def mm(mat1, mat2):
    r"""
    Performs a matrix multiplication of the sparse matrix :attr:`mat1`
    and dense matrix :attr:`mat2`. Similar to :func:`torch.mm`, If :attr:`mat1` is a
    :math:`(n \times m)` tensor, :attr:`mat2` is a :math:`(m \times p)` tensor, out will be a
    :math:`(n \times p)` dense tensor. :attr:`mat1` need to have `sparse_dim = 2`.
    This function also supports backward for both matrices. Note that the gradients of
    :attr:`mat1` is a coalesced sparse tensor.

    Args:
        mat1 (SparseTensor): the first sparse matrix to be multiplied
        mat2 (Tensor): the second dense matrix to be multiplied

    Example::

        >>> a = torch.randn(2, 3).to_sparse().requires_grad_(True)
        >>> a
        tensor(indices=tensor([[0, 0, 0, 1, 1, 1],
                               [0, 1, 2, 0, 1, 2]]),
               values=tensor([ 1.5901,  0.0183, -0.6146,  1.8061, -0.0112,  0.6302]),
               size=(2, 3), nnz=6, layout=torch.sparse_coo, requires_grad=True)

        >>> b = torch.randn(3, 2, requires_grad=True)
        >>> b
        tensor([[-0.6479,  0.7874],
                [-1.2056,  0.5641],
                [-1.1716, -0.9923]], requires_grad=True)

        >>> y = torch.sparse.mm(a, b)
        >>> y
        tensor([[-0.3323,  1.8723],
                [-1.8951,  0.7904]], grad_fn=<SparseAddmmBackward>)
        >>> y.sum().backward()
        >>> a.grad
        tensor(indices=tensor([[0, 0, 0, 1, 1, 1],
                               [0, 1, 2, 0, 1, 2]]),
               values=tensor([ 0.1394, -0.6415, -2.1639,  0.1394, -0.6415, -2.1639]),
               size=(2, 3), nnz=6, layout=torch.sparse_coo)
    """
    ...

def sum(input: Tensor, dim: Optional[Tuple[int]] = ..., dtype: Optional[int] = ...) -> Tensor:
    r"""
    Returns the sum of each row of SparseTensor :attr:`input` in the given
    dimensions :attr:`dim`. If :attr:`dim` is a list of dimensions,
    reduce over all of them. When sum over all ``sparse_dim``, this method
    returns a Tensor instead of SparseTensor.

    All summed :attr:`dim` are squeezed (see :func:`torch.squeeze`), resulting an output
    tensor having :attr:`dim` fewer dimensions than :attr:`input`.

    During backward, only gradients at ``nnz`` locations of :attr:`input`
    will propagate back. Note that the gradients of :attr:`input` is coalesced.

    Args:
        input (Tensor): the input SparseTensor
        dim (int or tuple of ints): a dimension or a list of dimensions to reduce. Default: reduce
            over all dims.
        dtype (:class:`torch.dtype`, optional): the desired data type of returned Tensor.
            Default: dtype of :attr:`input`.

    Example::

        >>> nnz = 3
        >>> dims = [5, 5, 2, 3]
        >>> I = torch.cat([torch.randint(0, dims[0], size=(nnz,)),
                           torch.randint(0, dims[1], size=(nnz,))], 0).reshape(2, nnz)
        >>> V = torch.randn(nnz, dims[2], dims[3])
        >>> size = torch.Size(dims)
        >>> S = torch.sparse_coo_tensor(I, V, size)
        >>> S
        tensor(indices=tensor([[2, 0, 3],
                               [2, 4, 1]]),
               values=tensor([[[-0.6438, -1.6467,  1.4004],
                               [ 0.3411,  0.0918, -0.2312]],

                              [[ 0.5348,  0.0634, -2.0494],
                               [-0.7125, -1.0646,  2.1844]],

                              [[ 0.1276,  0.1874, -0.6334],
                               [-1.9682, -0.5340,  0.7483]]]),
               size=(5, 5, 2, 3), nnz=3, layout=torch.sparse_coo)

        # when sum over only part of sparse_dims, return a SparseTensor
        >>> torch.sparse.sum(S, [1, 3])
        tensor(indices=tensor([[0, 2, 3]]),
               values=tensor([[-1.4512,  0.4073],
                              [-0.8901,  0.2017],
                              [-0.3183, -1.7539]]),
               size=(5, 2), nnz=3, layout=torch.sparse_coo)

        # when sum over all sparse dim, return a dense Tensor
        # with summed dims squeezed
        >>> torch.sparse.sum(S, [0, 1, 3])
        tensor([-2.6596, -1.1450])
    """
    ...

def softmax(input: Tensor, dim: int, dtype: Optional[DType] = ...) -> Tensor:
    r"""Applies a softmax function.

    Softmax is defined as:

    :math:`\text{Softmax}(x_{i}) = \frac{exp(x_i)}{\sum_j exp(x_j)}`

    where :math:`i, j` run over sparse tensor indicies and unspecified
    entries are ignores. This is equivalent to defining unspecifed
    entries as negative infinity so that :max:`exp(x_k) = 0` when the
    entry with index :math:`k` has not specified.

    It is applied to all slices along `dim`, and will re-scale them so
    that the elements lie in the range `[0, 1]` and sum to 1.

    Arguments:
        input (Tensor): input
        dim (int): A dimension along which softmax will be computed.
        dtype (:class:`torch.dtype`, optional): the desired data type
          of returned tensor.  If specified, the input tensor is
          casted to :attr:`dtype` before the operation is
          performed. This is useful for preventing data type
          overflows. Default: None
    """
    ...

def log_softmax(input: Tensor, dim: int, dtype: Optional[DType] = ...) -> Tensor:
    r"""Applies a softmax function followed by logarithm.

    See :class:`~torch.sparse.softmax` for more details.

    Arguments:
        input (Tensor): input
        dim (int): A dimension along which softmax will be computed.
        dtype (:class:`torch.dtype`, optional): the desired data type
          of returned tensor.  If specified, the input tensor is
          casted to :attr:`dtype` before the operation is
          performed. This is useful for preventing data type
          overflows. Default: None
    """
    ...

"""
This type stub file was generated by pyright.
"""

from .onnx import *

"""
This type stub file was generated by pyright.
"""

import torch
from typing import Optional, Tuple
from ._jit_internal import List, _overload as overload, boolean_dispatch

Tensor = torch.Tensor
def broadcast_tensors(*tensors):
    r"""broadcast_tensors(*tensors) -> List of Tensors

    Broadcasts the given tensors according to :ref:`broadcasting-semantics`.

    Args:
        *tensors: any number of tensors of the same type

    .. warning::

        More than one element of a broadcasted tensor may refer to a single
        memory location. As a result, in-place operations (especially ones that
        are vectorized) may result in incorrect behavior. If you need to write
        to the tensors, please clone them first.

    Example::

        >>> x = torch.arange(3).view(1, 3)
        >>> y = torch.arange(2).view(2, 1)
        >>> a, b = torch.broadcast_tensors(x, y)
        >>> a.size()
        torch.Size([2, 3])
        >>> a
        tensor([[0, 1, 2],
                [0, 1, 2]])
    """
    ...

def split(tensor, split_size_or_sections, dim=...):
    r"""Splits the tensor into chunks. Each chunk is a view of the original tensor.

    If :attr:`split_size_or_sections` is an integer type, then :attr:`tensor` will
    be split into equally sized chunks (if possible). Last chunk will be smaller if
    the tensor size along the given dimension :attr:`dim` is not divisible by
    :attr:`split_size`.

    If :attr:`split_size_or_sections` is a list, then :attr:`tensor` will be split
    into ``len(split_size_or_sections)`` chunks with sizes in :attr:`dim` according
    to :attr:`split_size_or_sections`.

    Arguments:
        tensor (Tensor): tensor to split.
        split_size_or_sections (int) or (list(int)): size of a single chunk or
            list of sizes for each chunk
        dim (int): dimension along which to split the tensor.

    Example::
        >>> a = torch.arange(10).reshape(5,2)
        >>> a
        tensor([[0, 1],
                [2, 3],
                [4, 5],
                [6, 7],
                [8, 9]])
        >>> torch.split(a, 2)
        (tensor([[0, 1],
                 [2, 3]]),
         tensor([[4, 5],
                 [6, 7]]),
         tensor([[8, 9]]))
        >>> torch.split(a, [1,4])
        (tensor([[0, 1]]),
         tensor([[2, 3],
                 [4, 5],
                 [6, 7],
                 [8, 9]]))
    """
    ...

def lu_unpack(LU_data: Tensor, LU_pivots: Tensor, unpack_data: bool = ..., unpack_pivots: bool = ...) -> Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor]]:
    r"""Unpacks the data and pivots from a LU factorization of a tensor.

    Returns a tuple of tensors as ``(the pivots, the L tensor, the U tensor)``.

    Arguments:
        LU_data (Tensor): the packed LU factorization data
        LU_pivots (Tensor): the packed LU factorization pivots
        unpack_data (bool): flag indicating if the data should be unpacked
        unpack_pivots (bool): flag indicating if the pivots should be unpacked

    Examples::

        >>> A = torch.randn(2, 3, 3)
        >>> A_LU, pivots = A.lu()
        >>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
        >>>
        >>> # can recover A from factorization
        >>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))

        >>> # LU factorization of a rectangular matrix:
        >>> A = torch.randn(2, 3, 2)
        >>> A_LU, pivots = A.lu()
        >>> P, A_L, A_U = torch.lu_unpack(A_LU, pivots)
        >>> P
        tensor([[[1., 0., 0.],
                 [0., 1., 0.],
                 [0., 0., 1.]],

                [[0., 0., 1.],
                 [0., 1., 0.],
                 [1., 0., 0.]]])
        >>> A_L
        tensor([[[ 1.0000,  0.0000],
                 [ 0.4763,  1.0000],
                 [ 0.3683,  0.1135]],

                [[ 1.0000,  0.0000],
                 [ 0.2957,  1.0000],
                 [-0.9668, -0.3335]]])
        >>> A_U
        tensor([[[ 2.1962,  1.0881],
                 [ 0.0000, -0.8681]],

                [[-1.0947,  0.3736],
                 [ 0.0000,  0.5718]]])
        >>> A_ = torch.bmm(P, torch.bmm(A_L, A_U))
        >>> torch.norm(A_ - A)
        tensor(2.9802e-08)
    """
    ...

def einsum(equation, *operands):
    r"""einsum(equation, *operands) -> Tensor

This function provides a way of computing multilinear expressions (i.e. sums of products) using the
Einstein summation convention.

Args:
    equation (string): The equation is given in terms of lower case letters (indices) to be associated
           with each dimension of the operands and result. The left hand side lists the operands
           dimensions, separated by commas. There should be one index letter per tensor dimension.
           The right hand side follows after `->` and gives the indices for the output.
           If the `->` and right hand side are omitted, it implicitly defined as the alphabetically
           sorted list of all indices appearing exactly once in the left hand side.
           The indices not apprearing in the output are summed over after multiplying the operands
           entries.
           If an index appears several times for the same operand, a diagonal is taken.
           Ellipses `...` represent a fixed number of dimensions. If the right hand side is inferred,
           the ellipsis dimensions are at the beginning of the output.
    operands (Tensor): The operands to compute the Einstein sum of.

.. note::

    This function does not optimize the given expression, so a different formula for the same computation may
    run faster or consume less memory. Projects like opt_einsum (https://optimized-einsum.readthedocs.io/en/stable/)
    can optimize the formula for you.

Examples::

    >>> x = torch.randn(5)
    >>> y = torch.randn(4)
    >>> torch.einsum('i,j->ij', x, y)  # outer product
    tensor([[-0.0570, -0.0286, -0.0231,  0.0197],
            [ 1.2616,  0.6335,  0.5113, -0.4351],
            [ 1.4452,  0.7257,  0.5857, -0.4984],
            [-0.4647, -0.2333, -0.1883,  0.1603],
            [-1.1130, -0.5588, -0.4510,  0.3838]])


    >>> A = torch.randn(3,5,4)
    >>> l = torch.randn(2,5)
    >>> r = torch.randn(2,4)
    >>> torch.einsum('bn,anm,bm->ba', l, A, r) # compare torch.nn.functional.bilinear
    tensor([[-0.3430, -5.2405,  0.4494],
            [ 0.3311,  5.5201, -3.0356]])


    >>> As = torch.randn(3,2,5)
    >>> Bs = torch.randn(3,5,4)
    >>> torch.einsum('bij,bjk->bik', As, Bs) # batch matrix multiplication
    tensor([[[-1.0564, -1.5904,  3.2023,  3.1271],
             [-1.6706, -0.8097, -0.8025, -2.1183]],

            [[ 4.2239,  0.3107, -0.5756, -0.2354],
             [-1.4558, -0.3460,  1.5087, -0.8530]],

            [[ 2.8153,  1.8787, -4.3839, -1.2112],
             [ 0.3728, -2.1131,  0.0921,  0.8305]]])

    >>> A = torch.randn(3, 3)
    >>> torch.einsum('ii->i', A) # diagonal
    tensor([-0.7825,  0.8291, -0.1936])

    >>> A = torch.randn(4, 3, 3)
    >>> torch.einsum('...ii->...i', A) # batch diagonal
    tensor([[-1.0864,  0.7292,  0.0569],
            [-0.9725, -1.0270,  0.6493],
            [ 0.5832, -1.1716, -1.5084],
            [ 0.4041, -1.1690,  0.8570]])

    >>> A = torch.randn(2, 3, 4, 5)
    >>> torch.einsum('...ij->...ji', A).shape # batch permute
    torch.Size([2, 3, 5, 4])
"""
    ...

def meshgrid(*tensors):
    r"""Take :math:`N` tensors, each of which can be either scalar or 1-dimensional
vector, and create :math:`N` N-dimensional grids, where the :math:`i` :sup:`th` grid is defined by
expanding the :math:`i` :sup:`th` input over dimensions defined by other inputs.


    Args:
        tensors (list of Tensor): list of scalars or 1 dimensional tensors. Scalars will be
        treated as tensors of size :math:`(1,)` automatically

    Returns:
        seq (sequence of Tensors): If the input has :math:`k` tensors of size
        :math:`(N_1,), (N_2,), \ldots , (N_k,)`, then the output would also have :math:`k` tensors,
        where all tensors are of size :math:`(N_1, N_2, \ldots , N_k)`.

    Example::

        >>> x = torch.tensor([1, 2, 3])
        >>> y = torch.tensor([4, 5, 6])
        >>> grid_x, grid_y = torch.meshgrid(x, y)
        >>> grid_x
        tensor([[1, 1, 1],
                [2, 2, 2],
                [3, 3, 3]])
        >>> grid_y
        tensor([[4, 5, 6],
                [4, 5, 6],
                [4, 5, 6]])
    """
    ...

def stft(input: Tensor, n_fft: int, hop_length: Optional[int] = ..., win_length: Optional[int] = ..., window: Optional[Tensor] = ..., center: bool = ..., pad_mode: str = ..., normalized: bool = ..., onesided: bool = ...) -> Tensor:
    r"""Short-time Fourier transform (STFT).

    Ignoring the optional batch dimension, this method computes the following
    expression:

    .. math::
        X[m, \omega] = \sum_{k = 0}^{\text{win\_length-1}}%
                            \text{window}[k]\ \text{input}[m \times \text{hop\_length} + k]\ %
                            \exp\left(- j \frac{2 \pi \cdot \omega k}{\text{win\_length}}\right),

    where :math:`m` is the index of the sliding window, and :math:`\omega` is
    the frequency that :math:`0 \leq \omega < \text{n\_fft}`. When
    :attr:`onesided` is the default value ``True``,

    * :attr:`input` must be either a 1-D time sequence or a 2-D batch of time
      sequences.

    * If :attr:`hop_length` is ``None`` (default), it is treated as equal to
      ``floor(n_fft / 4)``.

    * If :attr:`win_length` is ``None`` (default), it is treated as equal to
      :attr:`n_fft`.

    * :attr:`window` can be a 1-D tensor of size :attr:`win_length`, e.g., from
      :meth:`torch.hann_window`. If :attr:`window` is ``None`` (default), it is
      treated as if having :math:`1` everywhere in the window. If
      :math:`\text{win\_length} < \text{n\_fft}`, :attr:`window` will be padded on
      both sides to length :attr:`n_fft` before being applied.

    * If :attr:`center` is ``True`` (default), :attr:`input` will be padded on
      both sides so that the :math:`t`-th frame is centered at time
      :math:`t \times \text{hop\_length}`. Otherwise, the :math:`t`-th frame
      begins at time  :math:`t \times \text{hop\_length}`.

    * :attr:`pad_mode` determines the padding method used on :attr:`input` when
      :attr:`center` is ``True``. See :meth:`torch.nn.functional.pad` for
      all available options. Default is ``"reflect"``.

    * If :attr:`onesided` is ``True`` (default), only values for :math:`\omega`
      in :math:`\left[0, 1, 2, \dots, \left\lfloor \frac{\text{n\_fft}}{2} \right\rfloor + 1\right]`
      are returned because the real-to-complex Fourier transform satisfies the
      conjugate symmetry, i.e., :math:`X[m, \omega] = X[m, \text{n\_fft} - \omega]^*`.

    * If :attr:`normalized` is ``True`` (default is ``False``), the function
      returns the normalized STFT results, i.e., multiplied by :math:`(\text{frame\_length})^{-0.5}`.

    Returns the real and the imaginary parts together as one tensor of size
    :math:`(* \times N \times T \times 2)`, where :math:`*` is the optional
    batch size of :attr:`input`, :math:`N` is the number of frequencies where
    STFT is applied, :math:`T` is the total number of frames used, and each pair
    in the last dimension represents a complex number as the real part and the
    imaginary part.

    .. warning::
      This function changed signature at version 0.4.1. Calling with the
      previous signature may cause error or return incorrect result.

    Arguments:
        input (Tensor): the input tensor
        n_fft (int): size of Fourier transform
        hop_length (int, optional): the distance between neighboring sliding window
            frames. Default: ``None`` (treated as equal to ``floor(n_fft / 4)``)
        win_length (int, optional): the size of window frame and STFT filter.
            Default: ``None``  (treated as equal to :attr:`n_fft`)
        window (Tensor, optional): the optional window function.
            Default: ``None`` (treated as window of all :math:`1` s)
        center (bool, optional): whether to pad :attr:`input` on both sides so
            that the :math:`t`-th frame is centered at time :math:`t \times \text{hop\_length}`.
            Default: ``True``
        pad_mode (string, optional): controls the padding method used when
            :attr:`center` is ``True``. Default: ``"reflect"``
        normalized (bool, optional): controls whether to return the normalized STFT results
             Default: ``False``
        onesided (bool, optional): controls whether to return half of results to
            avoid redundancy Default: ``True``

    Returns:
        Tensor: A tensor containing the STFT result with shape described above

    """
    ...

def istft(input: Tensor, n_fft: int, hop_length: Optional[int] = ..., win_length: Optional[int] = ..., window: Optional[Tensor] = ..., center: bool = ..., normalized: bool = ..., onesided: bool = ..., length: Optional[int] = ...) -> Tensor:
    r"""Inverse short time Fourier Transform. This is expected to be the inverse of :func:`~torch.stft`.
    It has the same parameters (+ additional optional parameter of :attr:`length`) and it should return the
    least squares estimation of the original signal. The algorithm will check using the NOLA condition (
    nonzero overlap).

    Important consideration in the parameters :attr:`window` and :attr:`center` so that the envelop
    created by the summation of all the windows is never zero at certain point in time. Specifically,
    :math:`\sum_{t=-\infty}^{\infty} w^2[n-t\times hop\_length] \cancel{=} 0`.

    Since :func:`~torch.stft` discards elements at the end of the signal if they do not fit in a frame,
    ``istft`` may return a shorter signal than the original signal (can occur if :attr:`center` is False
    since the signal isn't padded).

    If :attr:`center` is ``True``, then there will be padding e.g. ``'constant'``, ``'reflect'``, etc.
    Left padding can be trimmed off exactly because they can be calculated but right padding cannot be
    calculated without additional information.

    Example: Suppose the last window is:
    ``[17, 18, 0, 0, 0]`` vs ``[18, 0, 0, 0, 0]``

    The :attr:`n_fft`, :attr:`hop_length`, :attr:`win_length` are all the same which prevents the calculation
    of right padding. These additional values could be zeros or a reflection of the signal so providing
    :attr:`length` could be useful. If :attr:`length` is ``None`` then padding will be aggressively removed
    (some loss of signal).

    [1] D. W. Griffin and J. S. Lim, "Signal estimation from modified short-time Fourier transform,"
    IEEE Trans. ASSP, vol.32, no.2, pp.236-243, Apr. 1984.

    Arguments:
        input (Tensor): The input tensor. Expected to be output of :func:`~torch.stft`,
            either 3D (``fft_size``, ``n_frame``, 2) or 4D (``channel``, ``fft_size``, ``n_frame``, 2).
        n_fft (int): Size of Fourier transform
        hop_length (Optional[int]): The distance between neighboring sliding window frames.
            (Default: ``n_fft // 4``)
        win_length (Optional[int]): The size of window frame and STFT filter. (Default: ``n_fft``)
        window (Optional[torch.Tensor]): The optional window function.
            (Default: ``torch.ones(win_length)``)
        center (bool): Whether :attr:`input` was padded on both sides so that the :math:`t`-th frame is
            centered at time :math:`t \times \text{hop\_length}`.
            (Default: ``True``)
        normalized (bool): Whether the STFT was normalized. (Default: ``False``)
        onesided (bool): Whether the STFT is onesided. (Default: ``True``)
        length (Optional[int]): The amount to trim the signal by (i.e. the
            original signal length). (Default: whole signal)

    Returns:
        Tensor: Least squares estimation of the original signal of size (..., signal_length)
    """
    ...

_return_inverse_false = boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_return_counts, if_false=_return_output, module_name=__name__, func_name='unique')
_return_inverse_true = boolean_dispatch(arg_name='return_counts', arg_index=3, default=False, if_true=_unique_impl, if_false=_return_inverse, module_name=__name__, func_name='unique')
unique = boolean_dispatch(arg_name='return_inverse', arg_index=2, default=False, if_true=_return_inverse_true, if_false=_return_inverse_false, module_name=__name__, func_name='unique')
_consecutive_return_inverse_false = boolean_dispatch(arg_name='return_counts', arg_index=1, default=False, if_true=_consecutive_return_counts, if_false=_consecutive_return_output, module_name=__name__, func_name='unique_consecutive')
_consecutive_return_inverse_true = boolean_dispatch(arg_name='return_counts', arg_index=1, default=False, if_true=_unique_consecutive_impl, if_false=_consecutive_return_inverse, module_name=__name__, func_name='unique_consecutive')
unique_consecutive = boolean_dispatch(arg_name='return_inverse', arg_index=2, default=False, if_true=_consecutive_return_inverse_true, if_false=_consecutive_return_inverse_false, module_name=__name__, func_name='unique_consecutive')
def tensordot(a, b, dims=...):
    r"""Returns a contraction of a and b over multiple dimensions.

    :attr:`tensordot` implements a generalized matrix product.

    Args:
      a (Tensor): Left tensor to contract
      b (Tensor): Right tensor to contract
      dims (int or tuple of two lists of integers): number of dimensions to
         contract or explicit lists of dimensions for :attr:`a` and
         :attr:`b` respectively

    When called with a non-negative integer argument :attr:`dims` = :math:`d`, and
    the number of dimensions of :attr:`a` and :attr:`b` is :math:`m` and :math:`n`,
    respectively, :func:`~torch.tensordot` computes

    .. math::
        r_{i_0,...,i_{m-d}, i_d,...,i_n}
          = \sum_{k_0,...,k_{d-1}} a_{i_0,...,i_{m-d},k_0,...,k_{d-1}} \times b_{k_0,...,k_{d-1}, i_d,...,i_n}.

    When called with :attr:`dims` of the list form, the given dimensions will be contracted
    in place of the last :math:`d` of :attr:`a` and the first :math:`d` of :math:`b`. The sizes
    in these dimensions must match, but :func:`~torch.tensordot` will deal with broadcasted
    dimensions.

    Examples::

        >>> a = torch.arange(60.).reshape(3, 4, 5)
        >>> b = torch.arange(24.).reshape(4, 3, 2)
        >>> torch.tensordot(a, b, dims=([1, 0], [0, 1]))
        tensor([[4400., 4730.],
                [4532., 4874.],
                [4664., 5018.],
                [4796., 5162.],
                [4928., 5306.]])

        >>> a = torch.randn(3, 4, 5, device='cuda')
        >>> b = torch.randn(4, 5, 6, device='cuda')
        >>> c = torch.tensordot(a, b, dims=2).cpu()
        tensor([[ 8.3504, -2.5436,  6.2922,  2.7556, -1.0732,  3.2741],
                [ 3.3161,  0.0704,  5.0187, -0.4079, -4.3126,  4.8744],
                [ 0.8223,  3.9445,  3.2168, -0.2400,  3.4117,  1.7780]])

    """
    ...

def cartesian_prod(*tensors):
    """Do cartesian product of the given sequence of tensors. The behavior is similar to
    python's `itertools.product`.

    Arguments:
        *tensors: any number of 1 dimensional tensors.

    Returns:
        Tensor: A tensor equivalent to converting all the input tensors into lists,
            do `itertools.product` on these lists, and finally convert the resulting list
            into tensor.

    Example::

        >>> a = [1, 2, 3]
        >>> b = [4, 5]
        >>> list(itertools.product(a, b))
        [(1, 4), (1, 5), (2, 4), (2, 5), (3, 4), (3, 5)]
        >>> tensor_a = torch.tensor(a)
        >>> tensor_b = torch.tensor(b)
        >>> torch.cartesian_prod(tensor_a, tensor_b)
        tensor([[1, 4],
                [1, 5],
                [2, 4],
                [2, 5],
                [3, 4],
                [3, 5]])
    """
    ...

def block_diag(*tensors):
    """Create a block diagonal matrix from provided tensors.

    Arguments:
        *tensors: One or more tensors with 0, 1, or 2 dimensions.

    Returns:
        Tensor: A 2 dimensional tensor with all the input tensors arranged in
            order such that their upper left and lower right corners are
            diagonally adjacent. All other elements are set to 0.

    Example::

        >>> import torch
        >>> A = torch.tensor([[0, 1], [1, 0]])
        >>> B = torch.tensor([[3, 4, 5], [6, 7, 8]])
        >>> C = torch.tensor(7)
        >>> D = torch.tensor([1, 2, 3])
        >>> E = torch.tensor([[4], [5], [6]])
        >>> torch.block_diag(A, B, C, D, E)
        tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0],
                [1, 0, 0, 0, 0, 0, 0, 0, 0, 0],
                [0, 0, 3, 4, 5, 0, 0, 0, 0, 0],
                [0, 0, 6, 7, 8, 0, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 7, 0, 0, 0, 0],
                [0, 0, 0, 0, 0, 0, 1, 2, 3, 0],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 4],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 5],
                [0, 0, 0, 0, 0, 0, 0, 0, 0, 6]])
    """
    ...

def cdist(x1: Tensor, x2: Tensor, p: float = ..., compute_mode: str = ...) -> Tensor:
    r"""Computes batched the p-norm distance between each pair of the two collections of row vectors.

    Args:
        x1 (Tensor): input tensor of shape :math:`B \times P \times M`.
        x2 (Tensor): input tensor of shape :math:`B \times R \times M`.
        p: p value for the p-norm distance to calculate between each vector pair
            :math:`\in [0, \infty]`.
        compute_mode:
            'use_mm_for_euclid_dist_if_necessary' - will use matrix multiplication approach to calculate
            euclidean distance (p = 2) if P > 25 or R > 25
            'use_mm_for_euclid_dist' - will always use matrix multiplication approach to calculate
            euclidean distance (p = 2)
            'donot_use_mm_for_euclid_dist' - will never use matrix multiplication approach to calculate
            euclidean distance (p = 2)
            Default: use_mm_for_euclid_dist_if_necessary.

    If x1 has shape :math:`B \times P \times M` and x2 has shape :math:`B \times R \times M` then the
    output will have shape :math:`B \times P \times R`.

    This function is equivalent to `scipy.spatial.distance.cdist(input,'minkowski', p=p)`
    if :math:`p \in (0, \infty)`. When :math:`p = 0` it is equivalent to
    `scipy.spatial.distance.cdist(input, 'hamming') * M`. When :math:`p = \infty`, the closest
    scipy function is `scipy.spatial.distance.cdist(xn, lambda x, y: np.abs(x - y).max())`.

    Example:

        >>> a = torch.tensor([[0.9041,  0.0196], [-0.3108, -2.4423], [-0.4821,  1.059]])
        >>> a
        tensor([[ 0.9041,  0.0196],
                [-0.3108, -2.4423],
                [-0.4821,  1.0590]])
        >>> b = torch.tensor([[-2.1763, -0.4713], [-0.6986,  1.3702]])
        >>> b
        tensor([[-2.1763, -0.4713],
                [-0.6986,  1.3702]])
        >>> torch.cdist(a, b, p=2)
        tensor([[3.1193, 2.0959],
                [2.7138, 3.8322],
                [2.2830, 0.3791]])
    """
    ...

@overload
def norm(input: Tensor, p: str = ..., dim: Optional[List[int]] = ..., keepdim: bool = ..., out: Optional[Tensor] = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

@overload
def norm(input: Tensor, p: Optional[number] = ..., dim: Optional[List[int]] = ..., keepdim: bool = ..., out: Optional[Tensor] = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

@overload
def norm(input: Tensor, p: Optional[number] = ..., dim: Optional[int] = ..., keepdim: bool = ..., out: Optional[Tensor] = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

@overload
def norm(input: Tensor, p: str = ..., dim: Optional[int] = ..., keepdim: bool = ..., out: Optional[Tensor] = ..., dtype: Optional[int] = ...) -> Tensor:
    ...

def norm(input, p=..., dim=..., keepdim=..., out=..., dtype=...):
    r"""Returns the matrix norm or vector norm of a given tensor.

    Args:
        input (Tensor): the input tensor
        p (int, float, inf, -inf, 'fro', 'nuc', optional): the order of norm. Default: ``'fro'``
            The following norms can be calculated:

            =====  ============================  ==========================
            ord    matrix norm                   vector norm
            =====  ============================  ==========================
            None   Frobenius norm                2-norm
            'fro'  Frobenius norm                --
            'nuc'  nuclear norm                  --
            Other  as vec norm when dim is None  sum(abs(x)**ord)**(1./ord)
            =====  ============================  ==========================

        dim (int, 2-tuple of ints, 2-list of ints, optional): If it is an int,
            vector norm will be calculated, if it is 2-tuple of ints, matrix norm
            will be calculated. If the value is None, matrix norm will be calculated
            when the input tensor only has two dimensions, vector norm will be
            calculated when the input tensor only has one dimension. If the input
            tensor has more than two dimensions, the vector norm will be applied to
            last dimension.
        keepdim (bool, optional): whether the output tensors have :attr:`dim`
            retained or not. Ignored if :attr:`dim` = ``None`` and
            :attr:`out` = ``None``. Default: ``False``
        out (Tensor, optional): the output tensor. Ignored if
            :attr:`dim` = ``None`` and :attr:`out` = ``None``.
        dtype (:class:`torch.dtype`, optional): the desired data type of
            returned tensor. If specified, the input tensor is casted to
            :attr:'dtype' while performing the operation. Default: None.


    Example::

        >>> import torch
        >>> a = torch.arange(9, dtype= torch.float) - 4
        >>> b = a.reshape((3, 3))
        >>> torch.norm(a)
        tensor(7.7460)
        >>> torch.norm(b)
        tensor(7.7460)
        >>> torch.norm(a, float('inf'))
        tensor(4.)
        >>> torch.norm(b, float('inf'))
        tensor(4.)
        >>> c = torch.tensor([[ 1, 2, 3],[-1, 1, 4]] , dtype= torch.float)
        >>> torch.norm(c, dim=0)
        tensor([1.4142, 2.2361, 5.0000])
        >>> torch.norm(c, dim=1)
        tensor([3.7417, 4.2426])
        >>> torch.norm(c, p=1, dim=1)
        tensor([6., 6.])
        >>> d = torch.arange(8, dtype= torch.float).reshape(2,2,2)
        >>> torch.norm(d, dim=(1,2))
        tensor([ 3.7417, 11.2250])
        >>> torch.norm(d[0, :, :]), torch.norm(d[1, :, :])
        (tensor(3.7417), tensor(11.2250))
    """
    ...

def chain_matmul(*matrices):
    r"""Returns the matrix product of the :math:`N` 2-D tensors. This product is efficiently computed
    using the matrix chain order algorithm which selects the order in which incurs the lowest cost in terms
    of arithmetic operations (`[CLRS]`_). Note that since this is a function to compute the product, :math:`N`
    needs to be greater than or equal to 2; if equal to 2 then a trivial matrix-matrix product is returned.
    If :math:`N` is 1, then this is a no-op - the original matrix is returned as is.


    Args:
        matrices (Tensors...): a sequence of 2 or more 2-D tensors whose product is to be determined.


    Returns:
        Tensor: if the :math:`i^{th}` tensor was of dimensions :math:`p_{i} \times p_{i + 1}`, then the product
        would be of dimensions :math:`p_{1} \times p_{N + 1}`.

    Example::

        >>> a = torch.randn(3, 4)
        >>> b = torch.randn(4, 5)
        >>> c = torch.randn(5, 6)
        >>> d = torch.randn(6, 7)
        >>> torch.chain_matmul(a, b, c, d)
        tensor([[ -2.3375,  -3.9790,  -4.1119,  -6.6577,   9.5609, -11.5095,  -3.2614],
                [ 21.4038,   3.3378,  -8.4982,  -5.2457, -10.2561,  -2.4684,   2.7163],
                [ -0.9647,  -5.8917,  -2.3213,  -5.2284,  12.8615, -12.2816,  -2.5095]])

    .. _`[CLRS]`: https://mitpress.mit.edu/books/introduction-algorithms-third-edition
    """
    ...

lu = boolean_dispatch(arg_name='get_infos', arg_index=2, default=False, if_true=_lu_with_infos, if_false=_lu_no_infos, module_name=__name__, func_name='lu')
def align_tensors(*tensors):
    ...

"""
This type stub file was generated by pyright.
"""

import types

class _ClassNamespace(types.ModuleType):
    def __init__(self, name) -> None:
        ...
    
    def __getattr__(self, attr):
        ...
    


class _Classes(types.ModuleType):
    def __init__(self) -> None:
        ...
    
    def __getattr__(self, name):
        ...
    
    @property
    def loaded_libraries(self):
        ...
    
    def load_library(self, path):
        """
        Loads a shared library from the given path into the current process.

        The library being loaded may run global initialization code to register
        custom classes with the PyTorch JIT runtime. This allows dynamically
        loading custom classes. For this, you should compile your class
        and the static registration code into a shared library object, and then
        call ``torch.classes.load_library('path/to/libcustom.so')`` to load the
        shared object.

        After the library is loaded, it is added to the
        ``torch.classes.loaded_libraries`` attribute, a set that may be inspected
        for the paths of all libraries loaded using this function.

        Arguments:
            path (str): A path to a shared library to load.
        """
        ...
    


classes = _Classes()
"""
This type stub file was generated by pyright.
"""

import os

if os.path.basename(os.path.dirname(__file__)) == 'shared':
    torch_parent = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
else:
    torch_parent = os.path.dirname(os.path.dirname(__file__))
def get_file_path(*path_components):
    ...

def get_file_path_2(*path_components):
    ...

def get_writable_path(path):
    ...

def prepare_multiprocessing_environment(path):
    ...

def resolve_library_path(path):
    ...

def get_source_lines_and_file(obj, error_msg=...):
    """
    Wrapper around inspect.getsourcelines and inspect.getsourcefile.

    Returns: (sourcelines, file_lino, filename)
    """
    ...

TEST_MASTER_ADDR = '127.0.0.1'
TEST_MASTER_PORT = 29500
USE_GLOBAL_DEPS = True
USE_RTLD_GLOBAL_WITH_LIBTORCH = False
"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

class HalfNormal(TransformedDistribution):
    r"""
    Creates a half-normal distribution parameterized by `scale` where::

        X ~ Normal(0, scale)
        Y = |X| ~ HalfNormal(scale)

    Example::

        >>> m = HalfNormal(torch.tensor([1.0]))
        >>> m.sample()  # half-normal distributed with scale=1
        tensor([ 0.1046])

    Args:
        scale (float or Tensor): scale of the full Normal distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def scale(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, prob):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily

class Normal(ExponentialFamily):
    r"""
    Creates a normal (also called Gaussian) distribution parameterized by
    :attr:`loc` and :attr:`scale`.

    Example::

        >>> m = Normal(torch.tensor([0.0]), torch.tensor([1.0]))
        >>> m.sample()  # normally distributed with loc=0 and scale=1
        tensor([ 0.1046])

    Args:
        loc (float or Tensor): mean of the distribution (often referred to as mu)
        scale (float or Tensor): standard deviation of the distribution
            (often referred to as sigma)
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    _mean_carrier_measure = ...
    @property
    def mean(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class FisherSnedecor(Distribution):
    r"""
    Creates a Fisher-Snedecor distribution parameterized by :attr:`df1` and :attr:`df2`.

    Example::

        >>> m = FisherSnedecor(torch.tensor([1.0]), torch.tensor([2.0]))
        >>> m.sample()  # Fisher-Snedecor-distributed with df1=1 and df2=2
        tensor([ 0.2453])

    Args:
        df1 (float or Tensor): degrees of freedom parameter 1
        df2 (float or Tensor): degrees of freedom parameter 2
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, df1, df2, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class Geometric(Distribution):
    r"""
    Creates a Geometric distribution parameterized by :attr:`probs`,
    where :attr:`probs` is the probability of success of Bernoulli trials.
    It represents the probability that in :math:`k + 1` Bernoulli trials, the
    first :math:`k` trials failed, before seeing a success.

    Samples are non-negative integers [0, :math:`\inf`).

    Example::

        >>> m = Geometric(torch.tensor([0.3]))
        >>> m.sample()  # underlying Bernoulli has 30% chance 1; 70% chance 0
        tensor([ 2.])

    Args:
        probs (Number, Tensor): the probability of sampling `1`. Must be in range (0, 1]
        logits (Number, Tensor): the log-odds of sampling `1`.
    """
    arg_constraints = ...
    support = ...
    def __init__(self, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class Categorical(Distribution):
    r"""
    Creates a categorical distribution parameterized by either :attr:`probs` or
    :attr:`logits` (but not both).

    .. note::
        It is equivalent to the distribution that :func:`torch.multinomial`
        samples from.

    Samples are integers from :math:`\{0, \ldots, K-1\}` where `K` is ``probs.size(-1)``.

    If :attr:`probs` is 1D with length-`K`, each element is the relative
    probability of sampling the class at that index.

    If :attr:`probs` is 2D, it is treated as a batch of relative probability
    vectors.

    .. note:: :attr:`probs` must be non-negative, finite and have a non-zero sum,
              and it will be normalized to sum to 1.

    See also: :func:`torch.multinomial`

    Example::

        >>> m = Categorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
        >>> m.sample()  # equal probability of 0, 1, 2, 3
        tensor(3)

    Args:
        probs (Tensor): event probabilities
        logits (Tensor): event log-odds
    """
    arg_constraints = ...
    has_enumerate_support = ...
    def __init__(self, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    
    def enumerate_support(self, expand=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class LowRankMultivariateNormal(Distribution):
    r"""
    Creates a multivariate normal distribution with covariance matrix having a low-rank form
    parameterized by :attr:`cov_factor` and :attr:`cov_diag`::

        covariance_matrix = cov_factor @ cov_factor.T + cov_diag

    Example:

        >>> m = LowRankMultivariateNormal(torch.zeros(2), torch.tensor([[1.], [0.]]), torch.ones(2))
        >>> m.sample()  # normally distributed with mean=`[0,0]`, cov_factor=`[[1],[0]]`, cov_diag=`[1,1]`
        tensor([-0.2102, -0.5429])

    Args:
        loc (Tensor): mean of the distribution with shape `batch_shape + event_shape`
        cov_factor (Tensor): factor part of low-rank form of covariance matrix with shape
            `batch_shape + event_shape + (rank,)`
        cov_diag (Tensor): diagonal part of low-rank form of covariance matrix with shape
            `batch_shape + event_shape`

    Note:
        The computation for determinant and inverse of covariance matrix is avoided when
        `cov_factor.shape[1] << cov_factor.shape[0]` thanks to `Woodbury matrix identity
        <https://en.wikipedia.org/wiki/Woodbury_matrix_identity>`_ and
        `matrix determinant lemma <https://en.wikipedia.org/wiki/Matrix_determinant_lemma>`_.
        Thanks to these formulas, we just need to compute the determinant and inverse of
        the small size "capacitance" matrix::

            capacitance = I + cov_factor.T @ inv(cov_diag) @ cov_factor
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, cov_factor, cov_diag, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @lazy_property
    def variance(self):
        ...
    
    @lazy_property
    def scale_tril(self):
        ...
    
    @lazy_property
    def covariance_matrix(self):
        ...
    
    @lazy_property
    def precision_matrix(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from functools import total_ordering

_KL_REGISTRY = {  }
_KL_MEMOIZE = {  }
def register_kl(type_p, type_q):
    """
    Decorator to register a pairwise function with :meth:`kl_divergence`.
    Usage::

        @register_kl(Normal, Normal)
        def kl_normal_normal(p, q):
            # insert implementation here

    Lookup returns the most specific (type,type) match ordered by subclass. If
    the match is ambiguous, a `RuntimeWarning` is raised. For example to
    resolve the ambiguous situation::

        @register_kl(BaseP, DerivedQ)
        def kl_version1(p, q): ...
        @register_kl(DerivedP, BaseQ)
        def kl_version2(p, q): ...

    you should register a third most-specific implementation, e.g.::

        register_kl(DerivedP, DerivedQ)(kl_version1)  # Break the tie.

    Args:
        type_p (type): A subclass of :class:`~torch.distributions.Distribution`.
        type_q (type): A subclass of :class:`~torch.distributions.Distribution`.
    """
    ...

@total_ordering
class _Match(object):
    __slots__ = ...
    def __init__(self, *types) -> None:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def __le__(self, other) -> bool:
        ...
    


def kl_divergence(p, q):
    r"""
    Compute Kullback-Leibler divergence :math:`KL(p \| q)` between two distributions.

    .. math::

        KL(p \| q) = \int p(x) \log\frac {p(x)} {q(x)} \,dx

    Args:
        p (Distribution): A :class:`~torch.distributions.Distribution` object.
        q (Distribution): A :class:`~torch.distributions.Distribution` object.

    Returns:
        Tensor: A batch of KL divergences of shape `batch_shape`.

    Raises:
        NotImplementedError: If the distribution types have not been registered via
            :meth:`register_kl`.
    """
    ...

_euler_gamma = 0.5772156649015329
"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.transformed_distribution import TransformedDistribution

class ExpRelaxedCategorical(Distribution):
    r"""
    Creates a ExpRelaxedCategorical parameterized by
    :attr:`temperature`, and either :attr:`probs` or :attr:`logits` (but not both).
    Returns the log of a point in the simplex. Based on the interface to
    :class:`OneHotCategorical`.

    Implementation based on [1].

    See also: :func:`torch.distributions.OneHotCategorical`

    Args:
        temperature (Tensor): relaxation temperature
        probs (Tensor): event probabilities
        logits (Tensor): the log probability of each event.

    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random Variables
    (Maddison et al, 2017)

    [2] Categorical Reparametrization with Gumbel-Softmax
    (Jang et al, 2017)
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, temperature, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def param_shape(self):
        ...
    
    @property
    def logits(self):
        ...
    
    @property
    def probs(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


class RelaxedOneHotCategorical(TransformedDistribution):
    r"""
    Creates a RelaxedOneHotCategorical distribution parametrized by
    :attr:`temperature`, and either :attr:`probs` or :attr:`logits`.
    This is a relaxed version of the :class:`OneHotCategorical` distribution, so
    its samples are on simplex, and are reparametrizable.

    Example::

        >>> m = RelaxedOneHotCategorical(torch.tensor([2.2]),
                                         torch.tensor([0.1, 0.2, 0.3, 0.4]))
        >>> m.sample()
        tensor([ 0.1294,  0.2324,  0.3859,  0.2523])

    Args:
        temperature (Tensor): relaxation temperature
        probs (Tensor): event probabilities
        logits (Tensor): the log probability of each event.
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, temperature, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def temperature(self):
        ...
    
    @property
    def logits(self):
        ...
    
    @property
    def probs(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from .bernoulli import Bernoulli
from .beta import Beta
from .binomial import Binomial
from .categorical import Categorical
from .cauchy import Cauchy
from .chi2 import Chi2
from .constraint_registry import biject_to, transform_to
from .continuous_bernoulli import ContinuousBernoulli
from .dirichlet import Dirichlet
from .distribution import Distribution
from .exp_family import ExponentialFamily
from .exponential import Exponential
from .fishersnedecor import FisherSnedecor
from .gamma import Gamma
from .geometric import Geometric
from .gumbel import Gumbel
from .half_cauchy import HalfCauchy
from .half_normal import HalfNormal
from .independent import Independent
from .kl import kl_divergence, register_kl
from .laplace import Laplace
from .log_normal import LogNormal
from .logistic_normal import LogisticNormal
from .lowrank_multivariate_normal import LowRankMultivariateNormal
from .mixture_same_family import MixtureSameFamily
from .multinomial import Multinomial
from .multivariate_normal import MultivariateNormal
from .negative_binomial import NegativeBinomial
from .normal import Normal
from .one_hot_categorical import OneHotCategorical
from .pareto import Pareto
from .poisson import Poisson
from .relaxed_bernoulli import RelaxedBernoulli
from .relaxed_categorical import RelaxedOneHotCategorical
from .studentT import StudentT
from .transformed_distribution import TransformedDistribution
from .transforms import *
from .uniform import Uniform
from .von_mises import VonMises
from .weibull import Weibull

r"""
The ``distributions`` package contains parameterizable probability distributions
and sampling functions. This allows the construction of stochastic computation
graphs and stochastic gradient estimators for optimization. This package
generally follows the design of the `TensorFlow Distributions`_ package.

.. _`TensorFlow Distributions`:
    https://arxiv.org/abs/1711.10604

It is not possible to directly backpropagate through random samples. However,
there are two main methods for creating surrogate functions that can be
backpropagated through. These are the score function estimator/likelihood ratio
estimator/REINFORCE and the pathwise derivative estimator. REINFORCE is commonly
seen as the basis for policy gradient methods in reinforcement learning, and the
pathwise derivative estimator is commonly seen in the reparameterization trick
in variational autoencoders. Whilst the score function only requires the value
of samples :math:`f(x)`, the pathwise derivative requires the derivative
:math:`f'(x)`. The next sections discuss these two in a reinforcement learning
example. For more details see
`Gradient Estimation Using Stochastic Computation Graphs`_ .

.. _`Gradient Estimation Using Stochastic Computation Graphs`:
     https://arxiv.org/abs/1506.05254

Score function
^^^^^^^^^^^^^^

When the probability density function is differentiable with respect to its
parameters, we only need :meth:`~torch.distributions.Distribution.sample` and
:meth:`~torch.distributions.Distribution.log_prob` to implement REINFORCE:

.. math::

    \Delta\theta  = \alpha r \frac{\partial\log p(a|\pi^\theta(s))}{\partial\theta}

where :math:`\theta` are the parameters, :math:`\alpha` is the learning rate,
:math:`r` is the reward and :math:`p(a|\pi^\theta(s))` is the probability of
taking action :math:`a` in state :math:`s` given policy :math:`\pi^\theta`.

In practice we would sample an action from the output of a network, apply this
action in an environment, and then use ``log_prob`` to construct an equivalent
loss function. Note that we use a negative because optimizers use gradient
descent, whilst the rule above assumes gradient ascent. With a categorical
policy, the code for implementing REINFORCE would be as follows::

    probs = policy_network(state)
    # Note that this is equivalent to what used to be called multinomial
    m = Categorical(probs)
    action = m.sample()
    next_state, reward = env.step(action)
    loss = -m.log_prob(action) * reward
    loss.backward()

Pathwise derivative
^^^^^^^^^^^^^^^^^^^

The other way to implement these stochastic/policy gradients would be to use the
reparameterization trick from the
:meth:`~torch.distributions.Distribution.rsample` method, where the
parameterized random variable can be constructed via a parameterized
deterministic function of a parameter-free random variable. The reparameterized
sample therefore becomes differentiable. The code for implementing the pathwise
derivative would be as follows::

    params = policy_network(state)
    m = Normal(*params)
    # Any distribution with .has_rsample == True could work based on the application
    action = m.rsample()
    next_state, reward = env.step(action)  # Assuming that reward is differentiable
    loss = -reward
    loss.backward()
"""
"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.distribution import Distribution

class TransformedDistribution(Distribution):
    r"""
    Extension of the Distribution class, which applies a sequence of Transforms
    to a base distribution.  Let f be the composition of transforms applied::

        X ~ BaseDistribution
        Y = f(X) ~ TransformedDistribution(BaseDistribution, f)
        log p(Y) = log p(X) + log |det (dX/dY)|

    Note that the ``.event_shape`` of a :class:`TransformedDistribution` is the
    maximum shape of its base distribution and its transforms, since transforms
    can introduce correlations among events.

    An example for the usage of :class:`TransformedDistribution` would be::

        # Building a Logistic Distribution
        # X ~ Uniform(0, 1)
        # f = a + b * logit(X)
        # Y ~ f(X) ~ Logistic(a, b)
        base_distribution = Uniform(0, 1)
        transforms = [SigmoidTransform().inv, AffineTransform(loc=a, scale=b)]
        logistic = TransformedDistribution(base_distribution, transforms)

    For more examples, please look at the implementations of
    :class:`~torch.distributions.gumbel.Gumbel`,
    :class:`~torch.distributions.half_cauchy.HalfCauchy`,
    :class:`~torch.distributions.half_normal.HalfNormal`,
    :class:`~torch.distributions.log_normal.LogNormal`,
    :class:`~torch.distributions.pareto.Pareto`,
    :class:`~torch.distributions.weibull.Weibull`,
    :class:`~torch.distributions.relaxed_bernoulli.RelaxedBernoulli` and
    :class:`~torch.distributions.relaxed_categorical.RelaxedOneHotCategorical`
    """
    arg_constraints = ...
    def __init__(self, base_distribution, transforms, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @property
    def has_rsample(self):
        ...
    
    def sample(self, sample_shape=...):
        """
        Generates a sample_shape shaped sample or sample_shape shaped batch of
        samples if the distribution parameters are batched. Samples first from
        base distribution and applies `transform()` for every transform in the
        list.
        """
        ...
    
    def rsample(self, sample_shape=...):
        """
        Generates a sample_shape shaped reparameterized sample or sample_shape
        shaped batch of reparameterized samples if the distribution parameters
        are batched. Samples first from base distribution and applies
        `transform()` for every transform in the list.
        """
        ...
    
    def log_prob(self, value):
        """
        Scores the sample by inverting the transform(s) and computing the score
        using the score of the base distribution and the log abs det jacobian.
        """
        ...
    
    def cdf(self, value):
        """
        Computes the cumulative distribution function by inverting the
        transform(s) and computing the score of the base distribution.
        """
        ...
    
    def icdf(self, value):
        """
        Computes the inverse cumulative distribution function using
        transform(s) and computing the score of the base distribution.
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily

class Gamma(ExponentialFamily):
    r"""
    Creates a Gamma distribution parameterized by shape :attr:`concentration` and :attr:`rate`.

    Example::

        >>> m = Gamma(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample()  # Gamma distributed with concentration=1 and rate=1
        tensor([ 0.1046])

    Args:
        concentration (float or Tensor): shape parameter of the distribution
            (often referred to as alpha)
        rate (float or Tensor): rate = 1 / scale of the distribution
            (often referred to as beta)
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    _mean_carrier_measure = ...
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, concentration, rate, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily
from torch.distributions.utils import lazy_property

class ContinuousBernoulli(ExponentialFamily):
    r"""
    Creates a continuous Bernoulli distribution parameterized by :attr:`probs`
    or :attr:`logits` (but not both).

    The distribution is supported in [0, 1] and parameterized by 'probs' (in
    (0,1)) or 'logits' (real-valued). Note that, unlike the Bernoulli, 'probs'
    does not correspond to a probability and 'logits' does not correspond to
    log-odds, but the same names are used due to the similarity with the
    Bernoulli. See [1] for more details.

    Example::

        >>> m = ContinuousBernoulli(torch.tensor([0.3]))
        >>> m.sample()
        tensor([ 0.2538])

    Args:
        probs (Number, Tensor): (0,1) valued parameters
        logits (Number, Tensor): real valued parameters whose sigmoid matches 'probs'

    [1] The continuous Bernoulli: fixing a pervasive error in variational
    autoencoders, Loaiza-Ganem G and Cunningham JP, NeurIPS 2019.
    https://arxiv.org/abs/1907.06845
    """
    arg_constraints = ...
    support = ...
    _mean_carrier_measure = ...
    has_rsample = ...
    def __init__(self, probs=..., logits=..., lims=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.transformed_distribution import TransformedDistribution
from torch.distributions.utils import lazy_property

class LogitRelaxedBernoulli(Distribution):
    r"""
    Creates a LogitRelaxedBernoulli distribution parameterized by :attr:`probs`
    or :attr:`logits` (but not both), which is the logit of a RelaxedBernoulli
    distribution.

    Samples are logits of values in (0, 1). See [1] for more details.

    Args:
        temperature (Tensor): relaxation temperature
        probs (Number, Tensor): the probability of sampling `1`
        logits (Number, Tensor): the log-odds of sampling `1`

    [1] The Concrete Distribution: A Continuous Relaxation of Discrete Random
    Variables (Maddison et al, 2017)

    [2] Categorical Reparametrization with Gumbel-Softmax
    (Jang et al, 2017)
    """
    arg_constraints = ...
    support = ...
    def __init__(self, temperature, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


class RelaxedBernoulli(TransformedDistribution):
    r"""
    Creates a RelaxedBernoulli distribution, parametrized by
    :attr:`temperature`, and either :attr:`probs` or :attr:`logits`
    (but not both). This is a relaxed version of the `Bernoulli` distribution,
    so the values are in (0, 1), and has reparametrizable samples.

    Example::

        >>> m = RelaxedBernoulli(torch.tensor([2.2]),
                                 torch.tensor([0.1, 0.2, 0.3, 0.99]))
        >>> m.sample()
        tensor([ 0.2951,  0.3442,  0.8918,  0.9021])

    Args:
        temperature (Tensor): relaxation temperature
        probs (Number, Tensor): the probability of sampling `1`
        logits (Number, Tensor): the log-odds of sampling `1`
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, temperature, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def temperature(self):
        ...
    
    @property
    def logits(self):
        ...
    
    @property
    def probs(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.distribution import Distribution

class Uniform(Distribution):
    r"""
    Generates uniformly distributed random samples from the half-open interval
    ``[low, high)``.

    Example::

        >>> m = Uniform(torch.tensor([0.0]), torch.tensor([5.0]))
        >>> m.sample()  # uniformly distributed in the range [0.0, 5.0)
        tensor([ 2.3418])

    Args:
        low (float or Tensor): lower range (inclusive).
        high (float or Tensor): upper range (exclusive).
    """
    arg_constraints = ...
    has_rsample = ...
    @property
    def mean(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, low, high, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

import torch
from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

_I0_COEF_SMALL = [1, 3.5156229, 3.0899424, 1.2067492, 0.2659732, 0.0360768, 0.0045813]
_I0_COEF_LARGE = [0.39894228, 0.01328592, 0.00225319, - 0.00157565, 0.00916281, - 0.02057706, 0.02635537, - 0.01647633, 0.00392377]
_I1_COEF_SMALL = [0.5, 0.87890594, 0.51498869, 0.15084934, 0.02658733, 0.00301532, 0.00032411]
_I1_COEF_LARGE = [0.39894228, - 0.03988024, - 0.00362018, 0.00163801, - 0.01031555, 0.02282967, - 0.02895312, 0.01787654, - 0.00420059]
_COEF_SMALL = [_I0_COEF_SMALL, _I1_COEF_SMALL]
_COEF_LARGE = [_I0_COEF_LARGE, _I1_COEF_LARGE]
class VonMises(Distribution):
    """
    A circular von Mises distribution.

    This implementation uses polar coordinates. The ``loc`` and ``value`` args
    can be any real number (to facilitate unconstrained optimization), but are
    interpreted as angles modulo 2 pi.

    Example::
        >>> m = dist.VonMises(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample() # von Mises distributed with loc=1 and concentration=1
        tensor([1.9777])

    :param torch.Tensor loc: an angle in radians.
    :param torch.Tensor concentration: concentration parameter
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, concentration, validate_args=...) -> None:
        ...
    
    def log_prob(self, value):
        ...
    
    @torch.no_grad()
    def sample(self, sample_shape=...):
        """
        The sampling algorithm for the von Mises distribution is based on the following paper:
        Best, D. J., and Nicholas I. Fisher.
        "Efficient simulation of the von Mises distribution." Applied Statistics (1979): 152-157.
        """
        ...
    
    def expand(self, batch_shape):
        ...
    
    @property
    def mean(self):
        """
        The provided mean is the circular one.
        """
        ...
    
    @lazy_property
    def variance(self):
        """
        The provided variance is the circular one.
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class OneHotCategorical(Distribution):
    r"""
    Creates a one-hot categorical distribution parameterized by :attr:`probs` or
    :attr:`logits`.

    Samples are one-hot coded vectors of size ``probs.size(-1)``.

    .. note:: :attr:`probs` must be non-negative, finite and have a non-zero sum,
              and it will be normalized to sum to 1.

    See also: :func:`torch.distributions.Categorical` for specifications of
    :attr:`probs` and :attr:`logits`.

    Example::

        >>> m = OneHotCategorical(torch.tensor([ 0.25, 0.25, 0.25, 0.25 ]))
        >>> m.sample()  # equal probability of 0, 1, 2, 3
        tensor([ 0.,  0.,  0.,  1.])

    Args:
        probs (Tensor): event probabilities
        logits (Tensor): event log probabilities
    """
    arg_constraints = ...
    support = ...
    has_enumerate_support = ...
    def __init__(self, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def probs(self):
        ...
    
    @property
    def logits(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    
    def enumerate_support(self, expand=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.transformed_distribution import TransformedDistribution

class Pareto(TransformedDistribution):
    r"""
    Samples from a Pareto Type 1 distribution.

    Example::

        >>> m = Pareto(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample()  # sample from a Pareto distribution with scale=1 and alpha=1
        tensor([ 1.5623])

    Args:
        scale (float or Tensor): Scale parameter of the distribution
        alpha (float or Tensor): Shape parameter of the distribution
    """
    arg_constraints = ...
    def __init__(self, scale, alpha, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

class Distribution(object):
    r"""
    Distribution is the abstract base class for probability distributions.
    """
    has_rsample = ...
    has_enumerate_support = ...
    _validate_args = ...
    support = ...
    arg_constraints = ...
    @staticmethod
    def set_default_validate_args(value):
        ...
    
    def __init__(self, batch_shape=..., event_shape=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        """
        Returns a new distribution instance (or populates an existing instance
        provided by a derived class) with batch dimensions expanded to
        `batch_shape`. This method calls :class:`~torch.Tensor.expand` on
        the distribution's parameters. As such, this does not allocate new
        memory for the expanded distribution instance. Additionally,
        this does not repeat any args checking or parameter broadcasting in
        `__init__.py`, when an instance is first created.

        Args:
            batch_shape (torch.Size): the desired expanded size.
            _instance: new instance provided by subclasses that
                need to override `.expand`.

        Returns:
            New distribution instance with batch dimensions expanded to
            `batch_size`.
        """
        ...
    
    @property
    def batch_shape(self):
        """
        Returns the shape over which parameters are batched.
        """
        ...
    
    @property
    def event_shape(self):
        """
        Returns the shape of a single sample (without batching).
        """
        ...
    
    @property
    def arg_constraints(self):
        """
        Returns a dictionary from argument names to
        :class:`~torch.distributions.constraints.Constraint` objects that
        should be satisfied by each argument of this distribution. Args that
        are not tensors need not appear in this dict.
        """
        ...
    
    @property
    def support(self):
        """
        Returns a :class:`~torch.distributions.constraints.Constraint` object
        representing this distribution's support.
        """
        ...
    
    @property
    def mean(self):
        """
        Returns the mean of the distribution.
        """
        ...
    
    @property
    def variance(self):
        """
        Returns the variance of the distribution.
        """
        ...
    
    @property
    def stddev(self):
        """
        Returns the standard deviation of the distribution.
        """
        ...
    
    def sample(self, sample_shape=...):
        """
        Generates a sample_shape shaped sample or sample_shape shaped batch of
        samples if the distribution parameters are batched.
        """
        ...
    
    def rsample(self, sample_shape=...):
        """
        Generates a sample_shape shaped reparameterized sample or sample_shape
        shaped batch of reparameterized samples if the distribution parameters
        are batched.
        """
        ...
    
    def sample_n(self, n):
        """
        Generates n samples or n batches of samples if the distribution
        parameters are batched.
        """
        ...
    
    def log_prob(self, value):
        """
        Returns the log of the probability density/mass function evaluated at
        `value`.

        Args:
            value (Tensor):
        """
        ...
    
    def cdf(self, value):
        """
        Returns the cumulative density/mass function evaluated at
        `value`.

        Args:
            value (Tensor):
        """
        ...
    
    def icdf(self, value):
        """
        Returns the inverse cumulative density/mass function evaluated at
        `value`.

        Args:
            value (Tensor):
        """
        ...
    
    def enumerate_support(self, expand=...):
        """
        Returns tensor containing all values supported by a discrete
        distribution. The result will enumerate over dimension 0, so the shape
        of the result will be `(cardinality,) + batch_shape + event_shape`
        (where `event_shape = ()` for univariate distributions).

        Note that this enumerates over all batched tensors in lock-step
        `[[0, 0], [1, 1], ...]`. With `expand=False`, enumeration happens
        along dim 0, but with the remaining batch dimensions being
        singleton dimensions, `[[0], [1], ..`.

        To iterate over the full Cartesian product use
        `itertools.product(m.enumerate_support())`.

        Args:
            expand (bool): whether to expand the support over the
                batch dims to match the distribution's `batch_shape`.

        Returns:
            Tensor iterating over dimension 0.
        """
        ...
    
    def entropy(self):
        """
        Returns entropy of distribution, batched over batch_shape.

        Returns:
            Tensor of shape batch_shape.
        """
        ...
    
    def perplexity(self):
        """
        Returns perplexity of distribution, batched over batch_shape.

        Returns:
            Tensor of shape batch_shape.
        """
        ...
    
    def __repr__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.autograd import Function
from torch.autograd.function import once_differentiable
from torch.distributions.exp_family import ExponentialFamily

class _Dirichlet(Function):
    @staticmethod
    def forward(ctx, concentration):
        ...
    
    @staticmethod
    @once_differentiable
    def backward(ctx, grad_output):
        ...
    


class Dirichlet(ExponentialFamily):
    r"""
    Creates a Dirichlet distribution parameterized by concentration :attr:`concentration`.

    Example::

        >>> m = Dirichlet(torch.tensor([0.5, 0.5]))
        >>> m.sample()  # Dirichlet distributed with concentrarion concentration
        tensor([ 0.1046,  0.8954])

    Args:
        concentration (Tensor): concentration parameter of the distribution
            (often referred to as alpha)
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, concentration, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

class HalfCauchy(TransformedDistribution):
    r"""
    Creates a half-normal distribution parameterized by `scale` where::

        X ~ Cauchy(0, scale)
        Y = |X| ~ HalfCauchy(scale)

    Example::

        >>> m = HalfCauchy(torch.tensor([1.0]))
        >>> m.sample()  # half-cauchy distributed with scale=1
        tensor([ 2.3214])

    Args:
        scale (float or Tensor): scale of the full Cauchy distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def scale(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, prob):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class Binomial(Distribution):
    r"""
    Creates a Binomial distribution parameterized by :attr:`total_count` and
    either :attr:`probs` or :attr:`logits` (but not both). :attr:`total_count` must be
    broadcastable with :attr:`probs`/:attr:`logits`.

    Example::

        >>> m = Binomial(100, torch.tensor([0 , .2, .8, 1]))
        >>> x = m.sample()
        tensor([   0.,   22.,   71.,  100.])

        >>> m = Binomial(torch.tensor([[5.], [10.]]), torch.tensor([0.5, 0.8]))
        >>> x = m.sample()
        tensor([[ 4.,  5.],
                [ 7.,  6.]])

    Args:
        total_count (int or Tensor): number of Bernoulli trials
        probs (Tensor): Event probabilities
        logits (Tensor): Event log-odds
    """
    arg_constraints = ...
    has_enumerate_support = ...
    def __init__(self, total_count=..., probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def enumerate_support(self, expand=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.utils import lazy_property

class Transform(object):
    """
    Abstract class for invertable transformations with computable log
    det jacobians. They are primarily used in
    :class:`torch.distributions.TransformedDistribution`.

    Caching is useful for transforms whose inverses are either expensive or
    numerically unstable. Note that care must be taken with memoized values
    since the autograd graph may be reversed. For example while the following
    works with or without caching::

        y = t(x)
        t.log_abs_det_jacobian(x, y).backward()  # x will receive gradients.

    However the following will error when caching due to dependency reversal::

        y = t(x)
        z = t.inv(y)
        grad(z.sum(), [y])  # error because z is x

    Derived classes should implement one or both of :meth:`_call` or
    :meth:`_inverse`. Derived classes that set `bijective=True` should also
    implement :meth:`log_abs_det_jacobian`.

    Args:
        cache_size (int): Size of cache. If zero, no caching is done. If one,
            the latest single value is cached. Only 0 and 1 are supported.

    Attributes:
        domain (:class:`~torch.distributions.constraints.Constraint`):
            The constraint representing valid inputs to this transform.
        codomain (:class:`~torch.distributions.constraints.Constraint`):
            The constraint representing valid outputs to this transform
            which are inputs to the inverse transform.
        bijective (bool): Whether this transform is bijective. A transform
            ``t`` is bijective iff ``t.inv(t(x)) == x`` and
            ``t(t.inv(y)) == y`` for every ``x`` in the domain and ``y`` in
            the codomain. Transforms that are not bijective should at least
            maintain the weaker pseudoinverse properties
            ``t(t.inv(t(x)) == t(x)`` and ``t.inv(t(t.inv(y))) == t.inv(y)``.
        sign (int or Tensor): For bijective univariate transforms, this
            should be +1 or -1 depending on whether transform is monotone
            increasing or decreasing.
        event_dim (int): Number of dimensions that are correlated together in
            the transform ``event_shape``. This should be 0 for pointwise
            transforms, 1 for transforms that act jointly on vectors, 2 for
            transforms that act jointly on matrices, etc.
    """
    bijective = ...
    event_dim = ...
    def __init__(self, cache_size=...) -> None:
        ...
    
    @property
    def inv(self):
        """
        Returns the inverse :class:`Transform` of this transform.
        This should satisfy ``t.inv.inv is t``.
        """
        ...
    
    @property
    def sign(self):
        """
        Returns the sign of the determinant of the Jacobian, if applicable.
        In general this only makes sense for bijective transforms.
        """
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def __ne__(self, other) -> bool:
        ...
    
    def __call__(self, x):
        """
        Computes the transform `x => y`.
        """
        ...
    
    def log_abs_det_jacobian(self, x, y):
        """
        Computes the log det jacobian `log |dy/dx|` given input and output.
        """
        ...
    
    def __repr__(self):
        ...
    


class _InverseTransform(Transform):
    """
    Inverts a single :class:`Transform`.
    This class is private; please instead use the ``Transform.inv`` property.
    """
    def __init__(self, transform) -> None:
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    
    @property
    def bijective(self):
        ...
    
    @property
    def sign(self):
        ...
    
    @property
    def event_dim(self):
        ...
    
    @property
    def inv(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def __call__(self, x):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class ComposeTransform(Transform):
    """
    Composes multiple transforms in a chain.
    The transforms being composed are responsible for caching.

    Args:
        parts (list of :class:`Transform`): A list of transforms to compose.
        cache_size (int): Size of cache. If zero, no caching is done. If one,
            the latest single value is cached. Only 0 and 1 are supported.
    """
    def __init__(self, parts, cache_size=...) -> None:
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    
    @lazy_property
    def bijective(self):
        ...
    
    @lazy_property
    def sign(self):
        ...
    
    @lazy_property
    def event_dim(self):
        ...
    
    @property
    def inv(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __call__(self, x):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    def __repr__(self):
        ...
    


identity_transform = ComposeTransform([])
class ExpTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \exp(x)`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class PowerTransform(Transform):
    r"""
    Transform via the mapping :math:`y = x^{\text{exponent}}`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __init__(self, exponent, cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class SigmoidTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \frac{1}{1 + \exp(-x)}` and :math:`x = \text{logit}(y)`.
    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class TanhTransform(Transform):
    r"""
    Transform via the mapping :math:`y = \tanh(x)`.

    It is equivalent to
    ```
    ComposeTransform([AffineTransform(0., 2.), SigmoidTransform(), AffineTransform(-1., 2.)])
    ```
    However this might not be numerically stable, thus it is recommended to use `TanhTransform`
    instead.

    Note that one should use `cache_size=1` when it comes to `NaN/Inf` values.

    """
    domain = ...
    codomain = ...
    bijective = ...
    sign = ...
    @staticmethod
    def atanh(x):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class AbsTransform(Transform):
    r"""
    Transform via the mapping :math:`y = |x|`.
    """
    domain = ...
    codomain = ...
    def __eq__(self, other) -> bool:
        ...
    


class AffineTransform(Transform):
    r"""
    Transform via the pointwise affine mapping :math:`y = \text{loc} + \text{scale} \times x`.

    Args:
        loc (Tensor or float): Location parameter.
        scale (Tensor or float): Scale parameter.
        event_dim (int): Optional size of `event_shape`. This should be zero
            for univariate random variables, 1 for distributions over vectors,
            2 for distributions over matrices, etc.
    """
    domain = ...
    codomain = ...
    bijective = ...
    def __init__(self, loc, scale, event_dim=..., cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def __eq__(self, other) -> bool:
        ...
    
    @property
    def sign(self):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class SoftmaxTransform(Transform):
    r"""
    Transform from unconstrained space to the simplex via :math:`y = \exp(x)` then
    normalizing.

    This is not bijective and cannot be used for HMC. However this acts mostly
    coordinate-wise (except for the final normalization), and thus is
    appropriate for coordinate-wise optimization algorithms.
    """
    domain = ...
    codomain = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    


class StickBreakingTransform(Transform):
    """
    Transform from unconstrained space to the simplex of one additional
    dimension via a stick-breaking process.

    This transform arises as an iterated sigmoid transform in a stick-breaking
    construction of the `Dirichlet` distribution: the first logit is
    transformed via sigmoid to the first probability and the probability of
    everything else, and then the process recurses.

    This is bijective and appropriate for use in HMC; however it mixes
    coordinates together and is less appropriate for optimization.
    """
    domain = ...
    codomain = ...
    bijective = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    


class LowerCholeskyTransform(Transform):
    """
    Transform from unconstrained matrices to lower-triangular matrices with
    nonnegative diagonal entries.

    This is useful for parameterizing positive definite matrices in terms of
    their Cholesky factorization.
    """
    domain = ...
    codomain = ...
    event_dim = ...
    def __eq__(self, other) -> bool:
        ...
    


class CatTransform(Transform):
    """
    Transform functor that applies a sequence of transforms `tseq`
    component-wise to each submatrix at `dim`, of length `lengths[dim]`,
    in a way compatible with :func:`torch.cat`.

    Example::
       x0 = torch.cat([torch.range(1, 10), torch.range(1, 10)], dim=0)
       x = torch.cat([x0, x0], dim=0)
       t0 = CatTransform([ExpTransform(), identity_transform], dim=0, lengths=[10, 10])
       t = CatTransform([t0, t0], dim=0, lengths=[20, 20])
       y = t(x)
    """
    def __init__(self, tseq, dim=..., lengths=..., cache_size=...) -> None:
        ...
    
    @lazy_property
    def length(self):
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    @property
    def bijective(self):
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    


class StackTransform(Transform):
    """
    Transform functor that applies a sequence of transforms `tseq`
    component-wise to each submatrix at `dim`
    in a way compatible with :func:`torch.stack`.

    Example::
       x = torch.stack([torch.range(1, 10), torch.range(1, 10)], dim=1)
       t = StackTransform([ExpTransform(), identity_transform], dim=1)
       y = t(x)
    """
    def __init__(self, tseq, dim=..., cache_size=...) -> None:
        ...
    
    def with_cache(self, cache_size=...):
        ...
    
    def log_abs_det_jacobian(self, x, y):
        ...
    
    @property
    def bijective(self):
        ...
    
    @constraints.dependent_property
    def domain(self):
        ...
    
    @constraints.dependent_property
    def codomain(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class Cauchy(Distribution):
    r"""
    Samples from a Cauchy (Lorentz) distribution. The distribution of the ratio of
    independent normally distributed random variables with means `0` follows a
    Cauchy distribution.

    Example::

        >>> m = Cauchy(torch.tensor([0.0]), torch.tensor([1.0]))
        >>> m.sample()  # sample from a Cauchy distribution with loc=0 and scale=1
        tensor([ 2.3214])

    Args:
        loc (float or Tensor): mode or median of the distribution.
        scale (float or Tensor): half width at half maximum.
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily

class Beta(ExponentialFamily):
    r"""
    Beta distribution parameterized by :attr:`concentration1` and :attr:`concentration0`.

    Example::

        >>> m = Beta(torch.tensor([0.5]), torch.tensor([0.5]))
        >>> m.sample()  # Beta distributed with concentration concentration1 and concentration0
        tensor([ 0.1046])

    Args:
        concentration1 (float or Tensor): 1st concentration parameter of the distribution
            (often referred to as alpha)
        concentration0 (float or Tensor): 2nd concentration parameter of the distribution
            (often referred to as beta)
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, concentration1, concentration0, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    
    @property
    def concentration1(self):
        ...
    
    @property
    def concentration0(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

def broadcast_all(*values):
    r"""
    Given a list of values (possibly containing numbers), returns a list where each
    value is broadcasted based on the following rules:
      - `torch.*Tensor` instances are broadcasted as per :ref:`_broadcasting-semantics`.
      - numbers.Number instances (scalars) are upcast to tensors having
        the same size and type as the first tensor passed to `values`.  If all the
        values are scalars, then they are upcasted to scalar Tensors.

    Args:
        values (list of `numbers.Number` or `torch.*Tensor`)

    Raises:
        ValueError: if any of the values is not a `numbers.Number` or
            `torch.*Tensor` instance
    """
    ...

def logits_to_probs(logits, is_binary=...):
    r"""
    Converts a tensor of logits into probabilities. Note that for the
    binary case, each value denotes log odds, whereas for the
    multi-dimensional case, the values along the last dimension denote
    the log probabilities (possibly unnormalized) of the events.
    """
    ...

def clamp_probs(probs):
    ...

def probs_to_logits(probs, is_binary=...):
    r"""
    Converts a tensor of probabilities into logits. For the binary case,
    this denotes the probability of occurrence of the event indexed by `1`.
    For the multi-dimensional case, the values along the last dimension
    denote the probabilities of occurrence of each of the events.
    """
    ...

class lazy_property(object):
    r"""
    Used as a decorator for lazy loading of class attributes. This uses a
    non-data descriptor that calls the wrapped method to compute the property on
    first call; thereafter replacing the wrapped method into an instance
    attribute.
    """
    def __init__(self, wrapped) -> None:
        ...
    
    def __get__(self, instance, obj_type=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

r"""
PyTorch provides two global :class:`ConstraintRegistry` objects that link
:class:`~torch.distributions.constraints.Constraint` objects to
:class:`~torch.distributions.transforms.Transform` objects. These objects both
input constraints and return transforms, but they have different guarantees on
bijectivity.

1. ``biject_to(constraint)`` looks up a bijective
   :class:`~torch.distributions.transforms.Transform` from ``constraints.real``
   to the given ``constraint``. The returned transform is guaranteed to have
   ``.bijective = True`` and should implement ``.log_abs_det_jacobian()``.
2. ``transform_to(constraint)`` looks up a not-necessarily bijective
   :class:`~torch.distributions.transforms.Transform` from ``constraints.real``
   to the given ``constraint``. The returned transform is not guaranteed to
   implement ``.log_abs_det_jacobian()``.

The ``transform_to()`` registry is useful for performing unconstrained
optimization on constrained parameters of probability distributions, which are
indicated by each distribution's ``.arg_constraints`` dict. These transforms often
overparameterize a space in order to avoid rotation; they are thus more
suitable for coordinate-wise optimization algorithms like Adam::

    loc = torch.zeros(100, requires_grad=True)
    unconstrained = torch.zeros(100, requires_grad=True)
    scale = transform_to(Normal.arg_constraints['scale'])(unconstrained)
    loss = -Normal(loc, scale).log_prob(data).sum()

The ``biject_to()`` registry is useful for Hamiltonian Monte Carlo, where
samples from a probability distribution with constrained ``.support`` are
propagated in an unconstrained space, and algorithms are typically rotation
invariant.::

    dist = Exponential(rate)
    unconstrained = torch.zeros(100, requires_grad=True)
    sample = biject_to(dist.support)(unconstrained)
    potential_energy = -dist.log_prob(sample).sum()

.. note::

    An example where ``transform_to`` and ``biject_to`` differ is
    ``constraints.simplex``: ``transform_to(constraints.simplex)`` returns a
    :class:`~torch.distributions.transforms.SoftmaxTransform` that simply
    exponentiates and normalizes its inputs; this is a cheap and mostly
    coordinate-wise operation appropriate for algorithms like SVI. In
    contrast, ``biject_to(constraints.simplex)`` returns a
    :class:`~torch.distributions.transforms.StickBreakingTransform` that
    bijects its input down to a one-fewer-dimensional space; this a more
    expensive less numerically stable transform but is needed for algorithms
    like HMC.

The ``biject_to`` and ``transform_to`` objects can be extended by user-defined
constraints and transforms using their ``.register()`` method either as a
function on singleton constraints::

    transform_to.register(my_constraint, my_transform)

or as a decorator on parameterized constraints::

    @transform_to.register(MyConstraintClass)
    def my_factory(constraint):
        assert isinstance(constraint, MyConstraintClass)
        return MyTransform(constraint.param1, constraint.param2)

You can create your own registry by creating a new :class:`ConstraintRegistry`
object.
"""
class ConstraintRegistry(object):
    """
    Registry to link constraints to transforms.
    """
    def __init__(self) -> None:
        ...
    
    def register(self, constraint, factory=...):
        """
        Registers a :class:`~torch.distributions.constraints.Constraint`
        subclass in this registry. Usage::

            @my_registry.register(MyConstraintClass)
            def construct_transform(constraint):
                assert isinstance(constraint, MyConstraint)
                return MyTransform(constraint.arg_constraints)

        Args:
            constraint (subclass of :class:`~torch.distributions.constraints.Constraint`):
                A subclass of :class:`~torch.distributions.constraints.Constraint`, or
                a singleton object of the desired class.
            factory (callable): A callable that inputs a constraint object and returns
                a  :class:`~torch.distributions.transforms.Transform` object.
        """
        ...
    
    def __call__(self, constraint):
        """
        Looks up a transform to constrained space, given a constraint object.
        Usage::

            constraint = Normal.arg_constraints['scale']
            scale = transform_to(constraint)(torch.zeros(1))  # constrained
            u = transform_to(constraint).inv(scale)           # unconstrained

        Args:
            constraint (:class:`~torch.distributions.constraints.Constraint`):
                A constraint object.

        Returns:
            A :class:`~torch.distributions.transforms.Transform` object.

        Raises:
            `NotImplementedError` if no transform has been registered.
        """
        ...
    


biject_to = ConstraintRegistry()
transform_to = ConstraintRegistry()
"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

class Weibull(TransformedDistribution):
    r"""
    Samples from a two-parameter Weibull distribution.

    Example:

        >>> m = Weibull(torch.tensor([1.0]), torch.tensor([1.0]))
        >>> m.sample()  # sample from a Weibull distribution with scale=1, concentration=1
        tensor([ 0.4784])

    Args:
        scale (float or Tensor): Scale parameter of distribution (lambda).
        concentration (float or Tensor): Concentration parameter of distribution (k/shape).
    """
    arg_constraints = ...
    support = ...
    def __init__(self, scale, concentration, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

class LogNormal(TransformedDistribution):
    r"""
    Creates a log-normal distribution parameterized by
    :attr:`loc` and :attr:`scale` where::

        X ~ Normal(loc, scale)
        Y = exp(X) ~ LogNormal(loc, scale)

    Example::

        >>> m = LogNormal(torch.tensor([0.0]), torch.tensor([1.0]))
        >>> m.sample()  # log-normal distributed with mean=0 and stddev=1
        tensor([ 0.1046])

    Args:
        loc (float or Tensor): mean of log of distribution
        scale (float or Tensor): standard deviation of log of the distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def loc(self):
        ...
    
    @property
    def scale(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class MultivariateNormal(Distribution):
    r"""
    Creates a multivariate normal (also called Gaussian) distribution
    parameterized by a mean vector and a covariance matrix.

    The multivariate normal distribution can be parameterized either
    in terms of a positive definite covariance matrix :math:`\mathbf{\Sigma}`
    or a positive definite precision matrix :math:`\mathbf{\Sigma}^{-1}`
    or a lower-triangular matrix :math:`\mathbf{L}` with positive-valued
    diagonal entries, such that
    :math:`\mathbf{\Sigma} = \mathbf{L}\mathbf{L}^\top`. This triangular matrix
    can be obtained via e.g. Cholesky decomposition of the covariance.

    Example:

        >>> m = MultivariateNormal(torch.zeros(2), torch.eye(2))
        >>> m.sample()  # normally distributed with mean=`[0,0]` and covariance_matrix=`I`
        tensor([-0.2102, -0.5429])

    Args:
        loc (Tensor): mean of the distribution
        covariance_matrix (Tensor): positive-definite covariance matrix
        precision_matrix (Tensor): positive-definite precision matrix
        scale_tril (Tensor): lower-triangular factor of covariance, with positive-valued diagonal

    Note:
        Only one of :attr:`covariance_matrix` or :attr:`precision_matrix` or
        :attr:`scale_tril` can be specified.

        Using :attr:`scale_tril` will be more efficient: all computations internally
        are based on :attr:`scale_tril`. If :attr:`covariance_matrix` or
        :attr:`precision_matrix` is passed instead, it is only used to compute
        the corresponding lower triangular matrices using a Cholesky decomposition.
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, covariance_matrix=..., precision_matrix=..., scale_tril=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @lazy_property
    def scale_tril(self):
        ...
    
    @lazy_property
    def covariance_matrix(self):
        ...
    
    @lazy_property
    def precision_matrix(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily
from torch.distributions.utils import lazy_property

class Bernoulli(ExponentialFamily):
    r"""
    Creates a Bernoulli distribution parameterized by :attr:`probs`
    or :attr:`logits` (but not both).

    Samples are binary (0 or 1). They take the value `1` with probability `p`
    and `0` with probability `1 - p`.

    Example::

        >>> m = Bernoulli(torch.tensor([0.3]))
        >>> m.sample()  # 30% chance 1; 70% chance 0
        tensor([ 0.])

    Args:
        probs (Number, Tensor): the probability of sampling `1`
        logits (Number, Tensor): the log-odds of sampling `1`
    """
    arg_constraints = ...
    support = ...
    has_enumerate_support = ...
    _mean_carrier_measure = ...
    def __init__(self, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    
    def enumerate_support(self, expand=...):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions import constraints
from torch.distributions.distribution import Distribution

class Independent(Distribution):
    r"""
    Reinterprets some of the batch dims of a distribution as event dims.

    This is mainly useful for changing the shape of the result of
    :meth:`log_prob`. For example to create a diagonal Normal distribution with
    the same shape as a Multivariate Normal distribution (so they are
    interchangeable), you can::

        >>> loc = torch.zeros(3)
        >>> scale = torch.ones(3)
        >>> mvn = MultivariateNormal(loc, scale_tril=torch.diag(scale))
        >>> [mvn.batch_shape, mvn.event_shape]
        [torch.Size(()), torch.Size((3,))]
        >>> normal = Normal(loc, scale)
        >>> [normal.batch_shape, normal.event_shape]
        [torch.Size((3,)), torch.Size(())]
        >>> diagn = Independent(normal, 1)
        >>> [diagn.batch_shape, diagn.event_shape]
        [torch.Size(()), torch.Size((3,))]

    Args:
        base_distribution (torch.distributions.distribution.Distribution): a
            base distribution
        reinterpreted_batch_ndims (int): the number of batch dims to
            reinterpret as event dims
    """
    arg_constraints = ...
    def __init__(self, base_distribution, reinterpreted_batch_ndims, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def has_rsample(self):
        ...
    
    @property
    def has_enumerate_support(self):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    
    def enumerate_support(self, expand=...):
        ...
    
    def __repr__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class StudentT(Distribution):
    r"""
    Creates a Student's t-distribution parameterized by degree of
    freedom :attr:`df`, mean :attr:`loc` and scale :attr:`scale`.

    Example::

        >>> m = StudentT(torch.tensor([2.0]))
        >>> m.sample()  # Student's t-distributed with degrees of freedom=2
        tensor([ 0.1046])

    Args:
        df (float or Tensor): degrees of freedom
        loc (float or Tensor): mean of the distribution
        scale (float or Tensor): scale of the distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, df, loc=..., scale=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily

class Exponential(ExponentialFamily):
    r"""
    Creates a Exponential distribution parameterized by :attr:`rate`.

    Example::

        >>> m = Exponential(torch.tensor([1.0]))
        >>> m.sample()  # Exponential distributed with rate=1
        tensor([ 0.1046])

    Args:
        rate (float or Tensor): rate = 1 / scale of the distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    _mean_carrier_measure = ...
    @property
    def mean(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, rate, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions.utils import lazy_property

class NegativeBinomial(Distribution):
    r"""
    Creates a Negative Binomial distribution, i.e. distribution
    of the number of successful independent and identical Bernoulli trials
    before :attr:`total_count` failures are achieved. The probability
    of success of each Bernoulli trial is :attr:`probs`.

    Args:
        total_count (float or Tensor): non-negative number of negative Bernoulli
            trials to stop, although the distribution is still valid for real
            valued count
        probs (Tensor): Event probabilities of success in the half open interval [0, 1)
        logits (Tensor): Event log-odds for probabilities of success
    """
    arg_constraints = ...
    support = ...
    def __init__(self, total_count, probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @lazy_property
    def logits(self):
        ...
    
    @lazy_property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class Laplace(Distribution):
    r"""
    Creates a Laplace distribution parameterized by :attr:`loc` and :attr:`scale`.

    Example::

        >>> m = Laplace(torch.tensor([0.0]), torch.tensor([1.0]))
        >>> m.sample()  # Laplace distributed with loc=0, scale=1
        tensor([ 0.1046])

    Args:
        loc (float or Tensor): mean of the distribution
        scale (float or Tensor): scale of the distribution
    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def rsample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    
    def cdf(self, value):
        ...
    
    def icdf(self, value):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.exp_family import ExponentialFamily

class Poisson(ExponentialFamily):
    r"""
    Creates a Poisson distribution parameterized by :attr:`rate`, the rate parameter.

    Samples are nonnegative integers, with a pmf given by

    .. math::
      \mathrm{rate}^k \frac{e^{-\mathrm{rate}}}{k!}

    Example::

        >>> m = Poisson(torch.tensor([4]))
        >>> m.sample()
        tensor([ 3.])

    Args:
        rate (Number, Tensor): the rate parameter
    """
    arg_constraints = ...
    support = ...
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, rate, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions import constraints

class MixtureSameFamily(Distribution):
    r"""
    The `MixtureSameFamily` distribution implements a (batch of) mixture
    distribution where all component are from different parameterizations of
    the same distribution type. It is parameterized by a `Categorical`
    "selecting distribution" (over `k` component) and a component
    distribution, i.e., a `Distribution` with a rightmost batch shape
    (equal to `[k]`) which indexes each (batch of) component.

    Examples::

        # Construct Gaussian Mixture Model in 1D consisting of 5 equally
        # weighted normal distributions
        >>> mix = D.Categorical(torch.ones(5,))
        >>> comp = D.Normal(torch.randn(5,), torch.rand(5,))
        >>> gmm = MixtureSameFamily(mix, comp)

        # Construct Gaussian Mixture Modle in 2D consisting of 5 equally
        # weighted bivariate normal distributions
        >>> mix = D.Categorical(torch.ones(5,))
        >>> comp = D.Independent(D.Normal(
                     torch.randn(5,2), torch.rand(5,2)), 1)
        >>> gmm = MixtureSameFamily(mix, comp)

        # Construct a batch of 3 Gaussian Mixture Models in 2D each
        # consisting of 5 random weighted bivariate normal distributions
        >>> mix = D.Categorical(torch.rand(3,5))
        >>> comp = D.Independent(D.Normal(
                    torch.randn(3,5,2), torch.rand(3,5,2)), 1)
        >>> gmm = MixtureSameFamily(mix, comp)

    Args:
        mixture_distribution: `torch.distributions.Categorical`-like
            instance. Manages the probability of selecting component.
            The number of categories must match the rightmost batch
            dimension of the `component_distribution`. Must have either
            scalar `batch_shape` or `batch_shape` matching
            `component_distribution.batch_shape[:-1]`
        component_distribution: `torch.distributions.Distribution`-like
            instance. Right-most batch dimension indexes component.
    """
    arg_constraints = ...
    has_rsample = ...
    def __init__(self, mixture_distribution, component_distribution, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @property
    def mixture_distribution(self):
        ...
    
    @property
    def component_distribution(self):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def cdf(self, x):
        ...
    
    def log_prob(self, x):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def __repr__(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

class LogisticNormal(TransformedDistribution):
    r"""
    Creates a logistic-normal distribution parameterized by :attr:`loc` and :attr:`scale`
    that define the base `Normal` distribution transformed with the
    `StickBreakingTransform` such that::

        X ~ LogisticNormal(loc, scale)
        Y = log(X / (1 - X.cumsum(-1)))[..., :-1] ~ Normal(loc, scale)

    Args:
        loc (float or Tensor): mean of the base distribution
        scale (float or Tensor): standard deviation of the base distribution

    Example::

        >>> # logistic-normal distributed with mean=(0, 0, 0) and stddev=(1, 1, 1)
        >>> # of the base Normal distribution
        >>> m = distributions.LogisticNormal(torch.tensor([0.0] * 3), torch.tensor([1.0] * 3))
        >>> m.sample()
        tensor([ 0.7653,  0.0341,  0.0579,  0.1427])

    """
    arg_constraints = ...
    support = ...
    has_rsample = ...
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def loc(self):
        ...
    
    @property
    def scale(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

r"""
The following constraints are implemented:

- ``constraints.boolean``
- ``constraints.cat``
- ``constraints.dependent``
- ``constraints.greater_than(lower_bound)``
- ``constraints.integer_interval(lower_bound, upper_bound)``
- ``constraints.interval(lower_bound, upper_bound)``
- ``constraints.lower_cholesky``
- ``constraints.lower_triangular``
- ``constraints.nonnegative_integer``
- ``constraints.positive``
- ``constraints.positive_definite``
- ``constraints.positive_integer``
- ``constraints.real``
- ``constraints.real_vector``
- ``constraints.simplex``
- ``constraints.stack``
- ``constraints.unit_interval``
"""
class Constraint(object):
    """
    Abstract base class for constraints.

    A constraint object represents a region over which a variable is valid,
    e.g. within which a variable can be optimized.
    """
    def check(self, value):
        """
        Returns a byte tensor of `sample_shape + batch_shape` indicating
        whether each event in value satisfies this constraint.
        """
        ...
    
    def __repr__(self):
        ...
    


class _Dependent(Constraint):
    """
    Placeholder for variables whose support depends on other variables.
    These variables obey no simple coordinate-wise constraints.
    """
    def check(self, x):
        ...
    


def is_dependent(constraint):
    ...

class _DependentProperty(property, _Dependent):
    """
    Decorator that extends @property to act like a `Dependent` constraint when
    called on a class and act like a property when called on an object.

    Example::

        class Uniform(Distribution):
            def __init__(self, low, high):
                self.low = low
                self.high = high
            @constraints.dependent_property
            def support(self):
                return constraints.interval(self.low, self.high)
    """
    ...


class _Boolean(Constraint):
    """
    Constrain to the two values `{0, 1}`.
    """
    def check(self, value):
        ...
    


class _IntegerInterval(Constraint):
    """
    Constrain to an integer interval `[lower_bound, upper_bound]`.
    """
    def __init__(self, lower_bound, upper_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _IntegerLessThan(Constraint):
    """
    Constrain to an integer interval `(-inf, upper_bound]`.
    """
    def __init__(self, upper_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _IntegerGreaterThan(Constraint):
    """
    Constrain to an integer interval `[lower_bound, inf)`.
    """
    def __init__(self, lower_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _Real(Constraint):
    """
    Trivially constrain to the extended real line `[-inf, inf]`.
    """
    def check(self, value):
        ...
    


class _GreaterThan(Constraint):
    """
    Constrain to a real half line `(lower_bound, inf]`.
    """
    def __init__(self, lower_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _GreaterThanEq(Constraint):
    """
    Constrain to a real half line `[lower_bound, inf)`.
    """
    def __init__(self, lower_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _LessThan(Constraint):
    """
    Constrain to a real half line `[-inf, upper_bound)`.
    """
    def __init__(self, upper_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _Interval(Constraint):
    """
    Constrain to a real interval `[lower_bound, upper_bound]`.
    """
    def __init__(self, lower_bound, upper_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _HalfOpenInterval(Constraint):
    """
    Constrain to a real interval `[lower_bound, upper_bound)`.
    """
    def __init__(self, lower_bound, upper_bound) -> None:
        ...
    
    def check(self, value):
        ...
    
    def __repr__(self):
        ...
    


class _Simplex(Constraint):
    """
    Constrain to the unit simplex in the innermost (rightmost) dimension.
    Specifically: `x >= 0` and `x.sum(-1) == 1`.
    """
    def check(self, value):
        ...
    


class _LowerTriangular(Constraint):
    """
    Constrain to lower-triangular square matrices.
    """
    def check(self, value):
        ...
    


class _LowerCholesky(Constraint):
    """
    Constrain to lower-triangular square matrices with positive diagonals.
    """
    def check(self, value):
        ...
    


class _PositiveDefinite(Constraint):
    """
    Constrain to positive-definite matrices.
    """
    def check(self, value):
        ...
    


class _RealVector(Constraint):
    """
    Constrain to real-valued vectors. This is the same as `constraints.real`,
    but additionally reduces across the `event_shape` dimension.
    """
    def check(self, value):
        ...
    


class _Cat(Constraint):
    """
    Constraint functor that applies a sequence of constraints
    `cseq` at the submatrices at dimension `dim`,
    each of size `lengths[dim]`, in a way compatible with :func:`torch.cat`.
    """
    def __init__(self, cseq, dim=..., lengths=...) -> None:
        ...
    
    def check(self, value):
        ...
    


class _Stack(Constraint):
    """
    Constraint functor that applies a sequence of constraints
    `cseq` at the submatrices at dimension `dim`,
    in a way compatible with :func:`torch.stack`.
    """
    def __init__(self, cseq, dim=...) -> None:
        ...
    
    def check(self, value):
        ...
    


dependent = _Dependent()
dependent_property = _DependentProperty
boolean = _Boolean()
nonnegative_integer = _IntegerGreaterThan(0)
positive_integer = _IntegerGreaterThan(1)
integer_interval = _IntegerInterval
real = _Real()
real_vector = _RealVector()
positive = _GreaterThan(0)
greater_than = _GreaterThan
greater_than_eq = _GreaterThanEq
less_than = _LessThan
unit_interval = _Interval(0, 1)
interval = _Interval
half_open_interval = _HalfOpenInterval
simplex = _Simplex()
lower_triangular = _LowerTriangular()
lower_cholesky = _LowerCholesky()
positive_definite = _PositiveDefinite()
cat = _Cat
stack = _Stack
"""
This type stub file was generated by pyright.
"""

from torch.distributions.transformed_distribution import TransformedDistribution

euler_constant = 0.5772156649015329
class Gumbel(TransformedDistribution):
    r"""
    Samples from a Gumbel Distribution.

    Examples::

        >>> m = Gumbel(torch.tensor([1.0]), torch.tensor([2.0]))
        >>> m.sample()  # sample from Gumbel distribution with loc=1, scale=2
        tensor([ 1.0124])

    Args:
        loc (float or Tensor): Location parameter of the distribution
        scale (float or Tensor): Scale parameter of the distribution
    """
    arg_constraints = ...
    support = ...
    def __init__(self, loc, scale, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    def log_prob(self, value):
        ...
    
    @property
    def mean(self):
        ...
    
    @property
    def stddev(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def entropy(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution
from torch.distributions import constraints

class Multinomial(Distribution):
    r"""
    Creates a Multinomial distribution parameterized by :attr:`total_count` and
    either :attr:`probs` or :attr:`logits` (but not both). The innermost dimension of
    :attr:`probs` indexes over categories. All other dimensions index over batches.

    Note that :attr:`total_count` need not be specified if only :meth:`log_prob` is
    called (see example below)

    .. note:: :attr:`probs` must be non-negative, finite and have a non-zero sum,
              and it will be normalized to sum to 1.

    -   :meth:`sample` requires a single shared `total_count` for all
        parameters and samples.
    -   :meth:`log_prob` allows different `total_count` for each parameter and
        sample.

    Example::

        >>> m = Multinomial(100, torch.tensor([ 1., 1., 1., 1.]))
        >>> x = m.sample()  # equal probability of 0, 1, 2, 3
        tensor([ 21.,  24.,  30.,  25.])

        >>> Multinomial(probs=torch.tensor([1., 1., 1., 1.])).log_prob(x)
        tensor([-4.1338])

    Args:
        total_count (int): number of trials
        probs (Tensor): event probabilities
        logits (Tensor): event log probabilities
    """
    arg_constraints = ...
    @property
    def mean(self):
        ...
    
    @property
    def variance(self):
        ...
    
    def __init__(self, total_count=..., probs=..., logits=..., validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @constraints.dependent_property
    def support(self):
        ...
    
    @property
    def logits(self):
        ...
    
    @property
    def probs(self):
        ...
    
    @property
    def param_shape(self):
        ...
    
    def sample(self, sample_shape=...):
        ...
    
    def log_prob(self, value):
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.distribution import Distribution

class ExponentialFamily(Distribution):
    r"""
    ExponentialFamily is the abstract base class for probability distributions belonging to an
    exponential family, whose probability mass/density function has the form is defined below

    .. math::

        p_{F}(x; \theta) = \exp(\langle t(x), \theta\rangle - F(\theta) + k(x))

    where :math:`\theta` denotes the natural parameters, :math:`t(x)` denotes the sufficient statistic,
    :math:`F(\theta)` is the log normalizer function for a given family and :math:`k(x)` is the carrier
    measure.

    Note:
        This class is an intermediary between the `Distribution` class and distributions which belong
        to an exponential family mainly to check the correctness of the `.entropy()` and analytic KL
        divergence methods. We use this class to compute the entropy and KL divergence using the AD
        framework and Bregman divergences (courtesy of: Frank Nielsen and Richard Nock, Entropies and
        Cross-entropies of Exponential Families).
    """
    def entropy(self):
        """
        Method to compute the entropy using Bregman divergence of the log normalizer.
        """
        ...
    


"""
This type stub file was generated by pyright.
"""

from torch.distributions.gamma import Gamma

class Chi2(Gamma):
    r"""
    Creates a Chi2 distribution parameterized by shape parameter :attr:`df`.
    This is exactly equivalent to ``Gamma(alpha=0.5*df, beta=0.5)``

    Example::

        >>> m = Chi2(torch.tensor([1.0]))
        >>> m.sample()  # Chi2 distributed with shape df=1
        tensor([ 0.1046])

    Args:
        df (float or Tensor): shape parameter of the distribution
    """
    arg_constraints = ...
    def __init__(self, df, validate_args=...) -> None:
        ...
    
    def expand(self, batch_shape, _instance=...):
        ...
    
    @property
    def df(self):
        ...
    


"""
This type stub file was generated by pyright.
"""

"""
This global flag controls whether to assign new tensors to the parameters
instead of changing the existing parameters in-place when converting an `nn.Module`
using the following methods:
1. `module.cuda()` / `.cpu()` (for moving `module` between devices)
2. `module.float()` / `.double()` / `.half()` (for converting `module` to a different dtype)
3. `module.to()` / `.type()` (for changing `module`'s device or dtype)
4. `module._apply(fn)` (for generic functions applied to `module`)

Default: False
"""
_overwrite_module_params_on_conversion = False
def set_overwrite_module_params_on_conversion(value):
    ...

def get_overwrite_module_params_on_conversion():
    ...

"""
This type stub file was generated by pyright.
"""

import struct
from contextlib import contextmanager

DEFAULT_PROTOCOL = 2
LONG_SIZE = struct.Struct('=l').size
INT_SIZE = struct.Struct('=i').size
SHORT_SIZE = struct.Struct('=h').size
MAGIC_NUMBER = 1.195470371460388e+23
PROTOCOL_VERSION = 1001
STORAGE_KEY_SEPARATOR = ','
class SourceChangeWarning(Warning):
    ...


@contextmanager
def mkdtemp():
    ...

_package_registry = []
def register_package(priority, tagger, deserializer):
    ...

def check_module_version_greater_or_equal(module, req_version_tuple, error_if_malformed=...):
    '''
    Check if a module's version satisfies requirements

    Usually, a module's version string will be like 'x.y.z', which would be represented
    as a tuple (x, y, z), but sometimes it could be an unexpected format. If the version
    string does not match the given tuple's format up to the length of the tuple, then
    error and exit or emit a warning.

    Args:
        module: the module to check the version of
        req_version_tuple: tuple (usually of ints) representing the required version
        error_if_malformed: whether we should exit if module version string is malformed

    Returns:
        requirement_is_met: bool
    '''
    ...

def validate_cuda_device(location):
    ...

def location_tag(storage):
    ...

def default_restore_location(storage, location):
    ...

def normalize_storage_type(storage_type):
    ...

def storage_to_tensor_type(storage):
    ...

class _opener(object):
    def __init__(self, file_like) -> None:
        ...
    
    def __enter__(self):
        ...
    
    def __exit__(self, *args):
        ...
    


class _open_file(_opener):
    def __init__(self, name, mode) -> None:
        ...
    
    def __exit__(self, *args):
        ...
    


class _open_buffer_reader(_opener):
    def __init__(self, buffer) -> None:
        ...
    


class _open_buffer_writer(_opener):
    def __exit__(self, *args):
        ...
    


class _open_zipfile_reader(_opener):
    def __init__(self, name_or_buffer) -> None:
        ...
    


class _open_zipfile_writer_file(_opener):
    def __init__(self, name) -> None:
        ...
    
    def __exit__(self, *args):
        ...
    


class _open_zipfile_writer_buffer(_opener):
    def __init__(self, buffer) -> None:
        ...
    
    def __exit__(self, *args):
        ...
    


def save(obj, f, pickle_module=..., pickle_protocol=..., _use_new_zipfile_serialization=...):
    """Saves an object to a disk file.

    See also: :ref:`recommend-saving-models`

    Args:
        obj: saved object
        f: a file-like object (has to implement write and flush) or a string or
           os.PathLike object containing a file name
        pickle_module: module used for pickling metadata and objects
        pickle_protocol: can be specified to override the default protocol

    .. note::
        A common PyTorch convention is to save tensors using .pt file extension.

    .. note::
        The 1.6 release of PyTorch switched ``torch.save`` to use a new
        zipfile-based file format. ``torch.load`` still retains the ability to
        load files in the old format. If for any reason you want ``torch.save``
        to use the old format, pass the kwarg ``_use_new_zipfile_serialization=False``.

    Example:
        >>> # Save to file
        >>> x = torch.tensor([0, 1, 2, 3, 4])
        >>> torch.save(x, 'tensor.pt')
        >>> # Save to io.BytesIO buffer
        >>> buffer = io.BytesIO()
        >>> torch.save(x, buffer)
    """
    ...

def load(f, map_location=..., pickle_module=..., **pickle_load_args):
    """Loads an object saved with :func:`torch.save` from a file.

    :func:`torch.load` uses Python's unpickling facilities but treats storages,
    which underlie tensors, specially. They are first deserialized on the
    CPU and are then moved to the device they were saved from. If this fails
    (e.g. because the run time system doesn't have certain devices), an exception
    is raised. However, storages can be dynamically remapped to an alternative
    set of devices using the :attr:`map_location` argument.

    If :attr:`map_location` is a callable, it will be called once for each serialized
    storage with two arguments: storage and location. The storage argument
    will be the initial deserialization of the storage, residing on the CPU.
    Each serialized storage has a location tag associated with it which
    identifies the device it was saved from, and this tag is the second
    argument passed to :attr:`map_location`. The builtin location tags are ``'cpu'``
    for CPU tensors and ``'cuda:device_id'`` (e.g. ``'cuda:2'``) for CUDA tensors.
    :attr:`map_location` should return either ``None`` or a storage. If
    :attr:`map_location` returns a storage, it will be used as the final deserialized
    object, already moved to the right device. Otherwise, :func:`torch.load` will
    fall back to the default behavior, as if :attr:`map_location` wasn't specified.

    If :attr:`map_location` is a :class:`torch.device` object or a string containing
    a device tag, it indicates the location where all tensors should be loaded.

    Otherwise, if :attr:`map_location` is a dict, it will be used to remap location tags
    appearing in the file (keys), to ones that specify where to put the
    storages (values).

    User extensions can register their own location tags and tagging and
    deserialization methods using :func:`torch.serialization.register_package`.

    Args:
        f: a file-like object (has to implement :meth:`read`, :meth`readline`, :meth`tell`, and :meth`seek`),
            or a string or os.PathLike object containing a file name
        map_location: a function, :class:`torch.device`, string or a dict specifying how to remap storage
            locations
        pickle_module: module used for unpickling metadata and objects (has to
            match the :attr:`pickle_module` used to serialize file)
        pickle_load_args: (Python 3 only) optional keyword arguments passed over to
            :func:`pickle_module.load` and :func:`pickle_module.Unpickler`, e.g.,
            :attr:`errors=...`.

    .. warning::
        :func:`torch.load()` uses ``pickle`` module implicitly, which is known to be insecure.
        It is possible to construct malicious pickle data which will execute arbitrary code
        during unpickling. Never load data that could have come from an untrusted
        source, or that could have been tampered with. **Only load data you trust**.

    .. note::
        When you call :func:`torch.load()` on a file which contains GPU tensors, those tensors
        will be loaded to GPU by default. You can call ``torch.load(.., map_location='cpu')``
        and then :meth:`load_state_dict` to avoid GPU RAM surge when loading a model checkpoint.

    .. note::
        By default, we decode byte strings as ``utf-8``.  This is to avoid a common error
        case ``UnicodeDecodeError: 'ascii' codec can't decode byte 0x...``
        when loading files saved by Python 2 in Python 3.  If this default
        is incorrect, you may use an extra :attr:`encoding` keyword argument to specify how
        these objects should be loaded, e.g., :attr:`encoding='latin1'` decodes them
        to strings using ``latin1`` encoding, and :attr:`encoding='bytes'` keeps them
        as byte arrays which can be decoded later with ``byte_array.decode(...)``.

    Example:
        >>> torch.load('tensors.pt')
        # Load all tensors onto the CPU
        >>> torch.load('tensors.pt', map_location=torch.device('cpu'))
        # Load all tensors onto the CPU, using a function
        >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage)
        # Load all tensors onto GPU 1
        >>> torch.load('tensors.pt', map_location=lambda storage, loc: storage.cuda(1))
        # Map tensors from GPU 1 to GPU 0
        >>> torch.load('tensors.pt', map_location={'cuda:1':'cuda:0'})
        # Load tensor from io.BytesIO object
        >>> with open('tensor.pt', 'rb') as f:
                buffer = io.BytesIO(f.read())
        >>> torch.load(buffer)
        # Load a module with 'ascii' encoding for unpickling
        >>> torch.load('module.pt', encoding='ascii')
    """
    ...

"""
This type stub file was generated by pyright.
"""

import torch
from typing import Dict, Optional, Tuple
from torch import Tensor

"""Locally Optimal Block Preconditioned Conjugate Gradient methods.
"""
def lobpcg(A: Tensor, k: Optional[int] = ..., B: Optional[Tensor] = ..., X: Optional[Tensor] = ..., n: Optional[int] = ..., iK: Optional[Tensor] = ..., niter: Optional[int] = ..., tol: Optional[float] = ..., largest: Optional[bool] = ..., method: Optional[str] = ..., tracker: Optional[None] = ..., ortho_iparams: Optional[Dict[str, int]] = ..., ortho_fparams: Optional[Dict[str, float]] = ..., ortho_bparams: Optional[Dict[str, bool]] = ...) -> Tuple[Tensor, Tensor]:
    """Find the k largest (or smallest) eigenvalues and the corresponding
    eigenvectors of a symmetric positive defined generalized
    eigenvalue problem using matrix-free LOBPCG methods.

    This function is a front-end to the following LOBPCG algorithms
    selectable via `method` argument:

      `method="basic"` - the LOBPCG method introduced by Andrew
      Knyazev, see [Knyazev2001]. A less robust method, may fail when
      Cholesky is applied to singular input.

      `method="ortho"` - the LOBPCG method with orthogonal basis
      selection [StathopoulosEtal2002]. A robust method.

    Supported inputs are dense, sparse, and batches of dense matrices.

    .. note:: In general, the basic method spends least time per
      iteration. However, the robust methods converge much faster and
      are more stable. So, the usage of the basic method is generally
      not recommended but there exist cases where the usage of the
      basic method may be preferred.

    Arguments:

      A (Tensor): the input tensor of size :math:`(*, m, m)`

      B (Tensor, optional): the input tensor of size :math:`(*, m,
                  m)`. When not specified, `B` is interpereted as
                  identity matrix.

      X (tensor, optional): the input tensor of size :math:`(*, m, n)`
                  where `k <= n <= m`. When specified, it is used as
                  initial approximation of eigenvectors. X must be a
                  dense tensor.

      iK (tensor, optional): the input tensor of size :math:`(*, m,
                  m)`. When specified, it will be used as preconditioner.

      k (integer, optional): the number of requested
                  eigenpairs. Default is the number of :math:`X`
                  columns (when specified) or `1`.

      n (integer, optional): if :math:`X` is not specified then `n`
                  specifies the size of the generated random
                  approximation of eigenvectors. Default value for `n`
                  is `k`. If :math:`X` is specifed, the value of `n`
                  (when specified) must be the number of :math:`X`
                  columns.

      tol (float, optional): residual tolerance for stopping
                 criterion. Default is `feps ** 0.5` where `feps` is
                 smallest non-zero floating-point number of the given
                 input tensor `A` data type.

      largest (bool, optional): when True, solve the eigenproblem for
                 the largest eigenvalues. Otherwise, solve the
                 eigenproblem for smallest eigenvalues. Default is
                 `True`.

      method (str, optional): select LOBPCG method. See the
                 description of the function above. Default is
                 "ortho".

      niter (int, optional): maximum number of iterations. When
                 reached, the iteration process is hard-stopped and
                 the current approximation of eigenpairs is returned.
                 For infinite iteration but until convergence criteria
                 is met, use `-1`.

      tracker (callable, optional) : a function for tracing the
                 iteration process. When specified, it is called at
                 each iteration step with LOBPCG instance as an
                 argument. The LOBPCG instance holds the full state of
                 the iteration process in the following attributes:

                   `iparams`, `fparams`, `bparams` - dictionaries of
                   integer, float, and boolean valued input
                   parameters, respectively

                   `ivars`, `fvars`, `bvars`, `tvars` - dictionaries
                   of integer, float, boolean, and Tensor valued
                   iteration variables, respectively.

                   `A`, `B`, `iK` - input Tensor arguments.

                   `E`, `X`, `S`, `R` - iteration Tensor variables.

                 For instance:

                   `ivars["istep"]` - the current iteration step
                   `X` - the current approximation of eigenvectors
                   `E` - the current approximation of eigenvalues
                   `R` - the current residual
                   `ivars["converged_count"]` - the current number of converged eigenpairs
                   `tvars["rerr"]` - the current state of convergence criteria

                 Note that when `tracker` stores Tensor objects from
                 the LOBPCG instance, it must make copies of these.

                 If `tracker` sets `bvars["force_stop"] = True`, the
                 iteration process will be hard-stopped.

      ortho_iparams, ortho_fparams, ortho_bparams (dict, optional):
                 various parameters to LOBPCG algorithm when using
                 `method="ortho"`.

    Returns:

      E (Tensor): tensor of eigenvalues of size :math:`(*, k)`

      X (Tensor): tensor of eigenvectors of size :math:`(*, m, k)`

    References:

      [Knyazev2001] Andrew V. Knyazev. (2001) Toward the Optimal
      Preconditioned Eigensolver: Locally Optimal Block Preconditioned
      Conjugate Gradient Method. SIAM J. Sci. Comput., 23(2),
      517-541. (25 pages)
      https://epubs.siam.org/doi/abs/10.1137/S1064827500366124

      [StathopoulosEtal2002] Andreas Stathopoulos and Kesheng
      Wu. (2002) A Block Orthogonalization Procedure with Constant
      Synchronization Requirements. SIAM J. Sci. Comput., 23(6),
      2165-2182. (18 pages)
      https://epubs.siam.org/doi/10.1137/S1064827500370883

      [DuerschEtal2018] Jed A. Duersch, Meiyue Shao, Chao Yang, Ming
      Gu. (2018) A Robust and Efficient Implementation of LOBPCG.
      SIAM J. Sci. Comput., 40(5), C655-C676. (22 pages)
      https://epubs.siam.org/doi/abs/10.1137/17M1129830

    """
    ...

class LOBPCG(object):
    """Worker class of LOBPCG methods.
    """
    def __init__(self, A: Optional[Tensor], B: Optional[Tensor], X: Tensor, iK: Optional[Tensor], iparams: Dict[str, int], fparams: Dict[str, float], bparams: Dict[str, bool], method: str, tracker: Optional[None]) -> None:
        ...
    
    def __str__(self) -> str:
        ...
    
    def update(self):
        """Set and update iteration variables.
        """
        ...
    
    def update_residual(self):
        """Update residual R from A, B, X, E.
        """
        ...
    
    def update_converged_count(self):
        """Determine the number of converged eigenpairs using backward stable
        convergence criterion, see discussion in Sec 4.3 of [DuerschEtal2018].

        Users may redefine this method for custom convergence criteria.
        """
        ...
    
    def stop_iteration(self):
        """Return True to stop iterations.

        Note that tracker (if defined) can force-stop iterations by
        setting ``worker.bvars['force_stop'] = True``.
        """
        ...
    
    def run(self):
        """Run LOBPCG iterations.

        Use this method as a template for implementing LOBPCG
        iteration scheme with custom tracker that is compatible with
        TorchScript.
        """
        ...
    
    @torch.jit.unused
    def call_tracker(self):
        """Interface for tracking iteration process in Python mode.

        Tracking the iteration process is disabled in TorchScript
        mode. In fact, one should specify tracker=None when JIT
        compiling functions using lobpcg.
        """
        ...
    


LOBPCG_call_tracker_orig = LOBPCG.call_tracker
def LOBPCG_call_tracker(self):
    ...

