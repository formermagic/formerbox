"""
This type stub file was generated by pyright.
"""

import torch
from dataclasses import dataclass
from enum import Enum
from typing import Dict, List, Optional, Union
from torch.utils.data.dataset import Dataset
from ...models.auto.modeling_auto import MODEL_FOR_QUESTION_ANSWERING_MAPPING
from ...tokenization_utils import PreTrainedTokenizer
from ...utils import logging
from ..processors.squad import SquadFeatures

logger = logging.get_logger(__name__)
MODEL_CONFIG_CLASSES = list(MODEL_FOR_QUESTION_ANSWERING_MAPPING.keys())
MODEL_TYPES = tuple(conf.model_type for conf in MODEL_CONFIG_CLASSES)
@dataclass
class SquadDataTrainingArguments:
    """
    Arguments pertaining to what data we are going to input our model for training and eval.
    """
    model_type: str = ...
    data_dir: str = ...
    max_seq_length: int = ...
    doc_stride: int = ...
    max_query_length: int = ...
    max_answer_length: int = ...
    overwrite_cache: bool = ...
    version_2_with_negative: bool = ...
    null_score_diff_threshold: float = ...
    n_best_size: int = ...
    lang_id: int = ...
    threads: int = ...


class Split(Enum):
    train = ...
    dev = ...


class SquadDataset(Dataset):
    """
    This will be superseded by a framework-agnostic approach soon.
    """
    args: SquadDataTrainingArguments
    features: List[SquadFeatures]
    mode: Split
    is_language_sensitive: bool
    def __init__(self, args: SquadDataTrainingArguments, tokenizer: PreTrainedTokenizer, limit_length: Optional[int] = ..., mode: Union[str, Split] = ..., is_language_sensitive: Optional[bool] = ..., cache_dir: Optional[str] = ..., dataset_format: Optional[str] = ...) -> None:
        ...
    
    def __len__(self):
        ...
    
    def __getitem__(self, i) -> Dict[str, torch.Tensor]:
        ...
    


