"""
This type stub file was generated by pyright.
"""

from .tokenization_roberta import RobertaTokenizer
from .utils import logging

logger = logging.get_logger(__name__)
vocab_url = "https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-vocab.json"
merges_url = "https://s3.amazonaws.com/models.huggingface.co/bert/roberta-large-merges.txt"
_all_longformer_models = ["allenai/longformer-base-4096", "allenai/longformer-large-4096", "allenai/longformer-large-4096-finetuned-triviaqa", "allenai/longformer-base-4096-extra.pos.embd.only", "allenai/longformer-large-4096-extra.pos.embd.only"]
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = { "allenai/longformer-base-4096": 4096,"allenai/longformer-large-4096": 4096,"allenai/longformer-large-4096-finetuned-triviaqa": 4096,"allenai/longformer-base-4096-extra.pos.embd.only": 4096,"allenai/longformer-large-4096-extra.pos.embd.only": 4096 }
class LongformerTokenizer(RobertaTokenizer):
    r"""
    Construct a Longformer tokenizer.

    :class:`~transformers.LongformerTokenizer` is identical to :class:`~transformers.RobertaTokenizer`. Refer to
    the superclass for usage examples and documentation concerning parameters.
    """
    max_model_input_sizes = ...
    pretrained_vocab_files_map = ...


