"""
This type stub file was generated by pyright.
"""

from typing import List, Optional
from .file_utils import add_start_docstrings, is_sentencepiece_available
from .tokenization_reformer_fast import ReformerTokenizerFast
from .tokenization_utils_base import BatchEncoding, PREPARE_SEQ2SEQ_BATCH_DOCSTRING
from .tokenization_pegasus import PegasusTokenizer

if is_sentencepiece_available():
    ...
else:
    PegasusTokenizer = None
SPIECE_UNDERLINE = "â–"
VOCAB_FILES_NAMES = { "vocab_file": "spiece.model","tokenizer_file": "tokenizer.json" }
PRETRAINED_VOCAB_FILES_MAP = { "vocab_file": { "google/pegasus-xsum": "https://cdn.huggingface.co/google/pegasus-xsum/spiece.model" },"tokenizer_file": { "google/pegasus-xsum": "https://cdn.huggingface.co/google/pegasus-xsum/tokenizer.json" } }
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = { "google/pegasus-xsum": 512 }
class PegasusTokenizerFast(ReformerTokenizerFast):
    offset = ...
    vocab_files_names = ...
    pretrained_vocab_files_map = ...
    max_model_input_sizes = ...
    slow_tokenizer_class = ...
    def get_special_tokens_mask(self, token_ids_0: List, token_ids_1: Optional[List] = ..., already_has_special_tokens: bool = ...) -> List[int]:
        """Get list where entries are [1] if a token is [eos] or [pad] else 0."""
        ...
    
    def build_inputs_with_special_tokens(self, token_ids_0, token_ids_1=...) -> List[int]:
        """
        Build model inputs from a sequence by adding eos to the end. no bos token is added to the front.
        - single sequence: ``X </s>``
        - pair of sequences: ``A B </s>``  (not intended use)

        Args:
            token_ids_0 (:obj:`List[int]`):
                List of IDs to which the special tokens will be added
            token_ids_1 (:obj:`List[int]`, `optional`):
                Optional second list of IDs for sequence pairs.

        Returns:
            :obj:`List[int]`: list of `input IDs <../glossary.html#input-ids>`__ with the appropriate special tokens.
        """
        ...
    
    @add_start_docstrings(PREPARE_SEQ2SEQ_BATCH_DOCSTRING)
    def prepare_seq2seq_batch(self, src_texts: List[str], tgt_texts: Optional[List[str]] = ..., max_length: Optional[int] = ..., max_target_length: Optional[int] = ..., return_tensors: str = ..., truncation=..., padding=..., **unused) -> BatchEncoding:
        ...
    


