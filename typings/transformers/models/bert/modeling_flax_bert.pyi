"""
This type stub file was generated by pyright.
"""

import numpy as np
import flax.linen as nn
import jax
import jax.numpy as jnp
from typing import Callable, Dict, Tuple
from flax.core.frozen_dict import FrozenDict
from jax.random import PRNGKey
from ...file_utils import add_start_docstrings, add_start_docstrings_to_model_forward
from ...modeling_flax_utils import FlaxPreTrainedModel
from ...utils import logging
from .configuration_bert import BertConfig

logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "BertConfig"
_TOKENIZER_FOR_DOC = "BertTokenizer"
BERT_START_DOCSTRING = r"""

    This model inherits from :class:`~transformers.FlaxPreTrainedModel`. Check the superclass documentation for the
    generic methods the library implements for all its model (such as downloading, saving and converting weights from
    PyTorch models)

    This model is also a Flax Linen `flax.nn.Module
    <https://flax.readthedocs.io/en/latest/_autosummary/flax.nn.module.html>`__ subclass. Use it as a regular Flax
    Module and refer to the Flax documentation for all matter related to general usage and behavior.

    Finally, this model supports inherent JAX features such as:

    - `Just-In-Time (JIT) compilation <https://jax.readthedocs.io/en/latest/jax.html#just-in-time-compilation-jit>`__
    - `Automatic Differentiation <https://jax.readthedocs.io/en/latest/jax.html#automatic-differentiation>`__
    - `Vectorization <https://jax.readthedocs.io/en/latest/jax.html#vectorization-vmap>`__
    - `Parallelization <https://jax.readthedocs.io/en/latest/jax.html#parallelization-pmap>`__

    Parameters:
        config (:class:`~transformers.BertConfig`): Model configuration class with all the parameters of the model.
            Initializing with a config file does not load the weights associated with the model, only the
            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model
            weights.
"""
BERT_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (:obj:`numpy.ndarray` of shape :obj:`({0})`):
            Indices of input sequence tokens in the vocabulary.

            Indices can be obtained using :class:`~transformers.BertTokenizer`. See
            :meth:`transformers.PreTrainedTokenizer.encode` and :func:`transformers.PreTrainedTokenizer.__call__` for
            details.

            `What are input IDs? <../glossary.html#input-ids>`__
        attention_mask (:obj:`numpy.ndarray` of shape :obj:`({0})`, `optional`):
            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            `What are attention masks? <../glossary.html#attention-mask>`__
        token_type_ids (:obj:`numpy.ndarray` of shape :obj:`({0})`, `optional`):
            Segment token indices to indicate first and second portions of the inputs. Indices are selected in ``[0,
            1]``:

            - 0 corresponds to a `sentence A` token,
            - 1 corresponds to a `sentence B` token.

            `What are token type IDs? <../glossary.html#token-type-ids>`__
        position_ids (:obj:`numpy.ndarray` of shape :obj:`({0})`, `optional`):
            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range ``[0,
            config.max_position_embeddings - 1]``.
        return_dict (:obj:`bool`, `optional`):
            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.
"""
class FlaxBertLayerNorm(nn.Module):
    """
    Layer normalization (https://arxiv.org/abs/1607.06450). Operates on the last axis of the input data.
    """
    epsilon: float = ...
    dtype: jnp.dtype = ...
    bias: bool = ...
    scale: bool = ...
    scale_init: Callable[..., np.ndarray] = ...
    bias_init: Callable[..., np.ndarray] = ...
    @nn.compact
    def __call__(self, x):
        """
        Applies layer normalization on the input. It normalizes the activations of the layer for each given example in
        a batch independently, rather than across a batch like Batch Normalization. i.e. applies a transformation that
        maintains the mean activation within each example close to 0 and the activation standard deviation close to 1

        Args:
          x: the inputs

        Returns:
          Normalized inputs (the same shape as inputs).
        """
        ...
    


class FlaxBertEmbedding(nn.Module):
    """
    Specify a new class for doing the embedding stuff as Flax's one use 'embedding' for the parameter name and PyTorch
    use 'weight'
    """
    vocab_size: int
    hidden_size: int
    kernel_init_scale: float = ...
    emb_init: Callable[..., np.ndarray] = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, inputs):
        ...
    


class FlaxBertEmbeddings(nn.Module):
    """Construct the embeddings from word, position and token_type embeddings."""
    vocab_size: int
    hidden_size: int
    type_vocab_size: int
    max_length: int
    kernel_init_scale: float = ...
    dropout_rate: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, input_ids, token_type_ids, position_ids, attention_mask, deterministic: bool = ...):
        ...
    


class FlaxBertAttention(nn.Module):
    num_heads: int
    head_size: int
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states, attention_mask, deterministic: bool = ...):
        ...
    


class FlaxBertIntermediate(nn.Module):
    output_size: int
    hidden_act: str = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states):
        ...
    


class FlaxBertOutput(nn.Module):
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, intermediate_output, attention_output, deterministic: bool = ...):
        ...
    


class FlaxBertLayer(nn.Module):
    num_heads: int
    head_size: int
    intermediate_size: int
    hidden_act: str = ...
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states, attention_mask, deterministic: bool = ...):
        ...
    


class FlaxBertLayerCollection(nn.Module):
    """
    Stores N BertLayer(s)
    """
    num_layers: int
    num_heads: int
    head_size: int
    intermediate_size: int
    hidden_act: str = ...
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, inputs, attention_mask, deterministic: bool = ...):
        ...
    


class FlaxBertEncoder(nn.Module):
    num_layers: int
    num_heads: int
    head_size: int
    intermediate_size: int
    hidden_act: str = ...
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states, attention_mask, deterministic: bool = ...):
        ...
    


class FlaxBertPooler(nn.Module):
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states):
        ...
    


class FlaxBertPredictionHeadTransform(nn.Module):
    hidden_act: str = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states):
        ...
    


class FlaxBertLMPredictionHead(nn.Module):
    vocab_size: int
    hidden_act: str = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states):
        ...
    


class FlaxBertOnlyMLMHead(nn.Module):
    vocab_size: int
    hidden_act: str = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, hidden_states):
        ...
    


class FlaxBertPreTrainedModel(FlaxPreTrainedModel):
    """
    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained
    models.
    """
    config_class = ...
    base_model_prefix = ...
    def init(self, rng: jax.random.PRNGKey, input_shape: Tuple) -> FrozenDict:
        ...
    
    @staticmethod
    def convert_from_pytorch(pt_state: Dict, config: BertConfig) -> Dict:
        ...
    


@add_start_docstrings("The bare Bert Model transformer outputting raw hidden-states without any specific head on top.", BERT_START_DOCSTRING)
class FlaxBertModel(FlaxBertPreTrainedModel):
    """
    The model can behave as an encoder (with only self-attention) as well as a decoder, in which case a layer of
    cross-attention is added between the self-attention layers, following the architecture described in `Attention is
    all you need <https://arxiv.org/abs/1706.03762>`__ by Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit,
    Llion Jones, Aidan N. Gomez, Lukasz Kaiser and Illia Polosukhin.
    """
    def __init__(self, config: BertConfig, input_shape: Tuple = ..., seed: int = ..., dtype: jnp.dtype = ..., **kwargs) -> None:
        ...
    
    @add_start_docstrings_to_model_forward(BERT_INPUTS_DOCSTRING.format("batch_size, sequence_length"))
    def __call__(self, input_ids, attention_mask=..., token_type_ids=..., position_ids=..., params: dict = ..., dropout_rng: PRNGKey = ..., train: bool = ...):
        ...
    


class FlaxBertModule(nn.Module):
    vocab_size: int
    hidden_size: int
    type_vocab_size: int
    max_length: int
    num_encoder_layers: int
    num_heads: int
    head_size: int
    intermediate_size: int
    hidden_act: str = ...
    dropout_rate: float = ...
    kernel_init_scale: float = ...
    dtype: jnp.dtype = ...
    add_pooling_layer: bool = ...
    @nn.compact
    def __call__(self, input_ids, attention_mask, token_type_ids, position_ids, deterministic: bool = ...):
        ...
    


class FlaxBertForMaskedLM(FlaxBertPreTrainedModel):
    def __init__(self, config: BertConfig, input_shape: Tuple = ..., seed: int = ..., dtype: jnp.dtype = ..., **kwargs) -> None:
        ...
    
    def __call__(self, input_ids, attention_mask=..., token_type_ids=..., position_ids=..., params: dict = ..., dropout_rng: PRNGKey = ..., train: bool = ...):
        ...
    


class FlaxBertForMaskedLMModule(nn.Module):
    vocab_size: int
    hidden_size: int
    intermediate_size: int
    head_size: int
    num_heads: int
    num_encoder_layers: int
    type_vocab_size: int
    max_length: int
    hidden_act: str
    dropout_rate: float = ...
    dtype: jnp.dtype = ...
    @nn.compact
    def __call__(self, input_ids, attention_mask=..., token_type_ids=..., position_ids=..., deterministic: bool = ...):
        ...
    


