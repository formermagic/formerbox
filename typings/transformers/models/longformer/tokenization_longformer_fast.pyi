"""
This type stub file was generated by pyright.
"""

from ...utils import logging
from ..roberta.tokenization_roberta_fast import RobertaTokenizerFast

logger = logging.get_logger(__name__)
vocab_url = "https://huggingface.co/roberta-large/resolve/main/vocab.json"
merges_url = "https://huggingface.co/roberta-large/resolve/main/merges.txt"
tokenizer_url = "https://huggingface.co/roberta-large/resolve/main/tokenizer.json"
_all_longformer_models = ["allenai/longformer-base-4096", "allenai/longformer-large-4096", "allenai/longformer-large-4096-finetuned-triviaqa", "allenai/longformer-base-4096-extra.pos.embd.only", "allenai/longformer-large-4096-extra.pos.embd.only"]
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = { "allenai/longformer-base-4096": 4096,"allenai/longformer-large-4096": 4096,"allenai/longformer-large-4096-finetuned-triviaqa": 4096,"allenai/longformer-base-4096-extra.pos.embd.only": 4096,"allenai/longformer-large-4096-extra.pos.embd.only": 4096 }
class LongformerTokenizerFast(RobertaTokenizerFast):
    r"""
    Construct a "fast" Longformer tokenizer (backed by HuggingFace's `tokenizers` library).

    :class:`~transformers.LongformerTokenizerFast` is identical to :class:`~transformers.RobertaTokenizerFast`. Refer
    to the superclass for usage examples and documentation concerning parameters.
    """
    max_model_input_sizes = ...
    pretrained_vocab_files_map = ...
    slow_tokenizer_class = ...


