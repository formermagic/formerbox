"""
This type stub file was generated by pyright.
"""

from ...utils import logging
from ..roberta.tokenization_roberta import RobertaTokenizer

logger = logging.get_logger(__name__)
vocab_url = "https://huggingface.co/roberta-large/resolve/main/vocab.json"
merges_url = "https://huggingface.co/roberta-large/resolve/main/merges.txt"
_all_longformer_models = ["allenai/longformer-base-4096", "allenai/longformer-large-4096", "allenai/longformer-large-4096-finetuned-triviaqa", "allenai/longformer-base-4096-extra.pos.embd.only", "allenai/longformer-large-4096-extra.pos.embd.only"]
PRETRAINED_POSITIONAL_EMBEDDINGS_SIZES = { "allenai/longformer-base-4096": 4096,"allenai/longformer-large-4096": 4096,"allenai/longformer-large-4096-finetuned-triviaqa": 4096,"allenai/longformer-base-4096-extra.pos.embd.only": 4096,"allenai/longformer-large-4096-extra.pos.embd.only": 4096 }
class LongformerTokenizer(RobertaTokenizer):
    r"""
    Construct a Longformer tokenizer.

    :class:`~transformers.LongformerTokenizer` is identical to :class:`~transformers.RobertaTokenizer`. Refer to the
    superclass for usage examples and documentation concerning parameters.
    """
    max_model_input_sizes = ...
    pretrained_vocab_files_map = ...


