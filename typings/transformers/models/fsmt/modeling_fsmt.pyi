"""
This type stub file was generated by pyright.
"""

from typing import Any, Dict, Optional, Tuple
from torch import Tensor, nn
from ...file_utils import add_code_sample_docstrings, add_end_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, replace_return_docstrings
from ...modeling_outputs import Seq2SeqLMOutput, Seq2SeqModelOutput
from ...modeling_utils import PreTrainedModel
from ...utils import logging
from .configuration_fsmt import FSMTConfig

"""PyTorch Fairseq model, ported from https://github.com/pytorch/fairseq/tree/master/examples/wmt19"""
logger = logging.get_logger(__name__)
_CONFIG_FOR_DOC = "FSMTConfig"
_TOKENIZER_FOR_DOC = "FSMTTokenizer"
FSMT_START_DOCSTRING = r"""

    This model inherits from :class:`~transformers.PreTrainedModel`. Check the superclass documentation for the generic
    methods the library implements for all its model (such as downloading or saving, resizing the input embeddings,
    pruning heads etc.)

    This model is also a PyTorch `torch.nn.Module <https://pytorch.org/docs/stable/nn.html#torch.nn.Module>`__
    subclass. Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to
    general usage and behavior.

    Parameters:
        config (:class:`~transformers.FSMTConfig`): Model configuration class with all the parameters of the model.
            Initializing with a config file does not load the weights associated with the model, only the
            configuration. Check out the :meth:`~transformers.PreTrainedModel.from_pretrained` method to load the model
            weights.

"""
FSMT_GENERATION_EXAMPLE = r"""
    Translation example::

        from transformers import FSMTTokenizer, FSMTForConditionalGeneration

        mname = "facebook/wmt19-ru-en"
        model = FSMTForConditionalGeneration.from_pretrained(mname)
        tokenizer = FSMTTokenizer.from_pretrained(mname)

        src_text = "Машинное обучение - это здорово, не так ли?"
        input_ids = tokenizer.encode(src_text, return_tensors='pt')
        outputs = model.generate(input_ids, num_beams=5, num_return_sequences=3)
        for i, output in enumerate(outputs):
            decoded = tokenizer.decode(output, skip_special_tokens=True)
            print(f"{i}: {decoded})
         # 1: Machine learning is great, isn't it? ...

"""
FSMT_INPUTS_DOCSTRING = r"""
    Args:
        input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`):
            Indices of input sequence tokens in the vocabulary.

            IIndices can be obtained using :class:`~transformers.FSTMTokenizer`. See
            :meth:`transformers.PreTrainedTokenizer.encode` and :meth:`transformers.PreTrainedTokenizer.__call__` for
            details.

            `What are input IDs? <../glossary.html#input-ids>`__
        attention_mask (:obj:`torch.Tensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Mask to avoid performing attention on padding token indices. Mask values selected in ``[0, 1]``:

            - 1 for tokens that are **not masked**,
            - 0 for tokens that are **masked**.

            `What are attention masks? <../glossary.html#attention-mask>`__
        decoder_input_ids (:obj:`torch.LongTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):
            Provide for translation and summarization training. By default, the model will create this tensor by
            shifting the input_ids right, following the paper.
        decoder_attention_mask (:obj:`torch.BoolTensor` of shape :obj:`(batch_size, target_sequence_length)`, `optional`):
            Default behavior: generate a tensor that ignores pad tokens in :obj:`decoder_input_ids`. Causal mask will
            also be used by default. If you want to change padding behavior, you should read
            :func:`modeling_fstm._prepare_fstm_decoder_inputs` and modify. See diagram 1 in the paper for more info on
            the default strategy
        encoder_outputs (:obj:`Tuple(torch.FloatTensor)`, `optional`):
            Tuple consists of (:obj:`last_hidden_state`, `optional`: :obj:`hidden_states`, `optional`:
            :obj:`attentions`) :obj:`last_hidden_state` of shape :obj:`(batch_size, sequence_length, hidden_size)` is a
            sequence of hidden-states at the output of the last layer of the encoder. Used in the cross-attention of
            the decoder.
        past_key_values (:obj:`Tuple(torch.FloatTensor)` of length :obj:`config.n_layers` with each tuple having 4 tensors of shape :obj:`(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`):
            Contains precomputed key and value hidden-states of the attention blocks. Can be used to speed up decoding.
            If :obj:`past_key_values` are used, the user can optionally input only the last :obj:`decoder_input_ids`
            (those that don't have their past key value states given to this model) of shape :obj:`(batch_size, 1)`
            instead of all :obj:`decoder_input_ids` of shape :obj:`(batch_size, sequence_length)`.
        use_cache (:obj:`bool`, `optional`, defaults to :obj:`True`):
            If set to :obj:`True`, :obj:`past_key_values` key value states are returned and can be used to speed up
            decoding (see :obj:`past_key_values`).
        output_attentions (:obj:`bool`, `optional`):
            Whether or not to return the attentions tensors of all attention layers. See ``attentions`` under returned
            tensors for more detail.
        output_hidden_states (:obj:`bool`, `optional`):
            Whether or not to return the hidden states of all layers. See ``hidden_states`` under returned tensors for
            more detail.
        return_dict (:obj:`bool`, `optional`):
            Whether or not to return a :class:`~transformers.file_utils.ModelOutput` instead of a plain tuple.
"""
def invert_mask(attention_mask):
    """Turns 1->0, 0->1, False->True, True-> False"""
    ...

class PretrainedFSMTModel(PreTrainedModel):
    config_class = ...
    base_model_prefix = ...
    @property
    def dummy_inputs(self):
        ...
    


def shift_tokens_right(input_ids, pad_token_id):
    """Shift input ids one token to the right, and wrap the last non pad token (usually <eos>)."""
    ...

def make_padding_mask(input_ids, padding_idx=...):
    """True for pad tokens"""
    ...

class EncoderLayer(nn.Module):
    def __init__(self, config: FSMTConfig) -> None:
        ...
    
    def forward(self, x, encoder_padding_mask, output_attentions=...):
        """
        Args:
            x (Tensor): input to the layer of shape `(seq_len, batch, embed_dim)`
            encoder_padding_mask (ByteTensor): binary ByteTensor of shape
                `(batch, src_len)` where padding elements are indicated by ``1``.
            for t_tgt, t_src is excluded (or masked out), =0 means it is
            included in attention

        Returns:
            encoded output of shape `(seq_len, batch, embed_dim)`
        """
        ...
    


class FSMTEncoder(nn.Module):
    """
    Transformer encoder consisting of *config.encoder_layers* self attention layers. Each layer is a
    :class:`EncoderLayer`.

    Args:
        config: FSMTConfig
    """
    def __init__(self, config: FSMTConfig, embed_tokens) -> None:
        ...
    
    def forward(self, input_ids, attention_mask=..., output_attentions=..., output_hidden_states=..., return_dict=...):
        """
        Args:
            input_ids (LongTensor): tokens in the source language of shape
                `(batch, src_len)`
            attention_mask (torch.LongTensor): indicating which indices are padding tokens

        Returns:
            BaseModelOutput or Tuple comprised of:

                - **x** (Tensor): the last encoder layer's output of shape `(src_len, batch, embed_dim)`
                - **encoder_states** (tuple(torch.FloatTensor)): all intermediate hidden states of shape `(src_len,
                  batch, embed_dim)`. Only populated if *output_hidden_states:* is True.
                - **all_attentions** (tuple(torch.FloatTensor)): Attention weights for each layer.
                During training might not be of length n_layers because of layer dropout.
        """
        ...
    


class DecoderLayer(nn.Module):
    def __init__(self, config: FSMTConfig) -> None:
        ...
    
    def forward(self, x, encoder_hidden_states, encoder_attn_mask=..., layer_state=..., causal_mask=..., decoder_padding_mask=..., output_attentions=...):
        ...
    


class FSMTDecoder(nn.Module):
    """
    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`

    Args:
        config: FSMTConfig
        embed_tokens (torch.nn.Embedding): output embedding
    """
    def __init__(self, config: FSMTConfig, embed_tokens: nn.Embedding) -> None:
        ...
    
    def forward(self, input_ids, encoder_hidden_states, encoder_padding_mask, decoder_padding_mask, decoder_causal_mask, past_key_values=..., use_cache=..., output_attentions=..., output_hidden_states=..., return_dict=...):
        """
        Includes several features from "Jointly Learning to Align and Translate with Transformer Models" (Garg et al.,
        EMNLP 2019).

        Args:
            input_ids (LongTensor): previous decoder outputs of shape
                `(batch, tgt_len)`, for teacher forcing
            encoder_hidden_states: output from the encoder, used for
                encoder-side attention
            encoder_padding_mask: for ignoring pad tokens
            past_key_values (dict or None): dictionary used for storing state during generation

        Returns:
            BaseModelOutputWithPast or tuple:

                - the decoder's features of shape `(batch, tgt_len, embed_dim)`
                - the cache
                - hidden states
                - attentions
        """
        ...
    


class Attention(nn.Module):
    """Multi-headed attention from 'Attention Is All You Need' paper"""
    def __init__(self, embed_dim, num_heads, dropout=..., bias=..., encoder_decoder_attention=...) -> None:
        ...
    
    def forward(self, query, key: Optional[Tensor], key_padding_mask: Optional[Tensor] = ..., layer_state: Optional[Dict[str, Optional[Tensor]]] = ..., attn_mask: Optional[Tensor] = ..., output_attentions=...) -> Tuple[Tensor, Optional[Tensor]]:
        """Input shape: Time(SeqLen) x Batch x Channel"""
        ...
    


def fill_with_neg_inf(t):
    """FP16-compatible function that fills a input_ids with -inf."""
    ...

@add_start_docstrings("The bare FSMT Model outputting raw hidden-states without any specific head on top.", FSMT_START_DOCSTRING)
class FSMTModel(PretrainedFSMTModel):
    def __init__(self, config: FSMTConfig) -> None:
        ...
    
    @add_start_docstrings_to_model_forward(FSMT_INPUTS_DOCSTRING)
    @add_code_sample_docstrings(tokenizer_class=_TOKENIZER_FOR_DOC, checkpoint="facebook/wmt19-ru-en", output_type=Seq2SeqModelOutput, config_class=_CONFIG_FOR_DOC)
    def forward(self, input_ids, attention_mask=..., decoder_input_ids=..., decoder_attention_mask=..., encoder_outputs: Optional[Tuple] = ..., past_key_values=..., use_cache=..., output_attentions=..., output_hidden_states=..., return_dict=...):
        ...
    
    def get_input_embeddings(self):
        ...
    
    def set_input_embeddings(self, value):
        ...
    
    def get_output_embeddings(self):
        ...
    
    def set_output_embeddings(self, value):
        ...
    


@add_start_docstrings("The FSMT Model with a language modeling head. Can be used for summarization.", FSMT_START_DOCSTRING)
class FSMTForConditionalGeneration(PretrainedFSMTModel):
    base_model_prefix = ...
    _keys_to_ignore_on_load_missing = ...
    _keys_to_ignore_on_save = ...
    def __init__(self, config: FSMTConfig) -> None:
        ...
    
    def resize_token_embeddings(self, new_num_tokens: int) -> nn.Embedding:
        ...
    
    @add_start_docstrings_to_model_forward(FSMT_INPUTS_DOCSTRING)
    @replace_return_docstrings(output_type=Seq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)
    @add_end_docstrings(FSMT_GENERATION_EXAMPLE)
    def forward(self, input_ids, attention_mask=..., decoder_input_ids=..., decoder_attention_mask=..., encoder_outputs=..., past_key_values=..., labels=..., use_cache=..., output_attentions=..., output_hidden_states=..., return_dict=...):
        r"""
        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):
            Labels for computing the masked language modeling loss. Indices should either be in ``[0, ...,
            config.vocab_size]`` or -100 (see ``input_ids`` docstring). Tokens with indices set to ``-100`` are ignored
            (masked), the loss is only computed for the tokens with labels in ``[0, ..., config.vocab_size]``.

        Returns:

        """
        ...
    
    def prepare_inputs_for_generation(self, decoder_input_ids, past=..., attention_mask=..., use_cache=..., encoder_outputs=..., **kwargs):
        ...
    
    def adjust_logits_during_generation(self, logits, cur_len, max_length):
        ...
    
    def get_encoder(self):
        ...
    
    def get_output_embeddings(self):
        ...
    


class SinusoidalPositionalEmbedding(nn.Embedding):
    """
    This module produces sinusoidal positional embeddings of any length.

    We don't want to save the weight of this embedding since it's not trained (deterministic) and it can be huge.

    Padding symbols are ignored.

    These embeddings get automatically extended in forward if more positions is needed.
    """
    def __init__(self, num_positions, embedding_dim, padding_idx) -> None:
        ...
    
    def make_weight(self, num_positions, embedding_dim, padding_idx):
        ...
    
    @staticmethod
    def get_embedding(num_embeddings, embedding_dim, padding_idx):
        """
        Build sinusoidal embeddings.

        This matches the implementation in tensor2tensor, but differs slightly from the description in Section 3.5 of
        "Attention Is All You Need".
        """
        ...
    
    @staticmethod
    def make_positions(tensor, padding_idx: int):
        """
        Replace non-padding symbols with their position numbers.

        Position numbers begin at padding_idx+1. Padding symbols are ignored.
        """
        ...
    
    def forward(self, input, incremental_state: Optional[Any] = ..., timestep: Optional[Tensor] = ...):
        """Input is expected to be of size [bsz x seqlen]."""
        ...
    


