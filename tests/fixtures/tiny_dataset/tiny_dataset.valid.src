 # !/usr/bin/env ▁ python <encdom> from nose . tools import * <newline> import networkx as nx <newline> class TestVitality : <newline> <indent> def test_closeness_vitality_unweighted ( self ) : <newline> <indent> G = nx . cycle_graph ( 3 ) <newline> v = nx . closeness_vitality ( G ) <newline> assert_equal ( v , { 0 : 4.0 , 1 : 4.0 , 2 : 4.0 } ) <newline> <dedent> def test_closeness_vitality_weighted ( self ) : <newline> <indent> G = nx . Graph ( ) <newline> G . add_cycle ( [ 0 , 1 , 2 ] , weight = 2 ) <newline> v = nx . closeness_vitality ( G , weight = 'weight' ) <newline> assert_equal ( v , { 0 : 8.0 , 1 : 8.0 , 2 : 8.0 } ) <newline> <dedent> def test_closeness_vitality_unweighted_digraph ( self ) : <newline> <indent> G = nx . DiGraph ( ) <newline> G . add_cycle ( [ 0 , 1 , 2 ] ) <newline> v = nx . closeness_vitality ( G ) <newline> assert_equal ( v , { 0 : 8.0 , 1 : 8.0 , 2 : 8.0 } ) <newline> <dedent> def test_closeness_vitality_weighted_digraph ( self ) : <newline> <indent> G = nx . DiGraph ( ) <newline> G . add_cycle ( [ 0 , 1 , 2 ] , weight = 2 ) <newline> v = nx . closeness_vitality ( G , weight = 'weight' ) <newline> assert_equal ( v , { 0 : 16.0 , 1 : 16.0 , 2 : 16.0 } ) <newline> <dedent> <dedent>
 # !/usr/bin/env ▁ python3 <encdom>  """ ▁ turtle-example-suite: <strnewline> <strnewline> ▁ tdemo_fractalCurves.py <strnewline> <strnewline> This ▁ program ▁ draws ▁ two ▁ fractal-curve-designs: <strnewline> (1) ▁ A ▁ hilbert ▁ curve ▁ (in ▁ a ▁ box) <strnewline> (2) ▁ A ▁ combination ▁ of ▁ Koch-curves. <strnewline> <strnewline> The ▁ CurvesTurtle ▁ class ▁ and ▁ the ▁ fractal-curve- <strnewline> methods ▁ are ▁ taken ▁ from ▁ the ▁ PythonCard ▁ example <strnewline> scripts ▁ for ▁ turtle-graphics. <strnewline> """  <newline> from turtle import * <newline> from time import sleep , clock <newline> class CurvesTurtle ( Pen ) : <newline>  # ▁ example ▁ derived ▁ from <encdom>  # ▁ Turtle ▁ Geometry: ▁ The ▁ Computer ▁ as ▁ a ▁ Medium ▁ for ▁ Exploring ▁ Mathematics <encdom>  # ▁ by ▁ Harold ▁ Abelson ▁ and ▁ Andrea ▁ diSessa <encdom>  # ▁ p. ▁ 96-98 <encdom> <indent> def hilbert ( self , size , level , parity ) : <newline> <indent> if level == 0 : <newline> <indent> return <newline>  # ▁ rotate ▁ and ▁ draw ▁ first ▁ subcurve ▁ with ▁ opposite ▁ parity ▁ to ▁ big ▁ curve <encdom> <dedent> self . left ( parity * 90 ) <newline> self . hilbert ( size , level - 1 , - parity ) <newline>  # ▁ interface ▁ to ▁ and ▁ draw ▁ second ▁ subcurve ▁ with ▁ same ▁ parity ▁ as ▁ big ▁ curve <encdom> self . forward ( size ) <newline> self . right ( parity * 90 ) <newline> self . hilbert ( size , level - 1 , parity ) <newline>  # ▁ third ▁ subcurve <encdom> self . forward ( size ) <newline> self . hilbert ( size , level - 1 , parity ) <newline>  # ▁ fourth ▁ subcurve <encdom> self . right ( parity * 90 ) <newline> self . forward ( size ) <newline> self . hilbert ( size , level - 1 , - parity ) <newline>  # ▁ a ▁ final ▁ turn ▁ is ▁ needed ▁ to ▁ make ▁ the ▁ turtle <encdom>  # ▁ end ▁ up ▁ facing ▁ outward ▁ from ▁ the ▁ large ▁ square <encdom> self . left ( parity * 90 ) <newline>  # ▁ Visual ▁ Modeling ▁ with ▁ Logo: ▁ A ▁ Structural ▁ Approach ▁ to ▁ Seeing <encdom>  # ▁ by ▁ James ▁ Clayson <encdom>  # ▁ Koch ▁ curve, ▁ after ▁ Helge ▁ von ▁ Koch ▁ who ▁ introduced ▁ this ▁ geometric ▁ figure ▁ in ▁ 1904 <encdom>  # ▁ p. ▁ 146 <encdom> <dedent> def fractalgon ( self , n , rad , lev , dir ) : <newline> <indent> import math <newline>  # ▁ if ▁ dir ▁ = ▁ 1 ▁ turn ▁ outward <encdom>  # ▁ if ▁ dir ▁ = ▁ -1 ▁ turn ▁ inward <encdom> edge = 2 * rad * math . sin ( math . pi / n ) <newline> self . pu ( ) <newline> self . fd ( rad ) <newline> self . pd ( ) <newline> self . rt ( 180 - ( 90 * ( n - 2 ) / n ) ) <newline> for i in range ( n ) : <newline> <indent> self . fractal ( edge , lev , dir ) <newline> self . rt ( 360 / n ) <newline> <dedent> self . lt ( 180 - ( 90 * ( n - 2 ) / n ) ) <newline> self . pu ( ) <newline> self . bk ( rad ) <newline> self . pd ( ) <newline>  # ▁ p. ▁ 146 <encdom> <dedent> def fractal ( self , dist , depth , dir ) : <newline> <indent> if depth < 1 : <newline> <indent> self . fd ( dist ) <newline> return <newline> <dedent> self . fractal ( dist / 3 , depth - 1 , dir ) <newline> self . lt ( 60 * dir ) <newline> self . fractal ( dist / 3 , depth - 1 , dir ) <newline> self . rt ( 120 * dir ) <newline> self . fractal ( dist / 3 , depth - 1 , dir ) <newline> self . lt ( 60 * dir ) <newline> self . fractal ( dist / 3 , depth - 1 , dir ) <newline> <dedent> <dedent> def main ( ) : <newline> <indent> ft = CurvesTurtle ( ) <newline> ft . reset ( ) <newline> ft . speed ( 0 ) <newline> ft . ht ( ) <newline> ft . getscreen ( ) . tracer ( 1 , 0 ) <newline> ft . pu ( ) <newline> size = 6 <newline> ft . setpos ( - 33 * size , - 32 * size ) <newline> ft . pd ( ) <newline> ta = clock ( ) <newline> ft . fillcolor ( "red" ) <newline> ft . begin_fill ( ) <newline> ft . fd ( size ) <newline> ft . hilbert ( size , 6 , 1 ) <newline>  # ▁ frame <encdom> ft . fd ( size ) <newline> for i in range ( 3 ) : <newline> <indent> ft . lt ( 90 ) <newline> ft . fd ( size * ( 64 + i % 2 ) ) <newline> <dedent> ft . pu ( ) <newline> for i in range ( 2 ) : <newline> <indent> ft . fd ( size ) <newline> ft . rt ( 90 ) <newline> <dedent> ft . pd ( ) <newline> for i in range ( 4 ) : <newline> <indent> ft . fd ( size * ( 66 + i % 2 ) ) <newline> ft . rt ( 90 ) <newline> <dedent> ft . end_fill ( ) <newline> tb = clock ( ) <newline> res = "Hilbert: ▁ %.2fsec. ▁ " % ( tb - ta ) <newline> sleep ( 3 ) <newline> ft . reset ( ) <newline> ft . speed ( 0 ) <newline> ft . ht ( ) <newline> ft . getscreen ( ) . tracer ( 1 , 0 ) <newline> ta = clock ( ) <newline> ft . color ( "black" , "blue" ) <newline> ft . begin_fill ( ) <newline> ft . fractalgon ( 3 , 250 , 4 , 1 ) <newline> ft . end_fill ( ) <newline> ft . begin_fill ( ) <newline> ft . color ( "red" ) <newline> ft . fractalgon ( 3 , 200 , 4 , - 1 ) <newline> ft . end_fill ( ) <newline> tb = clock ( ) <newline> res += "Koch: ▁ %.2fsec." % ( tb - ta ) <newline> return res <newline> <dedent> if __name__ == '__main__' : <newline> <indent> msg = main ( ) <newline> print ( msg ) <newline> mainloop ( ) <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ Copyright ▁ (c) ▁ 2013 ▁ Ole ▁ Krause-Sparmann <encdom>  # ▁ Permission ▁ is ▁ hereby ▁ granted, ▁ free ▁ of ▁ charge, ▁ to ▁ any ▁ person ▁ obtaining ▁ a ▁ copy <encdom>  # ▁ of ▁ this ▁ software ▁ and ▁ associated ▁ documentation ▁ files ▁ (the ▁"Software"), ▁ to ▁ deal <encdom>  # ▁ in ▁ the ▁ Software ▁ without ▁ restriction, ▁ including ▁ without ▁ limitation ▁ the ▁ rights <encdom>  # ▁ to ▁ use, ▁ copy, ▁ modify, ▁ merge, ▁ publish, ▁ distribute, ▁ sublicense, ▁ and/or ▁ sell <encdom>  # ▁ copies ▁ of ▁ the ▁ Software, ▁ and ▁ to ▁ permit ▁ persons ▁ to ▁ whom ▁ the ▁ Software ▁ is <encdom>  # ▁ furnished ▁ to ▁ do ▁ so, ▁ subject ▁ to ▁ the ▁ following ▁ conditions: <encdom>  # ▁ The ▁ above ▁ copyright ▁ notice ▁ and ▁ this ▁ permission ▁ notice ▁ shall ▁ be ▁ included ▁ in <encdom>  # ▁ all ▁ copies ▁ or ▁ substantial ▁ portions ▁ of ▁ the ▁ Software. <encdom>  # ▁ THE ▁ SOFTWARE ▁ IS ▁ PROVIDED ▁"AS ▁ IS", ▁ WITHOUT ▁ WARRANTY ▁ OF ▁ ANY ▁ KIND, ▁ EXPRESS ▁ OR <encdom>  # ▁ IMPLIED, ▁ INCLUDING ▁ BUT ▁ NOT ▁ LIMITED ▁ TO ▁ THE ▁ WARRANTIES ▁ OF ▁ MERCHANTABILITY, <encdom>  # ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE ▁ AND ▁ NONINFRINGEMENT. ▁ IN ▁ NO ▁ EVENT ▁ SHALL ▁ THE <encdom>  # ▁ AUTHORS ▁ OR ▁ COPYRIGHT ▁ HOLDERS ▁ BE ▁ LIABLE ▁ FOR ▁ ANY ▁ CLAIM, ▁ DAMAGES ▁ OR ▁ OTHER <encdom>  # ▁ LIABILITY, ▁ WHETHER ▁ IN ▁ AN ▁ ACTION ▁ OF ▁ CONTRACT, ▁ TORT ▁ OR ▁ OTHERWISE, ▁ ARISING ▁ FROM, <encdom>  # ▁ OUT ▁ OF ▁ OR ▁ IN ▁ CONNECTION ▁ WITH ▁ THE ▁ SOFTWARE ▁ OR ▁ THE ▁ USE ▁ OR ▁ OTHER ▁ DEALINGS ▁ IN <encdom>  # ▁ THE ▁ SOFTWARE. <encdom> import numpy <newline> import scipy <newline> import unittest <newline> from nearpy . hashes import RandomBinaryProjections , RandomDiscretizedProjections , PCABinaryProjections , PCADiscretizedProjections <newline> class TestRandomBinaryProjections ( unittest . TestCase ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . rbp = RandomBinaryProjections ( 'testHash' , 10 ) <newline> self . rbp . reset ( 100 ) <newline> <dedent> def test_hash_format ( self ) : <newline> <indent> h = self . rbp . hash_vector ( numpy . random . randn ( 100 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> self . assertEqual ( len ( h [ 0 ] ) , 10 ) <newline> for c in h [ 0 ] : <newline> <indent> self . assertTrue ( c == '1' or c == '0' ) <newline> <dedent> <dedent> def test_hash_deterministic ( self ) : <newline> <indent> x = numpy . random . randn ( 100 ) <newline> first_hash = self . rbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . rbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> def test_hash_format_sparse ( self ) : <newline> <indent> h = self . rbp . hash_vector ( scipy . sparse . rand ( 100 , 1 , density = 0.1 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> self . assertEqual ( len ( h [ 0 ] ) , 10 ) <newline> for c in h [ 0 ] : <newline> <indent> self . assertTrue ( c == '1' or c == '0' ) <newline> <dedent> <dedent> def test_hash_deterministic_sparse ( self ) : <newline> <indent> x = scipy . sparse . rand ( 100 , 1 , density = 0.1 ) <newline> first_hash = self . rbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . rbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> <dedent> class TestRandomDiscretizedProjections ( unittest . TestCase ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . rbp = RandomDiscretizedProjections ( 'testHash' , 10 , 0.1 ) <newline> self . rbp . reset ( 100 ) <newline> <dedent> def test_hash_format ( self ) : <newline> <indent> h = self . rbp . hash_vector ( numpy . random . randn ( 100 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> <dedent> def test_hash_deterministic ( self ) : <newline> <indent> x = numpy . random . randn ( 100 ) <newline> first_hash = self . rbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . rbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> def test_hash_format_sparse ( self ) : <newline> <indent> h = self . rbp . hash_vector ( scipy . sparse . rand ( 100 , 1 , density = 0.1 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> <dedent> def test_hash_deterministic_sparse ( self ) : <newline> <indent> x = scipy . sparse . rand ( 100 , 1 , density = 0.1 ) <newline> first_hash = self . rbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . rbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> <dedent> class TestPCABinaryProjections ( unittest . TestCase ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . vectors = numpy . random . randn ( 10 , 100 ) <newline> self . pbp = PCABinaryProjections ( 'pbp' , 4 , self . vectors ) <newline> <dedent> def test_hash_format ( self ) : <newline> <indent> h = self . pbp . hash_vector ( numpy . random . randn ( 10 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> self . assertEqual ( len ( h [ 0 ] ) , 4 ) <newline> for c in h [ 0 ] : <newline> <indent> self . assertTrue ( c == '1' or c == '0' ) <newline> <dedent> <dedent> def test_hash_deterministic ( self ) : <newline> <indent> x = numpy . random . randn ( 10 ) <newline> first_hash = self . pbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . pbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> def test_hash_format_sparse ( self ) : <newline> <indent> h = self . pbp . hash_vector ( scipy . sparse . rand ( 10 , 1 , density = 0.6 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> self . assertEqual ( len ( h [ 0 ] ) , 4 ) <newline> for c in h [ 0 ] : <newline> <indent> self . assertTrue ( c == '1' or c == '0' ) <newline> <dedent> <dedent> def test_hash_deterministic_sparse ( self ) : <newline> <indent> x = scipy . sparse . rand ( 10 , 1 , density = 0.6 ) <newline> first_hash = self . pbp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . pbp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> <dedent> class TestPCADiscretizedProjections ( unittest . TestCase ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . vectors = numpy . random . randn ( 10 , 100 ) <newline> self . pdp = PCADiscretizedProjections ( 'pdp' , 4 , self . vectors , 0.1 ) <newline> <dedent> def test_hash_format ( self ) : <newline> <indent> h = self . pdp . hash_vector ( numpy . random . randn ( 10 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> <dedent> def test_hash_deterministic ( self ) : <newline> <indent> x = numpy . random . randn ( 10 ) <newline> first_hash = self . pdp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . pdp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> def test_hash_format_sparse ( self ) : <newline> <indent> h = self . pdp . hash_vector ( scipy . sparse . rand ( 10 , 1 , density = 0.6 ) ) <newline> self . assertEqual ( len ( h ) , 1 ) <newline> self . assertEqual ( type ( h [ 0 ] ) , type ( '' ) ) <newline> <dedent> def test_hash_deterministic_sparse ( self ) : <newline> <indent> x = scipy . sparse . rand ( 10 , 1 , density = 0.6 ) <newline> first_hash = self . pdp . hash_vector ( x ) [ 0 ] <newline> for k in range ( 100 ) : <newline> <indent> self . assertEqual ( first_hash , self . pdp . hash_vector ( x ) [ 0 ] ) <newline> <dedent> <dedent> <dedent> if __name__ == '__main__' : <newline> <indent> unittest . main ( ) <newline> <dedent>
 # !/usr/bin/env ▁ python <encdom>  """ <strnewline> Copyright ▁ (c) ▁ 2006-2015 ▁ sqlmap ▁ developers ▁ (http://sqlmap.org/) <strnewline> See ▁ the ▁ file ▁'doc/COPYING' ▁ for ▁ copying ▁ permission <strnewline> """  <newline> from lib . core . enums import DBMS <newline> from lib . core . settings import PGSQL_SYSTEM_DBS <newline> from lib . core . unescaper import unescaper <newline> from plugins . dbms . postgresql . enumeration import Enumeration <newline> from plugins . dbms . postgresql . filesystem import Filesystem <newline> from plugins . dbms . postgresql . fingerprint import Fingerprint <newline> from plugins . dbms . postgresql . syntax import Syntax <newline> from plugins . dbms . postgresql . takeover import Takeover <newline> from plugins . generic . misc import Miscellaneous <newline> class PostgreSQLMap ( Syntax , Fingerprint , Enumeration , Filesystem , Miscellaneous , Takeover ) : <newline> <indent>  """ <strnewline> ▁ This ▁ class ▁ defines ▁ PostgreSQL ▁ methods <strnewline> ▁ """  <newline> def __init__ ( self ) : <newline> <indent> self . excludeDbsList = PGSQL_SYSTEM_DBS <newline> self . sysUdfs = {  # ▁ UDF ▁ name: ▁ UDF ▁ parameters' ▁ input ▁ data-type ▁ and ▁ return ▁ data-type <encdom> "sys_exec" : { "input" : [ "text" ] , "return" : "int4" } , "sys_eval" : { "input" : [ "text" ] , "return" : "text" } , "sys_bineval" : { "input" : [ "text" ] , "return" : "int4" } , "sys_fileread" : { "input" : [ "text" ] , "return" : "text" } } <newline> Syntax . __init__ ( self ) <newline> Fingerprint . __init__ ( self ) <newline> Enumeration . __init__ ( self ) <newline> Filesystem . __init__ ( self ) <newline> Miscellaneous . __init__ ( self ) <newline> Takeover . __init__ ( self ) <newline> <dedent> unescaper [ DBMS . PGSQL ] = Syntax . escape <newline> <dedent>
 # ▁ -*- ▁ encoding: ▁ utf-8 ▁ -*- <encdom>  # ▁ vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4: <encdom>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  """ <strnewline> End-to-end ▁ tests ▁ related ▁ to ▁ the ▁ cohort ▁ management ▁ on ▁ the ▁ LMS ▁ Instructor ▁ Dashboard <strnewline> """  <newline> import os <newline> import uuid <newline> from datetime import datetime <newline> import unicodecsv <newline> from bok_choy . promise import EmptyPromise <newline> from nose . plugins . attrib import attr <newline> from pytz import UTC , utc <newline> from common . test . acceptance . fixtures . course import CourseFixture , XBlockFixtureDesc <newline> from common . test . acceptance . pages . common . auto_auth import AutoAuthPage <newline> from common . test . acceptance . pages . lms . instructor_dashboard import DataDownloadPage , InstructorDashboardPage <newline> from common . test . acceptance . pages . studio . settings_group_configurations import GroupConfigurationsPage <newline> from common . test . acceptance . tests . discussion . helpers import CohortTestMixin <newline> from common . test . acceptance . tests . helpers import EventsTestMixin , UniqueCourseTest , create_user_partition_json <newline> from xmodule . partitions . partitions import Group <newline> @ attr ( shard = 8 ) <newline> class CohortConfigurationTest ( EventsTestMixin , UniqueCourseTest , CohortTestMixin ) : <newline> <indent>  """ <strnewline> ▁ Tests ▁ for ▁ cohort ▁ management ▁ on ▁ the ▁ LMS ▁ Instructor ▁ Dashboard <strnewline> ▁ """  <newline> def setUp ( self ) : <newline> <indent>  """ <strnewline> ▁ Set ▁ up ▁ a ▁ cohorted ▁ course <strnewline> ▁ """  <newline> super ( CohortConfigurationTest , self ) . setUp ( ) <newline>  # ▁ create ▁ course ▁ with ▁ cohorts <encdom> self . manual_cohort_name = "ManualCohort1" <newline> self . auto_cohort_name = "AutoCohort1" <newline> self . course_fixture = CourseFixture ( ** self . course_info ) . install ( ) <newline> self . setup_cohort_config ( self . course_fixture , auto_cohort_groups = [ self . auto_cohort_name ] ) <newline> self . manual_cohort_id = self . add_manual_cohort ( self . course_fixture , self . manual_cohort_name ) <newline>  # ▁ create ▁ a ▁ non-instructor ▁ who ▁ will ▁ be ▁ registered ▁ for ▁ the ▁ course ▁ and ▁ in ▁ the ▁ manual ▁ cohort. <encdom> self . student_name , self . student_email = self . _generate_unique_user_data ( ) <newline> self . student_id = AutoAuthPage ( self . browser , username = self . student_name , email = self . student_email , course_id = self . course_id , staff = False ) . visit ( ) . get_user_id ( ) <newline> self . add_user_to_cohort ( self . course_fixture , self . student_name , self . manual_cohort_id ) <newline>  # ▁ create ▁ a ▁ second ▁ student ▁ user <encdom> self . other_student_name , self . other_student_email = self . _generate_unique_user_data ( ) <newline> self . other_student_id = AutoAuthPage ( self . browser , username = self . other_student_name , email = self . other_student_email , course_id = self . course_id , staff = False ) . visit ( ) . get_user_id ( ) <newline>  # ▁ login ▁ as ▁ an ▁ instructor <encdom> self . instructor_name , self . instructor_email = self . _generate_unique_user_data ( ) <newline> self . instructor_id = AutoAuthPage ( self . browser , username = self . instructor_name , email = self . instructor_email , course_id = self . course_id , staff = True ) . visit ( ) . get_user_id ( ) <newline>  # ▁ go ▁ to ▁ the ▁ membership ▁ page ▁ on ▁ the ▁ instructor ▁ dashboard <encdom> self . instructor_dashboard_page = InstructorDashboardPage ( self . browser , self . course_id ) <newline> self . instructor_dashboard_page . visit ( ) <newline> self . cohort_management_page = self . instructor_dashboard_page . select_cohort_management ( ) <newline> <dedent> def verify_cohort_description ( self , cohort_name , expected_description ) : <newline> <indent>  """ <strnewline> ▁ Selects ▁ the ▁ cohort ▁ with ▁ the ▁ given ▁ name ▁ and ▁ verifies ▁ the ▁ expected ▁ description ▁ is ▁ presented. <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( cohort_name ) <newline> self . assertEquals ( self . cohort_management_page . get_selected_cohort ( ) , cohort_name ) <newline> self . assertIn ( expected_description , self . cohort_management_page . get_cohort_group_setup ( ) ) <newline> <dedent> def test_cohort_description ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ the ▁ cohort ▁ configuration ▁ management ▁ in ▁ the ▁ instructor ▁ dashboard ▁ specifies ▁ whether <strnewline> ▁ students ▁ are ▁ automatically ▁ or ▁ manually ▁ assigned ▁ to ▁ specific ▁ cohorts. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ manual ▁ cohort ▁ and ▁ an ▁ automatic ▁ cohort ▁ defined <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ manual ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ There ▁ is ▁ text ▁ specifying ▁ that ▁ students ▁ are ▁ only ▁ added ▁ to ▁ the ▁ cohort ▁ manually <strnewline> ▁ And ▁ when ▁ I ▁ view ▁ the ▁ automatic ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ There ▁ is ▁ text ▁ specifying ▁ that ▁ students ▁ are ▁ automatically ▁ added ▁ to ▁ the ▁ cohort <strnewline> ▁ """  <newline> self . verify_cohort_description ( self . manual_cohort_name , 'Learners ▁ are ▁ added ▁ to ▁ this ▁ cohort ▁ only ▁ when ▁ you ▁ provide ▁ ' 'their ▁ email ▁ addresses ▁ or ▁ usernames ▁ on ▁ this ▁ page' , ) <newline> self . verify_cohort_description ( self . auto_cohort_name , 'Learners ▁ are ▁ added ▁ to ▁ this ▁ cohort ▁ automatically' , ) <newline> <dedent> def test_no_content_groups ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ if ▁ the ▁ course ▁ has ▁ no ▁ content ▁ groups ▁ defined ▁ (user_partitions ▁ of ▁ type ▁ cohort), <strnewline> ▁ the ▁ settings ▁ in ▁ the ▁ cohort ▁ management ▁ tab ▁ reflect ▁ this <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ but ▁ no ▁ content ▁ groups <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ cohort ▁ is ▁ not ▁ linked ▁ to ▁ a ▁ content ▁ group <strnewline> ▁ And ▁ there ▁ is ▁ text ▁ stating ▁ that ▁ no ▁ content ▁ groups ▁ are ▁ defined <strnewline> ▁ And ▁ I ▁ cannot ▁ select ▁ the ▁ radio ▁ button ▁ to ▁ enable ▁ content ▁ group ▁ association <strnewline> ▁ And ▁ there ▁ is ▁ a ▁ link ▁ I ▁ can ▁ select ▁ to ▁ open ▁ Group ▁ settings ▁ in ▁ Studio <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( self . manual_cohort_name ) <newline> self . assertIsNone ( self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> self . assertEqual ( "Warning: \n No ▁ content ▁ groups ▁ exist. ▁ Create ▁ a ▁ content ▁ group" , self . cohort_management_page . get_cohort_related_content_group_message ( ) ) <newline> self . assertFalse ( self . cohort_management_page . select_content_group_radio_button ( ) ) <newline> self . cohort_management_page . select_studio_group_settings ( ) <newline> group_settings_page = GroupConfigurationsPage ( self . browser , self . course_info [ 'org' ] , self . course_info [ 'number' ] , self . course_info [ 'run' ] ) <newline> group_settings_page . wait_for_page ( ) <newline> <dedent> def test_add_students_to_cohort_success ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ When ▁ students ▁ are ▁ added ▁ to ▁ a ▁ cohort, ▁ the ▁ appropriate ▁ notification ▁ is ▁ shown. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ two ▁ cohorts <strnewline> ▁ And ▁ there ▁ is ▁ a ▁ user ▁ in ▁ one ▁ cohort <strnewline> ▁ And ▁ there ▁ is ▁ a ▁ user ▁ in ▁ neither ▁ cohort <strnewline> ▁ When ▁ I ▁ add ▁ the ▁ two ▁ users ▁ to ▁ the ▁ cohort ▁ that ▁ initially ▁ had ▁ no ▁ users <strnewline> ▁ Then ▁ there ▁ are ▁ 2 ▁ users ▁ in ▁ total ▁ in ▁ the ▁ cohort <strnewline> ▁ And ▁ I ▁ get ▁ a ▁ notification ▁ that ▁ 2 ▁ users ▁ have ▁ been ▁ added ▁ to ▁ the ▁ cohort <strnewline> ▁ And ▁ I ▁ get ▁ a ▁ notification ▁ that ▁ 1 ▁ user ▁ was ▁ moved ▁ from ▁ the ▁ other ▁ cohort <strnewline> ▁ And ▁ the ▁ user ▁ input ▁ field ▁ is ▁ empty <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> start_time = datetime . now ( UTC ) <newline> self . cohort_management_page . select_cohort ( self . auto_cohort_name ) <newline> self . assertEqual ( 0 , self . cohort_management_page . get_selected_cohort_count ( ) ) <newline> self . cohort_management_page . add_students_to_selected_cohort ( [ self . student_name , self . instructor_name ] ) <newline>  # ▁ Wait ▁ for ▁ the ▁ number ▁ of ▁ users ▁ in ▁ the ▁ cohort ▁ to ▁ change, ▁ indicating ▁ that ▁ the ▁ add ▁ operation ▁ is ▁ complete. <encdom> EmptyPromise ( lambda : 2 == self . cohort_management_page . get_selected_cohort_count ( ) , 'Waiting ▁ for ▁ added ▁ students' ) . fulfill ( ) <newline> confirmation_messages = self . cohort_management_page . get_cohort_confirmation_messages ( ) <newline> self . assertEqual ( [ "2 ▁ learners ▁ have ▁ been ▁ added ▁ to ▁ this ▁ cohort." , "1 ▁ learner ▁ was ▁ moved ▁ from ▁ " + self . manual_cohort_name ] , confirmation_messages ) <newline> self . assertEqual ( "" , self . cohort_management_page . get_cohort_student_input_field_value ( ) ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_added" , "time" : { "$gt" : start_time } , "event.user_id" : { "$in" : [ int ( self . instructor_id ) , int ( self . student_id ) ] } , "event.cohort_name" : self . auto_cohort_name , } ) . count ( ) , 2 ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_removed" , "time" : { "$gt" : start_time } , "event.user_id" : int ( self . student_id ) , "event.cohort_name" : self . manual_cohort_name , } ) . count ( ) , 1 ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_add_requested" , "time" : { "$gt" : start_time } , "event.user_id" : int ( self . instructor_id ) , "event.cohort_name" : self . auto_cohort_name , "event.previous_cohort_name" : None , } ) . count ( ) , 1 ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_add_requested" , "time" : { "$gt" : start_time } , "event.user_id" : int ( self . student_id ) , "event.cohort_name" : self . auto_cohort_name , "event.previous_cohort_name" : self . manual_cohort_name , } ) . count ( ) , 1 ) <newline> <dedent> def test_add_students_to_cohort_failure ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ When ▁ errors ▁ occur ▁ when ▁ adding ▁ students ▁ to ▁ a ▁ cohort, ▁ the ▁ appropriate ▁ notification ▁ is ▁ shown. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ and ▁ a ▁ user ▁ already ▁ in ▁ it <strnewline> ▁ When ▁ I ▁ add ▁ the ▁ user ▁ already ▁ in ▁ a ▁ cohort ▁ to ▁ that ▁ same ▁ cohort <strnewline> ▁ And ▁ I ▁ add ▁ a ▁ non-existing ▁ user ▁ to ▁ that ▁ cohort <strnewline> ▁ Then ▁ there ▁ is ▁ no ▁ change ▁ in ▁ the ▁ number ▁ of ▁ students ▁ in ▁ the ▁ cohort <strnewline> ▁ And ▁ I ▁ get ▁ a ▁ notification ▁ that ▁ one ▁ user ▁ was ▁ already ▁ in ▁ the ▁ cohort <strnewline> ▁ And ▁ I ▁ get ▁ a ▁ notification ▁ that ▁ one ▁ user ▁ is ▁ unknown <strnewline> ▁ And ▁ the ▁ user ▁ input ▁ field ▁ still ▁ contains ▁ the ▁ incorrect ▁ email ▁ addresses <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( self . manual_cohort_name ) <newline> self . assertEqual ( 1 , self . cohort_management_page . get_selected_cohort_count ( ) ) <newline> self . cohort_management_page . add_students_to_selected_cohort ( [ self . student_name , "unknown_user" ] ) <newline>  # ▁ Wait ▁ for ▁ notification ▁ messages ▁ to ▁ appear, ▁ indicating ▁ that ▁ the ▁ add ▁ operation ▁ is ▁ complete. <encdom> EmptyPromise ( lambda : 2 == len ( self . cohort_management_page . get_cohort_confirmation_messages ( ) ) , 'Waiting ▁ for ▁ notification' ) . fulfill ( ) <newline> self . assertEqual ( 1 , self . cohort_management_page . get_selected_cohort_count ( ) ) <newline> self . assertEqual ( [ "0 ▁ learners ▁ have ▁ been ▁ added ▁ to ▁ this ▁ cohort." , "1 ▁ learner ▁ was ▁ already ▁ in ▁ the ▁ cohort" ] , self . cohort_management_page . get_cohort_confirmation_messages ( ) ) <newline> self . assertEqual ( [ "There ▁ was ▁ an ▁ error ▁ when ▁ trying ▁ to ▁ add ▁ learners:" , "Unknown ▁ username: ▁ unknown_user" ] , self . cohort_management_page . get_cohort_error_messages ( ) ) <newline> self . assertEqual ( self . student_name + ",unknown_user," , self . cohort_management_page . get_cohort_student_input_field_value ( ) ) <newline> <dedent> def _verify_cohort_settings ( self , cohort_name , assignment_type = None , new_cohort_name = None , new_assignment_type = None , verify_updated = False ) : <newline> <indent>  """ <strnewline> ▁ Create ▁ a ▁ new ▁ cohort ▁ and ▁ verify ▁ the ▁ new ▁ and ▁ existing ▁ settings. <strnewline> ▁ """  <newline> start_time = datetime . now ( UTC ) <newline> self . assertNotIn ( cohort_name , self . cohort_management_page . get_cohorts ( ) ) <newline> self . cohort_management_page . add_cohort ( cohort_name , assignment_type = assignment_type ) <newline> self . assertEqual ( 0 , self . cohort_management_page . get_selected_cohort_count ( ) ) <newline>  # ▁ After ▁ adding ▁ the ▁ cohort, ▁ it ▁ should ▁ automatically ▁ be ▁ selected ▁ and ▁ its <encdom>  # ▁ assignment_type ▁ should ▁ be ▁"manual" ▁ as ▁ this ▁ is ▁ the ▁ default ▁ assignment ▁ type <encdom> _assignment_type = assignment_type or 'manual' <newline> msg = "Waiting ▁ for ▁ currently ▁ selected ▁ cohort ▁ assignment ▁ type" <newline> EmptyPromise ( lambda : _assignment_type == self . cohort_management_page . get_cohort_associated_assignment_type ( ) , msg ) . fulfill ( ) <newline>  # ▁ Go ▁ back ▁ to ▁ Manage ▁ Students ▁ Tab <encdom> self . cohort_management_page . select_manage_settings ( ) <newline> self . cohort_management_page . add_students_to_selected_cohort ( [ self . instructor_name ] ) <newline>  # ▁ Wait ▁ for ▁ the ▁ number ▁ of ▁ users ▁ in ▁ the ▁ cohort ▁ to ▁ change, ▁ indicating ▁ that ▁ the ▁ add ▁ operation ▁ is ▁ complete. <encdom> EmptyPromise ( lambda : 1 == self . cohort_management_page . get_selected_cohort_count ( ) , 'Waiting ▁ for ▁ student ▁ to ▁ be ▁ added' ) . fulfill ( ) <newline> self . assertFalse ( self . cohort_management_page . is_assignment_settings_disabled ) <newline> self . assertEqual ( '' , self . cohort_management_page . assignment_settings_message ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.created" , "time" : { "$gt" : start_time } , "event.cohort_name" : cohort_name , } ) . count ( ) , 1 ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.creation_requested" , "time" : { "$gt" : start_time } , "event.cohort_name" : cohort_name , } ) . count ( ) , 1 ) <newline> if verify_updated : <newline> <indent> self . cohort_management_page . select_cohort ( cohort_name ) <newline> self . cohort_management_page . select_cohort_settings ( ) <newline> self . cohort_management_page . set_cohort_name ( new_cohort_name ) <newline> self . cohort_management_page . set_assignment_type ( new_assignment_type ) <newline> self . cohort_management_page . save_cohort_settings ( ) <newline>  # ▁ If ▁ cohort ▁ name ▁ is ▁ empty, ▁ then ▁ we ▁ should ▁ get/see ▁ an ▁ error ▁ message. <encdom> if not new_cohort_name : <newline> <indent> confirmation_messages = self . cohort_management_page . get_cohort_settings_messages ( type = 'error' ) <newline> self . assertEqual ( [ "The ▁ cohort ▁ cannot ▁ be ▁ saved" , "You ▁ must ▁ specify ▁ a ▁ name ▁ for ▁ the ▁ cohort" ] , confirmation_messages ) <newline> <dedent> else : <newline> <indent> confirmation_messages = self . cohort_management_page . get_cohort_settings_messages ( ) <newline> self . assertEqual ( [ "Saved ▁ cohort" ] , confirmation_messages ) <newline> self . assertEqual ( new_cohort_name , self . cohort_management_page . cohort_name_in_header ) <newline> self . assertIn ( new_cohort_name , self . cohort_management_page . get_cohorts ( ) ) <newline> self . assertEqual ( 1 , self . cohort_management_page . get_selected_cohort_count ( ) ) <newline> self . assertEqual ( new_assignment_type , self . cohort_management_page . get_cohort_associated_assignment_type ( ) ) <newline> <dedent> <dedent> <dedent> def _create_csv_file ( self , filename , csv_text_as_lists ) : <newline> <indent>  """ <strnewline> ▁ Create ▁ a ▁ csv ▁ file ▁ with ▁ the ▁ provided ▁ list ▁ of ▁ lists. <strnewline> <strnewline> ▁ :param ▁ filename: ▁ this ▁ is ▁ the ▁ name ▁ that ▁ will ▁ be ▁ used ▁ for ▁ the ▁ csv ▁ file. ▁ Its ▁ location ▁ will <strnewline> ▁ be ▁ under ▁ the ▁ test ▁ upload ▁ data ▁ directory <strnewline> ▁ :param ▁ csv_text_as_lists: ▁ provide ▁ the ▁ contents ▁ of ▁ the ▁ csv ▁ file ▁ int ▁ he ▁ form ▁ of ▁ a ▁ list ▁ of ▁ lists <strnewline> ▁ """  <newline> filename = self . instructor_dashboard_page . get_asset_path ( filename ) <newline> with open ( filename , 'w+' ) as csv_file : <newline> <indent> writer = unicodecsv . writer ( csv_file ) <newline> for line in csv_text_as_lists : <newline> <indent> writer . writerow ( line ) <newline> <dedent> <dedent> self . addCleanup ( os . remove , filename ) <newline> <dedent> def _generate_unique_user_data ( self ) : <newline> <indent>  """ <strnewline> ▁ Produce ▁ unique ▁ username ▁ and ▁ e-mail. <strnewline> ▁ """  <newline> unique_username = 'user' + str ( uuid . uuid4 ( ) . hex ) [ : 12 ] <newline> unique_email = unique_username + "@example.com" <newline> return unique_username , unique_email <newline> <dedent> def test_add_new_cohort ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ A ▁ new ▁ manual ▁ cohort ▁ can ▁ be ▁ created, ▁ and ▁ a ▁ student ▁ assigned ▁ to ▁ it. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ When ▁ I ▁ add ▁ a ▁ new ▁ manual ▁ cohort ▁ to ▁ the ▁ course ▁ via ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ Then ▁ the ▁ new ▁ cohort ▁ is ▁ displayed ▁ and ▁ has ▁ no ▁ users ▁ in ▁ it <strnewline> ▁ And ▁ assignment ▁ type ▁ of ▁ displayed ▁ cohort ▁ to ▁"manual" ▁ because ▁ this ▁ is ▁ the ▁ default <strnewline> ▁ And ▁ when ▁ I ▁ add ▁ the ▁ user ▁ to ▁ the ▁ new ▁ cohort <strnewline> ▁ Then ▁ the ▁ cohort ▁ has ▁ 1 ▁ user <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> cohort_name = str ( uuid . uuid4 ( ) . get_hex ( ) [ 0 : 20 ] ) <newline> self . _verify_cohort_settings ( cohort_name = cohort_name , assignment_type = None ) <newline> <dedent> def test_add_new_cohort_with_manual_assignment_type ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ A ▁ new ▁ cohort ▁ with ▁ manual ▁ assignment ▁ type ▁ can ▁ be ▁ created, ▁ and ▁ a ▁ student ▁ assigned ▁ to ▁ it. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ When ▁ I ▁ add ▁ a ▁ new ▁ manual ▁ cohort ▁ with ▁ manual ▁ assignment ▁ type ▁ to ▁ the ▁ course ▁ via ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ Then ▁ the ▁ new ▁ cohort ▁ is ▁ displayed ▁ and ▁ has ▁ no ▁ users ▁ in ▁ it <strnewline> ▁ And ▁ assignment ▁ type ▁ of ▁ displayed ▁ cohort ▁ is ▁"manual" <strnewline> ▁ And ▁ when ▁ I ▁ add ▁ the ▁ user ▁ to ▁ the ▁ new ▁ cohort <strnewline> ▁ Then ▁ the ▁ cohort ▁ has ▁ 1 ▁ user <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> cohort_name = str ( uuid . uuid4 ( ) . get_hex ( ) [ 0 : 20 ] ) <newline> self . _verify_cohort_settings ( cohort_name = cohort_name , assignment_type = 'manual' ) <newline> <dedent> def test_add_new_cohort_with_random_assignment_type ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ A ▁ new ▁ cohort ▁ with ▁ random ▁ assignment ▁ type ▁ can ▁ be ▁ created, ▁ and ▁ a ▁ student ▁ assigned ▁ to ▁ it. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ When ▁ I ▁ add ▁ a ▁ new ▁ manual ▁ cohort ▁ with ▁ random ▁ assignment ▁ type ▁ to ▁ the ▁ course ▁ via ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ Then ▁ the ▁ new ▁ cohort ▁ is ▁ displayed ▁ and ▁ has ▁ no ▁ users ▁ in ▁ it <strnewline> ▁ And ▁ assignment ▁ type ▁ of ▁ displayed ▁ cohort ▁ is ▁"random" <strnewline> ▁ And ▁ when ▁ I ▁ add ▁ the ▁ user ▁ to ▁ the ▁ new ▁ cohort <strnewline> ▁ Then ▁ the ▁ cohort ▁ has ▁ 1 ▁ user <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> cohort_name = str ( uuid . uuid4 ( ) . get_hex ( ) [ 0 : 20 ] ) <newline> self . _verify_cohort_settings ( cohort_name = cohort_name , assignment_type = 'random' ) <newline> <dedent> def test_update_existing_cohort_settings ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ Update ▁ existing ▁ cohort ▁ settings(cohort ▁ name, ▁ assignment ▁ type) <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ When ▁ I ▁ add ▁ a ▁ new ▁ cohort ▁ with ▁ random ▁ assignment ▁ type ▁ to ▁ the ▁ course ▁ via ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ Then ▁ the ▁ new ▁ cohort ▁ is ▁ displayed ▁ and ▁ has ▁ no ▁ users ▁ in ▁ it <strnewline> ▁ And ▁ assignment ▁ type ▁ of ▁ displayed ▁ cohort ▁ is ▁"random" <strnewline> ▁ And ▁ when ▁ I ▁ add ▁ the ▁ user ▁ to ▁ the ▁ new ▁ cohort <strnewline> ▁ Then ▁ the ▁ cohort ▁ has ▁ 1 ▁ user <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ Then ▁ I ▁ select ▁ the ▁ cohort ▁ (that ▁ you ▁ just ▁ created) ▁ from ▁ existing ▁ cohorts <strnewline> ▁ Then ▁ I ▁ change ▁ its ▁ name ▁ and ▁ assignment ▁ type ▁ set ▁ to ▁"manual" <strnewline> ▁ Then ▁ I ▁ Save ▁ the ▁ settings <strnewline> ▁ And ▁ cohort ▁ with ▁ new ▁ name ▁ is ▁ present ▁ in ▁ cohorts ▁ dropdown ▁ list <strnewline> ▁ And ▁ cohort ▁ assignment ▁ type ▁ should ▁ be ▁"manual" <strnewline> ▁ """  <newline> cohort_name = str ( uuid . uuid4 ( ) . get_hex ( ) [ 0 : 20 ] ) <newline> new_cohort_name = '{old}__NEW' . format ( old = cohort_name ) <newline> self . _verify_cohort_settings ( cohort_name = cohort_name , assignment_type = 'random' , new_cohort_name = new_cohort_name , new_assignment_type = 'manual' , verify_updated = True ) <newline> <dedent> def test_update_existing_cohort_settings_with_empty_cohort_name ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ Update ▁ existing ▁ cohort ▁ settings(cohort ▁ name, ▁ assignment ▁ type). <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ When ▁ I ▁ add ▁ a ▁ new ▁ cohort ▁ with ▁ random ▁ assignment ▁ type ▁ to ▁ the ▁ course ▁ via ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ Then ▁ the ▁ new ▁ cohort ▁ is ▁ displayed ▁ and ▁ has ▁ no ▁ users ▁ in ▁ it <strnewline> ▁ And ▁ assignment ▁ type ▁ of ▁ displayed ▁ cohort ▁ is ▁"random" <strnewline> ▁ And ▁ when ▁ I ▁ add ▁ the ▁ user ▁ to ▁ the ▁ new ▁ cohort <strnewline> ▁ Then ▁ the ▁ cohort ▁ has ▁ 1 ▁ user <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ Then ▁ I ▁ select ▁ a ▁ cohort ▁ from ▁ existing ▁ cohorts <strnewline> ▁ Then ▁ I ▁ set ▁ its ▁ name ▁ as ▁ empty ▁ string ▁ and ▁ assignment ▁ type ▁ set ▁ to ▁"manual" <strnewline> ▁ And ▁ I ▁ click ▁ on ▁ Save ▁ button <strnewline> ▁ Then ▁ I ▁ should ▁ see ▁ an ▁ error ▁ message <strnewline> ▁ """  <newline> cohort_name = str ( uuid . uuid4 ( ) . get_hex ( ) [ 0 : 20 ] ) <newline> new_cohort_name = '' <newline> self . _verify_cohort_settings ( cohort_name = cohort_name , assignment_type = 'random' , new_cohort_name = new_cohort_name , new_assignment_type = 'manual' , verify_updated = True ) <newline> <dedent> def test_default_cohort_assignment_settings ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ Cohort ▁ assignment ▁ settings ▁ are ▁ disabled ▁ for ▁ default ▁ cohort. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ user ▁ in ▁ the ▁ course <strnewline> ▁ And ▁ I ▁ have ▁ added ▁ a ▁ manual ▁ cohort <strnewline> ▁ And ▁ I ▁ have ▁ added ▁ a ▁ random ▁ cohort <strnewline> ▁ When ▁ I ▁ select ▁ the ▁ random ▁ cohort <strnewline> ▁ Then ▁ cohort ▁ assignment ▁ settings ▁ are ▁ disabled <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( "AutoCohort1" ) <newline> self . cohort_management_page . select_cohort_settings ( ) <newline> self . assertTrue ( self . cohort_management_page . is_assignment_settings_disabled ) <newline> message = "There ▁ must ▁ be ▁ one ▁ cohort ▁ to ▁ which ▁ students ▁ can ▁ automatically ▁ be ▁ assigned." <newline> self . assertEqual ( message , self . cohort_management_page . assignment_settings_message ) <newline> <dedent> def test_cohort_enable_disable ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ Cohort ▁ Enable/Disable ▁ checkbox ▁ related ▁ functionality ▁ is ▁ working ▁ as ▁ intended. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ cohorted ▁ course ▁ with ▁ a ▁ user. <strnewline> ▁ And ▁ I ▁ can ▁ see ▁ the ▁ `Enable ▁ Cohorts` ▁ checkbox ▁ is ▁ checked. <strnewline> ▁ And ▁ cohort ▁ management ▁ controls ▁ are ▁ visible. <strnewline> ▁ When ▁ I ▁ uncheck ▁ the ▁ `Enable ▁ Cohorts` ▁ checkbox. <strnewline> ▁ Then ▁ cohort ▁ management ▁ controls ▁ are ▁ not ▁ visible. <strnewline> ▁ And ▁ When ▁ I ▁ reload ▁ the ▁ page. <strnewline> ▁ Then ▁ I ▁ can ▁ see ▁ the ▁ `Enable ▁ Cohorts` ▁ checkbox ▁ is ▁ unchecked. <strnewline> ▁ And ▁ cohort ▁ management ▁ controls ▁ are ▁ not ▁ visible. <strnewline> ▁ """  <newline> self . assertTrue ( self . cohort_management_page . is_cohorted ) <newline> self . assertTrue ( self . cohort_management_page . cohort_management_controls_visible ( ) ) <newline> self . cohort_management_page . is_cohorted = False <newline> self . assertFalse ( self . cohort_management_page . cohort_management_controls_visible ( ) ) <newline> self . browser . refresh ( ) <newline> self . cohort_management_page . wait_for_page ( ) <newline> self . assertFalse ( self . cohort_management_page . is_cohorted ) <newline> self . assertFalse ( self . cohort_management_page . cohort_management_controls_visible ( ) ) <newline> <dedent> def test_link_to_data_download ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ a ▁ link ▁ is ▁ present ▁ from ▁ the ▁ cohort ▁ configuration ▁ in <strnewline> ▁ the ▁ instructor ▁ dashboard ▁ to ▁ the ▁ Data ▁ Download ▁ section. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ LMS ▁ instructor ▁ dashboard <strnewline> ▁ There ▁ is ▁ a ▁ link ▁ to ▁ take ▁ me ▁ to ▁ the ▁ Data ▁ Download ▁ section ▁ of ▁ the ▁ Instructor ▁ Dashboard. <strnewline> ▁ """  <newline> self . cohort_management_page . select_data_download ( ) <newline> data_download_page = DataDownloadPage ( self . browser ) <newline> data_download_page . wait_for_page ( ) <newline> <dedent> def test_cohort_by_csv_both_columns ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ the ▁ instructor ▁ can ▁ upload ▁ a ▁ file ▁ with ▁ user ▁ and ▁ cohort ▁ assignments, ▁ using ▁ both ▁ emails ▁ and ▁ usernames. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ two ▁ cohorts ▁ defined <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ I ▁ can ▁ upload ▁ a ▁ CSV ▁ file ▁ with ▁ assignments ▁ of ▁ users ▁ to ▁ cohorts ▁ via ▁ both ▁ usernames ▁ and ▁ emails <strnewline> ▁ Then ▁ I ▁ can ▁ download ▁ a ▁ file ▁ with ▁ results <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> csv_contents = [ [ 'username' , 'email' , 'ignored_column' , 'cohort' ] , [ self . instructor_name , '' , 'June' , 'ManualCohort1' ] , [ '' , self . student_email , 'Spring' , 'AutoCohort1' ] , [ self . other_student_name , '' , 'Fall' , 'ManualCohort1' ] , ] <newline> filename = "cohort_csv_both_columns_1.csv" <newline> self . _create_csv_file ( filename , csv_contents ) <newline> self . _verify_csv_upload_acceptable_file ( filename ) <newline> <dedent> def test_cohort_by_csv_only_email ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ the ▁ instructor ▁ can ▁ upload ▁ a ▁ file ▁ with ▁ user ▁ and ▁ cohort ▁ assignments, ▁ using ▁ only ▁ emails. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ two ▁ cohorts ▁ defined <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ I ▁ can ▁ upload ▁ a ▁ CSV ▁ file ▁ with ▁ assignments ▁ of ▁ users ▁ to ▁ cohorts ▁ via ▁ only ▁ emails <strnewline> ▁ Then ▁ I ▁ can ▁ download ▁ a ▁ file ▁ with ▁ results <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> csv_contents = [ [ 'email' , 'cohort' ] , [ self . instructor_email , 'ManualCohort1' ] , [ self . student_email , 'AutoCohort1' ] , [ self . other_student_email , 'ManualCohort1' ] , ] <newline> filename = "cohort_csv_emails_only.csv" <newline> self . _create_csv_file ( filename , csv_contents ) <newline> self . _verify_csv_upload_acceptable_file ( filename ) <newline> <dedent> def test_cohort_by_csv_only_username ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ the ▁ instructor ▁ can ▁ upload ▁ a ▁ file ▁ with ▁ user ▁ and ▁ cohort ▁ assignments, ▁ using ▁ only ▁ usernames. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ two ▁ cohorts ▁ defined <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ I ▁ can ▁ upload ▁ a ▁ CSV ▁ file ▁ with ▁ assignments ▁ of ▁ users ▁ to ▁ cohorts ▁ via ▁ only ▁ usernames <strnewline> ▁ Then ▁ I ▁ can ▁ download ▁ a ▁ file ▁ with ▁ results <strnewline> ▁ And ▁ appropriate ▁ events ▁ have ▁ been ▁ emitted <strnewline> ▁ """  <newline> csv_contents = [ [ 'username' , 'cohort' ] , [ self . instructor_name , 'ManualCohort1' ] , [ self . student_name , 'AutoCohort1' ] , [ self . other_student_name , 'ManualCohort1' ] , ] <newline> filename = "cohort_users_only_username1.csv" <newline> self . _create_csv_file ( filename , csv_contents ) <newline> self . _verify_csv_upload_acceptable_file ( filename ) <newline>  # ▁ TODO: ▁ Change ▁ unicode_hello_in_korean ▁ = ▁ u'ßßßßß' ▁ to ▁ u'안녕하세요', ▁ after ▁ up ▁ gradation ▁ of ▁ Chrome ▁ driver. ▁ See ▁ TNL-3944 <encdom> <dedent> def test_cohort_by_csv_unicode ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ the ▁ instructor ▁ can ▁ upload ▁ a ▁ file ▁ with ▁ user ▁ and ▁ cohort ▁ assignments, ▁ using ▁ both ▁ emails ▁ and ▁ usernames. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ two ▁ cohorts ▁ defined <strnewline> ▁ And ▁ I ▁ add ▁ another ▁ cohort ▁ with ▁ a ▁ unicode ▁ name <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ I ▁ can ▁ upload ▁ a ▁ CSV ▁ file ▁ with ▁ assignments ▁ of ▁ users ▁ to ▁ the ▁ unicode ▁ cohort ▁ via ▁ both ▁ usernames ▁ and ▁ emails <strnewline> ▁ Then ▁ I ▁ can ▁ download ▁ a ▁ file ▁ with ▁ results <strnewline> <strnewline> ▁ TODO: ▁ refactor ▁ events ▁ verification ▁ to ▁ handle ▁ this ▁ scenario. ▁ Events ▁ verification ▁ assumes ▁ movements <strnewline> ▁ between ▁ other ▁ cohorts ▁ (manual ▁ and ▁ auto). <strnewline> ▁ """  <newline> unicode_hello_in_korean = u'ßßßßßß' <newline> self . _verify_cohort_settings ( cohort_name = unicode_hello_in_korean , assignment_type = None ) <newline> csv_contents = [ [ 'username' , 'email' , 'cohort' ] , [ self . instructor_name , '' , unicode_hello_in_korean ] , [ '' , self . student_email , unicode_hello_in_korean ] , [ self . other_student_name , '' , unicode_hello_in_korean ] ] <newline> filename = "cohort_unicode_name.csv" <newline> self . _create_csv_file ( filename , csv_contents ) <newline> self . _verify_csv_upload_acceptable_file ( filename , skip_events = True ) <newline> <dedent> def _verify_csv_upload_acceptable_file ( self , filename , skip_events = None ) : <newline> <indent>  """ <strnewline> ▁ Helper ▁ method ▁ to ▁ verify ▁ cohort ▁ assignments ▁ after ▁ a ▁ successful ▁ CSV ▁ upload. <strnewline> <strnewline> ▁ When ▁ skip_events ▁ is ▁ specified, ▁ no ▁ assertions ▁ are ▁ made ▁ on ▁ events. <strnewline> ▁ """  <newline> start_time = datetime . now ( UTC ) <newline> self . cohort_management_page . upload_cohort_file ( filename ) <newline> self . _verify_cohort_by_csv_notification ( "Your ▁ file ▁'{}' ▁ has ▁ been ▁ uploaded. ▁ Allow ▁ a ▁ few ▁ minutes ▁ for ▁ processing." . format ( filename ) ) <newline> if not skip_events : <newline>  # ▁ student_user ▁ is ▁ moved ▁ from ▁ manual ▁ cohort ▁ to ▁ auto ▁ cohort <encdom> <indent> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_added" , "time" : { "$gt" : start_time } , "event.user_id" : { "$in" : [ int ( self . student_id ) ] } , "event.cohort_name" : self . auto_cohort_name , } ) . count ( ) , 1 ) <newline> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_removed" , "time" : { "$gt" : start_time } , "event.user_id" : int ( self . student_id ) , "event.cohort_name" : self . manual_cohort_name , } ) . count ( ) , 1 ) <newline>  # ▁ instructor_user ▁ (previously ▁ unassigned) ▁ is ▁ added ▁ to ▁ manual ▁ cohort <encdom> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_added" , "time" : { "$gt" : start_time } , "event.user_id" : { "$in" : [ int ( self . instructor_id ) ] } , "event.cohort_name" : self . manual_cohort_name , } ) . count ( ) , 1 ) <newline>  # ▁ other_student_user ▁ (previously ▁ unassigned) ▁ is ▁ added ▁ to ▁ manual ▁ cohort <encdom> self . assertEqual ( self . event_collection . find ( { "name" : "edx.cohort.user_added" , "time" : { "$gt" : start_time } , "event.user_id" : { "$in" : [ int ( self . other_student_id ) ] } , "event.cohort_name" : self . manual_cohort_name , } ) . count ( ) , 1 ) <newline>  # ▁ Verify ▁ the ▁ results ▁ can ▁ be ▁ downloaded. <encdom> <dedent> data_download = self . instructor_dashboard_page . select_data_download ( ) <newline> data_download . wait_for_available_report ( ) <newline> report = data_download . get_available_reports_for_download ( ) [ 0 ] <newline> base_file_name = "cohort_results_" <newline> self . assertIn ( "{}_{}" . format ( '_' . join ( [ self . course_info [ 'org' ] , self . course_info [ 'number' ] , self . course_info [ 'run' ] ] ) , base_file_name ) , report ) <newline> report_datetime = datetime . strptime ( report [ report . index ( base_file_name ) + len ( base_file_name ) : - len ( ".csv" ) ] , "%Y-%m-%d-%H%M" ) <newline> self . assertLessEqual ( start_time . replace ( second = 0 , microsecond = 0 ) , utc . localize ( report_datetime ) ) <newline> <dedent> def test_cohort_by_csv_wrong_file_type ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ if ▁ the ▁ instructor ▁ uploads ▁ a ▁ non-csv ▁ file, ▁ an ▁ error ▁ message ▁ is ▁ presented. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ cohorting ▁ enabled <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ And ▁ I ▁ upload ▁ a ▁ file ▁ without ▁ the ▁ CSV ▁ extension <strnewline> ▁ Then ▁ I ▁ get ▁ an ▁ error ▁ message ▁ stating ▁ that ▁ the ▁ file ▁ must ▁ have ▁ a ▁ CSV ▁ extension <strnewline> ▁ """  <newline> self . cohort_management_page . upload_cohort_file ( "image.jpg" ) <newline> self . _verify_cohort_by_csv_notification ( "The ▁ file ▁ must ▁ end ▁ with ▁ the ▁ extension ▁'.csv'." ) <newline> <dedent> def test_cohort_by_csv_missing_cohort ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ if ▁ the ▁ instructor ▁ uploads ▁ a ▁ csv ▁ file ▁ with ▁ no ▁ cohort ▁ column, ▁ an ▁ error ▁ message ▁ is ▁ presented. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ cohorting ▁ enabled <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ And ▁ I ▁ upload ▁ a ▁ CSV ▁ file ▁ that ▁ is ▁ missing ▁ the ▁ cohort ▁ column <strnewline> ▁ Then ▁ I ▁ get ▁ an ▁ error ▁ message ▁ stating ▁ that ▁ the ▁ file ▁ must ▁ have ▁ a ▁ cohort ▁ column <strnewline> ▁ """  <newline> self . cohort_management_page . upload_cohort_file ( "cohort_users_missing_cohort_column.csv" ) <newline> self . _verify_cohort_by_csv_notification ( "The ▁ file ▁ must ▁ contain ▁ a ▁'cohort' ▁ column ▁ containing ▁ cohort ▁ names." ) <newline> <dedent> def test_cohort_by_csv_missing_user ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ if ▁ the ▁ instructor ▁ uploads ▁ a ▁ csv ▁ file ▁ with ▁ no ▁ username ▁ or ▁ email ▁ column, ▁ an ▁ error ▁ message ▁ is ▁ presented. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ cohorting ▁ enabled <strnewline> ▁ When ▁ I ▁ go ▁ to ▁ the ▁ cohort ▁ management ▁ section ▁ of ▁ the ▁ instructor ▁ dashboard <strnewline> ▁ And ▁ I ▁ upload ▁ a ▁ CSV ▁ file ▁ that ▁ is ▁ missing ▁ both ▁ the ▁ username ▁ and ▁ email ▁ columns <strnewline> ▁ Then ▁ I ▁ get ▁ an ▁ error ▁ message ▁ stating ▁ that ▁ the ▁ file ▁ must ▁ have ▁ either ▁ a ▁ username ▁ or ▁ email ▁ column <strnewline> ▁ """  <newline> self . cohort_management_page . upload_cohort_file ( "cohort_users_missing_user_columns.csv" ) <newline> self . _verify_cohort_by_csv_notification ( "The ▁ file ▁ must ▁ contain ▁ a ▁'username' ▁ column, ▁ an ▁'email' ▁ column, ▁ or ▁ both." ) <newline> <dedent> def _verify_cohort_by_csv_notification ( self , expected_message ) : <newline> <indent>  """ <strnewline> ▁ Helper ▁ method ▁ to ▁ check ▁ the ▁ CSV ▁ file ▁ upload ▁ notification ▁ message. <strnewline> ▁ """  <newline>  # ▁ Wait ▁ for ▁ notification ▁ message ▁ to ▁ appear, ▁ indicating ▁ file ▁ has ▁ been ▁ uploaded. <encdom> EmptyPromise ( lambda : 1 == len ( self . cohort_management_page . get_csv_messages ( ) ) , 'Waiting ▁ for ▁ notification' ) . fulfill ( ) <newline> messages = self . cohort_management_page . get_csv_messages ( ) <newline> self . assertEquals ( expected_message , messages [ 0 ] ) <newline> <dedent> @ attr ( 'a11y' ) <newline> def test_cohorts_management_a11y ( self ) : <newline> <indent>  """ <strnewline> ▁ Run ▁ accessibility ▁ audit ▁ for ▁ cohort ▁ management. <strnewline> ▁ """  <newline> self . cohort_management_page . a11y_audit . check_for_accessibility_errors ( ) <newline> <dedent> <dedent> @ attr ( shard = 6 ) <newline> class CohortContentGroupAssociationTest ( UniqueCourseTest , CohortTestMixin ) : <newline> <indent>  """ <strnewline> ▁ Tests ▁ for ▁ linking ▁ between ▁ content ▁ groups ▁ and ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard. <strnewline> ▁ """  <newline> def setUp ( self ) : <newline> <indent>  """ <strnewline> ▁ Set ▁ up ▁ a ▁ cohorted ▁ course ▁ with ▁ a ▁ user_partition ▁ of ▁ scheme ▁"cohort". <strnewline> ▁ """  <newline> super ( CohortContentGroupAssociationTest , self ) . setUp ( ) <newline>  # ▁ create ▁ course ▁ with ▁ single ▁ cohort ▁ and ▁ two ▁ content ▁ groups ▁ (user_partition ▁ of ▁ type ▁"cohort") <encdom> self . cohort_name = "OnlyCohort" <newline> self . course_fixture = CourseFixture ( ** self . course_info ) . install ( ) <newline> self . setup_cohort_config ( self . course_fixture ) <newline> self . cohort_id = self . add_manual_cohort ( self . course_fixture , self . cohort_name ) <newline> self . course_fixture . _update_xblock ( self . course_fixture . _course_location , { "metadata" : { u"user_partitions" : [ create_user_partition_json ( 0 , 'Apples, ▁ Bananas' , 'Content ▁ Group ▁ Partition' , [ Group ( "0" , 'Apples' ) , Group ( "1" , 'Bananas' ) ] , scheme = "cohort" ) ] , } , } ) <newline>  # ▁ login ▁ as ▁ an ▁ instructor <encdom> self . instructor_name = "instructor_user" <newline> self . instructor_id = AutoAuthPage ( self . browser , username = self . instructor_name , email = "instructor_user@example.com" , course_id = self . course_id , staff = True ) . visit ( ) . get_user_id ( ) <newline>  # ▁ go ▁ to ▁ the ▁ membership ▁ page ▁ on ▁ the ▁ instructor ▁ dashboard <encdom> self . instructor_dashboard_page = InstructorDashboardPage ( self . browser , self . course_id ) <newline> self . instructor_dashboard_page . visit ( ) <newline> self . cohort_management_page = self . instructor_dashboard_page . select_cohort_management ( ) <newline> <dedent> def test_no_content_group_linked ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ In ▁ a ▁ course ▁ with ▁ content ▁ groups, ▁ cohorts ▁ are ▁ initially ▁ not ▁ linked ▁ to ▁ a ▁ content ▁ group <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ and ▁ content ▁ groups ▁ defined <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ cohort ▁ is ▁ not ▁ linked ▁ to ▁ a ▁ content ▁ group <strnewline> ▁ And ▁ there ▁ is ▁ no ▁ text ▁ stating ▁ that ▁ content ▁ groups ▁ are ▁ undefined <strnewline> ▁ And ▁ the ▁ content ▁ groups ▁ are ▁ listed ▁ in ▁ the ▁ selector <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( self . cohort_name ) <newline> self . assertIsNone ( self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> self . assertIsNone ( self . cohort_management_page . get_cohort_related_content_group_message ( ) ) <newline> self . assertEquals ( [ "Apples" , "Bananas" ] , self . cohort_management_page . get_all_content_groups ( ) ) <newline> <dedent> def test_link_to_content_group ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ In ▁ a ▁ course ▁ with ▁ content ▁ groups, ▁ cohorts ▁ can ▁ be ▁ linked ▁ to ▁ content ▁ groups <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ and ▁ content ▁ groups ▁ defined <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ And ▁ I ▁ link ▁ the ▁ cohort ▁ to ▁ one ▁ of ▁ the ▁ content ▁ groups ▁ and ▁ save <strnewline> ▁ Then ▁ there ▁ is ▁ a ▁ notification ▁ that ▁ my ▁ cohort ▁ has ▁ been ▁ saved <strnewline> ▁ And ▁ when ▁ I ▁ reload ▁ the ▁ page <strnewline> ▁ And ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ cohort ▁ is ▁ still ▁ linked ▁ to ▁ the ▁ content ▁ group <strnewline> ▁ """  <newline> self . _link_cohort_to_content_group ( self . cohort_name , "Bananas" ) <newline> self . assertEqual ( "Bananas" , self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> <dedent> def test_unlink_from_content_group ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ In ▁ a ▁ course ▁ with ▁ content ▁ groups, ▁ cohorts ▁ can ▁ be ▁ unlinked ▁ from ▁ content ▁ groups <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ and ▁ content ▁ groups ▁ defined <strnewline> ▁ When ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ And ▁ I ▁ link ▁ the ▁ cohort ▁ to ▁ one ▁ of ▁ the ▁ content ▁ groups ▁ and ▁ save <strnewline> ▁ Then ▁ there ▁ is ▁ a ▁ notification ▁ that ▁ my ▁ cohort ▁ has ▁ been ▁ saved <strnewline> ▁ And ▁ I ▁ reload ▁ the ▁ page <strnewline> ▁ And ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ And ▁ I ▁ unlink ▁ the ▁ cohort ▁ from ▁ any ▁ content ▁ group ▁ and ▁ save <strnewline> ▁ Then ▁ there ▁ is ▁ a ▁ notification ▁ that ▁ my ▁ cohort ▁ has ▁ been ▁ saved <strnewline> ▁ And ▁ when ▁ I ▁ reload ▁ the ▁ page <strnewline> ▁ And ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ cohort ▁ is ▁ not ▁ linked ▁ to ▁ any ▁ content ▁ group <strnewline> ▁ """  <newline> self . _link_cohort_to_content_group ( self . cohort_name , "Bananas" ) <newline> self . cohort_management_page . set_cohort_associated_content_group ( None ) <newline> self . _verify_settings_saved_and_reload ( self . cohort_name ) <newline> self . assertEqual ( None , self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> <dedent> def test_create_new_cohort_linked_to_content_group ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ In ▁ a ▁ course ▁ with ▁ content ▁ groups, ▁ a ▁ new ▁ cohort ▁ can ▁ be ▁ linked ▁ to ▁ a ▁ content ▁ group <strnewline> ▁ at ▁ time ▁ of ▁ creation. <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ and ▁ content ▁ groups ▁ defined <strnewline> ▁ When ▁ I ▁ create ▁ a ▁ new ▁ cohort ▁ and ▁ link ▁ it ▁ to ▁ a ▁ content ▁ group <strnewline> ▁ Then ▁ when ▁ I ▁ select ▁ settings ▁ I ▁ see ▁ that ▁ the ▁ cohort ▁ is ▁ linked ▁ to ▁ the ▁ content ▁ group <strnewline> ▁ And ▁ when ▁ I ▁ reload ▁ the ▁ page <strnewline> ▁ And ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ cohort ▁ is ▁ still ▁ linked ▁ to ▁ the ▁ content ▁ group <strnewline> ▁ """  <newline> new_cohort = "correctly ▁ linked ▁ cohort" <newline> self . _create_new_cohort_linked_to_content_group ( new_cohort , "Apples" ) <newline> self . browser . refresh ( ) <newline> self . cohort_management_page . wait_for_page ( ) <newline> self . cohort_management_page . select_cohort ( new_cohort ) <newline> self . assertEqual ( "Apples" , self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> <dedent> def test_missing_content_group ( self ) : <newline> <indent>  """ <strnewline> ▁ Scenario: ▁ In ▁ a ▁ course ▁ with ▁ content ▁ groups, ▁ if ▁ a ▁ cohort ▁ is ▁ associated ▁ with ▁ a ▁ content ▁ group ▁ that ▁ no ▁ longer <strnewline> ▁ exists, ▁ a ▁ warning ▁ message ▁ is ▁ shown <strnewline> <strnewline> ▁ Given ▁ I ▁ have ▁ a ▁ course ▁ with ▁ a ▁ cohort ▁ defined ▁ and ▁ content ▁ groups ▁ defined <strnewline> ▁ When ▁ I ▁ create ▁ a ▁ new ▁ cohort ▁ and ▁ link ▁ it ▁ to ▁ a ▁ content ▁ group <strnewline> ▁ And ▁ I ▁ delete ▁ that ▁ content ▁ group ▁ from ▁ the ▁ course <strnewline> ▁ And ▁ I ▁ reload ▁ the ▁ page <strnewline> ▁ And ▁ I ▁ view ▁ the ▁ cohort ▁ in ▁ the ▁ instructor ▁ dashboard ▁ and ▁ select ▁ settings <strnewline> ▁ Then ▁ the ▁ settings ▁ display ▁ a ▁ message ▁ that ▁ the ▁ content ▁ group ▁ no ▁ longer ▁ exists <strnewline> ▁ And ▁ when ▁ I ▁ select ▁ a ▁ different ▁ content ▁ group ▁ and ▁ save <strnewline> ▁ Then ▁ the ▁ error ▁ message ▁ goes ▁ away <strnewline> ▁ """  <newline> new_cohort = "linked ▁ to ▁ missing ▁ content ▁ group" <newline> self . _create_new_cohort_linked_to_content_group ( new_cohort , "Apples" ) <newline> self . course_fixture . _update_xblock ( self . course_fixture . _course_location , { "metadata" : { u"user_partitions" : [ create_user_partition_json ( 0 , 'Apples, ▁ Bananas' , 'Content ▁ Group ▁ Partition' , [ Group ( "2" , 'Pears' ) , Group ( "1" , 'Bananas' ) ] , scheme = "cohort" ) ] , } , } ) <newline> self . browser . refresh ( ) <newline> self . cohort_management_page . wait_for_page ( ) <newline> self . cohort_management_page . select_cohort ( new_cohort ) <newline> self . assertEqual ( "Deleted ▁ Content ▁ Group" , self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> self . assertEquals ( [ "Bananas" , "Pears" , "Deleted ▁ Content ▁ Group" ] , self . cohort_management_page . get_all_content_groups ( ) ) <newline> self . assertEqual ( "Warning: \n The ▁ previously ▁ selected ▁ content ▁ group ▁ was ▁ deleted. ▁ Select ▁ another ▁ content ▁ group." , self . cohort_management_page . get_cohort_related_content_group_message ( ) ) <newline> self . cohort_management_page . set_cohort_associated_content_group ( "Pears" ) <newline> confirmation_messages = self . cohort_management_page . get_cohort_settings_messages ( ) <newline> self . assertEqual ( [ "Saved ▁ cohort" ] , confirmation_messages ) <newline> self . assertIsNone ( self . cohort_management_page . get_cohort_related_content_group_message ( ) ) <newline> self . assertEquals ( [ "Bananas" , "Pears" ] , self . cohort_management_page . get_all_content_groups ( ) ) <newline> <dedent> def _create_new_cohort_linked_to_content_group ( self , new_cohort , cohort_group ) : <newline> <indent>  """ <strnewline> ▁ Creates ▁ a ▁ new ▁ cohort ▁ linked ▁ to ▁ a ▁ content ▁ group. <strnewline> ▁ """  <newline> self . cohort_management_page . add_cohort ( new_cohort , content_group = cohort_group ) <newline> self . assertEqual ( cohort_group , self . cohort_management_page . get_cohort_associated_content_group ( ) ) <newline> <dedent> def _link_cohort_to_content_group ( self , cohort_name , content_group ) : <newline> <indent>  """ <strnewline> ▁ Links ▁ a ▁ cohort ▁ to ▁ a ▁ content ▁ group. ▁ Saves ▁ the ▁ changes ▁ and ▁ verifies ▁ the ▁ cohort ▁ updated ▁ properly. <strnewline> ▁ Then ▁ refreshes ▁ the ▁ page ▁ and ▁ selects ▁ the ▁ cohort. <strnewline> ▁ """  <newline> self . cohort_management_page . select_cohort ( cohort_name ) <newline> self . cohort_management_page . set_cohort_associated_content_group ( content_group ) <newline> self . _verify_settings_saved_and_reload ( cohort_name ) <newline> <dedent> def _verify_settings_saved_and_reload ( self , cohort_name ) : <newline> <indent>  """ <strnewline> ▁ Verifies ▁ the ▁ confirmation ▁ message ▁ indicating ▁ that ▁ a ▁ cohort's ▁ settings ▁ have ▁ been ▁ updated. <strnewline> ▁ Then ▁ refreshes ▁ the ▁ page ▁ and ▁ selects ▁ the ▁ cohort. <strnewline> ▁ """  <newline> confirmation_messages = self . cohort_management_page . get_cohort_settings_messages ( ) <newline> self . assertEqual ( [ "Saved ▁ cohort" ] , confirmation_messages ) <newline> self . browser . refresh ( ) <newline> self . cohort_management_page . wait_for_page ( ) <newline> self . cohort_management_page . select_cohort ( cohort_name ) <newline> <dedent> <dedent>
 # ▁ Copyright ▁ 2011 ▁ OpenStack ▁ Foundation <encdom>  # ▁ All ▁ Rights ▁ Reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); ▁ you ▁ may <encdom>  # ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. ▁ You ▁ may ▁ obtain <encdom>  # ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, ▁ WITHOUT <encdom>  # ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. ▁ See ▁ the <encdom>  # ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and ▁ limitations <encdom>  # ▁ under ▁ the ▁ License. <encdom>  """ The ▁ deferred ▁ instance ▁ delete ▁ extension. """  <newline> import webob <newline> from nova . api . openstack import common <newline> from nova . api . openstack import extensions <newline> from nova . api . openstack import wsgi <newline> from nova import compute <newline> from nova import exception <newline> ALIAS = 'os-deferred-delete' <newline> authorize = extensions . os_compute_authorizer ( ALIAS ) <newline> class DeferredDeleteController ( wsgi . Controller ) : <newline> <indent> def __init__ ( self , * args , ** kwargs ) : <newline> <indent> super ( DeferredDeleteController , self ) . __init__ ( * args , ** kwargs ) <newline> self . compute_api = compute . API ( skip_policy_check = True ) <newline> <dedent> @ wsgi . response ( 202 ) <newline> @ extensions . expected_errors ( ( 404 , 409 , 403 ) ) <newline> @ wsgi . action ( 'restore' ) <newline> def _restore ( self , req , id , body ) : <newline> <indent>  """ Restore ▁ a ▁ previously ▁ deleted ▁ instance. """  <newline> context = req . environ [ "nova.context" ] <newline> authorize ( context ) <newline> instance = common . get_instance ( self . compute_api , context , id ) <newline> try : <newline> <indent> self . compute_api . restore ( context , instance ) <newline> <dedent> except exception . InstanceUnknownCell as error : <newline> <indent> raise webob . exc . HTTPNotFound ( explanation = error . format_message ( ) ) <newline> <dedent> except exception . QuotaError as error : <newline> <indent> raise webob . exc . HTTPForbidden ( explanation = error . format_message ( ) ) <newline> <dedent> except exception . InstanceInvalidState as state_error : <newline> <indent> common . raise_http_conflict_for_instance_invalid_state ( state_error , 'restore' , id ) <newline> <dedent> <dedent> @ wsgi . response ( 202 ) <newline> @ extensions . expected_errors ( ( 404 , 409 ) ) <newline> @ wsgi . action ( 'forceDelete' ) <newline> def _force_delete ( self , req , id , body ) : <newline> <indent>  """ Force ▁ delete ▁ of ▁ instance ▁ before ▁ deferred ▁ cleanup. """  <newline> context = req . environ [ "nova.context" ] <newline> authorize ( context ) <newline> instance = common . get_instance ( self . compute_api , context , id ) <newline> try : <newline> <indent> self . compute_api . force_delete ( context , instance ) <newline> <dedent> except exception . InstanceIsLocked as e : <newline> <indent> raise webob . exc . HTTPConflict ( explanation = e . format_message ( ) ) <newline> <dedent> <dedent> <dedent> class DeferredDelete ( extensions . V21APIExtensionBase ) : <newline> <indent>  """ Instance ▁ deferred ▁ delete. """  <newline> name = "DeferredDelete" <newline> alias = "os-deferred-delete" <newline> version = 1 <newline> def get_controller_extensions ( self ) : <newline> <indent> controller = DeferredDeleteController ( ) <newline> extension = extensions . ControllerExtension ( self , 'servers' , controller ) <newline> return [ extension ] <newline> <dedent> def get_resources ( self ) : <newline> <indent> return [ ] <newline> <dedent> <dedent>
from parsing import * <newline> def base12 ( n ) : <newline> <indent> digits = '0123456789ab' <newline> result = '' <newline> while n >= 12 : <newline> <indent> result = digits [ n % 12 ] + result <newline> n //= 12 <newline> <dedent> result = digits [ n ] + result <newline> return result <newline> <dedent> def mapping ( pattern , output ) : <newline> <indent> @ parser <newline> def mapping_implementation ( input ) : <newline> <indent> input . match ( Parser . coerce ( pattern ) ) <newline> return output <newline> <dedent> return mapping_implementation <newline> <dedent> def many1 ( parser ) : <newline> <indent> return many ( parser , at_least = 1 ) <newline> <dedent>
import collections <newline> import multiprocessing <newline> import pyes <newline> from pyes . exceptions import NotFoundException <newline> import pymongo <newline> import logging <newline> import time <newline> from bson . objectid import ObjectId <newline> from . spec import QuerySpecification <newline> from . oplog_watcher import OplogWatcher <newline> import base64 <newline> _indexes = [ ] <newline> _connections = { } <newline> class IndexMeta ( type ) : <newline> <indent> def __new__ ( mcs , name , bases , attrs ) : <newline> <indent> metaclass = attrs . get ( '__metaclass__' ) <newline> super_new = super ( IndexMeta , mcs ) . __new__ <newline> if metaclass and issubclass ( metaclass , IndexMeta ) : <newline> <indent> return super_new ( mcs , name , bases , attrs ) <newline> <dedent> terms = { } <newline> for attr_name , attr_value in attrs . items ( ) : <newline> <indent> if isinstance ( attr_value , Term ) : <newline> <indent> term = attr_value <newline> term . name = attr_name <newline> if term . index_name is None : <newline> <indent> term . index_name = term . name <newline> <dedent> terms [ attr_name ] = attr_value <newline> del attrs [ attr_name ] <newline> <dedent> <dedent> attrs [ 'terms' ] = terms <newline> meta = attrs . pop ( 'Meta' , None ) <newline> attrs [ '_meta' ] = { 'host' : getattr ( meta , 'host' ) , 'model' : getattr ( meta , 'model' ) , 'spec' : getattr ( meta , 'spec' , QuerySpecification ( ) ) , } <newline> new_cls = super_new ( mcs , name , bases , attrs ) <newline> index = new_cls . instance ( ) <newline> _indexes . append ( index ) <newline> index . model . _search_index = index <newline> return new_cls <newline> <dedent> <dedent> class Index ( object ) : <newline> <indent> __metaclass__ = IndexMeta <newline> def __init__ ( self ) : <newline> <indent> self . _meta = self . __class__ . _meta <newline> self . model = self . _meta [ 'model' ] <newline> self . spec = self . _meta [ 'spec' ] <newline> self . uri , _ , db = self . model . _meta [ 'db' ] . rpartition ( '/' ) <newline> self . namespace = '%s-%s' % ( db , self . model . _meta [ 'collection' ] ) <newline> self . doc_type = self . model . _name <newline> <dedent> @ classmethod <newline> def instance ( cls ) : <newline> <indent> if not hasattr ( cls , '_instance' ) : <newline> <indent> cls . _instance = cls ( ) <newline> <dedent> return cls . _instance <newline> <dedent> @ property <newline> def connection ( self ) : <newline> <indent> if not hasattr ( self , '_connection' ) : <newline> <indent> host = self . _meta [ 'host' ] <newline> if host not in _connections : <newline> <indent> _connections [ host ] = pyes . ES ( host ) <newline> <dedent> self . _connection = _connections [ host ] <newline> <dedent> return self . _connection <newline> <dedent> def search ( self , query , page = 1 , limit = 5 , filters = None ) : <newline> <indent> return search ( self , query , page , limit , filters ) <newline> <dedent> def indexer ( self ) : <newline> <indent> return Indexer ( self ) <newline> <dedent> <dedent> class Term ( object ) : <newline> <indent> def __init__ ( self , index_name = None , index = True , boost = 1.0 , null_value = None , coerce = None ) : <newline> <indent> self . name = None <newline> self . index_name = index_name <newline> self . index = index <newline> self . boost = boost <newline> self . null_value = null_value <newline> self . coerce = coerce <newline> <dedent> <dedent> class Indexer ( object ) : <newline> <indent> def __init__ ( self , index ) : <newline> <indent> self . index = index <newline> <dedent> def index_document ( self , obj , bulk = False ) : <newline> <indent> doc = { } <newline> for term in self . index . terms . values ( ) : <newline> <indent> if not term . index : <newline> <indent> continue <newline> <dedent> value = getattr ( obj , term . name ) <newline> if value is not None : <newline> <indent> if isinstance ( value , ObjectId ) : <newline> <indent> value = str ( value ) <newline> <dedent> if term . coerce is not None : <newline> <indent> value = term . coerce ( value ) <newline> <dedent> doc [ term . index_name ] = value <newline> <dedent> <dedent> self . _execute ( self . index . connection . index , doc , self . index . namespace , self . index . doc_type , id = base64 . b64encode ( str ( obj . id ) ) , bulk = bulk ) <newline> <dedent> def delete_document ( self , doc_id ) : <newline> <indent> self . _execute ( self . index . connection . delete , self . index . namespace , self . index . doc_type , base64 . b64encode ( str ( doc_id ) ) ) <newline> <dedent> def insert ( self , obj ) : <newline> <indent> obj = self . index . model . to_python ( obj ) <newline> logging . info ( 'Indexing ▁ %s ▁ (%s)' % ( self . index . model . _name , obj . id ) ) <newline> self . index_document ( obj ) <newline> <dedent> def update ( self , obj_id , raw ) : <newline> <indent> o = raw [ 'o' ] <newline> fields = self . index . terms . keys ( ) <newline> if o . has_key ( '$set' ) and len ( set ( fields ) - set ( o [ '$set' ] . keys ( ) ) ) < len ( fields ) : <newline> <indent> obj = self . index . model . objects . only ( * fields ) . filter ( self . index . spec ) . with_id ( obj_id ) <newline> if obj is not None : <newline> <indent> logging . info ( 'Updating ▁ %s ▁ (%s)' % ( self . index . model . _name , obj . id ) ) <newline> self . index_document ( obj ) <newline> <dedent> else : <newline> <indent> self . delete ( obj_id ) <newline> <dedent> <dedent> <dedent> def delete ( self , obj_id ) : <newline> <indent> logging . info ( 'Deleting ▁ %s ▁ (%s)' % ( self . index . model . _name , obj_id ) ) <newline> self . delete_document ( obj_id ) <newline> <dedent> def _execute ( self , func , * args , ** kwargs ) : <newline> <indent> attempts = 0 <newline> while attempts < 5 : <newline> <indent> try : <newline> <indent> func ( * args , ** kwargs ) <newline> break <newline> <dedent> except NotFoundException : <newline> <indent> break <newline> <dedent> except Exception : <newline> <indent> attempts += 1 <newline> logging . warning ( 'Retrying... ▁ (%d)' % attempts , exc_info = True ) <newline> time . sleep ( 1 ) <newline> <dedent> <dedent> <dedent> <dedent> class ResultSet ( object ) : <newline> <indent> def __init__ ( self , objects = None , total = 0 , elapsed_time = 0 , max_score = 0 ) : <newline> <indent> self . objects = objects or [ ] <newline> self . meta = { } <newline> self . total = total <newline> self . elapsed_time = elapsed_time <newline> self . max_score = max_score <newline> <dedent> def append ( self , value , meta ) : <newline> <indent> if value is not None : <newline> <indent> self . objects . append ( value ) <newline> self . meta [ value ] = meta <newline> <dedent> <dedent> def has_more ( self ) : <newline> <indent> return len ( self . objects ) < self . total <newline> <dedent> def __len__ ( self ) : <newline> <indent> return self . objects . __len__ ( ) <newline> <dedent> def __iter__ ( self ) : <newline> <indent> for obj in self . objects : <newline> <indent> yield obj , self . meta [ obj ] <newline> <dedent> <dedent> <dedent> def search ( indexes , query , page = 1 , limit = 5 , filters = None ) : <newline> <indent> if not isinstance ( indexes , list ) : <newline> <indent> indexes = [ indexes ] <newline> <dedent> namespaces = [ ] <newline> models = { } <newline> for i , index in enumerate ( indexes ) : <newline> <indent> if not isinstance ( index , Index ) : <newline> <indent> model = index <newline> for index in _indexes : <newline> <indent> if index . model == model : <newline> <indent> indexes [ i ] = index <newline> break <newline> <dedent> <dedent> <dedent> namespaces . append ( index . namespace ) <newline> models [ index . namespace ] = index . model <newline> <dedent> result_set = ResultSet ( ) <newline> result_set . query = query <newline> if isinstance ( query , ( str , unicode ) ) : <newline> <indent> if query . endswith ( ':' ) : <newline> <indent> query = query [ : - 1 ] <newline> <dedent> if any ( [ op in query for op in [ '?' , '*' , '~' , 'OR' , 'AND' , '+' , 'NOT' , '-' , ':' ] ] ) : <newline> <indent> query = pyes . StringQuery ( query ) <newline> <dedent> else : <newline> <indent> query = pyes . StringQuery ( query + '*' ) <newline> <dedent> <dedent> if not isinstance ( query , pyes . FilteredQuery ) and filters : <newline> <indent> term_filter = pyes . TermFilter ( ) <newline> for field , value in filters . iteritems ( ) : <newline> <indent> term_filter . add ( field , value ) <newline> <dedent> query = pyes . FilteredQuery ( query , term_filter ) <newline> <dedent> page = int ( page ) <newline> limit = int ( limit ) <newline> skip = ( page - 1 ) * limit <newline> try : <newline> <indent> response = _indexes [ 0 ] . connection . search ( query , indices = namespaces , ** { 'from' : str ( skip ) , 'size' : str ( limit ) } ) <newline> result_set . total = response . total <newline> result_set . elapsed_time = response . _results [ 'took' ] / 1000.0 <newline> result_set . max_score = response . max_score <newline> for i , hit in enumerate ( response . hits ) : <newline> <indent> result_set . append ( models [ hit [ '_index' ] ] . objects . with_id ( base64 . b64decode ( hit [ '_id' ] ) ) , { 'rank' : skip + i + 1 , 'score' : hit [ '_score' ] , 'relevance' : int ( hit [ '_score' ] / result_set . max_score * 100 ) } ) <newline> <dedent> <dedent> except pyes . exceptions . SearchPhaseExecutionException : <newline> <indent> pass <newline> <dedent> return result_set <newline> <dedent> def reindex ( only = None ) : <newline> <indent> logging . info ( 'Reindexing...' ) <newline> for index in _indexes : <newline> <indent> if only and index . namespace not in only : <newline> <indent> continue <newline> <dedent> try : <newline> <indent> index . connection . delete_index ( index . namespace ) <newline> <dedent> except pyes . exceptions . IndexMissingException : <newline> <indent> pass <newline> <dedent> index . connection . create_index ( index . namespace ) <newline> objects = index . model . objects . only ( * index . terms . keys ( ) ) . filter ( index . spec ) <newline> count = objects . count ( ) <newline> logging . info ( '%d ▁ object(s) ▁ from ▁ %s' % ( count , index . namespace ) ) <newline> indexer = Indexer ( index ) <newline> for i , obj in enumerate ( objects ) : <newline> <indent> i += 1 <newline> if not i % 10000 : <newline> <indent> logging . info ( '%d/%d' , i , count ) <newline> <dedent> indexer . index_document ( obj , bulk = True ) <newline> <dedent> indexer . index . connection . force_bulk ( ) <newline> <dedent> logging . info ( 'Done!' ) <newline> <dedent> def watch ( ) : <newline> <indent> hosts = collections . defaultdict ( list ) <newline> global _indexes <newline> for index in _indexes : <newline> <indent> hosts [ index . uri ] . append ( index ) <newline> <dedent> def target ( uri , indexes ) : <newline> <indent> namespaces = [ index . namespace . replace ( '-' , '.' ) for index in indexes ] <newline> logging . info ( 'Watching ▁ %s' % namespaces ) <newline> oplog_watcher = OplogWatcher ( pymongo . Connection ( uri ) , namespaces = namespaces ) <newline> for index in indexes : <newline> <indent> indexer = index . indexer ( ) <newline> for op in ( 'insert' , 'update' , 'delete' , ) : <newline> <indent> oplog_watcher . add_handler ( index . namespace . replace ( '-' , '.' ) , op , getattr ( indexer , op ) ) <newline> <dedent> <dedent> oplog_watcher . start ( ) <newline> <dedent> if len ( hosts ) > 1 : <newline> <indent> for uri , _indexes in hosts . items ( ) : <newline> <indent> multiprocessing . Process ( target = target , args = ( uri , _indexes ) ) . start ( ) <newline> <dedent> <dedent> else : <newline> <indent> target ( * hosts . items ( ) [ 0 ] ) <newline> <dedent> while True : <newline> <indent> time . sleep ( 1 ) <newline> <dedent> <dedent>
 # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ Headphones. <encdom>  # ▁ Headphones ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ Headphones ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ Headphones. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom>  # ▁ NZBGet ▁ support ▁ added ▁ by ▁ CurlyMo ▁ <curlymoo1@gmail.com> ▁ as ▁ a ▁ part ▁ of ▁ XBian ▁ - ▁ XBMC ▁ on ▁ the ▁ Raspberry ▁ Pi <encdom> from headphones import logger , searcher , db , importer , mb , lastfm , librarysync , helpers , notifiers <newline> from headphones . helpers import checked , radio , today , cleanName <newline> from mako . lookup import TemplateLookup <newline> from mako import exceptions <newline> from operator import itemgetter <newline> import headphones <newline> import threading <newline> import cherrypy <newline> import urllib2 <newline> import hashlib <newline> import random <newline> import urllib <newline> import json <newline> import time <newline> import cgi <newline> import sys <newline> import os <newline> import re <newline> try : <newline>  # ▁ pylint:disable=E0611 <encdom>  # ▁ ignore ▁ this ▁ error ▁ because ▁ we ▁ are ▁ catching ▁ the ▁ ImportError <encdom> <indent> from collections import OrderedDict <newline>  # ▁ pylint:enable=E0611 <encdom> <dedent> except ImportError : <newline>  # ▁ Python ▁ 2.6.x ▁ fallback, ▁ from ▁ libs <encdom> <indent> from ordereddict import OrderedDict <newline> <dedent> def serve_template ( templatename , ** kwargs ) : <newline> <indent> interface_dir = os . path . join ( str ( headphones . PROG_DIR ) , 'data/interfaces/' ) <newline> template_dir = os . path . join ( str ( interface_dir ) , headphones . CONFIG . INTERFACE ) <newline> _hplookup = TemplateLookup ( directories = [ template_dir ] ) <newline> try : <newline> <indent> template = _hplookup . get_template ( templatename ) <newline> return template . render ( ** kwargs ) <newline> <dedent> except : <newline> <indent> return exceptions . html_error_template ( ) . render ( ) <newline> <dedent> <dedent> class WebInterface ( object ) : <newline> <indent> @ cherrypy . expose <newline> def index ( self ) : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def home ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artists = myDB . select ( 'SELECT ▁ * ▁ from ▁ artists ▁ order ▁ by ▁ ArtistSortName ▁ COLLATE ▁ NOCASE' ) <newline> return serve_template ( templatename = "index.html" , title = "Home" , artists = artists ) <newline> <dedent> @ cherrypy . expose <newline> def artistPage ( self , ArtistID ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artist = myDB . action ( 'SELECT ▁ * ▁ FROM ▁ artists ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) . fetchone ( ) <newline>  # ▁ Don't ▁ redirect ▁ to ▁ the ▁ artist ▁ page ▁ until ▁ it ▁ has ▁ the ▁ bare ▁ minimum ▁ info ▁ inserted <encdom>  # ▁ Redirect ▁ to ▁ the ▁ home ▁ page ▁ if ▁ we ▁ still ▁ can't ▁ get ▁ it ▁ after ▁ 5 ▁ seconds <encdom> retry = 0 <newline> while not artist and retry < 5 : <newline> <indent> time . sleep ( 1 ) <newline> artist = myDB . action ( 'SELECT ▁ * ▁ FROM ▁ artists ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) . fetchone ( ) <newline> retry += 1 <newline> <dedent> if not artist : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> albums = myDB . select ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ ArtistID=? ▁ order ▁ by ▁ ReleaseDate ▁ DESC' , [ ArtistID ] ) <newline>  # ▁ Serve ▁ the ▁ extras ▁ up ▁ as ▁ a ▁ dict ▁ to ▁ make ▁ things ▁ easier ▁ for ▁ new ▁ templates ▁ (append ▁ new ▁ extras ▁ to ▁ the ▁ end) <encdom> extras_list = headphones . POSSIBLE_EXTRAS <newline> if artist [ 'Extras' ] : <newline> <indent> artist_extras = map ( int , artist [ 'Extras' ] . split ( ',' ) ) <newline> <dedent> else : <newline> <indent> artist_extras = [ ] <newline> <dedent> extras_dict = OrderedDict ( ) <newline> i = 1 <newline> for extra in extras_list : <newline> <indent> if i in artist_extras : <newline> <indent> extras_dict [ extra ] = "checked" <newline> <dedent> else : <newline> <indent> extras_dict [ extra ] = "" <newline> <dedent> i += 1 <newline> <dedent> return serve_template ( templatename = "artist.html" , title = artist [ 'ArtistName' ] , artist = artist , albums = albums , extras = extras_dict ) <newline> <dedent> @ cherrypy . expose <newline> def albumPage ( self , AlbumID ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> album = myDB . action ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) . fetchone ( ) <newline> retry = 0 <newline> while retry < 5 : <newline> <indent> if not album : <newline> <indent> time . sleep ( 1 ) <newline> album = myDB . action ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) . fetchone ( ) <newline> retry += 1 <newline> <dedent> else : <newline> <indent> break <newline> <dedent> <dedent> if not album : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> tracks = myDB . select ( 'SELECT ▁ * ▁ from ▁ tracks ▁ WHERE ▁ AlbumID=? ▁ ORDER ▁ BY ▁ CAST(TrackNumber ▁ AS ▁ INTEGER)' , [ AlbumID ] ) <newline> description = myDB . action ( 'SELECT ▁ * ▁ from ▁ descriptions ▁ WHERE ▁ ReleaseGroupID=?' , [ AlbumID ] ) . fetchone ( ) <newline> if not album [ 'ArtistName' ] : <newline> <indent> title = ' ▁ - ▁ ' <newline> <dedent> else : <newline> <indent> title = album [ 'ArtistName' ] + ' ▁ - ▁ ' <newline> <dedent> if not album [ 'AlbumTitle' ] : <newline> <indent> title = title + "" <newline> <dedent> else : <newline> <indent> title = title + album [ 'AlbumTitle' ] <newline> <dedent> return serve_template ( templatename = "album.html" , title = title , album = album , tracks = tracks , description = description ) <newline> <dedent> @ cherrypy . expose <newline> def search ( self , name , type ) : <newline> <indent> if len ( name ) == 0 : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> if type == 'artist' : <newline> <indent> searchresults = mb . findArtist ( name , limit = 100 ) <newline> <dedent> elif type == 'album' : <newline> <indent> searchresults = mb . findRelease ( name , limit = 100 ) <newline> <dedent> else : <newline> <indent> searchresults = mb . findSeries ( name , limit = 100 ) <newline> <dedent> return serve_template ( templatename = "searchresults.html" , title = 'Search ▁ Results ▁ for: ▁ "' + cgi . escape ( name ) + '"' , searchresults = searchresults , name = cgi . escape ( name ) , type = type ) <newline> <dedent> @ cherrypy . expose <newline> def addArtist ( self , artistid ) : <newline> <indent> thread = threading . Thread ( target = importer . addArtisttoDB , args = [ artistid ] ) <newline> thread . start ( ) <newline> thread . join ( 1 ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % artistid ) <newline> <dedent> @ cherrypy . expose <newline> def addSeries ( self , seriesid ) : <newline> <indent> thread = threading . Thread ( target = importer . addArtisttoDB , args = [ seriesid , False , False , "series" ] ) <newline> thread . start ( ) <newline> thread . join ( 1 ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % seriesid ) <newline> <dedent> @ cherrypy . expose <newline> def getExtras ( self , ArtistID , newstyle = False , ** kwargs ) : <newline>  # ▁ if ▁ calling ▁ this ▁ function ▁ without ▁ the ▁ newstyle, ▁ they're ▁ using ▁ the ▁ old ▁ format <encdom>  # ▁ which ▁ doesn't ▁ separate ▁ extras, ▁ so ▁ we'll ▁ grab ▁ all ▁ of ▁ them <encdom>  # ▁ If ▁ they ▁ are, ▁ we ▁ need ▁ to ▁ convert ▁ kwargs ▁ to ▁ string ▁ format <encdom> <indent> if not newstyle : <newline> <indent> extras = "1,2,3,4,5,6,7,8,9,10,11,12,13,14" <newline> <dedent> else : <newline> <indent> temp_extras_list = [ ] <newline> i = 1 <newline> for extra in headphones . POSSIBLE_EXTRAS : <newline> <indent> if extra in kwargs : <newline> <indent> temp_extras_list . append ( i ) <newline> <dedent> i += 1 <newline> <dedent> extras = ',' . join ( str ( n ) for n in temp_extras_list ) <newline> <dedent> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'IncludeExtras' : 1 , 'Extras' : extras } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> thread = threading . Thread ( target = importer . addArtisttoDB , args = [ ArtistID , True , False ] ) <newline> thread . start ( ) <newline> thread . join ( 1 ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def removeExtras ( self , ArtistID , ArtistName ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'IncludeExtras' : 0 } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> extraalbums = myDB . select ( 'SELECT ▁ AlbumID ▁ from ▁ albums ▁ WHERE ▁ ArtistID=? ▁ AND ▁ Status="Skipped" ▁ AND ▁ Type!="Album"' , [ ArtistID ] ) <newline> for album in extraalbums : <newline> <indent> myDB . action ( 'DELETE ▁ from ▁ tracks ▁ WHERE ▁ ArtistID=? ▁ AND ▁ AlbumID=?' , [ ArtistID , album [ 'AlbumID' ] ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ albums ▁ WHERE ▁ ArtistID=? ▁ AND ▁ AlbumID=?' , [ ArtistID , album [ 'AlbumID' ] ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ allalbums ▁ WHERE ▁ ArtistID=? ▁ AND ▁ AlbumID=?' , [ ArtistID , album [ 'AlbumID' ] ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ alltracks ▁ WHERE ▁ ArtistID=? ▁ AND ▁ AlbumID=?' , [ ArtistID , album [ 'AlbumID' ] ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ releases ▁ WHERE ▁ ReleaseGroupID=?' , [ album [ 'AlbumID' ] ] ) <newline> from headphones import cache <newline> c = cache . Cache ( ) <newline> c . remove_from_cache ( AlbumID = album [ 'AlbumID' ] ) <newline> <dedent> importer . finalize_update ( ArtistID , ArtistName ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def pauseArtist ( self , ArtistID ) : <newline> <indent> logger . info ( u"Pausing ▁ artist: ▁ " + ArtistID ) <newline> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'Status' : 'Paused' } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def resumeArtist ( self , ArtistID ) : <newline> <indent> logger . info ( u"Resuming ▁ artist: ▁ " + ArtistID ) <newline> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'Status' : 'Active' } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> def removeArtist ( self , ArtistID ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> namecheck = myDB . select ( 'SELECT ▁ ArtistName ▁ from ▁ artists ▁ where ▁ ArtistID=?' , [ ArtistID ] ) <newline> for name in namecheck : <newline> <indent> artistname = name [ 'ArtistName' ] <newline> <dedent> logger . info ( u"Deleting ▁ all ▁ traces ▁ of ▁ artist: ▁ " + artistname ) <newline> myDB . action ( 'DELETE ▁ from ▁ artists ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> from headphones import cache <newline> c = cache . Cache ( ) <newline> rgids = myDB . select ( 'SELECT ▁ AlbumID ▁ FROM ▁ albums ▁ WHERE ▁ ArtistID=? ▁ UNION ▁ SELECT ▁ AlbumID ▁ FROM ▁ allalbums ▁ WHERE ▁ ArtistID=?' , [ ArtistID , ArtistID ] ) <newline> for rgid in rgids : <newline> <indent> albumid = rgid [ 'AlbumID' ] <newline> myDB . action ( 'DELETE ▁ from ▁ releases ▁ WHERE ▁ ReleaseGroupID=?' , [ albumid ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ have ▁ WHERE ▁ Matched=?' , [ albumid ] ) <newline> c . remove_from_cache ( AlbumID = albumid ) <newline> myDB . action ( 'DELETE ▁ from ▁ descriptions ▁ WHERE ▁ ReleaseGroupID=?' , [ albumid ] ) <newline> <dedent> myDB . action ( 'DELETE ▁ from ▁ albums ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ tracks ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ allalbums ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ alltracks ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ have ▁ WHERE ▁ ArtistName=?' , [ artistname ] ) <newline> c . remove_from_cache ( ArtistID = ArtistID ) <newline> myDB . action ( 'DELETE ▁ from ▁ descriptions ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) <newline> myDB . action ( 'INSERT ▁ OR ▁ REPLACE ▁ into ▁ blacklist ▁ VALUES ▁ (?)' , [ ArtistID ] ) <newline> <dedent> @ cherrypy . expose <newline> def deleteArtist ( self , ArtistID ) : <newline> <indent> self . removeArtist ( ArtistID ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def scanArtist ( self , ArtistID ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artist_name = myDB . select ( 'SELECT ▁ DISTINCT ▁ ArtistName ▁ FROM ▁ artists ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) [ 0 ] [ 0 ] <newline> logger . info ( u"Scanning ▁ artist: ▁ %s" , artist_name ) <newline> full_folder_format = headphones . CONFIG . FOLDER_FORMAT <newline> folder_format = re . findall ( r'(.*?[Aa]rtist?)\.*' , full_folder_format ) [ 0 ] <newline> acceptable_formats = [ "$artist" , "$sortartist" , "$first/$artist" , "$first/$sortartist" ] <newline> if not folder_format . lower ( ) in acceptable_formats : <newline> <indent> logger . info ( "Can't ▁ determine ▁ the ▁ artist ▁ folder ▁ from ▁ the ▁ configured ▁ folder_format. ▁ Not ▁ scanning" ) <newline> return <newline>  # ▁ Format ▁ the ▁ folder ▁ to ▁ match ▁ the ▁ settings <encdom> <dedent> artist = artist_name . replace ( '/' , '_' ) <newline> if headphones . CONFIG . FILE_UNDERSCORES : <newline> <indent> artist = artist . replace ( ' ▁ ' , '_' ) <newline> <dedent> if artist . startswith ( 'The ▁ ' ) : <newline> <indent> sortname = artist [ 4 : ] + ", ▁ The" <newline> <dedent> else : <newline> <indent> sortname = artist <newline> <dedent> if sortname [ 0 ] . isdigit ( ) : <newline> <indent> firstchar = u'0-9' <newline> <dedent> else : <newline> <indent> firstchar = sortname [ 0 ] <newline> <dedent> values = { '$Artist' : artist , '$SortArtist' : sortname , '$First' : firstchar . upper ( ) , '$artist' : artist . lower ( ) , '$sortartist' : sortname . lower ( ) , '$first' : firstchar . lower ( ) , } <newline> folder = helpers . replace_all ( folder_format . strip ( ) , values , normalize = True ) <newline> folder = helpers . replace_illegal_chars ( folder , type = "folder" ) <newline> folder = folder . replace ( './' , '_/' ) . replace ( '/.' , '/_' ) <newline> if folder . endswith ( '.' ) : <newline> <indent> folder = folder [ : - 1 ] + '_' <newline> <dedent> if folder . startswith ( '.' ) : <newline> <indent> folder = '_' + folder [ 1 : ] <newline> <dedent> dirs = [ ] <newline> if headphones . CONFIG . MUSIC_DIR : <newline> <indent> dirs . append ( headphones . CONFIG . MUSIC_DIR ) <newline> <dedent> if headphones . CONFIG . DESTINATION_DIR : <newline> <indent> dirs . append ( headphones . CONFIG . DESTINATION_DIR ) <newline> <dedent> if headphones . CONFIG . LOSSLESS_DESTINATION_DIR : <newline> <indent> dirs . append ( headphones . CONFIG . LOSSLESS_DESTINATION_DIR ) <newline> <dedent> dirs = set ( dirs ) <newline> for dir in dirs : <newline> <indent> artistfolder = os . path . join ( dir , folder ) <newline> if not os . path . isdir ( artistfolder ) : <newline> <indent> logger . debug ( "Cannot ▁ find ▁ directory: ▁ " + artistfolder ) <newline> continue <newline> <dedent> threading . Thread ( target = librarysync . libraryScan , kwargs = { "dir" : artistfolder , "artistScan" : True , "ArtistID" : ArtistID , "ArtistName" : artist_name } ) . start ( ) <newline> <dedent> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def deleteEmptyArtists ( self ) : <newline> <indent> logger . info ( u"Deleting ▁ all ▁ empty ▁ artists" ) <newline> myDB = db . DBConnection ( ) <newline> emptyArtistIDs = [ row [ 'ArtistID' ] for row in myDB . select ( "SELECT ▁ ArtistID ▁ FROM ▁ artists ▁ WHERE ▁ LatestAlbum ▁ IS ▁ NULL" ) ] <newline> for ArtistID in emptyArtistIDs : <newline> <indent> self . removeArtist ( ArtistID ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def refreshArtist ( self , ArtistID ) : <newline> <indent> thread = threading . Thread ( target = importer . addArtisttoDB , args = [ ArtistID , False , True ] ) <newline> thread . start ( ) <newline> thread . join ( 1 ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def markAlbums ( self , ArtistID = None , action = None , ** args ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> if action == 'WantedNew' or action == 'WantedLossless' : <newline> <indent> newaction = 'Wanted' <newline> <dedent> else : <newline> <indent> newaction = action <newline> <dedent> for mbid in args : <newline> <indent> logger . info ( "Marking ▁ %s ▁ as ▁ %s" % ( mbid , newaction ) ) <newline> controlValueDict = { 'AlbumID' : mbid } <newline> newValueDict = { 'Status' : newaction } <newline> myDB . upsert ( "albums" , newValueDict , controlValueDict ) <newline> if action == 'Wanted' : <newline> <indent> searcher . searchforalbum ( mbid , new = False ) <newline> <dedent> if action == 'WantedNew' : <newline> <indent> searcher . searchforalbum ( mbid , new = True ) <newline> <dedent> if action == 'WantedLossless' : <newline> <indent> searcher . searchforalbum ( mbid , lossless = True ) <newline> <dedent> if ArtistID : <newline> <indent> ArtistIDT = ArtistID <newline> <dedent> else : <newline> <indent> ArtistIDT = myDB . action ( 'SELECT ▁ ArtistID ▁ FROM ▁ albums ▁ WHERE ▁ AlbumID=?' , [ mbid ] ) . fetchone ( ) [ 0 ] <newline> <dedent> myDB . action ( 'UPDATE ▁ artists ▁ SET ▁ TotalTracks=(SELECT ▁ COUNT(*) ▁ FROM ▁ tracks ▁ WHERE ▁ ArtistID ▁ = ▁ ? ▁ AND ▁ AlbumTitle ▁ IN ▁ (SELECT ▁ AlbumTitle ▁ FROM ▁ albums ▁ WHERE ▁ Status ▁ != ▁"Ignored")) ▁ WHERE ▁ ArtistID ▁ = ▁ ?' , [ ArtistIDT , ArtistIDT ] ) <newline> <dedent> if ArtistID : <newline> <indent> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> else : <newline> <indent> raise cherrypy . HTTPRedirect ( "upcoming" ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def addArtists ( self , action = None , ** args ) : <newline> <indent> if action == "add" : <newline> <indent> threading . Thread ( target = importer . artistlist_to_mbids , args = [ args , True ] ) . start ( ) <newline> <dedent> if action == "ignore" : <newline> <indent> myDB = db . DBConnection ( ) <newline> for artist in args : <newline> <indent> myDB . action ( 'DELETE ▁ FROM ▁ newartists ▁ WHERE ▁ ArtistName=?' , [ artist . decode ( headphones . SYS_ENCODING , 'replace' ) ] ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Ignored" ▁ WHERE ▁ ArtistName=?' , [ artist . decode ( headphones . SYS_ENCODING , 'replace' ) ] ) <newline> logger . info ( "Artist ▁ %s ▁ removed ▁ from ▁ new ▁ artist ▁ list ▁ and ▁ set ▁ to ▁ ignored" % artist ) <newline> <dedent> <dedent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def queueAlbum ( self , AlbumID , ArtistID = None , new = False , redirect = None , lossless = False ) : <newline> <indent> logger . info ( u"Marking ▁ album: ▁ " + AlbumID + " ▁ as ▁ wanted..." ) <newline> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'AlbumID' : AlbumID } <newline> if lossless : <newline> <indent> newValueDict = { 'Status' : 'Wanted ▁ Lossless' } <newline> logger . info ( "...lossless ▁ only!" ) <newline> <dedent> else : <newline> <indent> newValueDict = { 'Status' : 'Wanted' } <newline> <dedent> myDB . upsert ( "albums" , newValueDict , controlValueDict ) <newline> searcher . searchforalbum ( AlbumID , new ) <newline> if ArtistID : <newline> <indent> redirect = "artistPage?ArtistID=%s" % ArtistID <newline> <dedent> raise cherrypy . HTTPRedirect ( redirect ) <newline> <dedent> @ cherrypy . expose <newline> def choose_specific_download ( self , AlbumID ) : <newline> <indent> results = searcher . searchforalbum ( AlbumID , choose_specific_download = True ) <newline> results_as_dicts = [ ] <newline> for result in results : <newline> <indent> result_dict = { 'title' : result [ 0 ] , 'size' : result [ 1 ] , 'url' : result [ 2 ] , 'provider' : result [ 3 ] , 'kind' : result [ 4 ] , 'matches' : result [ 5 ] } <newline> results_as_dicts . append ( result_dict ) <newline> <dedent> s = json . dumps ( results_as_dicts ) <newline> cherrypy . response . headers [ 'Content-type' ] = 'application/json' <newline> return s <newline> <dedent> @ cherrypy . expose <newline> def download_specific_release ( self , AlbumID , title , size , url , provider , kind , ** kwargs ) : <newline>  # ▁ Handle ▁ situations ▁ where ▁ the ▁ torrent ▁ url ▁ contains ▁ arguments ▁ that ▁ are ▁ parsed <encdom> <indent> if kwargs : <newline> <indent> url = urllib2 . quote ( url , safe = ":?/=&" ) + '&' + urllib . urlencode ( kwargs ) <newline> <dedent> try : <newline> <indent> result = [ ( title , int ( size ) , url , provider , kind ) ] <newline> <dedent> except ValueError : <newline> <indent> result = [ ( title , float ( size ) , url , provider , kind ) ] <newline> <dedent> logger . info ( u"Making ▁ sure ▁ we ▁ can ▁ download ▁ the ▁ chosen ▁ result" ) <newline> ( data , bestqual ) = searcher . preprocess ( result ) <newline> if data and bestqual : <newline> <indent> myDB = db . DBConnection ( ) <newline> album = myDB . action ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) . fetchone ( ) <newline> searcher . send_to_downloader ( data , bestqual , album ) <newline> return json . dumps ( { 'result' : 'success' } ) <newline> <dedent> else : <newline> <indent> return json . dumps ( { 'result' : 'failure' } ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def unqueueAlbum ( self , AlbumID , ArtistID ) : <newline> <indent> logger . info ( u"Marking ▁ album: ▁ " + AlbumID + "as ▁ skipped..." ) <newline> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'AlbumID' : AlbumID } <newline> newValueDict = { 'Status' : 'Skipped' } <newline> myDB . upsert ( "albums" , newValueDict , controlValueDict ) <newline> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> @ cherrypy . expose <newline> def deleteAlbum ( self , AlbumID , ArtistID = None ) : <newline> <indent> logger . info ( u"Deleting ▁ all ▁ traces ▁ of ▁ album: ▁ " + AlbumID ) <newline> myDB = db . DBConnection ( ) <newline> myDB . action ( 'DELETE ▁ from ▁ have ▁ WHERE ▁ Matched=?' , [ AlbumID ] ) <newline> album = myDB . action ( 'SELECT ▁ ArtistID, ▁ ArtistName, ▁ AlbumTitle ▁ from ▁ albums ▁ where ▁ AlbumID=?' , [ AlbumID ] ) . fetchone ( ) <newline> if album : <newline> <indent> ArtistID = album [ 'ArtistID' ] <newline> myDB . action ( 'DELETE ▁ from ▁ have ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=?' , [ album [ 'ArtistName' ] , album [ 'AlbumTitle' ] ] ) <newline> <dedent> myDB . action ( 'DELETE ▁ from ▁ albums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ tracks ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ allalbums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ alltracks ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ releases ▁ WHERE ▁ ReleaseGroupID=?' , [ AlbumID ] ) <newline> myDB . action ( 'DELETE ▁ from ▁ descriptions ▁ WHERE ▁ ReleaseGroupID=?' , [ AlbumID ] ) <newline> from headphones import cache <newline> c = cache . Cache ( ) <newline> c . remove_from_cache ( AlbumID = AlbumID ) <newline> if ArtistID : <newline> <indent> raise cherrypy . HTTPRedirect ( "artistPage?ArtistID=%s" % ArtistID ) <newline> <dedent> else : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def switchAlbum ( self , AlbumID , ReleaseID ) : <newline> <indent>  """ <strnewline> ▁ Take ▁ the ▁ values ▁ from ▁ allalbums/alltracks ▁ (based ▁ on ▁ the ▁ ReleaseID) ▁ and <strnewline> ▁ swap ▁ it ▁ into ▁ the ▁ album ▁ & ▁ track ▁ tables <strnewline> ▁ """  <newline> from headphones import albumswitcher <newline> albumswitcher . switch ( AlbumID , ReleaseID ) <newline> raise cherrypy . HTTPRedirect ( "albumPage?AlbumID=%s" % AlbumID ) <newline> <dedent> @ cherrypy . expose <newline> def editSearchTerm ( self , AlbumID , SearchTerm ) : <newline> <indent> logger . info ( u"Updating ▁ search ▁ term ▁ for ▁ albumid: ▁ " + AlbumID ) <newline> myDB = db . DBConnection ( ) <newline> controlValueDict = { 'AlbumID' : AlbumID } <newline> newValueDict = { 'SearchTerm' : SearchTerm } <newline> myDB . upsert ( "albums" , newValueDict , controlValueDict ) <newline> raise cherrypy . HTTPRedirect ( "albumPage?AlbumID=%s" % AlbumID ) <newline> <dedent> @ cherrypy . expose <newline> def upcoming ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> upcoming = myDB . select ( "SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ ReleaseDate ▁ > ▁ date('now') ▁ order ▁ by ▁ ReleaseDate ▁ ASC" ) <newline> wanted = myDB . select ( "SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ Status='Wanted'" ) <newline> return serve_template ( templatename = "upcoming.html" , title = "Upcoming" , upcoming = upcoming , wanted = wanted ) <newline> <dedent> @ cherrypy . expose <newline> def manage ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> emptyArtists = myDB . select ( "SELECT ▁ * ▁ FROM ▁ artists ▁ WHERE ▁ LatestAlbum ▁ IS ▁ NULL" ) <newline> return serve_template ( templatename = "manage.html" , title = "Manage" , emptyArtists = emptyArtists ) <newline> <dedent> @ cherrypy . expose <newline> def manageArtists ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artists = myDB . select ( 'SELECT ▁ * ▁ from ▁ artists ▁ order ▁ by ▁ ArtistSortName ▁ COLLATE ▁ NOCASE' ) <newline> return serve_template ( templatename = "manageartists.html" , title = "Manage ▁ Artists" , artists = artists ) <newline> <dedent> @ cherrypy . expose <newline> def manageAlbums ( self , Status = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> if Status == "Upcoming" : <newline> <indent> albums = myDB . select ( "SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ ReleaseDate ▁ > ▁ date('now')" ) <newline> <dedent> elif Status : <newline> <indent> albums = myDB . select ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ Status=?' , [ Status ] ) <newline> <dedent> else : <newline> <indent> albums = myDB . select ( 'SELECT ▁ * ▁ from ▁ albums' ) <newline> <dedent> return serve_template ( templatename = "managealbums.html" , title = "Manage ▁ Albums" , albums = albums ) <newline> <dedent> @ cherrypy . expose <newline> def manageNew ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> newartists = myDB . select ( 'SELECT ▁ * ▁ from ▁ newartists' ) <newline> return serve_template ( templatename = "managenew.html" , title = "Manage ▁ New ▁ Artists" , newartists = newartists ) <newline> <dedent> @ cherrypy . expose <newline> def manageUnmatched ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> have_album_dictionary = [ ] <newline> headphones_album_dictionary = [ ] <newline> have_albums = myDB . select ( 'SELECT ▁ ArtistName, ▁ AlbumTitle, ▁ TrackTitle, ▁ CleanName ▁ from ▁ have ▁ WHERE ▁ Matched ▁ = ▁"Failed" ▁ GROUP ▁ BY ▁ AlbumTitle ▁ ORDER ▁ BY ▁ ArtistName' ) <newline> for albums in have_albums : <newline>  # Have ▁ to ▁ skip ▁ over ▁ manually ▁ matched ▁ tracks <encdom> <indent> if albums [ 'ArtistName' ] and albums [ 'AlbumTitle' ] and albums [ 'TrackTitle' ] : <newline> <indent> original_clean = helpers . cleanName ( albums [ 'ArtistName' ] + " ▁ " + albums [ 'AlbumTitle' ] + " ▁ " + albums [ 'TrackTitle' ] ) <newline>  # ▁ else: <encdom>  # ▁ original_clean ▁ = ▁ None <encdom> if original_clean == albums [ 'CleanName' ] : <newline> <indent> have_dict = { 'ArtistName' : albums [ 'ArtistName' ] , 'AlbumTitle' : albums [ 'AlbumTitle' ] } <newline> have_album_dictionary . append ( have_dict ) <newline> <dedent> <dedent> <dedent> headphones_albums = myDB . select ( 'SELECT ▁ ArtistName, ▁ AlbumTitle ▁ from ▁ albums ▁ ORDER ▁ BY ▁ ArtistName' ) <newline> for albums in headphones_albums : <newline> <indent> if albums [ 'ArtistName' ] and albums [ 'AlbumTitle' ] : <newline> <indent> headphones_dict = { 'ArtistName' : albums [ 'ArtistName' ] , 'AlbumTitle' : albums [ 'AlbumTitle' ] } <newline> headphones_album_dictionary . append ( headphones_dict ) <newline>  # unmatchedalbums ▁ = ▁ [f ▁ for ▁ f ▁ in ▁ have_album_dictionary ▁ if ▁ f ▁ not ▁ in ▁ [x ▁ for ▁ x ▁ in ▁ headphones_album_dictionary]] <encdom> <dedent> <dedent> check = set ( [ ( cleanName ( d [ 'ArtistName' ] ) . lower ( ) , cleanName ( d [ 'AlbumTitle' ] ) . lower ( ) ) for d in headphones_album_dictionary ] ) <newline> unmatchedalbums = [ d for d in have_album_dictionary if ( cleanName ( d [ 'ArtistName' ] ) . lower ( ) , cleanName ( d [ 'AlbumTitle' ] ) . lower ( ) ) not in check ] <newline> return serve_template ( templatename = "manageunmatched.html" , title = "Manage ▁ Unmatched ▁ Items" , unmatchedalbums = unmatchedalbums ) <newline> <dedent> @ cherrypy . expose <newline> def markUnmatched ( self , action = None , existing_artist = None , existing_album = None , new_artist = None , new_album = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> if action == "ignoreArtist" : <newline> <indent> artist = existing_artist <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Ignored" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ Matched ▁ = ▁"Failed"' , [ artist ] ) <newline> <dedent> elif action == "ignoreAlbum" : <newline> <indent> artist = existing_artist <newline> album = existing_album <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Ignored" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=? ▁ AND ▁ Matched ▁ = ▁"Failed"' , ( artist , album ) ) <newline> <dedent> elif action == "matchArtist" : <newline> <indent> existing_artist_clean = helpers . cleanName ( existing_artist ) . lower ( ) <newline> new_artist_clean = helpers . cleanName ( new_artist ) . lower ( ) <newline> if new_artist_clean != existing_artist_clean : <newline> <indent> have_tracks = myDB . action ( 'SELECT ▁ Matched, ▁ CleanName, ▁ Location, ▁ BitRate, ▁ Format ▁ FROM ▁ have ▁ WHERE ▁ ArtistName=?' , [ existing_artist ] ) <newline> update_count = 0 <newline> for entry in have_tracks : <newline> <indent> old_clean_filename = entry [ 'CleanName' ] <newline> if old_clean_filename . startswith ( existing_artist_clean ) : <newline> <indent> new_clean_filename = old_clean_filename . replace ( existing_artist_clean , new_artist_clean , 1 ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ CleanName=? ▁ WHERE ▁ ArtistName=? ▁ AND ▁ CleanName=?' , [ new_clean_filename , existing_artist , old_clean_filename ] ) <newline> controlValueDict = { "CleanName" : new_clean_filename } <newline> newValueDict = { "Location" : entry [ 'Location' ] , "BitRate" : entry [ 'BitRate' ] , "Format" : entry [ 'Format' ] } <newline>  # Attempt ▁ to ▁ match ▁ tracks ▁ with ▁ new ▁ CleanName <encdom> match_alltracks = myDB . action ( 'SELECT ▁ CleanName ▁ from ▁ alltracks ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) . fetchone ( ) <newline> if match_alltracks : <newline> <indent> myDB . upsert ( "alltracks" , newValueDict , controlValueDict ) <newline> <dedent> match_tracks = myDB . action ( 'SELECT ▁ CleanName, ▁ AlbumID ▁ from ▁ tracks ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) . fetchone ( ) <newline> if match_tracks : <newline> <indent> myDB . upsert ( "tracks" , newValueDict , controlValueDict ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Manual" ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) <newline> update_count += 1 <newline>  # This ▁ was ▁ throwing ▁ errors ▁ and ▁ I ▁ don't ▁ know ▁ why, ▁ but ▁ it ▁ seems ▁ to ▁ be ▁ working ▁ fine. <encdom>  # else: <encdom>  # logger.info("There ▁ was ▁ an ▁ error ▁ modifying ▁ Artist ▁ %s. ▁ This ▁ should ▁ not ▁ have ▁ happened" ▁ % ▁ existing_artist) <encdom> <dedent> <dedent> <dedent> logger . info ( "Manual ▁ matching ▁ yielded ▁ %s ▁ new ▁ matches ▁ for ▁ Artist: ▁ %s" % ( update_count , new_artist ) ) <newline> if update_count > 0 : <newline> <indent> librarysync . update_album_status ( ) <newline> <dedent> <dedent> else : <newline> <indent> logger . info ( "Artist ▁ %s ▁ already ▁ named ▁ appropriately; ▁ nothing ▁ to ▁ modify" % existing_artist ) <newline> <dedent> <dedent> elif action == "matchAlbum" : <newline> <indent> existing_artist_clean = helpers . cleanName ( existing_artist ) . lower ( ) <newline> new_artist_clean = helpers . cleanName ( new_artist ) . lower ( ) <newline> existing_album_clean = helpers . cleanName ( existing_album ) . lower ( ) <newline> new_album_clean = helpers . cleanName ( new_album ) . lower ( ) <newline> existing_clean_string = existing_artist_clean + " ▁ " + existing_album_clean <newline> new_clean_string = new_artist_clean + " ▁ " + new_album_clean <newline> if existing_clean_string != new_clean_string : <newline> <indent> have_tracks = myDB . action ( 'SELECT ▁ Matched, ▁ CleanName, ▁ Location, ▁ BitRate, ▁ Format ▁ FROM ▁ have ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=?' , ( existing_artist , existing_album ) ) <newline> update_count = 0 <newline> for entry in have_tracks : <newline> <indent> old_clean_filename = entry [ 'CleanName' ] <newline> if old_clean_filename . startswith ( existing_clean_string ) : <newline> <indent> new_clean_filename = old_clean_filename . replace ( existing_clean_string , new_clean_string , 1 ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ CleanName=? ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=? ▁ AND ▁ CleanName=?' , [ new_clean_filename , existing_artist , existing_album , old_clean_filename ] ) <newline> controlValueDict = { "CleanName" : new_clean_filename } <newline> newValueDict = { "Location" : entry [ 'Location' ] , "BitRate" : entry [ 'BitRate' ] , "Format" : entry [ 'Format' ] } <newline>  # Attempt ▁ to ▁ match ▁ tracks ▁ with ▁ new ▁ CleanName <encdom> match_alltracks = myDB . action ( 'SELECT ▁ CleanName ▁ from ▁ alltracks ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) . fetchone ( ) <newline> if match_alltracks : <newline> <indent> myDB . upsert ( "alltracks" , newValueDict , controlValueDict ) <newline> <dedent> match_tracks = myDB . action ( 'SELECT ▁ CleanName, ▁ AlbumID ▁ from ▁ tracks ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) . fetchone ( ) <newline> if match_tracks : <newline> <indent> myDB . upsert ( "tracks" , newValueDict , controlValueDict ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Manual" ▁ WHERE ▁ CleanName=?' , [ new_clean_filename ] ) <newline> album_id = match_tracks [ 'AlbumID' ] <newline> update_count += 1 <newline>  # This ▁ was ▁ throwing ▁ errors ▁ and ▁ I ▁ don't ▁ know ▁ why, ▁ but ▁ it ▁ seems ▁ to ▁ be ▁ working ▁ fine. <encdom>  # else: <encdom>  # logger.info("There ▁ was ▁ an ▁ error ▁ modifying ▁ Artist ▁ %s ▁ / ▁ Album ▁ %s ▁ with ▁ clean ▁ name ▁ %s" ▁ % ▁ (existing_artist, ▁ existing_album, ▁ existing_clean_string)) <encdom> <dedent> <dedent> <dedent> logger . info ( "Manual ▁ matching ▁ yielded ▁ %s ▁ new ▁ matches ▁ for ▁ Artist: ▁ %s ▁ / ▁ Album: ▁ %s" % ( update_count , new_artist , new_album ) ) <newline> if update_count > 0 : <newline> <indent> librarysync . update_album_status ( album_id ) <newline> <dedent> <dedent> else : <newline> <indent> logger . info ( "Artist ▁ %s ▁ / ▁ Album ▁ %s ▁ already ▁ named ▁ appropriately; ▁ nothing ▁ to ▁ modify" % ( existing_artist , existing_album ) ) <newline> <dedent> <dedent> <dedent> @ cherrypy . expose <newline> def manageManual ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> manual_albums = [ ] <newline> manualalbums = myDB . select ( 'SELECT ▁ ArtistName, ▁ AlbumTitle, ▁ TrackTitle, ▁ CleanName, ▁ Matched ▁ from ▁ have' ) <newline> for albums in manualalbums : <newline> <indent> if albums [ 'ArtistName' ] and albums [ 'AlbumTitle' ] and albums [ 'TrackTitle' ] : <newline> <indent> original_clean = helpers . cleanName ( albums [ 'ArtistName' ] + " ▁ " + albums [ 'AlbumTitle' ] + " ▁ " + albums [ 'TrackTitle' ] ) <newline> if albums [ 'Matched' ] == "Ignored" or albums [ 'Matched' ] == "Manual" or albums [ 'CleanName' ] != original_clean : <newline> <indent> if albums [ 'Matched' ] == "Ignored" : <newline> <indent> album_status = "Ignored" <newline> <dedent> elif albums [ 'Matched' ] == "Manual" or albums [ 'CleanName' ] != original_clean : <newline> <indent> album_status = "Matched" <newline> <dedent> manual_dict = { 'ArtistName' : albums [ 'ArtistName' ] , 'AlbumTitle' : albums [ 'AlbumTitle' ] , 'AlbumStatus' : album_status } <newline> if manual_dict not in manual_albums : <newline> <indent> manual_albums . append ( manual_dict ) <newline> <dedent> <dedent> <dedent> <dedent> manual_albums_sorted = sorted ( manual_albums , key = itemgetter ( 'ArtistName' , 'AlbumTitle' ) ) <newline> return serve_template ( templatename = "managemanual.html" , title = "Manage ▁ Manual ▁ Items" , manualalbums = manual_albums_sorted ) <newline> <dedent> @ cherrypy . expose <newline> def markManual ( self , action = None , existing_artist = None , existing_album = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> if action == "unignoreArtist" : <newline> <indent> artist = existing_artist <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Failed" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ Matched="Ignored"' , [ artist ] ) <newline> logger . info ( "Artist: ▁ %s ▁ successfully ▁ restored ▁ to ▁ unmatched ▁ list" % artist ) <newline> <dedent> elif action == "unignoreAlbum" : <newline> <indent> artist = existing_artist <newline> album = existing_album <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ Matched="Failed" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=? ▁ AND ▁ Matched="Ignored"' , ( artist , album ) ) <newline> logger . info ( "Album: ▁ %s ▁ successfully ▁ restored ▁ to ▁ unmatched ▁ list" % album ) <newline> <dedent> elif action == "unmatchArtist" : <newline> <indent> artist = existing_artist <newline> update_clean = myDB . select ( 'SELECT ▁ ArtistName, ▁ AlbumTitle, ▁ TrackTitle, ▁ CleanName, ▁ Matched ▁ from ▁ have ▁ WHERE ▁ ArtistName=?' , [ artist ] ) <newline> update_count = 0 <newline> for tracks in update_clean : <newline> <indent> original_clean = helpers . cleanName ( tracks [ 'ArtistName' ] + " ▁ " + tracks [ 'AlbumTitle' ] + " ▁ " + tracks [ 'TrackTitle' ] ) . lower ( ) <newline> album = tracks [ 'AlbumTitle' ] <newline> track_title = tracks [ 'TrackTitle' ] <newline> if tracks [ 'CleanName' ] != original_clean : <newline> <indent> myDB . action ( 'UPDATE ▁ tracks ▁ SET ▁ Location=?, ▁ BitRate=?, ▁ Format=? ▁ WHERE ▁ CleanName=?' , [ None , None , None , tracks [ 'CleanName' ] ] ) <newline> myDB . action ( 'UPDATE ▁ alltracks ▁ SET ▁ Location=?, ▁ BitRate=?, ▁ Format=? ▁ WHERE ▁ CleanName=?' , [ None , None , None , tracks [ 'CleanName' ] ] ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ CleanName=?, ▁ Matched="Failed" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=? ▁ AND ▁ TrackTitle=?' , ( original_clean , artist , album , track_title ) ) <newline> update_count += 1 <newline> <dedent> <dedent> if update_count > 0 : <newline> <indent> librarysync . update_album_status ( ) <newline> <dedent> logger . info ( "Artist: ▁ %s ▁ successfully ▁ restored ▁ to ▁ unmatched ▁ list" % artist ) <newline> <dedent> elif action == "unmatchAlbum" : <newline> <indent> artist = existing_artist <newline> album = existing_album <newline> update_clean = myDB . select ( 'SELECT ▁ ArtistName, ▁ AlbumTitle, ▁ TrackTitle, ▁ CleanName, ▁ Matched ▁ from ▁ have ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=?' , ( artist , album ) ) <newline> update_count = 0 <newline> for tracks in update_clean : <newline> <indent> original_clean = helpers . cleanName ( tracks [ 'ArtistName' ] + " ▁ " + tracks [ 'AlbumTitle' ] + " ▁ " + tracks [ 'TrackTitle' ] ) . lower ( ) <newline> track_title = tracks [ 'TrackTitle' ] <newline> if tracks [ 'CleanName' ] != original_clean : <newline> <indent> album_id_check = myDB . action ( 'SELECT ▁ AlbumID ▁ from ▁ tracks ▁ WHERE ▁ CleanName=?' , [ tracks [ 'CleanName' ] ] ) . fetchone ( ) <newline> if album_id_check : <newline> <indent> album_id = album_id_check [ 0 ] <newline> <dedent> myDB . action ( 'UPDATE ▁ tracks ▁ SET ▁ Location=?, ▁ BitRate=?, ▁ Format=? ▁ WHERE ▁ CleanName=?' , [ None , None , None , tracks [ 'CleanName' ] ] ) <newline> myDB . action ( 'UPDATE ▁ alltracks ▁ SET ▁ Location=?, ▁ BitRate=?, ▁ Format=? ▁ WHERE ▁ CleanName=?' , [ None , None , None , tracks [ 'CleanName' ] ] ) <newline> myDB . action ( 'UPDATE ▁ have ▁ SET ▁ CleanName=?, ▁ Matched="Failed" ▁ WHERE ▁ ArtistName=? ▁ AND ▁ AlbumTitle=? ▁ AND ▁ TrackTitle=?' , ( original_clean , artist , album , track_title ) ) <newline> update_count += 1 <newline> <dedent> <dedent> if update_count > 0 : <newline> <indent> librarysync . update_album_status ( album_id ) <newline> <dedent> logger . info ( "Album: ▁ %s ▁ successfully ▁ restored ▁ to ▁ unmatched ▁ list" % album ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def markArtists ( self , action = None , ** args ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artistsToAdd = [ ] <newline> for ArtistID in args : <newline> <indent> if action == 'delete' : <newline> <indent> self . removeArtist ( ArtistID ) <newline> <dedent> elif action == 'pause' : <newline> <indent> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'Status' : 'Paused' } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> <dedent> elif action == 'resume' : <newline> <indent> controlValueDict = { 'ArtistID' : ArtistID } <newline> newValueDict = { 'Status' : 'Active' } <newline> myDB . upsert ( "artists" , newValueDict , controlValueDict ) <newline> <dedent> else : <newline> <indent> artistsToAdd . append ( ArtistID ) <newline> <dedent> <dedent> if len ( artistsToAdd ) > 0 : <newline> <indent> logger . debug ( "Refreshing ▁ artists: ▁ %s" % artistsToAdd ) <newline> threading . Thread ( target = importer . addArtistIDListToDB , args = [ artistsToAdd ] ) . start ( ) <newline> <dedent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def importLastFM ( self , username ) : <newline> <indent> headphones . CONFIG . LASTFM_USERNAME = username <newline> headphones . CONFIG . write ( ) <newline> threading . Thread ( target = lastfm . getArtists ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def importLastFMTag ( self , tag , limit ) : <newline> <indent> threading . Thread ( target = lastfm . getTagTopArtists , args = ( tag , limit ) ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def importItunes ( self , path ) : <newline> <indent> headphones . CONFIG . PATH_TO_XML = path <newline> headphones . CONFIG . write ( ) <newline> thread = threading . Thread ( target = importer . itunesImport , args = [ path ] ) <newline> thread . start ( ) <newline> thread . join ( 10 ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def musicScan ( self , path , scan = 0 , redirect = None , autoadd = 0 , libraryscan = 0 ) : <newline> <indent> headphones . CONFIG . LIBRARYSCAN = libraryscan <newline> headphones . CONFIG . AUTO_ADD_ARTISTS = autoadd <newline> headphones . CONFIG . MUSIC_DIR = path <newline> headphones . CONFIG . write ( ) <newline> if scan : <newline> <indent> try : <newline> <indent> threading . Thread ( target = librarysync . libraryScan ) . start ( ) <newline> <dedent> except Exception as e : <newline> <indent> logger . error ( 'Unable ▁ to ▁ complete ▁ the ▁ scan: ▁ %s' % e ) <newline> <dedent> <dedent> if redirect : <newline> <indent> raise cherrypy . HTTPRedirect ( redirect ) <newline> <dedent> else : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def forceUpdate ( self ) : <newline> <indent> from headphones import updater <newline> threading . Thread ( target = updater . dbUpdate , args = [ False ] ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def forceFullUpdate ( self ) : <newline> <indent> from headphones import updater <newline> threading . Thread ( target = updater . dbUpdate , args = [ True ] ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def forceSearch ( self ) : <newline> <indent> from headphones import searcher <newline> threading . Thread ( target = searcher . searchforalbum ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def forcePostProcess ( self , dir = None , album_dir = None , keep_original_folder = False ) : <newline> <indent> from headphones import postprocessor <newline> threading . Thread ( target = postprocessor . forcePostProcess , kwargs = { 'dir' : dir , 'album_dir' : album_dir , 'keep_original_folder' : keep_original_folder == 'True' } ) . start ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def checkGithub ( self ) : <newline> <indent> from headphones import versioncheck <newline> versioncheck . checkGithub ( ) <newline> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def history ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> history = myDB . select (  ''' SELECT ▁ * ▁ from ▁ snatched ▁ WHERE ▁ Status ▁ NOT ▁ LIKE ▁"Seed%" ▁ order ▁ by ▁ DateAdded ▁ DESC '''  ) <newline> return serve_template ( templatename = "history.html" , title = "History" , history = history ) <newline> <dedent> @ cherrypy . expose <newline> def logs ( self ) : <newline> <indent> return serve_template ( templatename = "logs.html" , title = "Log" , lineList = headphones . LOG_LIST ) <newline> <dedent> @ cherrypy . expose <newline> def clearLogs ( self ) : <newline> <indent> headphones . LOG_LIST = [ ] <newline> logger . info ( "Web ▁ logs ▁ cleared" ) <newline> raise cherrypy . HTTPRedirect ( "logs" ) <newline> <dedent> @ cherrypy . expose <newline> def toggleVerbose ( self ) : <newline> <indent> headphones . VERBOSE = not headphones . VERBOSE <newline> logger . initLogger ( console = not headphones . QUIET , log_dir = headphones . CONFIG . LOG_DIR , verbose = headphones . VERBOSE ) <newline> logger . info ( "Verbose ▁ toggled, ▁ set ▁ to ▁ %s" , headphones . VERBOSE ) <newline> logger . debug ( "If ▁ you ▁ read ▁ this ▁ message, ▁ debug ▁ logging ▁ is ▁ available" ) <newline> raise cherrypy . HTTPRedirect ( "logs" ) <newline> <dedent> @ cherrypy . expose <newline> def getLog ( self , iDisplayStart = 0 , iDisplayLength = 100 , iSortCol_0 = 0 , sSortDir_0 = "desc" , sSearch = "" , ** kwargs ) : <newline> <indent> iDisplayStart = int ( iDisplayStart ) <newline> iDisplayLength = int ( iDisplayLength ) <newline> filtered = [ ] <newline> if sSearch == "" : <newline> <indent> filtered = headphones . LOG_LIST [ : : ] <newline> <dedent> else : <newline> <indent> filtered = [ row for row in headphones . LOG_LIST for column in row if sSearch . lower ( ) in column . lower ( ) ] <newline> <dedent> sortcolumn = 0 <newline> if iSortCol_0 == '1' : <newline> <indent> sortcolumn = 2 <newline> <dedent> elif iSortCol_0 == '2' : <newline> <indent> sortcolumn = 1 <newline> <dedent> filtered . sort ( key = lambda x : x [ sortcolumn ] , reverse = sSortDir_0 == "desc" ) <newline> rows = filtered [ iDisplayStart : ( iDisplayStart + iDisplayLength ) ] <newline> rows = [ [ row [ 0 ] , row [ 2 ] , row [ 1 ] ] for row in rows ] <newline> return json . dumps ( { 'iTotalDisplayRecords' : len ( filtered ) , 'iTotalRecords' : len ( headphones . LOG_LIST ) , 'aaData' : rows , } ) <newline> <dedent> @ cherrypy . expose <newline> def getArtists_json ( self , iDisplayStart = 0 , iDisplayLength = 100 , sSearch = "" , iSortCol_0 = '0' , sSortDir_0 = 'asc' , ** kwargs ) : <newline> <indent> iDisplayStart = int ( iDisplayStart ) <newline> iDisplayLength = int ( iDisplayLength ) <newline> filtered = [ ] <newline> totalcount = 0 <newline> myDB = db . DBConnection ( ) <newline> sortcolumn = 'ArtistSortName' <newline> sortbyhavepercent = False <newline> if iSortCol_0 == '2' : <newline> <indent> sortcolumn = 'Status' <newline> <dedent> elif iSortCol_0 == '3' : <newline> <indent> sortcolumn = 'ReleaseDate' <newline> <dedent> elif iSortCol_0 == '4' : <newline> <indent> sortbyhavepercent = True <newline> <dedent> if sSearch == "" : <newline> <indent> query = 'SELECT ▁ * ▁ from ▁ artists ▁ order ▁ by ▁ %s ▁ COLLATE ▁ NOCASE ▁ %s' % ( sortcolumn , sSortDir_0 ) <newline> filtered = myDB . select ( query ) <newline> totalcount = len ( filtered ) <newline> <dedent> else : <newline> <indent> query = 'SELECT ▁ * ▁ from ▁ artists ▁ WHERE ▁ ArtistSortName ▁ LIKE ▁ "%' + sSearch + '%" ▁ OR ▁ LatestAlbum ▁ LIKE ▁ "%' + sSearch + '%"' + 'ORDER ▁ BY ▁ %s ▁ COLLATE ▁ NOCASE ▁ %s' % ( sortcolumn , sSortDir_0 ) <newline> filtered = myDB . select ( query ) <newline> totalcount = myDB . select ( 'SELECT ▁ COUNT(*) ▁ from ▁ artists' ) [ 0 ] [ 0 ] <newline> <dedent> if sortbyhavepercent : <newline> <indent> filtered . sort ( key = lambda x : ( float ( x [ 'HaveTracks' ] ) / x [ 'TotalTracks' ] if x [ 'TotalTracks' ] > 0 else 0.0 , x [ 'HaveTracks' ] if x [ 'HaveTracks' ] else 0.0 ) , reverse = sSortDir_0 == "asc" ) <newline>  # can't ▁ figure ▁ out ▁ how ▁ to ▁ change ▁ the ▁ datatables ▁ default ▁ sorting ▁ order ▁ when ▁ its ▁ using ▁ an ▁ ajax ▁ datasource ▁ so ▁ ill <encdom>  # just ▁ reverse ▁ it ▁ here ▁ and ▁ the ▁ first ▁ click ▁ on ▁ the ▁"Latest ▁ Album" ▁ header ▁ will ▁ sort ▁ by ▁ descending ▁ release ▁ date <encdom> <dedent> if sortcolumn == 'ReleaseDate' : <newline> <indent> filtered . reverse ( ) <newline> <dedent> artists = filtered [ iDisplayStart : ( iDisplayStart + iDisplayLength ) ] <newline> rows = [ ] <newline> for artist in artists : <newline> <indent> row = { "ArtistID" : artist [ 'ArtistID' ] , "ArtistName" : artist [ "ArtistName" ] , "ArtistSortName" : artist [ "ArtistSortName" ] , "Status" : artist [ "Status" ] , "TotalTracks" : artist [ "TotalTracks" ] , "HaveTracks" : artist [ "HaveTracks" ] , "LatestAlbum" : "" , "ReleaseDate" : "" , "ReleaseInFuture" : "False" , "AlbumID" : "" , } <newline> if not row [ 'HaveTracks' ] : <newline> <indent> row [ 'HaveTracks' ] = 0 <newline> <dedent> if artist [ 'ReleaseDate' ] and artist [ 'LatestAlbum' ] : <newline> <indent> row [ 'ReleaseDate' ] = artist [ 'ReleaseDate' ] <newline> row [ 'LatestAlbum' ] = artist [ 'LatestAlbum' ] <newline> row [ 'AlbumID' ] = artist [ 'AlbumID' ] <newline> if artist [ 'ReleaseDate' ] > today ( ) : <newline> <indent> row [ 'ReleaseInFuture' ] = "True" <newline> <dedent> <dedent> elif artist [ 'LatestAlbum' ] : <newline> <indent> row [ 'ReleaseDate' ] = '' <newline> row [ 'LatestAlbum' ] = artist [ 'LatestAlbum' ] <newline> row [ 'AlbumID' ] = artist [ 'AlbumID' ] <newline> <dedent> rows . append ( row ) <newline> <dedent> dict = { 'iTotalDisplayRecords' : len ( filtered ) , 'iTotalRecords' : totalcount , 'aaData' : rows , } <newline> s = json . dumps ( dict ) <newline> cherrypy . response . headers [ 'Content-type' ] = 'application/json' <newline> return s <newline> <dedent> @ cherrypy . expose <newline> def getAlbumsByArtist_json ( self , artist = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> album_json = { } <newline> counter = 0 <newline> album_list = myDB . select ( "SELECT ▁ AlbumTitle ▁ from ▁ albums ▁ WHERE ▁ ArtistName=?" , [ artist ] ) <newline> for album in album_list : <newline> <indent> album_json [ counter ] = album [ 'AlbumTitle' ] <newline> counter += 1 <newline> <dedent> json_albums = json . dumps ( album_json ) <newline> cherrypy . response . headers [ 'Content-type' ] = 'application/json' <newline> return json_albums <newline> <dedent> @ cherrypy . expose <newline> def getArtistjson ( self , ArtistID , ** kwargs ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> artist = myDB . action ( 'SELECT ▁ * ▁ FROM ▁ artists ▁ WHERE ▁ ArtistID=?' , [ ArtistID ] ) . fetchone ( ) <newline> artist_json = json . dumps ( { 'ArtistName' : artist [ 'ArtistName' ] , 'Status' : artist [ 'Status' ] } ) <newline> return artist_json <newline> <dedent> @ cherrypy . expose <newline> def getAlbumjson ( self , AlbumID , ** kwargs ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> album = myDB . action ( 'SELECT ▁ * ▁ from ▁ albums ▁ WHERE ▁ AlbumID=?' , [ AlbumID ] ) . fetchone ( ) <newline> album_json = json . dumps ( { 'AlbumTitle' : album [ 'AlbumTitle' ] , 'ArtistName' : album [ 'ArtistName' ] , 'Status' : album [ 'Status' ] } ) <newline> return album_json <newline> <dedent> @ cherrypy . expose <newline> def clearhistory ( self , type = None , date_added = None , title = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> if type : <newline> <indent> if type == 'all' : <newline> <indent> logger . info ( u"Clearing ▁ all ▁ history" ) <newline> myDB . action ( 'DELETE ▁ from ▁ snatched ▁ WHERE ▁ Status ▁ NOT ▁ LIKE ▁"Seed%"' ) <newline> <dedent> else : <newline> <indent> logger . info ( u"Clearing ▁ history ▁ where ▁ status ▁ is ▁ %s" % type ) <newline> myDB . action ( 'DELETE ▁ from ▁ snatched ▁ WHERE ▁ Status=?' , [ type ] ) <newline> <dedent> <dedent> else : <newline> <indent> logger . info ( u"Deleting ▁'%s' ▁ from ▁ history" % title ) <newline> myDB . action ( 'DELETE ▁ from ▁ snatched ▁ WHERE ▁ Status ▁ NOT ▁ LIKE ▁"Seed%" ▁ AND ▁ Title=? ▁ AND ▁ DateAdded=?' , [ title , date_added ] ) <newline> <dedent> raise cherrypy . HTTPRedirect ( "history" ) <newline> <dedent> @ cherrypy . expose <newline> def generateAPI ( self ) : <newline> <indent> apikey = hashlib . sha224 ( str ( random . getrandbits ( 256 ) ) ) . hexdigest ( ) [ 0 : 32 ] <newline> logger . info ( "New ▁ API ▁ generated" ) <newline> return apikey <newline> <dedent> @ cherrypy . expose <newline> def forceScan ( self , keepmatched = None ) : <newline> <indent> myDB = db . DBConnection ( ) <newline>  # NEED ▁ TO ▁ MOVE ▁ THIS ▁ INTO ▁ A ▁ SEPARATE ▁ FUNCTION ▁ BEFORE ▁ RELEASE <encdom> myDB . select ( 'DELETE ▁ from ▁ Have' ) <newline> logger . info ( 'Removed ▁ all ▁ entries ▁ in ▁ local ▁ library ▁ database' ) <newline> myDB . select ( 'UPDATE ▁ alltracks ▁ SET ▁ Location=NULL, ▁ BitRate=NULL, ▁ Format=NULL' ) <newline> myDB . select ( 'UPDATE ▁ tracks ▁ SET ▁ Location=NULL, ▁ BitRate=NULL, ▁ Format=NULL' ) <newline> logger . info ( 'All ▁ tracks ▁ in ▁ library ▁ unmatched' ) <newline> myDB . action ( 'UPDATE ▁ artists ▁ SET ▁ HaveTracks=NULL' ) <newline> logger . info ( 'Reset ▁ track ▁ counts ▁ for ▁ all ▁ artists' ) <newline> myDB . action ( 'UPDATE ▁ albums ▁ SET ▁ Status="Skipped" ▁ WHERE ▁ Status="Skipped" ▁ OR ▁ Status="Downloaded"' ) <newline> logger . info ( 'Marking ▁ all ▁ unwanted ▁ albums ▁ as ▁ Skipped' ) <newline> try : <newline> <indent> threading . Thread ( target = librarysync . libraryScan ) . start ( ) <newline> <dedent> except Exception as e : <newline> <indent> logger . error ( 'Unable ▁ to ▁ complete ▁ the ▁ scan: ▁ %s' % e ) <newline> <dedent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> @ cherrypy . expose <newline> def config ( self ) : <newline> <indent> interface_dir = os . path . join ( headphones . PROG_DIR , 'data/interfaces/' ) <newline> interface_list = [ name for name in os . listdir ( interface_dir ) if os . path . isdir ( os . path . join ( interface_dir , name ) ) ] <newline> config = { "http_host" : headphones . CONFIG . HTTP_HOST , "http_username" : headphones . CONFIG . HTTP_USERNAME , "http_port" : headphones . CONFIG . HTTP_PORT , "http_password" : headphones . CONFIG . HTTP_PASSWORD , "launch_browser" : checked ( headphones . CONFIG . LAUNCH_BROWSER ) , "enable_https" : checked ( headphones . CONFIG . ENABLE_HTTPS ) , "https_cert" : headphones . CONFIG . HTTPS_CERT , "https_key" : headphones . CONFIG . HTTPS_KEY , "api_enabled" : checked ( headphones . CONFIG . API_ENABLED ) , "api_key" : headphones . CONFIG . API_KEY , "download_scan_interval" : headphones . CONFIG . DOWNLOAD_SCAN_INTERVAL , "update_db_interval" : headphones . CONFIG . UPDATE_DB_INTERVAL , "mb_ignore_age" : headphones . CONFIG . MB_IGNORE_AGE , "search_interval" : headphones . CONFIG . SEARCH_INTERVAL , "libraryscan_interval" : headphones . CONFIG . LIBRARYSCAN_INTERVAL , "sab_host" : headphones . CONFIG . SAB_HOST , "sab_username" : headphones . CONFIG . SAB_USERNAME , "sab_apikey" : headphones . CONFIG . SAB_APIKEY , "sab_password" : headphones . CONFIG . SAB_PASSWORD , "sab_category" : headphones . CONFIG . SAB_CATEGORY , "nzbget_host" : headphones . CONFIG . NZBGET_HOST , "nzbget_username" : headphones . CONFIG . NZBGET_USERNAME , "nzbget_password" : headphones . CONFIG . NZBGET_PASSWORD , "nzbget_category" : headphones . CONFIG . NZBGET_CATEGORY , "nzbget_priority" : headphones . CONFIG . NZBGET_PRIORITY , "transmission_host" : headphones . CONFIG . TRANSMISSION_HOST , "transmission_username" : headphones . CONFIG . TRANSMISSION_USERNAME , "transmission_password" : headphones . CONFIG . TRANSMISSION_PASSWORD , "utorrent_host" : headphones . CONFIG . UTORRENT_HOST , "utorrent_username" : headphones . CONFIG . UTORRENT_USERNAME , "utorrent_password" : headphones . CONFIG . UTORRENT_PASSWORD , "utorrent_label" : headphones . CONFIG . UTORRENT_LABEL , "nzb_downloader_sabnzbd" : radio ( headphones . CONFIG . NZB_DOWNLOADER , 0 ) , "nzb_downloader_nzbget" : radio ( headphones . CONFIG . NZB_DOWNLOADER , 1 ) , "nzb_downloader_blackhole" : radio ( headphones . CONFIG . NZB_DOWNLOADER , 2 ) , "torrent_downloader_blackhole" : radio ( headphones . CONFIG . TORRENT_DOWNLOADER , 0 ) , "torrent_downloader_transmission" : radio ( headphones . CONFIG . TORRENT_DOWNLOADER , 1 ) , "torrent_downloader_utorrent" : radio ( headphones . CONFIG . TORRENT_DOWNLOADER , 2 ) , "download_dir" : headphones . CONFIG . DOWNLOAD_DIR , "use_blackhole" : checked ( headphones . CONFIG . BLACKHOLE ) , "blackhole_dir" : headphones . CONFIG . BLACKHOLE_DIR , "usenet_retention" : headphones . CONFIG . USENET_RETENTION , "headphones_indexer" : checked ( headphones . CONFIG . HEADPHONES_INDEXER ) , "use_newznab" : checked ( headphones . CONFIG . NEWZNAB ) , "newznab_host" : headphones . CONFIG . NEWZNAB_HOST , "newznab_apikey" : headphones . CONFIG . NEWZNAB_APIKEY , "newznab_enabled" : checked ( headphones . CONFIG . NEWZNAB_ENABLED ) , "extra_newznabs" : headphones . CONFIG . get_extra_newznabs ( ) , "use_torznab" : checked ( headphones . CONFIG . TORZNAB ) , "torznab_host" : headphones . CONFIG . TORZNAB_HOST , "torznab_apikey" : headphones . CONFIG . TORZNAB_APIKEY , "torznab_enabled" : checked ( headphones . CONFIG . TORZNAB_ENABLED ) , "extra_torznabs" : headphones . CONFIG . get_extra_torznabs ( ) , "use_nzbsorg" : checked ( headphones . CONFIG . NZBSORG ) , "nzbsorg_uid" : headphones . CONFIG . NZBSORG_UID , "nzbsorg_hash" : headphones . CONFIG . NZBSORG_HASH , "use_omgwtfnzbs" : checked ( headphones . CONFIG . OMGWTFNZBS ) , "omgwtfnzbs_uid" : headphones . CONFIG . OMGWTFNZBS_UID , "omgwtfnzbs_apikey" : headphones . CONFIG . OMGWTFNZBS_APIKEY , "preferred_words" : headphones . CONFIG . PREFERRED_WORDS , "ignored_words" : headphones . CONFIG . IGNORED_WORDS , "required_words" : headphones . CONFIG . REQUIRED_WORDS , "ignore_clean_releases" : checked ( headphones . CONFIG . IGNORE_CLEAN_RELEASES ) , "torrentblackhole_dir" : headphones . CONFIG . TORRENTBLACKHOLE_DIR , "download_torrent_dir" : headphones . CONFIG . DOWNLOAD_TORRENT_DIR , "numberofseeders" : headphones . CONFIG . NUMBEROFSEEDERS , "use_kat" : checked ( headphones . CONFIG . KAT ) , "kat_proxy_url" : headphones . CONFIG . KAT_PROXY_URL , "kat_ratio" : headphones . CONFIG . KAT_RATIO , "use_piratebay" : checked ( headphones . CONFIG . PIRATEBAY ) , "piratebay_proxy_url" : headphones . CONFIG . PIRATEBAY_PROXY_URL , "piratebay_ratio" : headphones . CONFIG . PIRATEBAY_RATIO , "use_oldpiratebay" : checked ( headphones . CONFIG . OLDPIRATEBAY ) , "oldpiratebay_url" : headphones . CONFIG . OLDPIRATEBAY_URL , "oldpiratebay_ratio" : headphones . CONFIG . OLDPIRATEBAY_RATIO , "use_mininova" : checked ( headphones . CONFIG . MININOVA ) , "mininova_ratio" : headphones . CONFIG . MININOVA_RATIO , "use_waffles" : checked ( headphones . CONFIG . WAFFLES ) , "waffles_uid" : headphones . CONFIG . WAFFLES_UID , "waffles_passkey" : headphones . CONFIG . WAFFLES_PASSKEY , "waffles_ratio" : headphones . CONFIG . WAFFLES_RATIO , "use_rutracker" : checked ( headphones . CONFIG . RUTRACKER ) , "rutracker_user" : headphones . CONFIG . RUTRACKER_USER , "rutracker_password" : headphones . CONFIG . RUTRACKER_PASSWORD , "rutracker_ratio" : headphones . CONFIG . RUTRACKER_RATIO , "use_whatcd" : checked ( headphones . CONFIG . WHATCD ) , "whatcd_username" : headphones . CONFIG . WHATCD_USERNAME , "whatcd_password" : headphones . CONFIG . WHATCD_PASSWORD , "whatcd_ratio" : headphones . CONFIG . WHATCD_RATIO , "use_strike" : checked ( headphones . CONFIG . STRIKE ) , "strike_ratio" : headphones . CONFIG . STRIKE_RATIO , "pref_qual_0" : radio ( headphones . CONFIG . PREFERRED_QUALITY , 0 ) , "pref_qual_1" : radio ( headphones . CONFIG . PREFERRED_QUALITY , 1 ) , "pref_qual_2" : radio ( headphones . CONFIG . PREFERRED_QUALITY , 2 ) , "pref_qual_3" : radio ( headphones . CONFIG . PREFERRED_QUALITY , 3 ) , "preferred_bitrate" : headphones . CONFIG . PREFERRED_BITRATE , "preferred_bitrate_high" : headphones . CONFIG . PREFERRED_BITRATE_HIGH_BUFFER , "preferred_bitrate_low" : headphones . CONFIG . PREFERRED_BITRATE_LOW_BUFFER , "preferred_bitrate_allow_lossless" : checked ( headphones . CONFIG . PREFERRED_BITRATE_ALLOW_LOSSLESS ) , "detect_bitrate" : checked ( headphones . CONFIG . DETECT_BITRATE ) , "lossless_bitrate_from" : headphones . CONFIG . LOSSLESS_BITRATE_FROM , "lossless_bitrate_to" : headphones . CONFIG . LOSSLESS_BITRATE_TO , "freeze_db" : checked ( headphones . CONFIG . FREEZE_DB ) , "cue_split" : checked ( headphones . CONFIG . CUE_SPLIT ) , "cue_split_flac_path" : headphones . CONFIG . CUE_SPLIT_FLAC_PATH , "cue_split_shntool_path" : headphones . CONFIG . CUE_SPLIT_SHNTOOL_PATH , "move_files" : checked ( headphones . CONFIG . MOVE_FILES ) , "rename_files" : checked ( headphones . CONFIG . RENAME_FILES ) , "correct_metadata" : checked ( headphones . CONFIG . CORRECT_METADATA ) , "cleanup_files" : checked ( headphones . CONFIG . CLEANUP_FILES ) , "keep_nfo" : checked ( headphones . CONFIG . KEEP_NFO ) , "add_album_art" : checked ( headphones . CONFIG . ADD_ALBUM_ART ) , "album_art_format" : headphones . CONFIG . ALBUM_ART_FORMAT , "embed_album_art" : checked ( headphones . CONFIG . EMBED_ALBUM_ART ) , "embed_lyrics" : checked ( headphones . CONFIG . EMBED_LYRICS ) , "replace_existing_folders" : checked ( headphones . CONFIG . REPLACE_EXISTING_FOLDERS ) , "keep_original_folder" : checked ( headphones . CONFIG . KEEP_ORIGINAL_FOLDER ) , "destination_dir" : headphones . CONFIG . DESTINATION_DIR , "lossless_destination_dir" : headphones . CONFIG . LOSSLESS_DESTINATION_DIR , "folder_format" : headphones . CONFIG . FOLDER_FORMAT , "file_format" : headphones . CONFIG . FILE_FORMAT , "file_underscores" : checked ( headphones . CONFIG . FILE_UNDERSCORES ) , "include_extras" : checked ( headphones . CONFIG . INCLUDE_EXTRAS ) , "official_releases_only" : checked ( headphones . CONFIG . OFFICIAL_RELEASES_ONLY ) , "wait_until_release_date" : checked ( headphones . CONFIG . WAIT_UNTIL_RELEASE_DATE ) , "autowant_upcoming" : checked ( headphones . CONFIG . AUTOWANT_UPCOMING ) , "autowant_all" : checked ( headphones . CONFIG . AUTOWANT_ALL ) , "autowant_manually_added" : checked ( headphones . CONFIG . AUTOWANT_MANUALLY_ADDED ) , "do_not_process_unmatched" : checked ( headphones . CONFIG . DO_NOT_PROCESS_UNMATCHED ) , "keep_torrent_files" : checked ( headphones . CONFIG . KEEP_TORRENT_FILES ) , "prefer_torrents_0" : radio ( headphones . CONFIG . PREFER_TORRENTS , 0 ) , "prefer_torrents_1" : radio ( headphones . CONFIG . PREFER_TORRENTS , 1 ) , "prefer_torrents_2" : radio ( headphones . CONFIG . PREFER_TORRENTS , 2 ) , "magnet_links_0" : radio ( headphones . CONFIG . MAGNET_LINKS , 0 ) , "magnet_links_1" : radio ( headphones . CONFIG . MAGNET_LINKS , 1 ) , "magnet_links_2" : radio ( headphones . CONFIG . MAGNET_LINKS , 2 ) , "log_dir" : headphones . CONFIG . LOG_DIR , "cache_dir" : headphones . CONFIG . CACHE_DIR , "interface_list" : interface_list , "music_encoder" : checked ( headphones . CONFIG . MUSIC_ENCODER ) , "encoder" : headphones . CONFIG . ENCODER , "xldprofile" : headphones . CONFIG . XLDPROFILE , "bitrate" : int ( headphones . CONFIG . BITRATE ) , "encoder_path" : headphones . CONFIG . ENCODER_PATH , "advancedencoder" : headphones . CONFIG . ADVANCEDENCODER , "encoderoutputformat" : headphones . CONFIG . ENCODEROUTPUTFORMAT , "samplingfrequency" : headphones . CONFIG . SAMPLINGFREQUENCY , "encodervbrcbr" : headphones . CONFIG . ENCODERVBRCBR , "encoderquality" : headphones . CONFIG . ENCODERQUALITY , "encoderlossless" : checked ( headphones . CONFIG . ENCODERLOSSLESS ) , "encoder_multicore" : checked ( headphones . CONFIG . ENCODER_MULTICORE ) , "encoder_multicore_count" : int ( headphones . CONFIG . ENCODER_MULTICORE_COUNT ) , "delete_lossless_files" : checked ( headphones . CONFIG . DELETE_LOSSLESS_FILES ) , "growl_enabled" : checked ( headphones . CONFIG . GROWL_ENABLED ) , "growl_onsnatch" : checked ( headphones . CONFIG . GROWL_ONSNATCH ) , "growl_host" : headphones . CONFIG . GROWL_HOST , "growl_password" : headphones . CONFIG . GROWL_PASSWORD , "prowl_enabled" : checked ( headphones . CONFIG . PROWL_ENABLED ) , "prowl_onsnatch" : checked ( headphones . CONFIG . PROWL_ONSNATCH ) , "prowl_keys" : headphones . CONFIG . PROWL_KEYS , "prowl_priority" : headphones . CONFIG . PROWL_PRIORITY , "xbmc_enabled" : checked ( headphones . CONFIG . XBMC_ENABLED ) , "xbmc_host" : headphones . CONFIG . XBMC_HOST , "xbmc_username" : headphones . CONFIG . XBMC_USERNAME , "xbmc_password" : headphones . CONFIG . XBMC_PASSWORD , "xbmc_update" : checked ( headphones . CONFIG . XBMC_UPDATE ) , "xbmc_notify" : checked ( headphones . CONFIG . XBMC_NOTIFY ) , "lms_enabled" : checked ( headphones . CONFIG . LMS_ENABLED ) , "lms_host" : headphones . CONFIG . LMS_HOST , "plex_enabled" : checked ( headphones . CONFIG . PLEX_ENABLED ) , "plex_server_host" : headphones . CONFIG . PLEX_SERVER_HOST , "plex_client_host" : headphones . CONFIG . PLEX_CLIENT_HOST , "plex_username" : headphones . CONFIG . PLEX_USERNAME , "plex_password" : headphones . CONFIG . PLEX_PASSWORD , "plex_token" : headphones . CONFIG . PLEX_TOKEN , "plex_update" : checked ( headphones . CONFIG . PLEX_UPDATE ) , "plex_notify" : checked ( headphones . CONFIG . PLEX_NOTIFY ) , "nma_enabled" : checked ( headphones . CONFIG . NMA_ENABLED ) , "nma_apikey" : headphones . CONFIG . NMA_APIKEY , "nma_priority" : int ( headphones . CONFIG . NMA_PRIORITY ) , "nma_onsnatch" : checked ( headphones . CONFIG . NMA_ONSNATCH ) , "pushalot_enabled" : checked ( headphones . CONFIG . PUSHALOT_ENABLED ) , "pushalot_apikey" : headphones . CONFIG . PUSHALOT_APIKEY , "pushalot_onsnatch" : checked ( headphones . CONFIG . PUSHALOT_ONSNATCH ) , "synoindex_enabled" : checked ( headphones . CONFIG . SYNOINDEX_ENABLED ) , "pushover_enabled" : checked ( headphones . CONFIG . PUSHOVER_ENABLED ) , "pushover_onsnatch" : checked ( headphones . CONFIG . PUSHOVER_ONSNATCH ) , "pushover_keys" : headphones . CONFIG . PUSHOVER_KEYS , "pushover_apitoken" : headphones . CONFIG . PUSHOVER_APITOKEN , "pushover_priority" : headphones . CONFIG . PUSHOVER_PRIORITY , "pushbullet_enabled" : checked ( headphones . CONFIG . PUSHBULLET_ENABLED ) , "pushbullet_onsnatch" : checked ( headphones . CONFIG . PUSHBULLET_ONSNATCH ) , "pushbullet_apikey" : headphones . CONFIG . PUSHBULLET_APIKEY , "pushbullet_deviceid" : headphones . CONFIG . PUSHBULLET_DEVICEID , "subsonic_enabled" : checked ( headphones . CONFIG . SUBSONIC_ENABLED ) , "subsonic_host" : headphones . CONFIG . SUBSONIC_HOST , "subsonic_username" : headphones . CONFIG . SUBSONIC_USERNAME , "subsonic_password" : headphones . CONFIG . SUBSONIC_PASSWORD , "twitter_enabled" : checked ( headphones . CONFIG . TWITTER_ENABLED ) , "twitter_onsnatch" : checked ( headphones . CONFIG . TWITTER_ONSNATCH ) , "osx_notify_enabled" : checked ( headphones . CONFIG . OSX_NOTIFY_ENABLED ) , "osx_notify_onsnatch" : checked ( headphones . CONFIG . OSX_NOTIFY_ONSNATCH ) , "osx_notify_app" : headphones . CONFIG . OSX_NOTIFY_APP , "boxcar_enabled" : checked ( headphones . CONFIG . BOXCAR_ENABLED ) , "boxcar_onsnatch" : checked ( headphones . CONFIG . BOXCAR_ONSNATCH ) , "boxcar_token" : headphones . CONFIG . BOXCAR_TOKEN , "mirrorlist" : headphones . MIRRORLIST , "mirror" : headphones . CONFIG . MIRROR , "customhost" : headphones . CONFIG . CUSTOMHOST , "customport" : headphones . CONFIG . CUSTOMPORT , "customsleep" : headphones . CONFIG . CUSTOMSLEEP , "customauth" : checked ( headphones . CONFIG . CUSTOMAUTH ) , "customuser" : headphones . CONFIG . CUSTOMUSER , "custompass" : headphones . CONFIG . CUSTOMPASS , "hpuser" : headphones . CONFIG . HPUSER , "hppass" : headphones . CONFIG . HPPASS , "songkick_enabled" : checked ( headphones . CONFIG . SONGKICK_ENABLED ) , "songkick_apikey" : headphones . CONFIG . SONGKICK_APIKEY , "songkick_location" : headphones . CONFIG . SONGKICK_LOCATION , "songkick_filter_enabled" : checked ( headphones . CONFIG . SONGKICK_FILTER_ENABLED ) , "cache_sizemb" : headphones . CONFIG . CACHE_SIZEMB , "file_permissions" : headphones . CONFIG . FILE_PERMISSIONS , "folder_permissions" : headphones . CONFIG . FOLDER_PERMISSIONS , "mpc_enabled" : checked ( headphones . CONFIG . MPC_ENABLED ) , "email_enabled" : checked ( headphones . CONFIG . EMAIL_ENABLED ) , "email_from" : headphones . CONFIG . EMAIL_FROM , "email_to" : headphones . CONFIG . EMAIL_TO , "email_smtp_server" : headphones . CONFIG . EMAIL_SMTP_SERVER , "email_smtp_user" : headphones . CONFIG . EMAIL_SMTP_USER , "email_smtp_password" : headphones . CONFIG . EMAIL_SMTP_PASSWORD , "email_smtp_port" : int ( headphones . CONFIG . EMAIL_SMTP_PORT ) , "email_ssl" : checked ( headphones . CONFIG . EMAIL_SSL ) , "email_tls" : checked ( headphones . CONFIG . EMAIL_TLS ) , "email_onsnatch" : checked ( headphones . CONFIG . EMAIL_ONSNATCH ) , "idtag" : checked ( headphones . CONFIG . IDTAG ) } <newline>  # ▁ Need ▁ to ▁ convert ▁ EXTRAS ▁ to ▁ a ▁ dictionary ▁ we ▁ can ▁ pass ▁ to ▁ the ▁ config: <encdom>  # ▁ it'll ▁ come ▁ in ▁ as ▁ a ▁ string ▁ like ▁ 2,5,6,8 <encdom> extra_munges = { "dj-mix" : "dj_mix" , "mixtape/street" : "mixtape_street" } <newline> extras_list = [ extra_munges . get ( x , x ) for x in headphones . POSSIBLE_EXTRAS ] <newline> if headphones . CONFIG . EXTRAS : <newline> <indent> extras = map ( int , headphones . CONFIG . EXTRAS . split ( ',' ) ) <newline> <dedent> else : <newline> <indent> extras = [ ] <newline> <dedent> extras_dict = OrderedDict ( ) <newline> i = 1 <newline> for extra in extras_list : <newline> <indent> if i in extras : <newline> <indent> extras_dict [ extra ] = "checked" <newline> <dedent> else : <newline> <indent> extras_dict [ extra ] = "" <newline> <dedent> i += 1 <newline> <dedent> config [ "extras" ] = extras_dict <newline> return serve_template ( templatename = "config.html" , title = "Settings" , config = config ) <newline> <dedent> @ cherrypy . expose <newline> def configUpdate ( self , ** kwargs ) : <newline>  # ▁ Handle ▁ the ▁ variable ▁ config ▁ options. ▁ Note ▁ - ▁ keys ▁ with ▁ False ▁ values ▁ aren't ▁ getting ▁ passed <encdom> <indent> checked_configs = [ "launch_browser" , "enable_https" , "api_enabled" , "use_blackhole" , "headphones_indexer" , "use_newznab" , "newznab_enabled" , "use_torznab" , "torznab_enabled" , "use_nzbsorg" , "use_omgwtfnzbs" , "use_kat" , "use_piratebay" , "use_oldpiratebay" , "use_mininova" , "use_waffles" , "use_rutracker" , "use_whatcd" , "use_strike" , "preferred_bitrate_allow_lossless" , "detect_bitrate" , "ignore_clean_releases" , "freeze_db" , "cue_split" , "move_files" , "rename_files" , "correct_metadata" , "cleanup_files" , "keep_nfo" , "add_album_art" , "embed_album_art" , "embed_lyrics" , "replace_existing_folders" , "keep_original_folder" , "file_underscores" , "include_extras" , "official_releases_only" , "wait_until_release_date" , "autowant_upcoming" , "autowant_all" , "autowant_manually_added" , "do_not_process_unmatched" , "keep_torrent_files" , "music_encoder" , "encoderlossless" , "encoder_multicore" , "delete_lossless_files" , "growl_enabled" , "growl_onsnatch" , "prowl_enabled" , "prowl_onsnatch" , "xbmc_enabled" , "xbmc_update" , "xbmc_notify" , "lms_enabled" , "plex_enabled" , "plex_update" , "plex_notify" , "nma_enabled" , "nma_onsnatch" , "pushalot_enabled" , "pushalot_onsnatch" , "synoindex_enabled" , "pushover_enabled" , "pushover_onsnatch" , "pushbullet_enabled" , "pushbullet_onsnatch" , "subsonic_enabled" , "twitter_enabled" , "twitter_onsnatch" , "osx_notify_enabled" , "osx_notify_onsnatch" , "boxcar_enabled" , "boxcar_onsnatch" , "songkick_enabled" , "songkick_filter_enabled" , "mpc_enabled" , "email_enabled" , "email_ssl" , "email_tls" , "email_onsnatch" , "customauth" , "idtag" ] <newline> for checked_config in checked_configs : <newline> <indent> if checked_config not in kwargs : <newline>  # ▁ checked ▁ items ▁ should ▁ be ▁ zero ▁ or ▁ one. ▁ if ▁ they ▁ were ▁ not ▁ sent ▁ then ▁ the ▁ item ▁ was ▁ not ▁ checked <encdom> <indent> kwargs [ checked_config ] = 0 <newline> <dedent> <dedent> for plain_config , use_config in [ ( x [ 4 : ] , x ) for x in kwargs if x . startswith ( 'use_' ) ] : <newline>  # ▁ the ▁ use ▁ prefix ▁ is ▁ fairly ▁ nice ▁ in ▁ the ▁ html, ▁ but ▁ does ▁ not ▁ match ▁ the ▁ actual ▁ config <encdom> <indent> kwargs [ plain_config ] = kwargs [ use_config ] <newline> del kwargs [ use_config ] <newline> <dedent> extra_newznabs = [ ] <newline> for kwarg in [ x for x in kwargs if x . startswith ( 'newznab_host' ) ] : <newline> <indent> newznab_host_key = kwarg <newline> newznab_number = kwarg [ 12 : ] <newline> if len ( newznab_number ) : <newline> <indent> newznab_api_key = 'newznab_api' + newznab_number <newline> newznab_enabled_key = 'newznab_enabled' + newznab_number <newline> newznab_host = kwargs . get ( newznab_host_key , '' ) <newline> newznab_api = kwargs . get ( newznab_api_key , '' ) <newline> newznab_enabled = int ( kwargs . get ( newznab_enabled_key , 0 ) ) <newline> for key in [ newznab_host_key , newznab_api_key , newznab_enabled_key ] : <newline> <indent> if key in kwargs : <newline> <indent> del kwargs [ key ] <newline> <dedent> <dedent> extra_newznabs . append ( ( newznab_host , newznab_api , newznab_enabled ) ) <newline> <dedent> <dedent> extra_torznabs = [ ] <newline> for kwarg in [ x for x in kwargs if x . startswith ( 'torznab_host' ) ] : <newline> <indent> torznab_host_key = kwarg <newline> torznab_number = kwarg [ 12 : ] <newline> if len ( torznab_number ) : <newline> <indent> torznab_api_key = 'torznab_api' + torznab_number <newline> torznab_enabled_key = 'torznab_enabled' + torznab_number <newline> torznab_host = kwargs . get ( torznab_host_key , '' ) <newline> torznab_api = kwargs . get ( torznab_api_key , '' ) <newline> torznab_enabled = int ( kwargs . get ( torznab_enabled_key , 0 ) ) <newline> for key in [ torznab_host_key , torznab_api_key , torznab_enabled_key ] : <newline> <indent> if key in kwargs : <newline> <indent> del kwargs [ key ] <newline> <dedent> <dedent> extra_torznabs . append ( ( torznab_host , torznab_api , torznab_enabled ) ) <newline>  # ▁ Convert ▁ the ▁ extras ▁ to ▁ list ▁ then ▁ string. ▁ Coming ▁ in ▁ as ▁ 0 ▁ or ▁ 1 ▁ (append ▁ new ▁ extras ▁ to ▁ the ▁ end) <encdom> <dedent> <dedent> temp_extras_list = [ ] <newline> extra_munges = { "dj-mix" : "dj_mix" , "mixtape/street" : "mixtape_street" } <newline> expected_extras = [ extra_munges . get ( x , x ) for x in headphones . POSSIBLE_EXTRAS ] <newline> extras_list = [ kwargs . get ( x , 0 ) for x in expected_extras ] <newline> i = 1 <newline> for extra in extras_list : <newline> <indent> if extra : <newline> <indent> temp_extras_list . append ( i ) <newline> <dedent> i += 1 <newline> <dedent> for extra in expected_extras : <newline> <indent> temp = '%s_temp' % extra <newline> if temp in kwargs : <newline> <indent> del kwargs [ temp ] <newline> <dedent> if extra in kwargs : <newline> <indent> del kwargs [ extra ] <newline> <dedent> <dedent> headphones . CONFIG . EXTRAS = ',' . join ( str ( n ) for n in temp_extras_list ) <newline> headphones . CONFIG . clear_extra_newznabs ( ) <newline> headphones . CONFIG . clear_extra_torznabs ( ) <newline> headphones . CONFIG . process_kwargs ( kwargs ) <newline> for extra_newznab in extra_newznabs : <newline> <indent> headphones . CONFIG . add_extra_newznab ( extra_newznab ) <newline> <dedent> for extra_torznab in extra_torznabs : <newline> <indent> headphones . CONFIG . add_extra_torznab ( extra_torznab ) <newline>  # ▁ Sanity ▁ checking <encdom> <dedent> if headphones . CONFIG . SEARCH_INTERVAL and headphones . CONFIG . SEARCH_INTERVAL < 360 : <newline> <indent> logger . info ( "Search ▁ interval ▁ too ▁ low. ▁ Resetting ▁ to ▁ 6 ▁ hour ▁ minimum" ) <newline> headphones . CONFIG . SEARCH_INTERVAL = 360 <newline>  # ▁ Write ▁ the ▁ config <encdom> <dedent> headphones . CONFIG . write ( ) <newline>  # ▁ Reconfigure ▁ scheduler <encdom> headphones . initialize_scheduler ( ) <newline>  # ▁ Reconfigure ▁ musicbrainz ▁ database ▁ connection ▁ with ▁ the ▁ new ▁ values <encdom> mb . startmb ( ) <newline> raise cherrypy . HTTPRedirect ( "config" ) <newline> <dedent> @ cherrypy . expose <newline> def do_state_change ( self , signal , title , timer ) : <newline> <indent> headphones . SIGNAL = signal <newline> message = title + '...' <newline> return serve_template ( templatename = "shutdown.html" , title = title , message = message , timer = timer ) <newline> <dedent> @ cherrypy . expose <newline> def shutdown ( self ) : <newline> <indent> return self . do_state_change ( 'shutdown' , 'Shutting ▁ Down' , 15 ) <newline> <dedent> @ cherrypy . expose <newline> def restart ( self ) : <newline> <indent> return self . do_state_change ( 'restart' , 'Restarting' , 30 ) <newline> <dedent> @ cherrypy . expose <newline> def update ( self ) : <newline> <indent> return self . do_state_change ( 'update' , 'Updating' , 120 ) <newline> <dedent> @ cherrypy . expose <newline> def extras ( self ) : <newline> <indent> myDB = db . DBConnection ( ) <newline> cloudlist = myDB . select ( 'SELECT ▁ * ▁ from ▁ lastfmcloud' ) <newline> return serve_template ( templatename = "extras.html" , title = "Extras" , cloudlist = cloudlist ) <newline> <dedent> @ cherrypy . expose <newline> def addReleaseById ( self , rid , rgid = None ) : <newline> <indent> threading . Thread ( target = importer . addReleaseById , args = [ rid , rgid ] ) . start ( ) <newline> if rgid : <newline> <indent> raise cherrypy . HTTPRedirect ( "albumPage?AlbumID=%s" % rgid ) <newline> <dedent> else : <newline> <indent> raise cherrypy . HTTPRedirect ( "home" ) <newline> <dedent> <dedent> @ cherrypy . expose <newline> def updateCloud ( self ) : <newline> <indent> lastfm . getSimilar ( ) <newline> raise cherrypy . HTTPRedirect ( "extras" ) <newline> <dedent> @ cherrypy . expose <newline> def api ( self , * args , ** kwargs ) : <newline> <indent> from headphones . api import Api <newline> a = Api ( ) <newline> a . checkParams ( * args , ** kwargs ) <newline> return a . fetchData ( ) <newline> <dedent> @ cherrypy . expose <newline> def getInfo ( self , ArtistID = None , AlbumID = None ) : <newline> <indent> from headphones import cache <newline> info_dict = cache . getInfo ( ArtistID , AlbumID ) <newline> return json . dumps ( info_dict ) <newline> <dedent> @ cherrypy . expose <newline> def getArtwork ( self , ArtistID = None , AlbumID = None ) : <newline> <indent> from headphones import cache <newline> return cache . getArtwork ( ArtistID , AlbumID ) <newline> <dedent> @ cherrypy . expose <newline> def getThumb ( self , ArtistID = None , AlbumID = None ) : <newline> <indent> from headphones import cache <newline> return cache . getThumb ( ArtistID , AlbumID ) <newline>  # ▁ If ▁ you ▁ just ▁ want ▁ to ▁ get ▁ the ▁ last.fm ▁ image ▁ links ▁ for ▁ an ▁ album, ▁ make ▁ sure <encdom>  # ▁ to ▁ pass ▁ a ▁ releaseid ▁ and ▁ not ▁ a ▁ releasegroupid <encdom> <dedent> @ cherrypy . expose <newline> def getImageLinks ( self , ArtistID = None , AlbumID = None ) : <newline> <indent> from headphones import cache <newline> image_dict = cache . getImageLinks ( ArtistID , AlbumID ) <newline>  # ▁ Return ▁ the ▁ Cover ▁ Art ▁ Archive ▁ urls ▁ if ▁ not ▁ found ▁ on ▁ last.fm <encdom> if AlbumID and not image_dict : <newline> <indent> image_url = "http://coverartarchive.org/release/%s/front-500.jpg" % AlbumID <newline> thumb_url = "http://coverartarchive.org/release/%s/front-250.jpg" % AlbumID <newline> image_dict = { 'artwork' : image_url , 'thumbnail' : thumb_url } <newline> <dedent> elif AlbumID and ( not image_dict [ 'artwork' ] or not image_dict [ 'thumbnail' ] ) : <newline> <indent> if not image_dict [ 'artwork' ] : <newline> <indent> image_dict [ 'artwork' ] = "http://coverartarchive.org/release/%s/front-500.jpg" % AlbumID <newline> <dedent> if not image_dict [ 'thumbnail' ] : <newline> <indent> image_dict [ 'thumbnail' ] = "http://coverartarchive.org/release/%s/front-250.jpg" % AlbumID <newline> <dedent> <dedent> return json . dumps ( image_dict ) <newline> <dedent> @ cherrypy . expose <newline> def twitterStep1 ( self ) : <newline> <indent> cherrypy . response . headers [ 'Cache-Control' ] = "max-age=0,no-cache,no-store" <newline> tweet = notifiers . TwitterNotifier ( ) <newline> return tweet . _get_authorization ( ) <newline> <dedent> @ cherrypy . expose <newline> def twitterStep2 ( self , key ) : <newline> <indent> cherrypy . response . headers [ 'Cache-Control' ] = "max-age=0,no-cache,no-store" <newline> tweet = notifiers . TwitterNotifier ( ) <newline> result = tweet . _get_credentials ( key ) <newline> logger . info ( u"result: ▁ " + str ( result ) ) <newline> if result : <newline> <indent> return "Key ▁ verification ▁ successful" <newline> <dedent> else : <newline> <indent> return "Unable ▁ to ▁ verify ▁ key" <newline> <dedent> <dedent> @ cherrypy . expose <newline> def testTwitter ( self ) : <newline> <indent> cherrypy . response . headers [ 'Cache-Control' ] = "max-age=0,no-cache,no-store" <newline> tweet = notifiers . TwitterNotifier ( ) <newline> result = tweet . test_notify ( ) <newline> if result : <newline> <indent> return "Tweet ▁ successful, ▁ check ▁ your ▁ twitter ▁ to ▁ make ▁ sure ▁ it ▁ worked" <newline> <dedent> else : <newline> <indent> return "Error ▁ sending ▁ tweet" <newline> <dedent> <dedent> @ cherrypy . expose <newline> def osxnotifyregister ( self , app ) : <newline> <indent> cherrypy . response . headers [ 'Cache-Control' ] = "max-age=0,no-cache,no-store" <newline> from osxnotify import registerapp as osxnotify <newline> result , msg = osxnotify . registerapp ( app ) <newline> if result : <newline> <indent> osx_notify = notifiers . OSX_NOTIFY ( ) <newline> osx_notify . notify ( 'Registered' , result , 'Success ▁ :-)' ) <newline> logger . info ( 'Registered ▁ %s, ▁ to ▁ re-register ▁ a ▁ different ▁ app, ▁ delete ▁ this ▁ app ▁ first' % result ) <newline> <dedent> else : <newline> <indent> logger . warn ( msg ) <newline> <dedent> return msg <newline> <dedent> @ cherrypy . expose <newline> def testPushover ( self ) : <newline> <indent> logger . info ( u"Sending ▁ Pushover ▁ notification" ) <newline> pushover = notifiers . PUSHOVER ( ) <newline> result = pushover . notify ( "hooray!" , "This ▁ is ▁ a ▁ test" ) <newline> return str ( result ) <newline> <dedent> @ cherrypy . expose <newline> def testPlex ( self ) : <newline> <indent> logger . info ( u"Testing ▁ plex ▁ notifications" ) <newline> plex = notifiers . Plex ( ) <newline> plex . notify ( "hellooooo" , "test ▁ album!" , "" ) <newline> <dedent> @ cherrypy . expose <newline> def testPushbullet ( self ) : <newline> <indent> logger . info ( "Testing ▁ Pushbullet ▁ notifications" ) <newline> pushbullet = notifiers . PUSHBULLET ( ) <newline> pushbullet . notify ( "it ▁ works!" ) <newline> <dedent> <dedent> class Artwork ( object ) : <newline> <indent> @ cherrypy . expose <newline> def index ( self ) : <newline> <indent> return "Artwork" <newline> <dedent> @ cherrypy . expose <newline> def default ( self , ArtistOrAlbum = "" , ID = None ) : <newline> <indent> from headphones import cache <newline> ArtistID = None <newline> AlbumID = None <newline> if ArtistOrAlbum == "artist" : <newline> <indent> ArtistID = ID <newline> <dedent> elif ArtistOrAlbum == "album" : <newline> <indent> AlbumID = ID <newline> <dedent> relpath = cache . getArtwork ( ArtistID , AlbumID ) <newline> if not relpath : <newline> <indent> relpath = "data/interfaces/default/images/no-cover-art.png" <newline> basedir = os . path . dirname ( sys . argv [ 0 ] ) <newline> path = os . path . join ( basedir , relpath ) <newline> cherrypy . response . headers [ 'Content-type' ] = 'image/png' <newline> cherrypy . response . headers [ 'Cache-Control' ] = 'no-cache' <newline> <dedent> else : <newline> <indent> relpath = relpath . replace ( 'cache/' , '' , 1 ) <newline> path = os . path . join ( headphones . CONFIG . CACHE_DIR , relpath ) <newline> fileext = os . path . splitext ( relpath ) [ 1 ] [ 1 : : ] <newline> cherrypy . response . headers [ 'Content-type' ] = 'image/' + fileext <newline> cherrypy . response . headers [ 'Cache-Control' ] = 'max-age=31556926' <newline> <dedent> with open ( os . path . normpath ( path ) , "rb" ) as fp : <newline> <indent> return fp . read ( ) <newline> <dedent> <dedent> class Thumbs ( object ) : <newline> <indent> @ cherrypy . expose <newline> def index ( self ) : <newline> <indent> return "Here ▁ be ▁ thumbs" <newline> <dedent> @ cherrypy . expose <newline> def default ( self , ArtistOrAlbum = "" , ID = None ) : <newline> <indent> from headphones import cache <newline> ArtistID = None <newline> AlbumID = None <newline> if ArtistOrAlbum == "artist" : <newline> <indent> ArtistID = ID <newline> <dedent> elif ArtistOrAlbum == "album" : <newline> <indent> AlbumID = ID <newline> <dedent> relpath = cache . getThumb ( ArtistID , AlbumID ) <newline> if not relpath : <newline> <indent> relpath = "data/interfaces/default/images/no-cover-artist.png" <newline> basedir = os . path . dirname ( sys . argv [ 0 ] ) <newline> path = os . path . join ( basedir , relpath ) <newline> cherrypy . response . headers [ 'Content-type' ] = 'image/png' <newline> cherrypy . response . headers [ 'Cache-Control' ] = 'no-cache' <newline> <dedent> else : <newline> <indent> relpath = relpath . replace ( 'cache/' , '' , 1 ) <newline> path = os . path . join ( headphones . CONFIG . CACHE_DIR , relpath ) <newline> fileext = os . path . splitext ( relpath ) [ 1 ] [ 1 : : ] <newline> cherrypy . response . headers [ 'Content-type' ] = 'image/' + fileext <newline> cherrypy . response . headers [ 'Cache-Control' ] = 'max-age=31556926' <newline> <dedent> with open ( os . path . normpath ( path ) , "rb" ) as fp : <newline> <indent> return fp . read ( ) <newline> <dedent> <dedent> <dedent> thumbs = Thumbs ( ) <newline> <dedent> WebInterface . artwork = Artwork ( ) <newline>