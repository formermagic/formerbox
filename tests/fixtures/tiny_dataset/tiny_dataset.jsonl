{"id": "a6ed871bb00965c08e704794aab01734ca318005", "path": "obelisk-cardiograph.py", "repo_name": "veox/obelisk-cardiograph", "content": "#!/usr/bin/env python\n\n\"\"\"\nobelisk-cardiograph\nScript to monitor obelisk servers' heartbeat.\nAuthor: Noel Maersk <veox ta wemakethings tod net>\nLicense: Affero GNU GPLv3 (see LICENSE).\n\nA few examples from `zguide` were used, see:\nhttps://github.com/imatix/zguide\n\n\"\"\"\n\n\nimport zmq\n\n\n# This list is necessarily over 72 characters wide.\nserverlist = [{'address': 'tcp://obelisk.coinkite.com:9092', 'network': 'bitcoin'},\n              {'address': 'tcp://preacher.veox.pw:9092', 'network': 'bitcoin-testnet'}]\n\n\nclass Server(object):\n    \"\"\"\n    \"\"\"\n    def __init__(self, zmqcontext, properties):\n        \"\"\"\n        \"\"\"\n        # Consider using an initialiser wrapper as in\n        # https://stackoverflow.com/questions/1389180\n        # if the property list gets too long.\n        # Alternatively, find if there's a lib way to do it.\n        self._address = properties['address']\n        self._network = properties['network']\n        self.socket = zmqcontext.socket(zmq.SUB)\n\n    @property\n    def address(self):\n        \"\"\"tcp://<server-address>:<port>\"\"\"\n        return self._address\n\n    @address.setter\n    def address(self, value):\n        self._address = value\n\n    @property\n    def network(self):\n        \"\"\"Human-readable string description of the P2P network.\"\"\"\n        return self._network\n\n    @network.setter\n    def network(self, value):\n        self._network = value\n\n    def receive_heartbeat(self):\n        \"\"\"\n        \"\"\"\n        rawreply = self.socket.recv()\n        reply = rawreply[::-1]  # obelisk sends little-endian\n        return ':'.join(hex(x)[2:] for x in reply)\n\n    def connect(self, address = None):\n        \"\"\"\n        \"\"\"\n        if address == None:\n            address = self._address\n        self.socket.connect(address)\n        self.socket.setsockopt(zmq.SUBSCRIBE, b'')\n\n    def disconnect(self):\n        \"\"\"\n        \"\"\"\n        self.socket.close()\n\n\ndef main():\n    \"\"\" main method \"\"\"\n    context = zmq.Context()\n    servers = []\n    for i in serverlist:\n        server = Server(context, i)\n        server.connect()\n        servers.append(server)\n        \n    print(\"Entering main loop.\")\n    while True:\n        for server in servers:\n            print(server.network, server.address,\n                  server.receive_heartbeat())\n\n    # We never get here but clean up anyhow\n    for server in servers:\n        server.disconnect()\n    context.term()\n\n\nif __name__ == \"__main__\":\n    main()\n", "license": "agpl-3.0"}
{"id": "edeed256ce6fa26e4e7ebac8af4f6fb11a04ccfa", "path": "src/collectors/jcollectd/jcollectd.py", "repo_name": "Precis/Diamond", "content": "# coding=utf-8\n\n\"\"\"\nThe JCollectdCollector is capable of receiving Collectd network traffic\nas sent by the JCollectd jvm agent (and child Collectd processes).\n\nReason for developing this collector is allowing to use JCollectd, without\nthe need for Collectd.\n\nA few notes:\n\nThis collector starts a UDP server to receive data. This server runs in\na separate thread and puts it on a queue, waiting for the collect() method\nto pull. Because of this setup, the collector interval parameter is of\nless importance. What matters is the 'sendinterval' JCollectd parameter.\n\nSee https://github.com/emicklei/jcollectd for an up-to-date jcollect fork.\n\n#### Dependencies\n\n * jcollectd sending metrics\n\n\"\"\"\n\n\nimport threading\nimport re\nimport Queue\n\nimport diamond.collector\nimport diamond.metric\n\nimport collectd_network\n\n\nALIVE = True\n\n\nclass JCollectdCollector(diamond.collector.Collector):\n\n    def __init__(self, *args, **kwargs):\n        super(JCollectdCollector, self).__init__(*args, **kwargs)\n        self.listener_thread = None\n\n    def get_default_config(self):\n        \"\"\"\n        Returns the default collector settings\n        \"\"\"\n        config = super(JCollectdCollector, self).get_default_config()\n        config.update({\n            'path':     'jvm',\n            'listener_host': '127.0.0.1',\n            'listener_port': 25826,\n        })\n        return config\n\n    def collect(self):\n        if not self.listener_thread:\n            self.start_listener()\n\n        q = self.listener_thread.queue\n        while True:\n            try:\n                dp = q.get(False)\n                metric = self.make_metric(dp)\n            except Queue.Empty:\n                break\n            self.publish_metric(metric)\n\n    def start_listener(self):\n        self.listener_thread = ListenerThread(self.config['listener_host'],\n                                              self.config['listener_port'],\n                                              self.log)\n        self.listener_thread.start()\n\n    def stop_listener(self):\n        global ALIVE\n        ALIVE = False\n        self.listener_thread.join()\n        self.log.error('Listener thread is shut down.')\n\n    def make_metric(self, dp):\n\n        path = \".\".join((dp.host, self.config['path'], dp.name))\n\n        if 'path_prefix' in self.config:\n            prefix = self.config['path_prefix']\n            if prefix:\n                path = \".\".join((prefix, path))\n\n        if 'path_suffix' in self.config:\n            suffix = self.config['path_suffix']\n            if suffix:\n                path = \".\".join((path, suffix))\n\n        if dp.is_counter:\n            metric_type = \"COUNTER\"\n        else:\n            metric_type = \"GAUGE\"\n        metric = diamond.metric.Metric(path, dp.value, dp.time,\n                                       metric_type=metric_type)\n\n        return metric\n\n    def __del__(self):\n        if self.listener_thread:\n            self.stop_listener()\n\n\nclass ListenerThread(threading.Thread):\n\n    def __init__(self, host, port, log, poll_interval=0.4):\n        super(ListenerThread, self).__init__()\n        self.name = 'JCollectdListener'  # thread name\n\n        self.host = host\n        self.port = port\n        self.log = log\n        self.poll_interval = poll_interval\n\n        self.queue = Queue.Queue()\n\n    def run(self):\n        self.log.info('ListenerThread started on {0}:{1}(udp)'.format(\n            self.host, self.port))\n\n        rdr = collectd_network.Reader(self.host, self.port)\n\n        try:\n            while ALIVE:\n                try:\n                    items = rdr.interpret(poll_interval=self.poll_interval)\n                    self.send_to_collector(items)\n                except ValueError, e:\n                    self.log.warn('Dropping bad packet: {0}'.format(e))\n        except Exception, e:\n            self.log.error('caught exception: type={0}, exc={1}'.format(type(e),\n                                                                        e))\n\n        self.log.info('ListenerThread - stop')\n\n    def send_to_collector(self, items):\n        if items is None:\n            return\n\n        for item in items:\n            try:\n                metric = self.transform(item)\n                self.queue.put(metric)\n            except Queue.Full:\n                self.log.error('Queue to collector is FULL')\n            except Exception, e:\n                self.log.error('B00M! type={0}, exception={1}'.format(type(e),\n                                                                      e))\n\n    def transform(self, item):\n\n        parts = []\n\n        path = item.plugininstance\n        # extract jvm name from 'logstash-MemoryPool Eden Space'\n        if '-' in path:\n            (jvm, tail) = path.split('-', 1)\n            path = tail\n        else:\n            jvm = 'unnamed'\n\n        # add JVM name\n        parts.append(jvm)\n\n        # add mbean name (e.g. 'java_lang')\n        parts.append(item.plugin)\n\n        # get typed mbean: 'MemoryPool Eden Space'\n        if ' ' in path:\n            (mb_type, mb_name) = path.split(' ', 1)\n            parts.append(mb_type)\n            parts.append(mb_name)\n        else:\n            parts.append(path)\n\n        # add property name\n        parts.append(item.typeinstance)\n\n        # construct full path, from safe parts\n        name = '.'.join([sanitize_word(part) for part in parts])\n\n        if item[0][0] == 0:\n            is_counter = True\n        else:\n            is_counter = False\n        dp = Datapoint(item.host, item.time, name, item[0][1], is_counter)\n\n        return dp\n\n\ndef sanitize_word(s):\n    \"\"\"Remove non-alphanumerical characters from metric word.\n    And trim excessive underscores.\n    \"\"\"\n    s = re.sub('[^\\w-]+', '_', s)\n    s = re.sub('__+', '_', s)\n    return s.strip('_')\n\n\nclass Datapoint(object):\n\n    def __init__(self, host, time, name, value, is_counter):\n        self.host = host\n        self.time = time\n        self.name = name\n        self.value = value\n        self.is_counter = is_counter\n", "license": "mit"}
{"id": "e59498afb5a86d822863138dc496fa7e78e493a4", "path": "images/lenny64-peon/usr/share/python-support/python-django/django/contrib/admindocs/tests/fields.py", "repo_name": "carlgao/lenga", "content": "from django.db import models\n\nclass CustomField(models.Field):\n    description = \"A custom field type\"\n\nclass DescriptionLackingField(models.Field):\n    pass\n", "license": "mit"}
{"id": "b06a6fe49a42d4164a388576cbb81e95cc2fd6cd", "path": "scripts/calibration_demo.py", "repo_name": "probml/pyprobml", "content": "# -*- coding: utf-8 -*-\n\n# V. Kuleshov and P. S. Liang, \u201cCalibrated Structured Prediction,\u201d in NIPS, 2015, pp. 3474\u20133482 [Online]. \n#Available: http://papers.nips.cc/paper/5658-calibrated-structured-prediction.pdf\n\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport os\n\neps = 0.01 # we make this non-zero for plotting purposes\nptrue = [eps, 1, 0.5] # expect[y|x]\nFcal = [0.5, 0.5, 0.5]\nFuncal = [0.2, 0.8, 0.4]\nFbal = [eps, 0.75, 0.75]\n\n#https://matplotlib.org/examples/api/barchart_demo.html\n\nwidth = 0.2\n\nfig, ax = plt.subplots()\nX = np.arange(3) \nbar_true = ax.bar(X, ptrue, width, color='r')\nbar_cal = ax.bar(X+width,  Fcal, width, color='g')\nbar_uncal = ax.bar(X+2*width, Funcal, width, color='b')\nbar_bal = ax.bar(X+3*width, Fbal, width, color='k')\n\nax.set_ylabel('probability')\nax.set_xticks(X+width)\nax.set_xticklabels(X)\n\nax.legend((bar_true[0], bar_cal[0], bar_uncal[0], bar_bal[0]),\n    ('true', 'cal', 'uncal', 'bal'))\n\nplt.show()\nplt.savefig(os.path.join('figures', 'calibration'))\n\n### Plot error\n\neps = 0 \nptrue = np.array([eps, 1, 0.5])\nFcal = np.array([0.5, 0.5, 0.5])\nFuncal = np.array([0.2, 0.8, 0.4])\nFbal = np.array([eps, 0.75, 0.75])\n\nerr_cal = (ptrue - Fcal)\nerr_uncal = (ptrue - Funcal)\nerr_bal = (ptrue - Fbal)\nfig, ax = plt.subplots()\n\nX = np.arange(3) \nbar_cal = ax.bar(X+width,  err_cal, width, color='g')\nbar_uncal = ax.bar(X+2*width, err_uncal, width, color='b')\nbar_bal = ax.bar(X+3*width, err_bal, width, color='k')\n\nax.set_ylabel('error')\nax.set_xticks(X+width)\nax.set_xticklabels(X)\n\nax.legend((bar_cal[0], bar_uncal[0], bar_bal[0]),\n    ('cal', 'uncal', 'bal'))\n\nplt.show()\nplt.savefig(os.path.join('figures', 'calibration_err'))\n", "license": "mit"}
{"id": "11dc4d25c14b221ec9aa4a1c7606ebd1ed01005c", "path": "nearpy/tests/hashes_tests.py", "repo_name": "wanji/NearPy", "content": "# -*- coding: utf-8 -*-\n\n# Copyright (c) 2013 Ole Krause-Sparmann\n\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n\nimport numpy\nimport scipy\nimport unittest\n\nfrom nearpy.hashes import RandomBinaryProjections, \\\n    RandomDiscretizedProjections, \\\n    PCABinaryProjections, PCADiscretizedProjections\n\n\nclass TestRandomBinaryProjections(unittest.TestCase):\n\n    def setUp(self):\n        self.rbp = RandomBinaryProjections('testHash', 10)\n        self.rbp.reset(100)\n\n    def test_hash_format(self):\n        h = self.rbp.hash_vector(numpy.random.randn(100))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n        self.assertEqual(len(h[0]), 10)\n        for c in h[0]:\n            self.assertTrue(c == '1' or c == '0')\n\n    def test_hash_deterministic(self):\n        x = numpy.random.randn(100)\n        first_hash = self.rbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.rbp.hash_vector(x)[0])\n\n    def test_hash_format_sparse(self):\n        h = self.rbp.hash_vector(scipy.sparse.rand(100, 1, density=0.1))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n        self.assertEqual(len(h[0]), 10)\n        for c in h[0]:\n            self.assertTrue(c == '1' or c == '0')\n\n    def test_hash_deterministic_sparse(self):\n        x = scipy.sparse.rand(100, 1, density=0.1)\n        first_hash = self.rbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.rbp.hash_vector(x)[0])\n\nclass TestRandomDiscretizedProjections(unittest.TestCase):\n\n    def setUp(self):\n        self.rbp = RandomDiscretizedProjections('testHash', 10, 0.1)\n        self.rbp.reset(100)\n\n    def test_hash_format(self):\n        h = self.rbp.hash_vector(numpy.random.randn(100))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n\n    def test_hash_deterministic(self):\n        x = numpy.random.randn(100)\n        first_hash = self.rbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.rbp.hash_vector(x)[0])\n\n    def test_hash_format_sparse(self):\n        h = self.rbp.hash_vector(scipy.sparse.rand(100, 1, density=0.1))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n\n    def test_hash_deterministic_sparse(self):\n        x = scipy.sparse.rand(100, 1, density=0.1)\n        first_hash = self.rbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.rbp.hash_vector(x)[0])\n\nclass TestPCABinaryProjections(unittest.TestCase):\n\n    def setUp(self):\n        self.vectors = numpy.random.randn(10, 100)\n        self.pbp = PCABinaryProjections('pbp', 4, self.vectors)\n\n    def test_hash_format(self):\n        h = self.pbp.hash_vector(numpy.random.randn(10))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n        self.assertEqual(len(h[0]), 4)\n        for c in h[0]:\n            self.assertTrue(c == '1' or c == '0')\n\n    def test_hash_deterministic(self):\n        x = numpy.random.randn(10)\n        first_hash = self.pbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.pbp.hash_vector(x)[0])\n\n    def test_hash_format_sparse(self):\n        h = self.pbp.hash_vector(scipy.sparse.rand(10, 1, density=0.6))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n        self.assertEqual(len(h[0]), 4)\n        for c in h[0]:\n            self.assertTrue(c == '1' or c == '0')\n\n    def test_hash_deterministic_sparse(self):\n        x = scipy.sparse.rand(10, 1, density=0.6)\n        first_hash = self.pbp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.pbp.hash_vector(x)[0])\n\n\nclass TestPCADiscretizedProjections(unittest.TestCase):\n\n    def setUp(self):\n        self.vectors = numpy.random.randn(10, 100)\n        self.pdp = PCADiscretizedProjections('pdp', 4, self.vectors, 0.1)\n\n    def test_hash_format(self):\n        h = self.pdp.hash_vector(numpy.random.randn(10))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n\n    def test_hash_deterministic(self):\n        x = numpy.random.randn(10)\n        first_hash = self.pdp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.pdp.hash_vector(x)[0])\n\n    def test_hash_format_sparse(self):\n        h = self.pdp.hash_vector(scipy.sparse.rand(10, 1, density=0.6))\n        self.assertEqual(len(h), 1)\n        self.assertEqual(type(h[0]), type(''))\n\n    def test_hash_deterministic_sparse(self):\n        x = scipy.sparse.rand(10, 1, density=0.6)\n        first_hash = self.pdp.hash_vector(x)[0]\n        for k in range(100):\n            self.assertEqual(first_hash, self.pdp.hash_vector(x)[0])\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "license": "mit"}
{"id": "9aa7dac76d0e9a056a05e623f0846e7d44af742f", "path": "pinax_theme_tester/configs/stripe.py", "repo_name": "pinax/pinax_theme_tester", "content": "import decimal\nfrom datetime import datetime\n\nfrom django.conf import settings\nfrom django.conf.urls import url, include\n\nfrom pinax.stripe.forms import PlanForm\n\nfrom .base import ViewConfig\n\ninvoices = [\n    dict(date=datetime(2017, 10, 1), subscription=dict(plan=dict(name=\"Pro\")), period_start=datetime(2017, 10, 1), period_end=datetime(2017, 10, 31), total=decimal.Decimal(\"9.99\"), paid=False),\n    dict(date=datetime(2017, 9, 1), subscription=dict(plan=dict(name=\"Pro\")), period_start=datetime(2017, 9, 1), period_end=datetime(2017, 9, 30), total=decimal.Decimal(\"9.99\"), paid=True),\n    dict(date=datetime(2017, 8, 1), subscription=dict(plan=dict(name=\"Beginner\")), period_start=datetime(2017, 8, 1), period_end=datetime(2017, 8, 31), total=decimal.Decimal(\"5.99\"), paid=True),\n    dict(date=datetime(2017, 7, 1), subscription=dict(plan=dict(name=\"Beginner\")), period_start=datetime(2017, 7, 1), period_end=datetime(2017, 7, 30), total=decimal.Decimal(\"5.99\"), paid=True),\n]\ncard = dict(pk=1, brand=\"Visa\", last4=\"4242\", exp_month=\"10\", exp_year=\"2030\", created_at=datetime(2016, 4, 5))\nmethods = [\n    card\n]\nsubscription = dict(pk=1, current_period_start=datetime(2017, 10, 1), current_period_end=datetime(2017, 10, 31), plan=dict(name=\"Pro\"), start=datetime(2017, 10, 1), status=\"active\", invoice_set=dict(all=invoices))\nsubscriptions = [\n    subscription\n]\n\npatch = \"http://pinaxproject.com/pinax-design/patches/pinax-stripe.svg\"\nlabel = \"stripe\"\ntitle = \"Pinax Stripe\"\n\nviews = [\n    ViewConfig(pattern=r\"^invoices-empty/$\", template=\"pinax/stripe/invoice_list.html\", name=\"invoice_list_empty\", pattern_kwargs={}, object_list=[]),\n    ViewConfig(pattern=r\"^invoices/$\", template=\"pinax/stripe/invoice_list.html\", name=\"pinax_stripe_invoice_list\", pattern_kwargs={}, object_list=invoices),\n    ViewConfig(pattern=r\"^methods-empty/$\", template=\"pinax/stripe/paymentmethod_list.html\", name=\"method_list_empty\", pattern_kwargs={}, object_list=[]),\n    ViewConfig(pattern=r\"^methods/$\", template=\"pinax/stripe/paymentmethod_list.html\", name=\"pinax_stripe_payment_method_list\", pattern_kwargs={}, object_list=methods),\n    ViewConfig(pattern=r\"^methods/create/$\", template=\"pinax/stripe/paymentmethod_create.html\", name=\"pinax_stripe_payment_method_create\", pattern_kwargs={}, PINAX_STRIPE_PUBLIC_KEY=settings.PINAX_STRIPE_PUBLIC_KEY),\n    ViewConfig(pattern=r\"^methods/update/(?P<pk>\\d+)/$\", template=\"pinax/stripe/paymentmethod_update.html\", name=\"pinax_stripe_payment_method_update\", pattern_kwargs={\"pk\": 1}, object=card),\n    ViewConfig(pattern=r\"^methods/delete/(?P<pk>\\d+)/\", template=\"pinax/stripe/paymentmethod_delete.html\", name=\"pinax_stripe_payment_method_delete\", pattern_kwargs={\"pk\": 1}, object=card),\n    ViewConfig(pattern=r\"^subscriptions-empty/$\", template=\"pinax/stripe/subscription_list.html\", name=\"subscription_list_empty\", pattern_kwargs={}, object_list=[]),\n    ViewConfig(pattern=r\"^subscriptions/$\", template=\"pinax/stripe/subscription_list.html\", name=\"pinax_stripe_subscription_list\", pattern_kwargs={}, object_list=subscriptions),\n    ViewConfig(pattern=r\"^subscriptions/create/$\", template=\"pinax/stripe/subscription_create.html\", name=\"pinax_stripe_subscription_create\", pattern_kwargs={}, form=PlanForm(), request=dict(user=dict(customer=dict(default_source=\"foo\")))),\n    ViewConfig(pattern=r\"^subscriptions/update/(?P<pk>\\d+)/$\", template=\"pinax/stripe/subscription_update.html\", name=\"pinax_stripe_subscription_update\", pattern_kwargs={\"pk\": 1}, object=subscription, form=PlanForm(), PINAX_STRIPE_PUBLIC_KEY=settings.PINAX_STRIPE_PUBLIC_KEY),\n    ViewConfig(pattern=r\"^subscriptions/delete/(?P<pk>\\d+)/\", template=\"pinax/stripe/subscription_delete.html\", name=\"pinax_stripe_subscription_delete\", pattern_kwargs={\"pk\": 1}, object=subscription),\n]\nurlpatterns = [\n    view.url()\n    for view in views\n]\nurl = url(r\"payments/\", include(\"pinax_theme_tester.configs.stripe\"))\n", "license": "mit"}
{"id": "29c00d37a341ec7c6854ce33c1011909a9b4e25e", "path": "openerp/tools/func.py", "repo_name": "cdrooom/odoo", "content": "# -*- coding: utf-8 -*-\n##############################################################################\n#\n#    OpenERP, Open Source Management Solution\n#    Copyright (C) 2004-2009 Tiny SPRL (<http://tiny.be>).\n#    Copyright (C) 2010, 2014 OpenERP s.a. (<http://openerp.com>).\n#\n#    This program is free software: you can redistribute it and/or modify\n#    it under the terms of the GNU Affero General Public License as\n#    published by the Free Software Foundation, either version 3 of the\n#    License, or (at your option) any later version.\n#\n#    This program is distributed in the hope that it will be useful,\n#    but WITHOUT ANY WARRANTY; without even the implied warranty of\n#    MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#    GNU Affero General Public License for more details.\n#\n#    You should have received a copy of the GNU Affero General Public License\n#    along with this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n##############################################################################\n\n__all__ = ['synchronized', 'lazy_property', 'classproperty', 'conditional']\n\nfrom functools import wraps\nfrom inspect import getsourcefile\n\n\nclass lazy_property(object):\n    \"\"\" Decorator for a lazy property of an object, i.e., an object attribute\n        that is determined by the result of a method call evaluated once. To\n        reevaluate the property, simply delete the attribute on the object, and\n        get it again.\n    \"\"\"\n    def __init__(self, fget):\n        self.fget = fget\n\n    def __get__(self, obj, cls):\n        if obj is None:\n            return self\n        value = self.fget(obj)\n        setattr(obj, self.fget.__name__, value)\n        return value\n\n    @property\n    def __doc__(self):\n        return self.fget.__doc__\n\n    @staticmethod\n    def reset_all(obj):\n        \"\"\" Reset all lazy properties on the instance `obj`. \"\"\"\n        cls = type(obj)\n        obj_dict = vars(obj)\n        for name in obj_dict.keys():\n            if isinstance(getattr(cls, name, None), lazy_property):\n                obj_dict.pop(name)\n\ndef conditional(condition, decorator):\n    \"\"\" Decorator for a conditionally applied decorator.\n\n        Example:\n\n           @conditional(get_config('use_cache'), ormcache)\n           def fn():\n               pass\n    \"\"\"\n    if condition:\n        return decorator\n    else:\n        return lambda fn: fn\n\ndef synchronized(lock_attr='_lock'):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(self, *args, **kwargs):\n            lock = getattr(self, lock_attr)\n            try:\n                lock.acquire()\n                return func(self, *args, **kwargs)\n            finally:\n                lock.release()\n        return wrapper\n    return decorator\n\ndef frame_codeinfo(fframe, back=0):\n    \"\"\" Return a (filename, line) pair for a previous frame .\n        @return (filename, lineno) where lineno is either int or string==''\n    \"\"\"\n    \n    try:\n        if not fframe:\n            return \"<unknown>\", ''\n        for i in range(back):\n            fframe = fframe.f_back\n        try:\n            fname = getsourcefile(fframe)\n        except TypeError:\n            fname = '<builtin>'\n        lineno = fframe.f_lineno or ''\n        return fname, lineno\n    except Exception:\n        return \"<unknown>\", ''\n\ndef compose(a, b):\n    \"\"\" Composes the callables ``a`` and ``b``. ``compose(a, b)(*args)`` is\n    equivalent to ``a(b(*args))``.\n\n    Can be used as a decorator by partially applying ``a``::\n\n         @partial(compose, a)\n         def b():\n            ...\n    \"\"\"\n    @wraps(b)\n    def wrapper(*args, **kwargs):\n        return a(b(*args, **kwargs))\n    return wrapper\n\n\nclass _ClassProperty(property):\n    def __get__(self, cls, owner):\n        return self.fget.__get__(None, owner)()\n\ndef classproperty(func):\n    return _ClassProperty(classmethod(func))\n", "license": "agpl-3.0"}
{"id": "1482bedf228b5835b35c6554b073593682456168", "path": "static/Brython3.1.1-20150328-091302/Lib/locale.py", "repo_name": "40023154/final0627", "content": "def getdefaultlocale():\n    return __BRYTHON__.language,None\n\ndef localeconv():\n        \"\"\" localeconv() -> dict.\n            Returns numeric and monetary locale-specific parameters.\n        \"\"\"\n        # 'C' locale default values\n        return {'grouping': [127],\n                'currency_symbol': '',\n                'n_sign_posn': 127,\n                'p_cs_precedes': 127,\n                'n_cs_precedes': 127,\n                'mon_grouping': [],\n                'n_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'decimal_point': '.',\n                'negative_sign': '',\n                'positive_sign': '',\n                'p_sep_by_space': 127,\n                'int_curr_symbol': '',\n                'p_sign_posn': 127,\n                'thousands_sep': '',\n                'mon_thousands_sep': '',\n                'frac_digits': 127,\n                'mon_decimal_point': '',\n                'int_frac_digits': 127}\n\ndef setlocale(category, value=None):\n        \"\"\" setlocale(integer,string=None) -> string.\n            Activates/queries locale processing.\n        \"\"\"\n        if value not in (None, '', 'C'):\n            raise Error('_locale emulation only supports \"C\" locale')\n        return 'C'\n\nCHAR_MAX = 127\nLC_ALL = 6\nLC_COLLATE = 3\nLC_CTYPE = 0\nLC_MESSAGES = 5\nLC_MONETARY = 4\nLC_NUMERIC = 1\nLC_TIME = 2\nError = ValueError\n\n\ndef getlocale(category=LC_CTYPE):\n\n    \"\"\" Returns the current setting for the given locale category as\n        tuple (language code, encoding).\n\n        category may be one of the LC_* value except LC_ALL. It\n        defaults to LC_CTYPE.\n\n        Except for the code 'C', the language code corresponds to RFC\n        1766.  code and encoding can be None in case the values cannot\n        be determined.\n\n    \"\"\"\n    return None, None\n", "license": "gpl-3.0"}
{"id": "8272d76503690b180ef42e0f05d3f0b96372ecb2", "path": "pelicanconf.py", "repo_name": "40423115/2016fallcadp_hw", "content": "#!/usr/bin/env python\n# -*- coding: utf-8 -*- #\nfrom __future__ import unicode_literals\n\nAUTHOR = 'KMOL'\nSITENAME = '2016Fall \u8ab2\u7a0b\u7db2\u8a8c (\u864e\u5c3e\u79d1\u5927MDE)'\n# \u4e0d\u8981\u7528\u6587\u7ae0\u6240\u5728\u76ee\u9304\u4f5c\u70ba\u985e\u5225\nUSE_FOLDER_AS_CATEGORY = False\n\n#PATH = 'content'\n\n#OUTPUT_PATH = 'output'\n\nTIMEZONE = 'Asia/Taipei'\n\nDEFAULT_LANG = 'en'\n\n# Feed generation is usually not desired when developing\nFEED_ALL_ATOM = None\nCATEGORY_FEED_ATOM = None\nTRANSLATION_FEED_ATOM = None\nAUTHOR_FEED_ATOM = None\nAUTHOR_FEED_RSS = None\n\n# Blogroll\nLINKS = (('Pelican', 'http://getpelican.com/'),\n         ('pelican-bootstrap3', 'https://github.com/DandyDev/pelican-bootstrap3/'),\n         ('pelican-plugins', 'https://github.com/getpelican/pelican-plugins'),\n         ('Tipue search', 'https://github.com/Tipue/Tipue-Search'),)\n\n# Social widget\n#SOCIAL = (('You can add links in your config file', '#'),('Another social link', '#'),)\n\nDEFAULT_PAGINATION = 10\n\n# Uncomment following line if you want document-relative URLs when developing\n#RELATIVE_URLS = True\n\n# \u5fc5\u9808\u7d55\u5c0d\u76ee\u9304\u6216\u76f8\u5c0d\u65bc\u8a2d\u5b9a\u6a94\u6848\u6240\u5728\u76ee\u9304\nPLUGIN_PATHS = ['plugin']\nPLUGINS = ['liquid_tags.notebook', 'summary', 'tipue_search', 'sitemap']\n\n# for sitemap plugin\nSITEMAP = {\n    'format': 'xml',\n    'priorities': {\n        'articles': 0.5,\n        'indexes': 0.5,\n        'pages': 0.5\n    },\n    'changefreqs': {\n        'articles': 'monthly',\n        'indexes': 'daily',\n        'pages': 'monthly'\n    }\n}\n\n# search is for Tipue search\nDIRECT_TEMPLATES = (('index', 'tags', 'categories', 'authors', 'archives', 'search'))\n\n# for pelican-bootstrap3 theme settings\n#TAG_CLOUD_MAX_ITEMS = 50\nDISPLAY_CATEGORIES_ON_SIDEBAR = True\nDISPLAY_RECENT_POSTS_ON_SIDEBAR = True\nDISPLAY_TAGS_ON_SIDEBAR = True\nDISPLAY_TAGS_INLINE = True\nTAGS_URL = \"tags.html\"\nCATEGORIES_URL = \"categories.html\"\n#SHOW_ARTICLE_AUTHOR = True\n\n#MENUITEMS = [('Home', '/'), ('Archives', '/archives.html'), ('Search', '/search.html')]\n", "license": "agpl-3.0"}
{"id": "00712420ee075820d713e74a24610ee809def44e", "path": "topi/python/topi/tensor.py", "repo_name": "sxjscience/tvm", "content": "# Licensed to the Apache Software Foundation (ASF) under one\n# or more contributor license agreements.  See the NOTICE file\n# distributed with this work for additional information\n# regarding copyright ownership.  The ASF licenses this file\n# to you under the Apache License, Version 2.0 (the\n# \"License\"); you may not use this file except in compliance\n# with the License.  You may obtain a copy of the License at\n#\n#   http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing,\n# software distributed under the License is distributed on an\n# \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY\n# KIND, either express or implied.  See the License for the\n# specific language governing permissions and limitations\n# under the License.\n# pylint: disable=invalid-name,consider-using-enumerate,unused-argument,len-as-condition\n\"\"\"Elementwise operators\"\"\"\nfrom __future__ import absolute_import as _abs\nfrom . import cpp\n\ndef elemwise_sum(xs):\n    \"\"\"Perform element-wise sum on inputs\n\n    Parameters\n    ----------\n    xs : list of tvm.te.Tensor\n        Input arguments.\n\n    Returns\n    -------\n    y : tvm.te.Tensor\n        The result.\n    \"\"\"\n    return cpp.elemwise_sum(xs)\n\n\ndef full(shape, dtype, fill_value):\n    \"\"\"Fill tensor with fill_value\n\n    Parameters\n    ----------\n    shape : tuple\n        Input tensor shape.\n    dtype : str\n        Data type\n    fill_value : float\n        Value to be filled\n\n    Returns\n    -------\n    y : tvm.te.Tensor\n        The result.\n    \"\"\"\n    return cpp.full(shape, dtype, fill_value)\n\n\ndef full_like(x, fill_value):\n    \"\"\"Construct a tensor with same shape as input tensor,\n       then fill tensor with fill_value.\n\n    Parameters\n    ----------\n    x : tvm.te.Tensor\n        Input argument.\n    fill_value : float\n        Value to be filled\n\n    Returns\n    -------\n    y : tvm.te.Tensor\n        The result.\n    \"\"\"\n    return cpp.full_like(x, fill_value)\n", "license": "apache-2.0"}
{"id": "878eaf4215db0b9e97c12e0b03b831d26acadc17", "path": "tests/spatial/search_indexes.py", "repo_name": "ericholscher/django-haystack", "content": "from haystack import indexes\nfrom spatial.models import Checkin\n\n\nclass CheckinSearchIndex(indexes.SearchIndex, indexes.Indexable):\n    text = indexes.CharField(document=True)\n    username = indexes.CharField(model_attr='username')\n    comment = indexes.CharField(model_attr='comment')\n    # Again, if you were using GeoDjango, this could be just:\n    #   location = indexes.LocationField(model_attr='location')\n    location = indexes.LocationField(model_attr='get_location')\n    created = indexes.DateTimeField(model_attr='created')\n\n    def get_model(self):\n        return Checkin\n\n    def prepare_text(self, obj):\n        # Because I don't feel like creating a template just for this.\n        return '\\n'.join([obj.comment, obj.username])\n", "license": "bsd-3-clause"}
{"id": "a8044215c2241197b1e5ef5bed736b3adee8e7d3", "path": "Raspberry/old/resto/2013.12.16/sck.py", "repo_name": "sernaleon/charlie", "content": "import sys, socket, struct\n\ns=socket.socket()\ns=socket.socket(socket.AF_INET,socket.SOCK_DGRAM)\ns.bind(('192.168.0.123', 5000))  \n\nwhile True:\n\tdata,addr=s.recvfrom(3)\n\t\n\tcmd = struct.unpack('B', data[0])[0]\n\tp1  = struct.unpack('B', data[1])[0]\n\tp2  = struct.unpack('B', data[2])[0]\n\tprint(addr,cmd,p1,p2)\t\n\t\ns.close()  \n\n", "license": "apache-2.0"}
{"id": "b61d965da90e1964ec6d914c8c55295db9d60cc2", "path": "test/integration/targets/async_extra_data/library/junkping.py", "repo_name": "Shaps/ansible", "content": "#!/usr/bin/python\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport json\n\n\ndef main():\n    print(\"junk_before_module_output\")\n    print(json.dumps(dict(changed=False, source='user')))\n    print(\"junk_after_module_output\")\n\n\nif __name__ == '__main__':\n    main()\n", "license": "gpl-3.0"}
{"id": "4a3ba3577e1aafa0fce8abfca85a1f7a8804b84b", "path": "tests/servers/test_basehttp.py", "repo_name": "poiati/django", "content": "from io import BytesIO\n\nfrom django.core.handlers.wsgi import WSGIRequest\nfrom django.core.servers.basehttp import WSGIRequestHandler\nfrom django.test import SimpleTestCase\nfrom django.test.client import RequestFactory\nfrom django.test.utils import captured_stderr\n\n\nclass Stub(object):\n    def __init__(self, **kwargs):\n        self.__dict__.update(kwargs)\n\n\nclass WSGIRequestHandlerTestCase(SimpleTestCase):\n\n    def test_log_message(self):\n        request = WSGIRequest(RequestFactory().get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        with captured_stderr() as stderr:\n            handler.log_message('GET %s %s', 'A', 'B')\n        self.assertIn('] GET A B', stderr.getvalue())\n\n    def test_https(self):\n        request = WSGIRequest(RequestFactory().get('/').environ)\n        request.makefile = lambda *args, **kwargs: BytesIO()\n\n        handler = WSGIRequestHandler(request, '192.168.0.2', None)\n\n        with captured_stderr() as stderr:\n            handler.log_message(\"GET %s %s\", str('\\x16\\x03'), \"4\")\n            self.assertIn(\n                \"You're accessing the development server over HTTPS, \"\n                \"but it only supports HTTP.\",\n                stderr.getvalue()\n            )\n\n    def test_strips_underscore_headers(self):\n        \"\"\"WSGIRequestHandler ignores headers containing underscores.\n\n        This follows the lead of nginx and Apache 2.4, and is to avoid\n        ambiguity between dashes and underscores in mapping to WSGI environ,\n        which can have security implications.\n        \"\"\"\n        def test_app(environ, start_response):\n            \"\"\"A WSGI app that just reflects its HTTP environ.\"\"\"\n            start_response('200 OK', [])\n            http_environ_items = sorted(\n                '%s:%s' % (k, v) for k, v in environ.items()\n                if k.startswith('HTTP_')\n            )\n            yield (','.join(http_environ_items)).encode('utf-8')\n\n        rfile = BytesIO()\n        rfile.write(b\"GET / HTTP/1.0\\r\\n\")\n        rfile.write(b\"Some-Header: good\\r\\n\")\n        rfile.write(b\"Some_Header: bad\\r\\n\")\n        rfile.write(b\"Other_Header: bad\\r\\n\")\n        rfile.seek(0)\n\n        # WSGIRequestHandler closes the output file; we need to make this a\n        # no-op so we can still read its contents.\n        class UnclosableBytesIO(BytesIO):\n            def close(self):\n                pass\n\n        wfile = UnclosableBytesIO()\n\n        def makefile(mode, *a, **kw):\n            if mode == 'rb':\n                return rfile\n            elif mode == 'wb':\n                return wfile\n\n        request = Stub(makefile=makefile)\n        server = Stub(base_environ={}, get_app=lambda: test_app)\n\n        # We don't need to check stderr, but we don't want it in test output\n        with captured_stderr():\n            # instantiating a handler runs the request as side effect\n            WSGIRequestHandler(request, '192.168.0.2', server)\n\n        wfile.seek(0)\n        body = list(wfile.readlines())[-1]\n\n        self.assertEqual(body, b'HTTP_SOME_HEADER:good')\n", "license": "bsd-3-clause"}
{"id": "3011ec57ef725d8766077712c7c18513e55210e3", "path": "apps/translations/utils.py", "repo_name": "magopian/olympia", "content": "from django.utils.encoding import force_unicode\n\nimport html5lib\nimport jinja2\n\n\ndef truncate_text(text, limit, killwords=False, end='...'):\n    \"\"\"Return as many characters as possible without going over the limit.\n\n    Return the truncated text and the characters left before the limit, if any.\n\n    \"\"\"\n    text = text.strip()\n    text_length = len(text)\n\n    if text_length < limit:\n        return text, limit - text_length\n\n    # Explicitly add \"end\" in any case, as Jinja can't know we're truncating\n    # for real here, even though we might be at the end of a word.\n    text = jinja2.filters.do_truncate(text, limit, killwords, end='')\n    return text + end, 0\n\n\ndef trim(tree, limit, killwords, end):\n    \"\"\"Truncate the text of an html5lib tree.\"\"\"\n    if tree.text:  # Root node's text.\n        tree.text, limit = truncate_text(tree.text, limit, killwords, end)\n    for child in tree:  # Immediate children.\n        if limit <= 0:\n            # We reached the limit, remove all remaining children.\n            tree.remove(child)\n        else:\n            # Recurse on the current child.\n            _parsed_tree, limit = trim(child, limit, killwords, end)\n    if tree.tail:  # Root node's tail text.\n        if limit <= 0:\n            tree.tail = ''\n        else:\n            tree.tail, limit = truncate_text(tree.tail, limit, killwords, end)\n    return tree, limit\n\n\ndef text_length(tree):\n    \"\"\"Find the length of the text content, excluding markup.\"\"\"\n    total = 0\n    for node in tree.getiterator():  # Traverse all the tree nodes.\n        # In etree, a node has a text and tail attribute.\n        # Eg: \"<b>inner text</b> tail text <em>inner text</em>\".\n        if node.text:\n            total += len(node.text.strip())\n        if node.tail:\n            total += len(node.tail.strip())\n    return total\n\n\ndef truncate(html, length, killwords=False, end='...'):\n    \"\"\"\n    Return a slice of ``html`` <= length chars.\n\n    killwords and end are currently ignored.\n\n    ONLY USE FOR KNOWN-SAFE HTML.\n    \"\"\"\n    tree = html5lib.parseFragment(html)\n    if text_length(tree) <= length:\n        return jinja2.Markup(html)\n    else:\n        # Get a truncated version of the tree.\n        short, _ = trim(tree, length, killwords, end)\n\n        # Serialize the parsed tree back to html.\n        walker = html5lib.treewalkers.getTreeWalker('etree')\n        stream = walker(short)\n        serializer = html5lib.serializer.htmlserializer.HTMLSerializer(\n            quote_attr_values=True, omit_optional_tags=False)\n        return jinja2.Markup(force_unicode(serializer.render(stream)))\n\n\ndef transfield_changed(field, initial, data):\n    \"\"\"\n    For forms, compares initial data against cleaned_data for TransFields.\n    Returns True if data is the same. Returns False if data is different.\n\n    Arguments:\n    field -- name of the form field as-is.\n    initial -- data in the form of {'description_en-us': 'x',\n                                    'description_en-br': 'y'}\n    data -- cleaned data in the form of {'description': {'init': '',\n                                                         'en-us': 'x',\n                                                         'en-br': 'y'}\n    \"\"\"\n    initial = [(k, v.localized_string) for k, v in initial.iteritems()\n               if '%s_' % field in k and v is not None]\n    data = [('%s_%s' % (field, k), v) for k, v in data[field].iteritems()\n            if k != 'init']\n    return set(initial) != set(data)\n", "license": "bsd-3-clause"}
{"id": "d61b1b43239304550c820a71abe0318c5bc29556", "path": "conjure/search.py", "repo_name": "GGOutfitters/conjure", "content": "import collections\nimport multiprocessing\nimport pyes\nfrom pyes.exceptions import NotFoundException\nimport pymongo\nimport logging\nimport time\nfrom bson.objectid import ObjectId\nfrom .spec import QuerySpecification\nfrom .oplog_watcher import OplogWatcher\nimport base64\n\n_indexes = []\n_connections = {}\n\n\nclass IndexMeta(type):\n    def __new__(mcs, name, bases, attrs):\n        metaclass = attrs.get('__metaclass__')\n        super_new = super(IndexMeta, mcs).__new__\n\n        if metaclass and issubclass(metaclass, IndexMeta):\n            return super_new(mcs, name, bases, attrs)\n\n        terms = {}\n\n        for attr_name, attr_value in attrs.items():\n            if isinstance(attr_value, Term):\n                term = attr_value\n\n                term.name = attr_name\n\n                if term.index_name is None:\n                    term.index_name = term.name\n\n                terms[attr_name] = attr_value\n\n                del attrs[attr_name]\n\n        attrs['terms'] = terms\n\n        meta = attrs.pop('Meta', None)\n\n        attrs['_meta'] = {\n            'host': getattr(meta, 'host'),\n            'model': getattr(meta, 'model'),\n            'spec': getattr(meta, 'spec', QuerySpecification()),\n        }\n\n        new_cls = super_new(mcs, name, bases, attrs)\n\n        index = new_cls.instance()\n\n        _indexes.append(index)\n        index.model._search_index = index\n\n        return new_cls\n\n\nclass Index(object):\n    __metaclass__ = IndexMeta\n\n    def __init__(self):\n        self._meta = self.__class__._meta\n        self.model = self._meta['model']\n        self.spec = self._meta['spec']\n        self.uri, _, db = self.model._meta['db'].rpartition('/')\n        self.namespace = '%s-%s' % (db, self.model._meta['collection'])\n        self.doc_type = self.model._name\n\n    @classmethod\n    def instance(cls):\n        if not hasattr(cls, '_instance'):\n            cls._instance = cls()\n\n        return cls._instance\n\n    @property\n    def connection(self):\n        if not hasattr(self, '_connection'):\n            host = self._meta['host']\n\n            if host not in _connections:\n                _connections[host] = pyes.ES(host)\n\n            self._connection = _connections[host]\n\n        return self._connection\n\n    def search(self, query, page=1, limit=5, filters=None):\n        return search(self, query, page, limit, filters)\n\n    def indexer(self):\n        return Indexer(self)\n\n\nclass Term(object):\n    def __init__(self, index_name=None, index=True, boost=1.0, null_value=None, coerce=None):\n        self.name = None\n        self.index_name = index_name\n        self.index = index\n        self.boost = boost\n        self.null_value = null_value\n        self.coerce = coerce\n\n\nclass Indexer(object):\n    def __init__(self, index):\n        self.index = index\n\n    def index_document(self, obj, bulk=False):\n        doc = {}\n\n        for term in self.index.terms.values():\n            if not term.index:\n                continue\n\n            value = getattr(obj, term.name)\n\n            if value is not None:\n                if isinstance(value, ObjectId):\n                    value = str(value)\n\n                if term.coerce is not None:\n                    value = term.coerce(value)\n\n                doc[term.index_name] = value\n\n        self._execute(self.index.connection.index, doc, self.index.namespace,\n                      self.index.doc_type, id=base64.b64encode(str(obj.id)), bulk=bulk)\n\n    def delete_document(self, doc_id):\n        self._execute(self.index.connection.delete, self.index.namespace,\n            self.index.doc_type, base64.b64encode(str(doc_id)))\n\n    def insert(self, obj):\n        obj = self.index.model.to_python(obj)\n        logging.info('Indexing %s (%s)' % (self.index.model._name, obj.id))\n        self.index_document(obj)\n\n    def update(self, obj_id, raw):\n        o = raw['o']\n\n        fields = self.index.terms.keys()\n\n        if o.has_key('$set') and len(set(fields) - set(o['$set'].keys())) < len(fields):\n            obj = self.index.model.objects.only(*fields).filter(self.index.spec).with_id(obj_id)\n\n            if obj is not None:\n                logging.info('Updating %s (%s)' % (self.index.model._name, obj.id))\n                self.index_document(obj)\n            else:\n                self.delete(obj_id)\n\n    def delete(self, obj_id):\n        logging.info('Deleting %s (%s)' % (self.index.model._name, obj_id))\n        self.delete_document(obj_id)\n\n    def _execute(self, func, *args, **kwargs):\n        attempts = 0\n\n        while attempts < 5:\n            try:\n                func(*args, **kwargs)\n                break\n            except NotFoundException:\n                break\n            except Exception:\n                attempts += 1\n                logging.warning('Retrying... (%d)' % attempts, exc_info=True)\n                time.sleep(1)\n\n\nclass ResultSet(object):\n    def __init__(self, objects=None, total=0, elapsed_time=0, max_score=0):\n        self.objects = objects or []\n        self.meta = {}\n        self.total = total\n        self.elapsed_time = elapsed_time\n        self.max_score = max_score\n\n    def append(self, value, meta):\n        if value is not None:\n            self.objects.append(value)\n            self.meta[value] = meta\n\n    def has_more(self):\n        return len(self.objects) < self.total\n\n    def __len__(self):\n        return self.objects.__len__()\n\n    def __iter__(self):\n        for obj in self.objects:\n            yield obj, self.meta[obj]\n\n\ndef search(indexes, query, page=1, limit=5, filters=None):\n    if not isinstance(indexes, list):\n        indexes = [indexes]\n\n    namespaces = []\n    models = {}\n\n    for i, index in enumerate(indexes):\n        if not isinstance(index, Index):\n            model = index\n\n            for index in _indexes:\n                if index.model == model:\n                    indexes[i] = index\n                    break\n\n        namespaces.append(index.namespace)\n        models[index.namespace] = index.model\n\n    result_set = ResultSet()\n    result_set.query = query\n\n    if isinstance(query, (str, unicode)):\n        if query.endswith(':'):\n            query = query[:-1]\n\n        if any([op in query for op in ['?', '*', '~', 'OR', 'AND', '+', 'NOT', '-', ':']]):\n            query = pyes.StringQuery(query)\n        else:\n            query = pyes.StringQuery(query + '*')\n\n    if not isinstance(query, pyes.FilteredQuery) and filters:\n        term_filter = pyes.TermFilter()\n\n        for field, value in filters.iteritems():\n            term_filter.add(field, value)\n\n        query = pyes.FilteredQuery(query, term_filter)\n\n    page = int(page)\n    limit = int(limit)\n    skip = (page - 1) * limit\n\n    try:\n        response = _indexes[0].connection.search(query, indices=namespaces, **{\n            'from': str(skip),\n            'size': str(limit)\n        })\n\n        result_set.total = response.total\n        result_set.elapsed_time = response._results['took'] / 1000.0\n        result_set.max_score = response.max_score\n\n        for i, hit in enumerate(response.hits):\n            result_set.append(models[hit['_index']].objects.with_id(base64.b64decode(hit['_id'])), {\n                'rank': skip + i + 1,\n                'score': hit['_score'],\n                'relevance': int(hit['_score'] / result_set.max_score * 100)\n            })\n    except pyes.exceptions.SearchPhaseExecutionException:\n        pass\n\n    return result_set\n\n\ndef reindex(only=None):\n    logging.info('Reindexing...')\n\n    for index in _indexes:\n        if only and index.namespace not in only:\n            continue\n\n        try:\n            index.connection.delete_index(index.namespace)\n        except pyes.exceptions.IndexMissingException:\n            pass\n\n        index.connection.create_index(index.namespace)\n\n        objects = index.model.objects.only(*index.terms.keys()).filter(index.spec)\n        count = objects.count()\n\n        logging.info('%d object(s) from %s' % (count, index.namespace))\n\n        indexer = Indexer(index)\n\n        for i, obj in enumerate(objects):\n            i += 1\n\n            if not i % 10000:\n                logging.info('%d/%d', i, count)\n\n            indexer.index_document(obj, bulk=True)\n\n        indexer.index.connection.force_bulk()\n\n    logging.info('Done!')\n\n\ndef watch():\n    hosts = collections.defaultdict(list)\n\n    global _indexes\n\n    for index in _indexes:\n        hosts[index.uri].append(index)\n\n    def target(uri, indexes):\n        namespaces = [index.namespace.replace('-', '.') for index in indexes]\n\n        logging.info('Watching %s' % namespaces)\n\n        oplog_watcher = OplogWatcher(pymongo.Connection(uri), namespaces=namespaces)\n\n        for index in indexes:\n            indexer = index.indexer()\n\n            for op in ('insert', 'update', 'delete',):\n                oplog_watcher.add_handler(index.namespace.replace('-', '.'), op, getattr(indexer, op))\n\n        oplog_watcher.start()\n\n    if len(hosts) > 1:\n        for uri, _indexes in hosts.items():\n            multiprocessing.Process(target=target, args=(uri, _indexes)).start()\n    else:\n        target(*hosts.items()[0])\n\n    while True:\n        time.sleep(1)\n", "license": "mit"}
{"id": "a511db9bf3ed4ea4d5e85885ea150a0b4a938af4", "path": "backend/resorts/models/conditions.py", "repo_name": "racmariano/skidom", "content": "# -*- coding: utf-8 -*-\r\nfrom __future__ import unicode_literals\r\n\r\nfrom .resort import Resort\r\n\r\nfrom django.db import models\r\nfrom django.contrib.postgres.fields import ArrayField\r\n\r\nfrom dynamic_scraper.models import Scraper, SchedulerRuntime\r\nfrom scrapy_djangoitem import DjangoItem\r\n\r\nimport datetime\r\n\r\n# Past and forecasted conditions for a resort\r\nclass Conditions(models.Model):\r\n\r\n    # Hard-coded attributes needed for scraping\r\n    resort = models.ForeignKey(Resort, null = True, default=6)\r\n    conditions_page_url = models.URLField(blank = True)\r\n    checker_runtime = models.ForeignKey(SchedulerRuntime, blank = True, null = True, on_delete = models.SET_NULL)\r\n\r\n    # Attributes collected during scraping\r\n    date = models.DateField(default = datetime.date.today)\r\n    base_temp = models.DecimalField(max_digits = 6, decimal_places = 2, default = 0)\r\n    summit_temp = models.DecimalField(max_digits = 6, decimal_places = 2, default = 0)\r\n    wind_speed = models.DecimalField(max_digits = 6, decimal_places = 2, default = 0)\r\n    base_depth = models.DecimalField(max_digits = 6, decimal_places = 2, default = 0)\r\n    num_trails_open = models.IntegerField(default = 0)\r\n    new_snow_24_hr = models.IntegerField(default = 0)\r\n    #past_n_day_snowfall = ArrayField(models.DecimalField(max_digits = 6, decimal_places = 2, default = 0), size = 15)\r\n    #past_n_day_wind_speed = ArrayField(models.DecimalField(max_digits = 6, decimal_places = 2, default = 0), size = 15)\r\n    #future_n_day_snowfall = ArrayField(models.DecimalField(max_digits = 6, decimal_places = 2, default = 0), size = 15)\r\n    #future_n_day_wind_speed = ArrayField(models.DecimalField(max_digits = 6, decimal_places = 2, default = 0), size = 15)\r\n\r\n    # For database querying\r\n    unique_id = models.CharField(default='', max_length = 200)\r\n\r\n    def __init__(self, *args, **kwargs):\r\n        super(Conditions, self).__init__(*args, **kwargs)\r\n        if not self.id:\r\n            day = datetime.date.today\r\n            self.conditions_page_url = self.resort.conditions_page_url\r\n            self.unique_id = self.resort.name+str(datetime.date.today())\r\n\r\n    def __unicode__(self):\r\n        return self.resort.name+\": \"+str(self.date)\r\n\r\n    def __str__(self):\r\n        return self.resort.name+\": \"+str(self.date)\r\n\r\n    class Meta:\r\n        verbose_name_plural = \"Conditions\"\r\n\r\nclass ConditionsItem(DjangoItem):\r\n    django_model = Conditions\r\n", "license": "mit"}
{"id": "17609b480b996291e4211ddf661c2def50a959b7", "path": "nova/virt/xenapi/client/session.py", "repo_name": "xuweiliang/Codelibrary", "content": "# Copyright 2013 OpenStack Foundation\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nimport ast\nimport contextlib\n\ntry:\n    import cPickle as pickle\nexcept ImportError:\n    import pickle\n\nimport errno\nimport socket\nimport time\n\nfrom eventlet import queue\nfrom eventlet import timeout\nfrom oslo_log import log as logging\nfrom oslo_utils import versionutils\nfrom six.moves import http_client\nfrom six.moves import range\nfrom six.moves import urllib\n\ntry:\n    import xmlrpclib\nexcept ImportError:\n    import six.moves.xmlrpc_client as xmlrpclib\n\nimport nova.conf\nfrom nova import context\nfrom nova import exception\nfrom nova.i18n import _, _LE, _LW\nfrom nova import objects\nfrom nova import version\nfrom nova.virt.xenapi.client import objects as cli_objects\nfrom nova.virt.xenapi import pool\nfrom nova.virt.xenapi import pool_states\n\nLOG = logging.getLogger(__name__)\n\nCONF = nova.conf.CONF\n\n\ndef apply_session_helpers(session):\n    session.VM = cli_objects.VM(session)\n    session.SR = cli_objects.SR(session)\n    session.VDI = cli_objects.VDI(session)\n    session.VIF = cli_objects.VIF(session)\n    session.VBD = cli_objects.VBD(session)\n    session.PBD = cli_objects.PBD(session)\n    session.PIF = cli_objects.PIF(session)\n    session.VLAN = cli_objects.VLAN(session)\n    session.host = cli_objects.Host(session)\n    session.network = cli_objects.Network(session)\n    session.pool = cli_objects.Pool(session)\n\n\nclass XenAPISession(object):\n    \"\"\"The session to invoke XenAPI SDK calls.\"\"\"\n\n    # This is not a config option as it should only ever be\n    # changed in development environments.\n    # MAJOR VERSION: Incompatible changes with the plugins\n    # MINOR VERSION: Compatible changes, new plguins, etc\n    PLUGIN_REQUIRED_VERSION = '1.7'\n\n    def __init__(self, url, user, pw):\n        version_string = version.version_string_with_package()\n        self.nova_version = ('%(vendor)s %(product)s %(version)s' %\n                             {'vendor': version.vendor_string(),\n                              'product': version.product_string(),\n                              'version': version_string})\n        import XenAPI\n        self.XenAPI = XenAPI\n        self._sessions = queue.Queue()\n        self.is_slave = False\n        self.host_checked = False\n        exception = self.XenAPI.Failure(_(\"Unable to log in to XenAPI \"\n                                          \"(is the Dom0 disk full?)\"))\n        self.url = self._create_first_session(url, user, pw, exception)\n        self._populate_session_pool(url, user, pw, exception)\n        self.host_uuid = self._get_host_uuid()\n        self.host_ref = self._get_host_ref()\n        self.product_version, self.product_brand = \\\n            self._get_product_version_and_brand()\n\n        self._verify_plugin_version()\n\n        apply_session_helpers(self)\n\n    def _login_with_password(self, user, pw, session, exception):\n        with timeout.Timeout(CONF.xenserver.login_timeout, exception):\n            session.login_with_password(user, pw,\n                                        self.nova_version, 'OpenStack')\n\n    def _verify_plugin_version(self):\n        requested_version = self.PLUGIN_REQUIRED_VERSION\n        current_version = self.call_plugin_serialized(\n            'nova_plugin_version', 'get_version')\n\n        if not versionutils.is_compatible(requested_version, current_version):\n            raise self.XenAPI.Failure(\n                _(\"Plugin version mismatch (Expected %(exp)s, got %(got)s)\") %\n                {'exp': requested_version, 'got': current_version})\n\n    def _create_first_session(self, url, user, pw, exception):\n        try:\n            session = self._create_session_and_login(url, user, pw, exception)\n        except self.XenAPI.Failure as e:\n            # if user and pw of the master are different, we're doomed!\n            if e.details[0] == 'HOST_IS_SLAVE':\n                master = e.details[1]\n                url = pool.swap_xapi_host(url, master)\n                session = self._create_session_and_login(url, user, pw,\n                                                         exception)\n                self.is_slave = True\n            else:\n                raise\n        self._sessions.put(session)\n        return url\n\n    def _populate_session_pool(self, url, user, pw, exception):\n        for i in range(CONF.xenserver.connection_concurrent - 1):\n            session = self._create_session_and_login(url, user, pw, exception)\n            self._sessions.put(session)\n\n    def _get_host_uuid(self):\n        if self.is_slave:\n            aggr = objects.AggregateList.get_by_host(\n                context.get_admin_context(),\n                CONF.host, key=pool_states.POOL_FLAG)[0]\n            if not aggr:\n                LOG.error(_LE('Host is member of a pool, but DB '\n                              'says otherwise'))\n                raise exception.AggregateHostNotFound()\n            return aggr.metadata[CONF.host]\n        else:\n            with self._get_session() as session:\n                host_ref = session.xenapi.session.get_this_host(session.handle)\n                return session.xenapi.host.get_uuid(host_ref)\n\n    def _get_product_version_and_brand(self):\n        \"\"\"Return a tuple of (major, minor, rev) for the host version and\n        a string of the product brand.\n        \"\"\"\n        software_version = self._get_software_version()\n\n        product_version_str = software_version.get('product_version')\n        # Product version is only set in some cases (e.g. XCP, XenServer) and\n        # not in others (e.g. xenserver-core, XAPI-XCP).\n        # In these cases, the platform version is the best number to use.\n        if product_version_str is None:\n            product_version_str = software_version.get('platform_version',\n                                                       '0.0.0')\n        product_brand = software_version.get('product_brand')\n        product_version = versionutils.convert_version_to_tuple(\n                                                        product_version_str)\n\n        return product_version, product_brand\n\n    def _get_software_version(self):\n        return self.call_xenapi('host.get_software_version', self.host_ref)\n\n    def get_session_id(self):\n        \"\"\"Return a string session_id.  Used for vnc consoles.\"\"\"\n        with self._get_session() as session:\n            return str(session._session)\n\n    @contextlib.contextmanager\n    def _get_session(self):\n        \"\"\"Return exclusive session for scope of with statement.\"\"\"\n        session = self._sessions.get()\n        try:\n            yield session\n        finally:\n            self._sessions.put(session)\n\n    def _get_host_ref(self):\n        \"\"\"Return the xenapi host on which nova-compute runs on.\"\"\"\n        with self._get_session() as session:\n            return session.xenapi.host.get_by_uuid(self.host_uuid)\n\n    def call_xenapi(self, method, *args):\n        \"\"\"Call the specified XenAPI method on a background thread.\"\"\"\n        with self._get_session() as session:\n            return session.xenapi_request(method, args)\n\n    def call_plugin(self, plugin, fn, args):\n        \"\"\"Call host.call_plugin on a background thread.\"\"\"\n        # NOTE(armando): pass the host uuid along with the args so that\n        # the plugin gets executed on the right host when using XS pools\n        args['host_uuid'] = self.host_uuid\n\n        with self._get_session() as session:\n            return self._unwrap_plugin_exceptions(\n                                 session.xenapi.host.call_plugin,\n                                 self.host_ref, plugin, fn, args)\n\n    def call_plugin_serialized(self, plugin, fn, *args, **kwargs):\n        params = {'params': pickle.dumps(dict(args=args, kwargs=kwargs))}\n        rv = self.call_plugin(plugin, fn, params)\n        return pickle.loads(rv)\n\n    def call_plugin_serialized_with_retry(self, plugin, fn, num_retries,\n                                          callback, retry_cb=None, *args,\n                                          **kwargs):\n        \"\"\"Allows a plugin to raise RetryableError so we can try again.\"\"\"\n        attempts = num_retries + 1\n        sleep_time = 0.5\n        for attempt in range(1, attempts + 1):\n            try:\n                if attempt > 1:\n                    time.sleep(sleep_time)\n                    sleep_time = min(2 * sleep_time, 15)\n\n                callback_result = None\n                if callback:\n                    callback_result = callback(kwargs)\n\n                msg = ('%(plugin)s.%(fn)s attempt %(attempt)d/%(attempts)d, '\n                       'callback_result: %(callback_result)s')\n                LOG.debug(msg,\n                          {'plugin': plugin, 'fn': fn, 'attempt': attempt,\n                           'attempts': attempts,\n                           'callback_result': callback_result})\n                return self.call_plugin_serialized(plugin, fn, *args, **kwargs)\n            except self.XenAPI.Failure as exc:\n                if self._is_retryable_exception(exc, fn):\n                    LOG.warning(_LW('%(plugin)s.%(fn)s failed. '\n                                    'Retrying call.'),\n                                {'plugin': plugin, 'fn': fn})\n                    if retry_cb:\n                        retry_cb(exc=exc)\n                else:\n                    raise\n            except socket.error as exc:\n                if exc.errno == errno.ECONNRESET:\n                    LOG.warning(_LW('Lost connection to XenAPI during call to '\n                                    '%(plugin)s.%(fn)s.  Retrying call.'),\n                                {'plugin': plugin, 'fn': fn})\n                    if retry_cb:\n                        retry_cb(exc=exc)\n                else:\n                    raise\n\n        raise exception.PluginRetriesExceeded(num_retries=num_retries)\n\n    def _is_retryable_exception(self, exc, fn):\n        _type, method, error = exc.details[:3]\n        if error == 'RetryableError':\n            LOG.debug(\"RetryableError, so retrying %(fn)s\", {'fn': fn},\n                      exc_info=True)\n            return True\n        elif \"signal\" in method:\n            LOG.debug(\"Error due to a signal, retrying %(fn)s\", {'fn': fn},\n                      exc_info=True)\n            return True\n        else:\n            return False\n\n    def _create_session(self, url):\n        \"\"\"Stubout point. This can be replaced with a mock session.\"\"\"\n        self.is_local_connection = url == \"unix://local\"\n        if self.is_local_connection:\n            return self.XenAPI.xapi_local()\n        return self.XenAPI.Session(url)\n\n    def _create_session_and_login(self, url, user, pw, exception):\n        session = self._create_session(url)\n        self._login_with_password(user, pw, session, exception)\n        return session\n\n    def _unwrap_plugin_exceptions(self, func, *args, **kwargs):\n        \"\"\"Parse exception details.\"\"\"\n        try:\n            return func(*args, **kwargs)\n        except self.XenAPI.Failure as exc:\n            LOG.debug(\"Got exception: %s\", exc)\n            if (len(exc.details) == 4 and\n                exc.details[0] == 'XENAPI_PLUGIN_EXCEPTION' and\n                    exc.details[2] == 'Failure'):\n                params = None\n                try:\n                    params = ast.literal_eval(exc.details[3])\n                except Exception:\n                    raise exc\n                raise self.XenAPI.Failure(params)\n            else:\n                raise\n        except xmlrpclib.ProtocolError as exc:\n            LOG.debug(\"Got exception: %s\", exc)\n            raise\n\n    def get_rec(self, record_type, ref):\n        try:\n            return self.call_xenapi('%s.get_record' % record_type, ref)\n        except self.XenAPI.Failure as e:\n            if e.details[0] != 'HANDLE_INVALID':\n                raise\n\n        return None\n\n    def get_all_refs_and_recs(self, record_type):\n        \"\"\"Retrieve all refs and recs for a Xen record type.\n\n        Handles race-conditions where the record may be deleted between\n        the `get_all` call and the `get_record` call.\n        \"\"\"\n\n        return self.call_xenapi('%s.get_all_records' % record_type).items()\n\n    @contextlib.contextmanager\n    def custom_task(self, label, desc=''):\n        \"\"\"Return exclusive session for scope of with statement.\"\"\"\n        name = 'nova-%s' % (label)\n        task_ref = self.call_xenapi(\"task.create\", name,\n                                       desc)\n        try:\n            LOG.debug('Created task %s with ref %s' % (name, task_ref))\n            yield task_ref\n        finally:\n            self.call_xenapi(\"task.destroy\", task_ref)\n            LOG.debug('Destroyed task ref %s' % (task_ref))\n\n    @contextlib.contextmanager\n    def http_connection(session):\n        conn = None\n\n        xs_url = urllib.parse.urlparse(session.url)\n        LOG.debug(\"Creating http(s) connection to %s\" % session.url)\n        if xs_url.scheme == 'http':\n            conn = http_client.HTTPConnection(xs_url.netloc)\n        elif xs_url.scheme == 'https':\n            conn = http_client.HTTPSConnection(xs_url.netloc)\n\n        conn.connect()\n        try:\n            yield conn\n        finally:\n            conn.close()\n", "license": "apache-2.0"}
{"id": "f1bfba0fb1bb1ddc3d3cc5791bf8d00352c9ff68", "path": "statsmodels/duration/tests/survival_enet_r_results.py", "repo_name": "alekz112/statsmodels", "content": "import numpy as np\n\ncoef_50_2_0 = np.array([-0.6748149,0.5219471])\n\ncoef_50_2_1 = np.array([-0.3464841,0.211115])\n\ncoef_100_5_0 = np.array([-0.4839566,-0.3130558,-0.1239565,0.3466049,0.5827503])\n\ncoef_100_5_1 = np.array([-0.1314948,0,0,0.0324285,0.2364489])\n\n", "license": "bsd-3-clause"}
{"id": "c2469f012d10cde582e7c2616e96134774616e46", "path": "tensorflow/python/profiler/pprof_profiler_test.py", "repo_name": "allenlavoie/tensorflow", "content": "# Copyright 2017 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the 'License');\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an 'AS IS' BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Tests for pprof_profiler.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport gzip\n\nfrom proto import profile_pb2\nfrom tensorflow.core.framework import step_stats_pb2\nfrom tensorflow.core.protobuf import config_pb2\nfrom tensorflow.python.framework import constant_op\nfrom tensorflow.python.ops import control_flow_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.profiler import pprof_profiler\n\n\nclass PprofProfilerTest(test.TestCase):\n\n  def testDataEmpty(self):\n    output_dir = test.get_temp_dir()\n    run_metadata = config_pb2.RunMetadata()\n    graph = test.mock.MagicMock()\n    graph.get_operations.return_value = []\n\n    profiles = pprof_profiler.get_profiles(graph, run_metadata)\n    self.assertEquals(0, len(profiles))\n    profile_files = pprof_profiler.profile(\n        graph, run_metadata, output_dir)\n    self.assertEquals(0, len(profile_files))\n\n  def testRunMetadataEmpty(self):\n    output_dir = test.get_temp_dir()\n    run_metadata = config_pb2.RunMetadata()\n    graph = test.mock.MagicMock()\n    op1 = test.mock.MagicMock()\n    op1.name = 'Add/123'\n    op1.traceback = [('a/b/file1', 10, 'some_var')]\n    op1.type = 'add'\n    graph.get_operations.return_value = [op1]\n\n    profiles = pprof_profiler.get_profiles(graph, run_metadata)\n    self.assertEquals(0, len(profiles))\n    profile_files = pprof_profiler.profile(\n        graph, run_metadata, output_dir)\n    self.assertEquals(0, len(profile_files))\n\n  def testValidProfile(self):\n    output_dir = test.get_temp_dir()\n    run_metadata = config_pb2.RunMetadata()\n\n    node1 = step_stats_pb2.NodeExecStats(\n        node_name='Add/123',\n        op_start_rel_micros=3,\n        op_end_rel_micros=5,\n        all_end_rel_micros=4)\n\n    run_metadata = config_pb2.RunMetadata()\n    device1 = run_metadata.step_stats.dev_stats.add()\n    device1.device = 'deviceA'\n    device1.node_stats.extend([node1])\n\n    graph = test.mock.MagicMock()\n    op1 = test.mock.MagicMock()\n    op1.name = 'Add/123'\n    op1.traceback = [\n        ('a/b/file1', 10, 'apply_op', 'abc'), ('a/c/file2', 12, 'my_op', 'def')]\n    op1.type = 'add'\n    graph.get_operations.return_value = [op1]\n\n    expected_proto = \"\"\"sample_type {\n  type: 5\n  unit: 5\n}\nsample_type {\n  type: 6\n  unit: 7\n}\nsample_type {\n  type: 8\n  unit: 7\n}\nsample {\n  value: 1\n  value: 4\n  value: 2\n  label {\n    key: 1\n    str: 2\n  }\n  label {\n    key: 3\n    str: 4\n  }\n}\nstring_table: \"\"\nstring_table: \"node_name\"\nstring_table: \"Add/123\"\nstring_table: \"op_type\"\nstring_table: \"add\"\nstring_table: \"count\"\nstring_table: \"all_time\"\nstring_table: \"nanoseconds\"\nstring_table: \"op_time\"\nstring_table: \"Device 1 of 1: deviceA\"\ncomment: 9\n\"\"\"\n    # Test with protos\n    profiles = pprof_profiler.get_profiles(graph, run_metadata)\n    self.assertEquals(1, len(profiles))\n    self.assertTrue('deviceA' in profiles)\n    self.assertEquals(expected_proto, str(profiles['deviceA']))\n    # Test with files\n    profile_files = pprof_profiler.profile(\n        graph, run_metadata, output_dir)\n    self.assertEquals(1, len(profile_files))\n    with gzip.open(profile_files[0]) as profile_file:\n      profile_contents = profile_file.read()\n      profile = profile_pb2.Profile()\n      profile.ParseFromString(profile_contents)\n      self.assertEquals(expected_proto, str(profile))\n\n  def testProfileWithWhileLoop(self):\n    options = config_pb2.RunOptions()\n    options.trace_level = config_pb2.RunOptions.FULL_TRACE\n    run_metadata = config_pb2.RunMetadata()\n\n    num_iters = 5\n    with self.test_session() as sess:\n      i = constant_op.constant(0)\n      c = lambda i: math_ops.less(i, num_iters)\n      b = lambda i: math_ops.add(i, 1)\n      r = control_flow_ops.while_loop(c, b, [i])\n      sess.run(r, options=options, run_metadata=run_metadata)\n      profiles = pprof_profiler.get_profiles(sess.graph, run_metadata)\n      self.assertEquals(1, len(profiles))\n      profile = next(iter(profiles.values()))\n      add_samples = []  # Samples for the while/Add node\n      for sample in profile.sample:\n        if profile.string_table[sample.label[0].str] == 'while/Add':\n          add_samples.append(sample)\n      # Values for same nodes are aggregated.\n      self.assertEquals(1, len(add_samples))\n      # Value of \"count\" should be equal to number of iterations.\n      self.assertEquals(num_iters, add_samples[0].value[0])\n\n\nif __name__ == '__main__':\n  test.main()\n", "license": "apache-2.0"}
{"id": "706468ea515ce03f5f7b2b6d84b36f5647f054eb", "path": "tengwar/misc/helpers.py", "repo_name": "dstruthers/python-tengwar", "content": "from parsing import *\n\ndef base12(n):\n    digits = '0123456789ab'\n    result = ''\n    while n >= 12:\n        result = digits[n % 12] + result\n        n //= 12\n        \n    result = digits[n] + result\n    return result\n\ndef mapping(pattern, output):\n    @parser\n    def mapping_implementation(input):\n        input.match(Parser.coerce(pattern))\n        return output\n    return mapping_implementation\n\ndef many1(parser):\n    return many(parser, at_least=1)\n", "license": "mit"}
{"id": "7a9598725ef4043ebfddb19c843e741388f5a82a", "path": "DailyProgrammer/DP20170803B.py", "repo_name": "DayGitH/Python-Challenges", "content": "\"\"\"\n[2017-08-03] Challenge #325 [Intermediate] Arrow maze\n\nhttps://www.reddit.com/r/dailyprogrammer/comments/6rb98p/20170803_challenge_325_intermediate_arrow_maze/\n\n#Description\nWe want to return home, but we have to go trough an [arrow maze](http://imgur.com/TjYhSB4).\nWe start at [a certain point](http://imgur.com/QTxERGr) an in a arrow maze you can only follow [the direction of the\narrow](http://imgur.com/a097dDJ).\nAt each node in the maze we can decide to change direction (depending on the new node) or follow the direction we where\ngoing.\nWhen done right, we should have a path to [home](http://imgur.com/UqD5Brf) \n#Formal Inputs & Outputs\n##Input description\nYou recieve on the first line the coordinates of the node where you will start and after that the maze.\n`n ne e se s sw w nw` are the direction you can travel to and `h` is your target in the maze.\n    (2,0)\n     e se se sw  s\n     s nw nw  n  w\n    ne  s  h  e sw\n    se  n  w ne sw\n    ne nw nw  n  n\nI have added extra whitespace for formatting reasons\n##Output description\nYou need to output the path to the center.\n    (2,0)\n    (3,1)\n    (3,0)\n    (1,2)\n    (1,3)\n    (1,1)\n    (0,0)\n    (4,0)\n    (4,1)\n    (0,1)\n    (0,4)\n    (2,2)\nyou can get creative and use acii art or even better\n#Notes/Hints\nIf you have a hard time starting from the beginning, then backtracking might be a good option.\n#Finally\nHave a good challenge idea?\nConsider submitting it to /r/dailyprogrammer_ideas\n\"\"\"\n\n\ndef main():\n    pass\n\n\nif __name__ == \"__main__\":\n    main()\n", "license": "mit"}
{"id": "c49ebdf48b6efbf7c465a25020e01259d13f3a21", "path": "tests/template_tests/filter_tests/test_wordcount.py", "repo_name": "rizumu/django", "content": "from django.template.defaultfilters import wordcount\nfrom django.test import SimpleTestCase\nfrom django.utils.safestring import mark_safe\n\nfrom ..utils import setup\n\n\nclass WordcountTests(SimpleTestCase):\n\n    @setup({'wordcount01': '{% autoescape off %}{{ a|wordcount }} {{ b|wordcount }}{% endautoescape %}'})\n    def test_wordcount01(self):\n        output = self.engine.render_to_string('wordcount01', {'a': 'a & b', 'b': mark_safe('a &amp; b')})\n        self.assertEqual(output, '3 3')\n\n    @setup({'wordcount02': '{{ a|wordcount }} {{ b|wordcount }}'})\n    def test_wordcount02(self):\n        output = self.engine.render_to_string('wordcount02', {'a': 'a & b', 'b': mark_safe('a &amp; b')})\n        self.assertEqual(output, '3 3')\n\n\nclass FunctionTests(SimpleTestCase):\n\n    def test_empty_string(self):\n        self.assertEqual(wordcount(''), 0)\n\n    def test_count_one(self):\n        self.assertEqual(wordcount('oneword'), 1)\n\n    def test_count_multiple(self):\n        self.assertEqual(wordcount('lots of words'), 3)\n\n    def test_non_string_input(self):\n        self.assertEqual(wordcount(123), 1)\n", "license": "bsd-3-clause"}
{"id": "0f112907240696c0cb74acbed5855c8a1c2df3e9", "path": "release/src/router/samba36/lib/dnspython/dns/rdtypes/IN/PX.py", "repo_name": "wkritzinger/asuswrt-merlin", "content": "# Copyright (C) 2003-2007, 2009, 2010 Nominum, Inc.\n#\n# Permission to use, copy, modify, and distribute this software and its\n# documentation for any purpose with or without fee is hereby granted,\n# provided that the above copyright notice and this permission notice\n# appear in all copies.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\" AND NOMINUM DISCLAIMS ALL WARRANTIES\n# WITH REGARD TO THIS SOFTWARE INCLUDING ALL IMPLIED WARRANTIES OF\n# MERCHANTABILITY AND FITNESS. IN NO EVENT SHALL NOMINUM BE LIABLE FOR\n# ANY SPECIAL, DIRECT, INDIRECT, OR CONSEQUENTIAL DAMAGES OR ANY DAMAGES\n# WHATSOEVER RESULTING FROM LOSS OF USE, DATA OR PROFITS, WHETHER IN AN\n# ACTION OF CONTRACT, NEGLIGENCE OR OTHER TORTIOUS ACTION, ARISING OUT\n# OF OR IN CONNECTION WITH THE USE OR PERFORMANCE OF THIS SOFTWARE.\n\nimport struct\n\nimport dns.exception\nimport dns.rdata\nimport dns.name\n\nclass PX(dns.rdata.Rdata):\n    \"\"\"PX record.\n\n    @ivar preference: the preference value\n    @type preference: int\n    @ivar map822: the map822 name\n    @type map822: dns.name.Name object\n    @ivar mapx400: the mapx400 name\n    @type mapx400: dns.name.Name object\n    @see: RFC 2163\"\"\"\n\n    __slots__ = ['preference', 'map822', 'mapx400']\n        \n    def __init__(self, rdclass, rdtype, preference, map822, mapx400):\n        super(PX, self).__init__(rdclass, rdtype)\n        self.preference = preference\n        self.map822 = map822\n        self.mapx400 = mapx400\n\n    def to_text(self, origin=None, relativize=True, **kw):\n        map822 = self.map822.choose_relativity(origin, relativize)\n        mapx400 = self.mapx400.choose_relativity(origin, relativize)\n        return '%d %s %s' % (self.preference, map822, mapx400)\n        \n    def from_text(cls, rdclass, rdtype, tok, origin = None, relativize = True):\n        preference = tok.get_uint16()\n        map822 = tok.get_name()\n        map822 = map822.choose_relativity(origin, relativize)\n        mapx400 = tok.get_name(None)\n        mapx400 = mapx400.choose_relativity(origin, relativize)\n        tok.get_eol()\n        return cls(rdclass, rdtype, preference, map822, mapx400)\n    \n    from_text = classmethod(from_text)\n\n    def to_wire(self, file, compress = None, origin = None):\n        pref = struct.pack(\"!H\", self.preference)\n        file.write(pref)\n        self.map822.to_wire(file, None, origin)\n        self.mapx400.to_wire(file, None, origin)\n        \n    def from_wire(cls, rdclass, rdtype, wire, current, rdlen, origin = None):\n        (preference, ) = struct.unpack('!H', wire[current : current + 2])\n        current += 2\n        rdlen -= 2\n        (map822, cused) = dns.name.from_wire(wire[: current + rdlen],\n                                               current)\n        if cused > rdlen:\n            raise dns.exception.FormError\n        current += cused\n        rdlen -= cused\n        if not origin is None:\n            map822 = map822.relativize(origin)\n        (mapx400, cused) = dns.name.from_wire(wire[: current + rdlen],\n                                              current)\n        if cused != rdlen:\n            raise dns.exception.FormError\n        if not origin is None:\n            mapx400 = mapx400.relativize(origin)\n        return cls(rdclass, rdtype, preference, map822, mapx400)\n\n    from_wire = classmethod(from_wire)\n\n    def choose_relativity(self, origin = None, relativize = True):\n        self.map822 = self.map822.choose_relativity(origin, relativize)\n        self.mapx400 = self.mapx400.choose_relativity(origin, relativize)\n\n    def _cmp(self, other):\n        sp = struct.pack(\"!H\", self.preference)\n        op = struct.pack(\"!H\", other.preference)\n        v = cmp(sp, op)\n        if v == 0:\n            v = cmp(self.map822, other.map822)\n            if v == 0:\n                v = cmp(self.mapx400, other.mapx400)\n        return v\n", "license": "gpl-2.0"}
{"id": "2f977768c6fab194a1d2a47530d2b84a95f793aa", "path": "website/addons/mendeley/settings/local-dist.py", "repo_name": "doublebits/osf.io", "content": "# -*- coding: utf-8 -*-\n\"\"\"Example Mendeley local settings file. Copy this file to local.py and change\nthese settings.\n\"\"\"\n# Get an app key and secret at http://dev.mendeley.com/\nMENDELEY_CLIENT_ID = 'changeme'\nMENDELEY_CLIENT_SECRET = 'changeme'\n", "license": "apache-2.0"}
{"id": "8abc9f3e14c17a2f7421e8a7472993bb686047f4", "path": "staging_dir/target-mipsel_r2_uClibc-0.9.33.2/root-ralink/usr/lib/python2.7/binhex.py", "repo_name": "hynnet/hiwifi-openwrt-HC5661-HC5761", "content": "\"\"\"Macintosh binhex compression/decompression.\n\neasy interface:\nbinhex(inputfilename, outputfilename)\nhexbin(inputfilename, outputfilename)\n\"\"\"\n\n#\n# Jack Jansen, CWI, August 1995.\n#\n# The module is supposed to be as compatible as possible. Especially the\n# easy interface should work \"as expected\" on any platform.\n# XXXX Note: currently, textfiles appear in mac-form on all platforms.\n# We seem to lack a simple character-translate in python.\n# (we should probably use ISO-Latin-1 on all but the mac platform).\n# XXXX The simple routines are too simple: they expect to hold the complete\n# files in-core. Should be fixed.\n# XXXX It would be nice to handle AppleDouble format on unix\n# (for servers serving macs).\n# XXXX I don't understand what happens when you get 0x90 times the same byte on\n# input. The resulting code (xx 90 90) would appear to be interpreted as an\n# escaped *value* of 0x90. All coders I've seen appear to ignore this nicety...\n#\nimport sys\nimport os\nimport struct\nimport binascii\n\n__all__ = [\"binhex\",\"hexbin\",\"Error\"]\n\nclass Error(Exception):\n    pass\n\n# States (what have we written)\n[_DID_HEADER, _DID_DATA, _DID_RSRC] = range(3)\n\n# Various constants\nREASONABLY_LARGE=32768  # Minimal amount we pass the rle-coder\nLINELEN=64\nRUNCHAR=chr(0x90)   # run-length introducer\n\n#\n# This code is no longer byte-order dependent\n\n#\n# Workarounds for non-mac machines.\ntry:\n    from Carbon.File import FSSpec, FInfo\n    from MacOS import openrf\n\n    def getfileinfo(name):\n        finfo = FSSpec(name).FSpGetFInfo()\n        dir, file = os.path.split(name)\n        # XXX Get resource/data sizes\n        fp = open(name, 'rb')\n        fp.seek(0, 2)\n        dlen = fp.tell()\n        fp = openrf(name, '*rb')\n        fp.seek(0, 2)\n        rlen = fp.tell()\n        return file, finfo, dlen, rlen\n\n    def openrsrc(name, *mode):\n        if not mode:\n            mode = '*rb'\n        else:\n            mode = '*' + mode[0]\n        return openrf(name, mode)\n\nexcept ImportError:\n    #\n    # Glue code for non-macintosh usage\n    #\n\n    class FInfo:\n        def __init__(self):\n            self.Type = '????'\n            self.Creator = '????'\n            self.Flags = 0\n\n    def getfileinfo(name):\n        finfo = FInfo()\n        # Quick check for textfile\n        fp = open(name)\n        data = open(name).read(256)\n        for c in data:\n            if not c.isspace() and (c<' ' or ord(c) > 0x7f):\n                break\n        else:\n            finfo.Type = 'TEXT'\n        fp.seek(0, 2)\n        dsize = fp.tell()\n        fp.close()\n        dir, file = os.path.split(name)\n        file = file.replace(':', '-', 1)\n        return file, finfo, dsize, 0\n\n    class openrsrc:\n        def __init__(self, *args):\n            pass\n\n        def read(self, *args):\n            return ''\n\n        def write(self, *args):\n            pass\n\n        def close(self):\n            pass\n\nclass _Hqxcoderengine:\n    \"\"\"Write data to the coder in 3-byte chunks\"\"\"\n\n    def __init__(self, ofp):\n        self.ofp = ofp\n        self.data = ''\n        self.hqxdata = ''\n        self.linelen = LINELEN-1\n\n    def write(self, data):\n        self.data = self.data + data\n        datalen = len(self.data)\n        todo = (datalen//3)*3\n        data = self.data[:todo]\n        self.data = self.data[todo:]\n        if not data:\n            return\n        self.hqxdata = self.hqxdata + binascii.b2a_hqx(data)\n        self._flush(0)\n\n    def _flush(self, force):\n        first = 0\n        while first <= len(self.hqxdata)-self.linelen:\n            last = first + self.linelen\n            self.ofp.write(self.hqxdata[first:last]+'\\n')\n            self.linelen = LINELEN\n            first = last\n        self.hqxdata = self.hqxdata[first:]\n        if force:\n            self.ofp.write(self.hqxdata + ':\\n')\n\n    def close(self):\n        if self.data:\n            self.hqxdata = \\\n                 self.hqxdata + binascii.b2a_hqx(self.data)\n        self._flush(1)\n        self.ofp.close()\n        del self.ofp\n\nclass _Rlecoderengine:\n    \"\"\"Write data to the RLE-coder in suitably large chunks\"\"\"\n\n    def __init__(self, ofp):\n        self.ofp = ofp\n        self.data = ''\n\n    def write(self, data):\n        self.data = self.data + data\n        if len(self.data) < REASONABLY_LARGE:\n            return\n        rledata = binascii.rlecode_hqx(self.data)\n        self.ofp.write(rledata)\n        self.data = ''\n\n    def close(self):\n        if self.data:\n            rledata = binascii.rlecode_hqx(self.data)\n            self.ofp.write(rledata)\n        self.ofp.close()\n        del self.ofp\n\nclass BinHex:\n    def __init__(self, name_finfo_dlen_rlen, ofp):\n        name, finfo, dlen, rlen = name_finfo_dlen_rlen\n        if type(ofp) == type(''):\n            ofname = ofp\n            ofp = open(ofname, 'w')\n        ofp.write('(This file must be converted with BinHex 4.0)\\n\\n:')\n        hqxer = _Hqxcoderengine(ofp)\n        self.ofp = _Rlecoderengine(hqxer)\n        self.crc = 0\n        if finfo is None:\n            finfo = FInfo()\n        self.dlen = dlen\n        self.rlen = rlen\n        self._writeinfo(name, finfo)\n        self.state = _DID_HEADER\n\n    def _writeinfo(self, name, finfo):\n        nl = len(name)\n        if nl > 63:\n            raise Error, 'Filename too long'\n        d = chr(nl) + name + '\\0'\n        d2 = finfo.Type + finfo.Creator\n\n        # Force all structs to be packed with big-endian\n        d3 = struct.pack('>h', finfo.Flags)\n        d4 = struct.pack('>ii', self.dlen, self.rlen)\n        info = d + d2 + d3 + d4\n        self._write(info)\n        self._writecrc()\n\n    def _write(self, data):\n        self.crc = binascii.crc_hqx(data, self.crc)\n        self.ofp.write(data)\n\n    def _writecrc(self):\n        # XXXX Should this be here??\n        # self.crc = binascii.crc_hqx('\\0\\0', self.crc)\n        if self.crc < 0:\n            fmt = '>h'\n        else:\n            fmt = '>H'\n        self.ofp.write(struct.pack(fmt, self.crc))\n        self.crc = 0\n\n    def write(self, data):\n        if self.state != _DID_HEADER:\n            raise Error, 'Writing data at the wrong time'\n        self.dlen = self.dlen - len(data)\n        self._write(data)\n\n    def close_data(self):\n        if self.dlen != 0:\n            raise Error, 'Incorrect data size, diff=%r' % (self.rlen,)\n        self._writecrc()\n        self.state = _DID_DATA\n\n    def write_rsrc(self, data):\n        if self.state < _DID_DATA:\n            self.close_data()\n        if self.state != _DID_DATA:\n            raise Error, 'Writing resource data at the wrong time'\n        self.rlen = self.rlen - len(data)\n        self._write(data)\n\n    def close(self):\n        if self.state < _DID_DATA:\n            self.close_data()\n        if self.state != _DID_DATA:\n            raise Error, 'Close at the wrong time'\n        if self.rlen != 0:\n            raise Error, \\\n                  \"Incorrect resource-datasize, diff=%r\" % (self.rlen,)\n        self._writecrc()\n        self.ofp.close()\n        self.state = None\n        del self.ofp\n\ndef binhex(inp, out):\n    \"\"\"(infilename, outfilename) - Create binhex-encoded copy of a file\"\"\"\n    finfo = getfileinfo(inp)\n    ofp = BinHex(finfo, out)\n\n    ifp = open(inp, 'rb')\n    # XXXX Do textfile translation on non-mac systems\n    while 1:\n        d = ifp.read(128000)\n        if not d: break\n        ofp.write(d)\n    ofp.close_data()\n    ifp.close()\n\n    ifp = openrsrc(inp, 'rb')\n    while 1:\n        d = ifp.read(128000)\n        if not d: break\n        ofp.write_rsrc(d)\n    ofp.close()\n    ifp.close()\n\nclass _Hqxdecoderengine:\n    \"\"\"Read data via the decoder in 4-byte chunks\"\"\"\n\n    def __init__(self, ifp):\n        self.ifp = ifp\n        self.eof = 0\n\n    def read(self, totalwtd):\n        \"\"\"Read at least wtd bytes (or until EOF)\"\"\"\n        decdata = ''\n        wtd = totalwtd\n        #\n        # The loop here is convoluted, since we don't really now how\n        # much to decode: there may be newlines in the incoming data.\n        while wtd > 0:\n            if self.eof: return decdata\n            wtd = ((wtd+2)//3)*4\n            data = self.ifp.read(wtd)\n            #\n            # Next problem: there may not be a complete number of\n            # bytes in what we pass to a2b. Solve by yet another\n            # loop.\n            #\n            while 1:\n                try:\n                    decdatacur, self.eof = \\\n                            binascii.a2b_hqx(data)\n                    break\n                except binascii.Incomplete:\n                    pass\n                newdata = self.ifp.read(1)\n                if not newdata:\n                    raise Error, \\\n                          'Premature EOF on binhex file'\n                data = data + newdata\n            decdata = decdata + decdatacur\n            wtd = totalwtd - len(decdata)\n            if not decdata and not self.eof:\n                raise Error, 'Premature EOF on binhex file'\n        return decdata\n\n    def close(self):\n        self.ifp.close()\n\nclass _Rledecoderengine:\n    \"\"\"Read data via the RLE-coder\"\"\"\n\n    def __init__(self, ifp):\n        self.ifp = ifp\n        self.pre_buffer = ''\n        self.post_buffer = ''\n        self.eof = 0\n\n    def read(self, wtd):\n        if wtd > len(self.post_buffer):\n            self._fill(wtd-len(self.post_buffer))\n        rv = self.post_buffer[:wtd]\n        self.post_buffer = self.post_buffer[wtd:]\n        return rv\n\n    def _fill(self, wtd):\n        self.pre_buffer = self.pre_buffer + self.ifp.read(wtd+4)\n        if self.ifp.eof:\n            self.post_buffer = self.post_buffer + \\\n                binascii.rledecode_hqx(self.pre_buffer)\n            self.pre_buffer = ''\n            return\n\n        #\n        # Obfuscated code ahead. We have to take care that we don't\n        # end up with an orphaned RUNCHAR later on. So, we keep a couple\n        # of bytes in the buffer, depending on what the end of\n        # the buffer looks like:\n        # '\\220\\0\\220' - Keep 3 bytes: repeated \\220 (escaped as \\220\\0)\n        # '?\\220' - Keep 2 bytes: repeated something-else\n        # '\\220\\0' - Escaped \\220: Keep 2 bytes.\n        # '?\\220?' - Complete repeat sequence: decode all\n        # otherwise: keep 1 byte.\n        #\n        mark = len(self.pre_buffer)\n        if self.pre_buffer[-3:] == RUNCHAR + '\\0' + RUNCHAR:\n            mark = mark - 3\n        elif self.pre_buffer[-1] == RUNCHAR:\n            mark = mark - 2\n        elif self.pre_buffer[-2:] == RUNCHAR + '\\0':\n            mark = mark - 2\n        elif self.pre_buffer[-2] == RUNCHAR:\n            pass # Decode all\n        else:\n            mark = mark - 1\n\n        self.post_buffer = self.post_buffer + \\\n            binascii.rledecode_hqx(self.pre_buffer[:mark])\n        self.pre_buffer = self.pre_buffer[mark:]\n\n    def close(self):\n        self.ifp.close()\n\nclass HexBin:\n    def __init__(self, ifp):\n        if type(ifp) == type(''):\n            ifp = open(ifp)\n        #\n        # Find initial colon.\n        #\n        while 1:\n            ch = ifp.read(1)\n            if not ch:\n                raise Error, \"No binhex data found\"\n            # Cater for \\r\\n terminated lines (which show up as \\n\\r, hence\n            # all lines start with \\r)\n            if ch == '\\r':\n                continue\n            if ch == ':':\n                break\n            if ch != '\\n':\n                dummy = ifp.readline()\n\n        hqxifp = _Hqxdecoderengine(ifp)\n        self.ifp = _Rledecoderengine(hqxifp)\n        self.crc = 0\n        self._readheader()\n\n    def _read(self, len):\n        data = self.ifp.read(len)\n        self.crc = binascii.crc_hqx(data, self.crc)\n        return data\n\n    def _checkcrc(self):\n        filecrc = struct.unpack('>h', self.ifp.read(2))[0] & 0xffff\n        #self.crc = binascii.crc_hqx('\\0\\0', self.crc)\n        # XXXX Is this needed??\n        self.crc = self.crc & 0xffff\n        if filecrc != self.crc:\n            raise Error, 'CRC error, computed %x, read %x' \\\n                  %(self.crc, filecrc)\n        self.crc = 0\n\n    def _readheader(self):\n        len = self._read(1)\n        fname = self._read(ord(len))\n        rest = self._read(1+4+4+2+4+4)\n        self._checkcrc()\n\n        type = rest[1:5]\n        creator = rest[5:9]\n        flags = struct.unpack('>h', rest[9:11])[0]\n        self.dlen = struct.unpack('>l', rest[11:15])[0]\n        self.rlen = struct.unpack('>l', rest[15:19])[0]\n\n        self.FName = fname\n        self.FInfo = FInfo()\n        self.FInfo.Creator = creator\n        self.FInfo.Type = type\n        self.FInfo.Flags = flags\n\n        self.state = _DID_HEADER\n\n    def read(self, *n):\n        if self.state != _DID_HEADER:\n            raise Error, 'Read data at wrong time'\n        if n:\n            n = n[0]\n            n = min(n, self.dlen)\n        else:\n            n = self.dlen\n        rv = ''\n        while len(rv) < n:\n            rv = rv + self._read(n-len(rv))\n        self.dlen = self.dlen - n\n        return rv\n\n    def close_data(self):\n        if self.state != _DID_HEADER:\n            raise Error, 'close_data at wrong time'\n        if self.dlen:\n            dummy = self._read(self.dlen)\n        self._checkcrc()\n        self.state = _DID_DATA\n\n    def read_rsrc(self, *n):\n        if self.state == _DID_HEADER:\n            self.close_data()\n        if self.state != _DID_DATA:\n            raise Error, 'Read resource data at wrong time'\n        if n:\n            n = n[0]\n            n = min(n, self.rlen)\n        else:\n            n = self.rlen\n        self.rlen = self.rlen - n\n        return self._read(n)\n\n    def close(self):\n        if self.rlen:\n            dummy = self.read_rsrc(self.rlen)\n        self._checkcrc()\n        self.state = _DID_RSRC\n        self.ifp.close()\n\ndef hexbin(inp, out):\n    \"\"\"(infilename, outfilename) - Decode binhexed file\"\"\"\n    ifp = HexBin(inp)\n    finfo = ifp.FInfo\n    if not out:\n        out = ifp.FName\n\n    ofp = open(out, 'wb')\n    # XXXX Do translation on non-mac systems\n    while 1:\n        d = ifp.read(128000)\n        if not d: break\n        ofp.write(d)\n    ofp.close()\n    ifp.close_data()\n\n    d = ifp.read_rsrc(128000)\n    if d:\n        ofp = openrsrc(out, 'wb')\n        ofp.write(d)\n        while 1:\n            d = ifp.read_rsrc(128000)\n            if not d: break\n            ofp.write(d)\n        ofp.close()\n\n    ifp.close()\n\ndef _test():\n    fname = sys.argv[1]\n    binhex(fname, fname+'.hqx')\n    hexbin(fname+'.hqx', fname+'.viahqx')\n    #hexbin(fname, fname+'.unpacked')\n    sys.exit(1)\n\nif __name__ == '__main__':\n    _test()\n", "license": "gpl-2.0"}
{"id": "2f9cc4d0081251898ee8bc5f1018c09e0b218e9f", "path": "nova/console/__init__.py", "repo_name": "yosshy/nova", "content": "#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"\n:mod:`nova.console` -- Console Proxy to set up VM console access\n      (i.e. with xvp)\n=====================================================\n\n.. automodule:: nova.console\n   :platform: Unix\n   :synopsis: Wrapper around console proxies such as xvp to set up\n              multitenant VM console access\n\"\"\"\n", "license": "apache-2.0"}
{"id": "b80c5969bd4b4f858d26858e5ddaba52acc77d67", "path": "tests/esctest/tests/decsel.py", "repo_name": "ronyfadel/iTerm2", "content": "from esc import NUL, blank\nimport escargs\nimport esccmd\nimport escio\nfrom esctypes import Point, Rect\nfrom escutil import AssertEQ, AssertScreenCharsInRectEqual, GetCursorPosition, knownBug\n\nclass DECSELTests(object):\n\n  def prepare(self):\n    \"\"\"Initializes the screen to abcdefghij on the first line with the cursor\n    on the 'e'.\"\"\"\n    esccmd.CUP(Point(1, 1))\n    escio.Write(\"abcdefghij\")\n    esccmd.CUP(Point(5, 1))\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_Default(self):\n    \"\"\"Should erase to right of cursor.\"\"\"\n    self.prepare()\n    esccmd.DECSEL()\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ \"abcd\" + 6 * NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_0(self):\n    \"\"\"Should erase to right of cursor.\"\"\"\n    self.prepare()\n    esccmd.DECSEL(0)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ \"abcd\" + 6 * NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_1(self):\n    \"\"\"Should erase to left of cursor.\"\"\"\n    self.prepare()\n    esccmd.DECSEL(1)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ 5 * blank() + \"fghij\" ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_2(self):\n    \"\"\"Should erase whole line.\"\"\"\n    self.prepare()\n    esccmd.DECSEL(2)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ 10 * NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_IgnoresScrollRegion(self):\n    \"\"\"Should erase whole line.\"\"\"\n    self.prepare()\n    esccmd.DECSET(esccmd.DECLRMM)\n    esccmd.DECSLRM(2, 4)\n    esccmd.CUP(Point(5, 1))\n    esccmd.DECSEL(2)\n    esccmd.DECRESET(esccmd.DECLRMM)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ 10 * NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_Default_Protection(self):\n    \"\"\"Should erase to right of cursor.\"\"\"\n    esccmd.DECSCA(1)\n    self.prepare()\n\n    # Write an X at 1,1 without protection\n    esccmd.DECSCA(0)\n    esccmd.CUP(Point(10, 1))\n    escio.Write(\"X\")\n    esccmd.CUP(Point(5, 1))\n\n    esccmd.DECSEL()\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ \"abcdefghi\" + NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_0_Protection(self):\n    \"\"\"All letters are protected so nothing should happen.\"\"\"\n    esccmd.DECSCA(1)\n    self.prepare()\n\n    # Write an X at 1,1 without protection\n    esccmd.DECSCA(0)\n    esccmd.CUP(Point(10, 1))\n    escio.Write(\"X\")\n\n    esccmd.CUP(Point(5, 1))\n    esccmd.DECSEL(0)\n\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ \"abcdefghi\" + NUL ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_1_Protection(self):\n    \"\"\"All letters are protected so nothing should happen.\"\"\"\n    esccmd.DECSCA(1)\n    self.prepare()\n\n    # Write an X at 1,1 without protection\n    esccmd.DECSCA(0)\n    esccmd.CUP(Point(1, 1))\n    escio.Write(\"X\")\n\n    esccmd.CUP(Point(5, 1))\n    esccmd.DECSEL(1)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ blank() + \"bcdefghij\" ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_2_Protection(self):\n    \"\"\"All letters are protected so nothing should happen.\"\"\"\n    esccmd.DECSCA(1)\n    self.prepare()\n\n    # Write an X at 1,1 without protection\n    esccmd.DECSCA(0)\n    esccmd.CUP(Point(1, 1))\n    escio.Write(\"X\")\n\n    esccmd.DECSEL(2)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ blank() + \"bcdefghij\" ])\n\n  @knownBug(terminal=\"iTerm2\", reason=\"Not implemented\")\n  def test_DECSEL_IgnoresScrollRegion_Protection(self):\n    \"\"\"All letters are protected so nothing should happen.\"\"\"\n    esccmd.DECSCA(1)\n    self.prepare()\n\n    # Write an X at 1,1 without protection\n    esccmd.DECSCA(0)\n    esccmd.CUP(Point(1, 1))\n    escio.Write(\"X\")\n\n    esccmd.DECSET(esccmd.DECLRMM)\n    esccmd.DECSLRM(2, 4)\n    esccmd.CUP(Point(5, 1))\n    esccmd.DECSEL(2)\n    esccmd.DECRESET(esccmd.DECLRMM)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 10, 1),\n                                 [ blank() + \"bcdefghij\" ])\n\n  @knownBug(terminal=\"xterm\",\n            reason=\"DECSEL respects ISO protection for backward compatibility reasons, per email from Thomas\")\n  @knownBug(terminal=\"iTerm2\", reason=\"DECSED not implemented\")\n  def test_DECSEL_doesNotRespectISOProtect(self):\n    \"\"\"DECSEL does not respect ISO protection.\"\"\"\n    escio.Write(\"a\")\n    esccmd.SPA()\n    escio.Write(\"b\")\n    esccmd.EPA()\n    esccmd.DECSEL(2)\n    AssertScreenCharsInRectEqual(Rect(1, 1, 2, 1), [ blank() * 2 ])\n\n", "license": "gpl-2.0"}
{"id": "cda8db477ebf649d6032df66ed4e0a15d1f51de7", "path": "GeoDanmarkChecker/fot/rules/validate/singlelayer/uniqueattributevalue.py", "repo_name": "Septima/qgis-GeoDanmarkCheck", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nRoutines for quality control of GeoDanmark map data\nCopyright (C) 2016\nDeveloped by Septima.dk for the Danish Agency for Data Supply and Efficiency\n\nThis program is free software: you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation, either version 3 of the License, or\n(at your option) any later version.\n\nThis program is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\nGNU General Public License for more details.\n\nYou should have received a copy of the GNU General Public License\nalong with this program.  If not, see <http://www.gnu.org/licenses/>.\n\"\"\"\n\nfrom .singlelayerrule import SingleLayerRule\nfrom ....geomutils.errorgeometry import createlinemarker\n\nclass UniqueAttributeValue(SingleLayerRule):\n    \"\"\"Check that all values for a given attribute are unique within the features.\n\n    Parameters\n    ----------\n    name : str\n        Name if this rule instance\n    feature_type : fot.FeatureType\n        Feature type to apply check to\n    attributename : str\n        Name of attribute which must be unique\n    filter : str\n        QGIS Filter Expression which is applied to features before evaluating this rule.\n\n    \"\"\"\n\n    def __init__(self, name, feature_type, attributename, filter=None):\n        super(UniqueAttributeValue, self).__init__(name, feature_type)\n        self.filter = filter\n        self.attributesneeded = attributename\n        self.attributename = attributename\n        self.attributevalues = {}\n\n    def checkmany(self, features, reporter, progressreporter):\n        progressreporter.begintask(self.name, len(features))\n        for feature in features:\n            try:\n                value = feature[self.attributename]\n                if value in self.attributevalues:\n                    # Wooops not unique!\n                    errorgeom = createlinemarker(feature, self.attributevalues[value])\n                    reporter.error(\n                        self.name,\n                        self.featuretype,\n                        self.attributename + '=\"' + unicode(value) + '\" not unique',\n                        errorgeom\n                    )\n                else:\n                    self.attributevalues[value] = feature\n            except Exception as e:\n                reporter.error(\n                    self.name,\n                    self.featuretype,\n                    \"Error processing attribute: {0} Message: {1}\".format(self.attributename, str(e)),\n                    feature\n                )\n            progressreporter.completed_one()", "license": "gpl-3.0"}
{"id": "cef77ba1bc62b0d1a352d913902fac41ca47c297", "path": "setup.py", "repo_name": "le9i0nx/ansible", "content": "\nimport json\nimport os\nimport os.path\nimport re\nimport sys\nfrom collections import defaultdict\nfrom distutils.command.build_scripts import build_scripts as BuildScripts\nfrom distutils.command.sdist import sdist as SDist\n\ntry:\n    from setuptools import setup, find_packages\n    from setuptools.command.build_py import build_py as BuildPy\n    from setuptools.command.install_lib import install_lib as InstallLib\n    from setuptools.command.install_scripts import install_scripts as InstallScripts\nexcept ImportError:\n    print(\"Ansible now needs setuptools in order to build. Install it using\"\n          \" your package manager (usually python-setuptools) or via pip (pip\"\n          \" install setuptools).\")\n    sys.exit(1)\n\nsys.path.insert(0, os.path.abspath('lib'))\nfrom ansible.release import __version__, __author__\n\n\nSYMLINK_CACHE = 'SYMLINK_CACHE.json'\n\n\ndef _find_symlinks(topdir, extension=''):\n    \"\"\"Find symlinks that should be maintained\n\n    Maintained symlinks exist in the bin dir or are modules which have\n    aliases.  Our heuristic is that they are a link in a certain path which\n    point to a file in the same directory.\n    \"\"\"\n    symlinks = defaultdict(list)\n    for base_path, dirs, files in os.walk(topdir):\n        for filename in files:\n            filepath = os.path.join(base_path, filename)\n            if os.path.islink(filepath) and filename.endswith(extension):\n                target = os.readlink(filepath)\n                if os.path.dirname(target) == '':\n                    link = filepath[len(topdir):]\n                    if link.startswith('/'):\n                        link = link[1:]\n                    symlinks[os.path.basename(target)].append(link)\n    return symlinks\n\n\ndef _cache_symlinks(symlink_data):\n    with open(SYMLINK_CACHE, 'w') as f:\n        f.write(json.dumps(symlink_data))\n\n\ndef _maintain_symlinks(symlink_type, base_path):\n    \"\"\"Switch a real file into a symlink\"\"\"\n    try:\n        # Try the cache first because going from git checkout to sdist is the\n        # only time we know that we're going to cache correctly\n        with open(SYMLINK_CACHE, 'r') as f:\n            symlink_data = json.loads(f.read())\n    except (IOError, OSError) as e:\n        # IOError on py2, OSError on py3.  Both have errno\n        if e.errno == 2:\n            # SYMLINKS_CACHE doesn't exist.  Fallback to trying to create the\n            # cache now.  Will work if we're running directly from a git\n            # checkout or from an sdist created earlier.\n            symlink_data = {'script': _find_symlinks('bin'),\n                            'library': _find_symlinks('lib', '.py'),\n                            }\n\n            # Sanity check that something we know should be a symlink was\n            # found.  We'll take that to mean that the current directory\n            # structure properly reflects symlinks in the git repo\n            if 'ansible-playbook' in symlink_data['script']['ansible']:\n                _cache_symlinks(symlink_data)\n            else:\n                raise\n        else:\n            raise\n    symlinks = symlink_data[symlink_type]\n\n    for source in symlinks:\n        for dest in symlinks[source]:\n            dest_path = os.path.join(base_path, dest)\n            if not os.path.islink(dest_path):\n                try:\n                    os.unlink(dest_path)\n                except OSError as e:\n                    if e.errno == 2:\n                        # File does not exist which is all we wanted\n                        pass\n                os.symlink(source, dest_path)\n\n\nclass BuildPyCommand(BuildPy):\n    def run(self):\n        BuildPy.run(self)\n        _maintain_symlinks('library', self.build_lib)\n\n\nclass BuildScriptsCommand(BuildScripts):\n    def run(self):\n        BuildScripts.run(self)\n        _maintain_symlinks('script', self.build_dir)\n\n\nclass InstallLibCommand(InstallLib):\n    def run(self):\n        InstallLib.run(self)\n        _maintain_symlinks('library', self.install_dir)\n\n\nclass InstallScriptsCommand(InstallScripts):\n    def run(self):\n        InstallScripts.run(self)\n        _maintain_symlinks('script', self.install_dir)\n\n\nclass SDistCommand(SDist):\n    def run(self):\n        # have to generate the cache of symlinks for release as sdist is the\n        # only command that has access to symlinks from the git repo\n        symlinks = {'script': _find_symlinks('bin'),\n                    'library': _find_symlinks('lib', '.py'),\n                    }\n        _cache_symlinks(symlinks)\n\n        SDist.run(self)\n\n\nwith open('requirements.txt') as requirements_file:\n    install_requirements = requirements_file.read().splitlines()\n    if not install_requirements:\n        print(\"Unable to read requirements from the requirements.txt file\"\n              \"That indicates this copy of the source code is incomplete.\")\n        sys.exit(2)\n\n# pycrypto or cryptography.   We choose a default but allow the user to\n# override it.  This translates into pip install of the sdist deciding what\n# package to install and also the runtime dependencies that pkg_resources\n# knows about\ncrypto_backend = os.environ.get('ANSIBLE_CRYPTO_BACKEND', None)\nif crypto_backend:\n    if crypto_backend.strip() == 'pycrypto':\n        # Attempt to set version requirements\n        crypto_backend = 'pycrypto >= 2.6'\n\n    install_requirements = [r for r in install_requirements if not (r.lower().startswith('pycrypto') or r.lower().startswith('cryptography'))]\n    install_requirements.append(crypto_backend)\n\n# specify any extra requirements for installation\nextra_requirements = dict()\nextra_requirements_dir = 'packaging/requirements'\nfor extra_requirements_filename in os.listdir(extra_requirements_dir):\n    filename_match = re.search(r'^requirements-(\\w*).txt$', extra_requirements_filename)\n    if filename_match:\n        with open(os.path.join(extra_requirements_dir, extra_requirements_filename)) as extra_requirements_file:\n            extra_requirements[filename_match.group(1)] = extra_requirements_file.read().splitlines()\n\n\nsetup(\n    # Use the distutils SDist so that symlinks are not expanded\n    # Use a custom Build for the same reason\n    cmdclass={\n        'build_py': BuildPyCommand,\n        'build_scripts': BuildScriptsCommand,\n        'install_lib': InstallLibCommand,\n        'install_scripts': InstallScriptsCommand,\n        'sdist': SDistCommand,\n    },\n    name='ansible',\n    version=__version__,\n    description='Radically simple IT automation',\n    author=__author__,\n    author_email='info@ansible.com',\n    url='https://ansible.com/',\n    license='GPLv3+',\n    # Ansible will also make use of a system copy of python-six and\n    # python-selectors2 if installed but use a Bundled copy if it's not.\n    install_requires=install_requirements,\n    package_dir={'': 'lib'},\n    packages=find_packages('lib'),\n    package_data={\n        '': [\n            'module_utils/powershell/*.psm1',\n            'module_utils/powershell/*/*.psm1',\n            'modules/windows/*.ps1',\n            'modules/windows/*/*.ps1',\n            'galaxy/data/*/*.*',\n            'galaxy/data/*/*/.*',\n            'galaxy/data/*/*/*.*',\n            'galaxy/data/*/tests/inventory',\n            'config/base.yml',\n        ],\n    },\n    classifiers=[\n        'Development Status :: 5 - Production/Stable',\n        'Environment :: Console',\n        'Intended Audience :: Developers',\n        'Intended Audience :: Information Technology',\n        'Intended Audience :: System Administrators',\n        'License :: OSI Approved :: GNU General Public License v3 or later (GPLv3+)',\n        'Natural Language :: English',\n        'Operating System :: POSIX',\n        'Programming Language :: Python :: 2.6',\n        'Programming Language :: Python :: 2.7',\n        'Topic :: System :: Installation/Setup',\n        'Topic :: System :: Systems Administration',\n        'Topic :: Utilities',\n    ],\n    scripts=[\n        'bin/ansible',\n        'bin/ansible-playbook',\n        'bin/ansible-pull',\n        'bin/ansible-doc',\n        'bin/ansible-galaxy',\n        'bin/ansible-console',\n        'bin/ansible-connection',\n        'bin/ansible-vault',\n    ],\n    data_files=[],\n    extras_require=extra_requirements,\n    # Installing as zip files would break due to references to __file__\n    zip_safe=False\n)\n", "license": "gpl-3.0"}
{"id": "c77eeb544e78bd38e9f32b5315026061c9c8a483", "path": "old_code_scripts/measure_resolution/lmfit-py/doc/sphinx/numpydoc/phantom_import.py", "repo_name": "DiamondLightSource/auto_tomo_calibration-experimental", "content": "\"\"\"\n==============\nphantom_import\n==============\n\nSphinx extension to make directives from ``sphinx.ext.autodoc`` and similar\nextensions to use docstrings loaded from an XML file.\n\nThis extension loads an XML file in the Pydocweb format [1] and\ncreates a dummy module that contains the specified docstrings. This\ncan be used to get the current docstrings from a Pydocweb instance\nwithout needing to rebuild the documented module.\n\n.. [1] http://code.google.com/p/pydocweb\n\n\"\"\"\nimport imp, sys, compiler, types, os, inspect, re\n\ndef setup(app):\n    app.connect('builder-inited', initialize)\n    app.add_config_value('phantom_import_file', None, True)\n\ndef initialize(app):\n    fn = app.config.phantom_import_file\n    if (fn and os.path.isfile(fn)):\n        print \"[numpydoc] Phantom importing modules from\", fn, \"...\"\n        import_phantom_module(fn)\n\n#------------------------------------------------------------------------------\n# Creating 'phantom' modules from an XML description\n#------------------------------------------------------------------------------\ndef import_phantom_module(xml_file):\n    \"\"\"\n    Insert a fake Python module to sys.modules, based on a XML file.\n\n    The XML file is expected to conform to Pydocweb DTD. The fake\n    module will contain dummy objects, which guarantee the following:\n\n    - Docstrings are correct.\n    - Class inheritance relationships are correct (if present in XML).\n    - Function argspec is *NOT* correct (even if present in XML).\n      Instead, the function signature is prepended to the function docstring.\n    - Class attributes are *NOT* correct; instead, they are dummy objects.\n\n    Parameters\n    ----------\n    xml_file : str\n        Name of an XML file to read\n    \n    \"\"\"\n    import lxml.etree as etree\n\n    object_cache = {}\n\n    tree = etree.parse(xml_file)\n    root = tree.getroot()\n\n    # Sort items so that\n    # - Base classes come before classes inherited from them\n    # - Modules come before their contents\n    all_nodes = dict([(n.attrib['id'], n) for n in root])\n    \n    def _get_bases(node, recurse=False):\n        bases = [x.attrib['ref'] for x in node.findall('base')]\n        if recurse:\n            j = 0\n            while True:\n                try:\n                    b = bases[j]\n                except IndexError: break\n                if b in all_nodes:\n                    bases.extend(_get_bases(all_nodes[b]))\n                j += 1\n        return bases\n\n    type_index = ['module', 'class', 'callable', 'object']\n    \n    def base_cmp(a, b):\n        x = cmp(type_index.index(a.tag), type_index.index(b.tag))\n        if x != 0: return x\n\n        if a.tag == 'class' and b.tag == 'class':\n            a_bases = _get_bases(a, recurse=True)\n            b_bases = _get_bases(b, recurse=True)\n            x = cmp(len(a_bases), len(b_bases))\n            if x != 0: return x\n            if a.attrib['id'] in b_bases: return -1\n            if b.attrib['id'] in a_bases: return 1\n        \n        return cmp(a.attrib['id'].count('.'), b.attrib['id'].count('.'))\n\n    nodes = root.getchildren()\n    nodes.sort(base_cmp)\n\n    # Create phantom items\n    for node in nodes:\n        name = node.attrib['id']\n        doc = (node.text or '').decode('string-escape') + \"\\n\"\n        if doc == \"\\n\": doc = \"\"\n\n        # create parent, if missing\n        parent = name\n        while True:\n            parent = '.'.join(parent.split('.')[:-1])\n            if not parent: break\n            if parent in object_cache: break\n            obj = imp.new_module(parent)\n            object_cache[parent] = obj\n            sys.modules[parent] = obj\n\n        # create object\n        if node.tag == 'module':\n            obj = imp.new_module(name)\n            obj.__doc__ = doc\n            sys.modules[name] = obj\n        elif node.tag == 'class':\n            bases = [object_cache[b] for b in _get_bases(node)\n                     if b in object_cache]\n            bases.append(object)\n            init = lambda self: None\n            init.__doc__ = doc\n            obj = type(name, tuple(bases), {'__doc__': doc, '__init__': init})\n            obj.__name__ = name.split('.')[-1]\n        elif node.tag == 'callable':\n            funcname = node.attrib['id'].split('.')[-1]\n            argspec = node.attrib.get('argspec')\n            if argspec:\n                argspec = re.sub('^[^(]*', '', argspec)\n                doc = \"%s%s\\n\\n%s\" % (funcname, argspec, doc)\n            obj = lambda: 0\n            obj.__argspec_is_invalid_ = True\n            obj.func_name = funcname\n            obj.__name__ = name\n            obj.__doc__ = doc\n            if inspect.isclass(object_cache[parent]):\n                obj.__objclass__ = object_cache[parent]\n        else:\n            class Dummy(object): pass\n            obj = Dummy()\n            obj.__name__ = name\n            obj.__doc__ = doc\n            if inspect.isclass(object_cache[parent]):\n                obj.__get__ = lambda: None\n        object_cache[name] = obj\n\n        if parent:\n            if inspect.ismodule(object_cache[parent]):\n                obj.__module__ = parent\n                setattr(object_cache[parent], name.split('.')[-1], obj)\n\n    # Populate items\n    for node in root:\n        obj = object_cache.get(node.attrib['id'])\n        if obj is None: continue\n        for ref in node.findall('ref'):\n            if node.tag == 'class':\n                if ref.attrib['ref'].startswith(node.attrib['id'] + '.'):\n                    setattr(obj, ref.attrib['name'],\n                            object_cache.get(ref.attrib['ref']))\n            else:\n                setattr(obj, ref.attrib['name'],\n                        object_cache.get(ref.attrib['ref']))\n", "license": "apache-2.0"}
{"id": "59d4f636584c55097a7b20e31974d21c7897b882", "path": "gramps/gen/plug/report/__init__.py", "repo_name": "Fedik/gramps", "content": "#\n# Gramps - a GTK+/GNOME based genealogy program\n#\n# Copyright (C) 2001      David R. Hampton\n# Copyright (C) 2001-2006 Donald N. Allingham\n# Copyright (C) 2007      Brian G. Matherly\n# Copyright (C) 2010      Jakim Friant\n#\n# This program is free software; you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation; either version 2 of the License, or\n# (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA.\n#\n\n# gen.plug.report.__init__\n#\n\n\"Report Generation Framework\"\n\nfrom ._constants import *\nfrom ._reportbase import Report\n\nfrom ._bibliography import Bibliography, Citation\n\nfrom ._options import MenuReportOptions, ReportOptions, DocOptions\n\nfrom ._book import BookList, Book, BookItem, append_styles\n", "license": "gpl-2.0"}
{"id": "121e8667c1b5d42004535b38316e9c73b7898f16", "path": "headphones/webserve.py", "repo_name": "WebSpider/headphones", "content": "#  This file is part of Headphones.\n#\n#  Headphones is free software: you can redistribute it and/or modify\n#  it under the terms of the GNU General Public License as published by\n#  the Free Software Foundation, either version 3 of the License, or\n#  (at your option) any later version.\n#\n#  Headphones is distributed in the hope that it will be useful,\n#  but WITHOUT ANY WARRANTY; without even the implied warranty of\n#  MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n#  GNU General Public License for more details.\n#\n#  You should have received a copy of the GNU General Public License\n#  along with Headphones.  If not, see <http://www.gnu.org/licenses/>.\n\n# NZBGet support added by CurlyMo <curlymoo1@gmail.com> as a part of XBian - XBMC on the Raspberry Pi\n\nfrom headphones import logger, searcher, db, importer, mb, lastfm, librarysync, helpers, notifiers\nfrom headphones.helpers import checked, radio, today, cleanName\n\nfrom mako.lookup import TemplateLookup\nfrom mako import exceptions\n\nfrom operator import itemgetter\n\nimport headphones\nimport threading\nimport cherrypy\nimport urllib2\nimport hashlib\nimport random\nimport urllib\nimport json\nimport time\nimport cgi\nimport sys\nimport os\nimport re\n\ntry:\n    # pylint:disable=E0611\n    # ignore this error because we are catching the ImportError\n    from collections import OrderedDict\n    # pylint:enable=E0611\nexcept ImportError:\n    # Python 2.6.x fallback, from libs\n    from ordereddict import OrderedDict\n\n\ndef serve_template(templatename, **kwargs):\n\n    interface_dir = os.path.join(str(headphones.PROG_DIR), 'data/interfaces/')\n    template_dir = os.path.join(str(interface_dir), headphones.CONFIG.INTERFACE)\n\n    _hplookup = TemplateLookup(directories=[template_dir])\n\n    try:\n        template = _hplookup.get_template(templatename)\n        return template.render(**kwargs)\n    except:\n        return exceptions.html_error_template().render()\n\n\nclass WebInterface(object):\n\n    @cherrypy.expose\n    def index(self):\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def home(self):\n        myDB = db.DBConnection()\n        artists = myDB.select('SELECT * from artists order by ArtistSortName COLLATE NOCASE')\n        return serve_template(templatename=\"index.html\", title=\"Home\", artists=artists)\n\n    @cherrypy.expose\n    def artistPage(self, ArtistID):\n        myDB = db.DBConnection()\n        artist = myDB.action('SELECT * FROM artists WHERE ArtistID=?', [ArtistID]).fetchone()\n\n        # Don't redirect to the artist page until it has the bare minimum info inserted\n        # Redirect to the home page if we still can't get it after 5 seconds\n        retry = 0\n\n        while not artist and retry < 5:\n            time.sleep(1)\n            artist = myDB.action('SELECT * FROM artists WHERE ArtistID=?', [ArtistID]).fetchone()\n            retry += 1\n\n        if not artist:\n            raise cherrypy.HTTPRedirect(\"home\")\n\n        albums = myDB.select('SELECT * from albums WHERE ArtistID=? order by ReleaseDate DESC', [ArtistID])\n\n        # Serve the extras up as a dict to make things easier for new templates (append new extras to the end)\n        extras_list = headphones.POSSIBLE_EXTRAS\n        if artist['Extras']:\n            artist_extras = map(int, artist['Extras'].split(','))\n        else:\n            artist_extras = []\n\n        extras_dict = OrderedDict()\n\n        i = 1\n        for extra in extras_list:\n            if i in artist_extras:\n                extras_dict[extra] = \"checked\"\n            else:\n                extras_dict[extra] = \"\"\n            i += 1\n\n        return serve_template(templatename=\"artist.html\", title=artist['ArtistName'], artist=artist, albums=albums, extras=extras_dict)\n\n    @cherrypy.expose\n    def albumPage(self, AlbumID):\n        myDB = db.DBConnection()\n        album = myDB.action('SELECT * from albums WHERE AlbumID=?', [AlbumID]).fetchone()\n\n        retry = 0\n        while retry < 5:\n            if not album:\n                time.sleep(1)\n                album = myDB.action('SELECT * from albums WHERE AlbumID=?', [AlbumID]).fetchone()\n                retry += 1\n            else:\n                break\n\n        if not album:\n            raise cherrypy.HTTPRedirect(\"home\")\n\n        tracks = myDB.select('SELECT * from tracks WHERE AlbumID=? ORDER BY CAST(TrackNumber AS INTEGER)', [AlbumID])\n        description = myDB.action('SELECT * from descriptions WHERE ReleaseGroupID=?', [AlbumID]).fetchone()\n\n        if not album['ArtistName']:\n            title = ' - '\n        else:\n            title = album['ArtistName'] + ' - '\n        if not album['AlbumTitle']:\n            title = title + \"\"\n        else:\n            title = title + album['AlbumTitle']\n        return serve_template(templatename=\"album.html\", title=title, album=album, tracks=tracks, description=description)\n\n    @cherrypy.expose\n    def search(self, name, type):\n        if len(name) == 0:\n            raise cherrypy.HTTPRedirect(\"home\")\n        if type == 'artist':\n            searchresults = mb.findArtist(name, limit=100)\n        elif type == 'album':\n            searchresults = mb.findRelease(name, limit=100)\n        else:\n            searchresults = mb.findSeries(name, limit=100)\n        return serve_template(templatename=\"searchresults.html\", title='Search Results for: \"' + cgi.escape(name) + '\"', searchresults=searchresults, name=cgi.escape(name), type=type)\n\n    @cherrypy.expose\n    def addArtist(self, artistid):\n        thread = threading.Thread(target=importer.addArtisttoDB, args=[artistid])\n        thread.start()\n        thread.join(1)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % artistid)\n\n    @cherrypy.expose\n    def addSeries(self, seriesid):\n        thread = threading.Thread(target=importer.addArtisttoDB, args=[seriesid, False, False, \"series\"])\n        thread.start()\n        thread.join(1)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % seriesid)\n\n    @cherrypy.expose\n    def getExtras(self, ArtistID, newstyle=False, **kwargs):\n        # if calling this function without the newstyle, they're using the old format\n        # which doesn't separate extras, so we'll grab all of them\n        #\n        # If they are, we need to convert kwargs to string format\n        if not newstyle:\n            extras = \"1,2,3,4,5,6,7,8,9,10,11,12,13,14\"\n        else:\n            temp_extras_list = []\n            i = 1\n            for extra in headphones.POSSIBLE_EXTRAS:\n                if extra in kwargs:\n                    temp_extras_list.append(i)\n                i += 1\n            extras = ','.join(str(n) for n in temp_extras_list)\n\n        myDB = db.DBConnection()\n        controlValueDict = {'ArtistID': ArtistID}\n        newValueDict = {'IncludeExtras': 1,\n                        'Extras': extras}\n        myDB.upsert(\"artists\", newValueDict, controlValueDict)\n        thread = threading.Thread(target=importer.addArtisttoDB, args=[ArtistID, True, False])\n        thread.start()\n        thread.join(1)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def removeExtras(self, ArtistID, ArtistName):\n        myDB = db.DBConnection()\n        controlValueDict = {'ArtistID': ArtistID}\n        newValueDict = {'IncludeExtras': 0}\n        myDB.upsert(\"artists\", newValueDict, controlValueDict)\n        extraalbums = myDB.select('SELECT AlbumID from albums WHERE ArtistID=? AND Status=\"Skipped\" AND Type!=\"Album\"', [ArtistID])\n        for album in extraalbums:\n            myDB.action('DELETE from tracks WHERE ArtistID=? AND AlbumID=?', [ArtistID, album['AlbumID']])\n            myDB.action('DELETE from albums WHERE ArtistID=? AND AlbumID=?', [ArtistID, album['AlbumID']])\n            myDB.action('DELETE from allalbums WHERE ArtistID=? AND AlbumID=?', [ArtistID, album['AlbumID']])\n            myDB.action('DELETE from alltracks WHERE ArtistID=? AND AlbumID=?', [ArtistID, album['AlbumID']])\n            myDB.action('DELETE from releases WHERE ReleaseGroupID=?', [album['AlbumID']])\n            from headphones import cache\n            c = cache.Cache()\n            c.remove_from_cache(AlbumID=album['AlbumID'])\n        importer.finalize_update(ArtistID, ArtistName)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def pauseArtist(self, ArtistID):\n        logger.info(u\"Pausing artist: \" + ArtistID)\n        myDB = db.DBConnection()\n        controlValueDict = {'ArtistID': ArtistID}\n        newValueDict = {'Status': 'Paused'}\n        myDB.upsert(\"artists\", newValueDict, controlValueDict)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def resumeArtist(self, ArtistID):\n        logger.info(u\"Resuming artist: \" + ArtistID)\n        myDB = db.DBConnection()\n        controlValueDict = {'ArtistID': ArtistID}\n        newValueDict = {'Status': 'Active'}\n        myDB.upsert(\"artists\", newValueDict, controlValueDict)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    def removeArtist(self, ArtistID):\n        myDB = db.DBConnection()\n        namecheck = myDB.select('SELECT ArtistName from artists where ArtistID=?', [ArtistID])\n        for name in namecheck:\n            artistname = name['ArtistName']\n        logger.info(u\"Deleting all traces of artist: \" + artistname)\n        myDB.action('DELETE from artists WHERE ArtistID=?', [ArtistID])\n\n        from headphones import cache\n        c = cache.Cache()\n\n        rgids = myDB.select('SELECT AlbumID FROM albums WHERE ArtistID=? UNION SELECT AlbumID FROM allalbums WHERE ArtistID=?', [ArtistID, ArtistID])\n        for rgid in rgids:\n            albumid = rgid['AlbumID']\n            myDB.action('DELETE from releases WHERE ReleaseGroupID=?', [albumid])\n            myDB.action('DELETE from have WHERE Matched=?', [albumid])\n            c.remove_from_cache(AlbumID=albumid)\n            myDB.action('DELETE from descriptions WHERE ReleaseGroupID=?', [albumid])\n\n        myDB.action('DELETE from albums WHERE ArtistID=?', [ArtistID])\n        myDB.action('DELETE from tracks WHERE ArtistID=?', [ArtistID])\n\n        myDB.action('DELETE from allalbums WHERE ArtistID=?', [ArtistID])\n        myDB.action('DELETE from alltracks WHERE ArtistID=?', [ArtistID])\n        myDB.action('DELETE from have WHERE ArtistName=?', [artistname])\n        c.remove_from_cache(ArtistID=ArtistID)\n        myDB.action('DELETE from descriptions WHERE ArtistID=?', [ArtistID])\n        myDB.action('INSERT OR REPLACE into blacklist VALUES (?)', [ArtistID])\n\n    @cherrypy.expose\n    def deleteArtist(self, ArtistID):\n        self.removeArtist(ArtistID)\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def scanArtist(self, ArtistID):\n\n        myDB = db.DBConnection()\n        artist_name = myDB.select('SELECT DISTINCT ArtistName FROM artists WHERE ArtistID=?', [ArtistID])[0][0]\n\n        logger.info(u\"Scanning artist: %s\", artist_name)\n\n        full_folder_format = headphones.CONFIG.FOLDER_FORMAT\n        folder_format = re.findall(r'(.*?[Aa]rtist?)\\.*', full_folder_format)[0]\n\n        acceptable_formats = [\"$artist\",\"$sortartist\",\"$first/$artist\",\"$first/$sortartist\"]\n\n        if not folder_format.lower() in acceptable_formats:\n            logger.info(\"Can't determine the artist folder from the configured folder_format. Not scanning\")\n            return\n\n        # Format the folder to match the settings\n        artist = artist_name.replace('/', '_')\n\n        if headphones.CONFIG.FILE_UNDERSCORES:\n            artist = artist.replace(' ', '_')\n\n        if artist.startswith('The '):\n            sortname = artist[4:] + \", The\"\n        else:\n            sortname = artist\n\n        if sortname[0].isdigit():\n            firstchar = u'0-9'\n        else:\n            firstchar = sortname[0]\n\n        values = {'$Artist': artist,\n                '$SortArtist': sortname,\n                '$First': firstchar.upper(),\n                '$artist': artist.lower(),\n                '$sortartist': sortname.lower(),\n                '$first': firstchar.lower(),\n            }\n\n        folder = helpers.replace_all(folder_format.strip(), values, normalize=True)\n\n        folder = helpers.replace_illegal_chars(folder, type=\"folder\")\n        folder = folder.replace('./', '_/').replace('/.', '/_')\n\n        if folder.endswith('.'):\n            folder = folder[:-1] + '_'\n\n        if folder.startswith('.'):\n            folder = '_' + folder[1:]\n\n        dirs = []\n        if headphones.CONFIG.MUSIC_DIR:\n            dirs.append(headphones.CONFIG.MUSIC_DIR)\n        if headphones.CONFIG.DESTINATION_DIR:\n            dirs.append(headphones.CONFIG.DESTINATION_DIR)\n        if headphones.CONFIG.LOSSLESS_DESTINATION_DIR:\n            dirs.append(headphones.CONFIG.LOSSLESS_DESTINATION_DIR)\n\n        dirs = set(dirs)\n\n        for dir in dirs:\n            artistfolder = os.path.join(dir, folder)\n            if not os.path.isdir(artistfolder):\n                logger.debug(\"Cannot find directory: \" + artistfolder)\n                continue\n            threading.Thread(target=librarysync.libraryScan, kwargs={\"dir\":artistfolder, \"artistScan\":True, \"ArtistID\":ArtistID, \"ArtistName\":artist_name}).start()\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def deleteEmptyArtists(self):\n        logger.info(u\"Deleting all empty artists\")\n        myDB = db.DBConnection()\n        emptyArtistIDs = [row['ArtistID'] for row in myDB.select(\"SELECT ArtistID FROM artists WHERE LatestAlbum IS NULL\")]\n        for ArtistID in emptyArtistIDs:\n            self.removeArtist(ArtistID)\n\n    @cherrypy.expose\n    def refreshArtist(self, ArtistID):\n        thread = threading.Thread(target=importer.addArtisttoDB, args=[ArtistID, False, True])\n        thread.start()\n        thread.join(1)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def markAlbums(self, ArtistID=None, action=None, **args):\n        myDB = db.DBConnection()\n        if action == 'WantedNew' or action == 'WantedLossless':\n            newaction = 'Wanted'\n        else:\n            newaction = action\n        for mbid in args:\n            logger.info(\"Marking %s as %s\" % (mbid, newaction))\n            controlValueDict = {'AlbumID': mbid}\n            newValueDict = {'Status': newaction}\n            myDB.upsert(\"albums\", newValueDict, controlValueDict)\n            if action == 'Wanted':\n                searcher.searchforalbum(mbid, new=False)\n            if action == 'WantedNew':\n                searcher.searchforalbum(mbid, new=True)\n            if action == 'WantedLossless':\n                searcher.searchforalbum(mbid, lossless=True)\n            if ArtistID:\n                ArtistIDT = ArtistID\n            else:\n                ArtistIDT = myDB.action('SELECT ArtistID FROM albums WHERE AlbumID=?', [mbid]).fetchone()[0]\n            myDB.action('UPDATE artists SET TotalTracks=(SELECT COUNT(*) FROM tracks WHERE ArtistID = ? AND AlbumTitle IN (SELECT AlbumTitle FROM albums WHERE Status != \"Ignored\")) WHERE ArtistID = ?', [ArtistIDT, ArtistIDT])\n        if ArtistID:\n            raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n        else:\n            raise cherrypy.HTTPRedirect(\"upcoming\")\n\n    @cherrypy.expose\n    def addArtists(self, action=None, **args):\n        if action == \"add\":\n            threading.Thread(target=importer.artistlist_to_mbids, args=[args, True]).start()\n        if action == \"ignore\":\n            myDB = db.DBConnection()\n            for artist in args:\n                myDB.action('DELETE FROM newartists WHERE ArtistName=?', [artist.decode(headphones.SYS_ENCODING, 'replace')])\n                myDB.action('UPDATE have SET Matched=\"Ignored\" WHERE ArtistName=?', [artist.decode(headphones.SYS_ENCODING, 'replace')])\n                logger.info(\"Artist %s removed from new artist list and set to ignored\" % artist)\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def queueAlbum(self, AlbumID, ArtistID=None, new=False, redirect=None, lossless=False):\n        logger.info(u\"Marking album: \" + AlbumID + \" as wanted...\")\n        myDB = db.DBConnection()\n        controlValueDict = {'AlbumID': AlbumID}\n        if lossless:\n            newValueDict = {'Status': 'Wanted Lossless'}\n            logger.info(\"...lossless only!\")\n        else:\n            newValueDict = {'Status': 'Wanted'}\n        myDB.upsert(\"albums\", newValueDict, controlValueDict)\n        searcher.searchforalbum(AlbumID, new)\n        if ArtistID:\n            redirect = \"artistPage?ArtistID=%s\" % ArtistID\n        raise cherrypy.HTTPRedirect(redirect)\n\n    @cherrypy.expose\n    def choose_specific_download(self, AlbumID):\n        results = searcher.searchforalbum(AlbumID, choose_specific_download=True)\n\n        results_as_dicts = []\n\n        for result in results:\n            result_dict = {\n                'title': result[0],\n                'size': result[1],\n                'url': result[2],\n                'provider': result[3],\n                'kind': result[4],\n                'matches': result[5]\n            }\n            results_as_dicts.append(result_dict)\n        s = json.dumps(results_as_dicts)\n        cherrypy.response.headers['Content-type'] = 'application/json'\n        return s\n\n    @cherrypy.expose\n    def download_specific_release(self, AlbumID, title, size, url, provider, kind, **kwargs):\n        # Handle situations where the torrent url contains arguments that are parsed\n        if kwargs:\n            url = urllib2.quote(url, safe=\":?/=&\") + '&' + urllib.urlencode(kwargs)\n        try:\n            result = [(title, int(size), url, provider, kind)]\n        except ValueError:\n            result = [(title, float(size), url, provider, kind)]\n\n        logger.info(u\"Making sure we can download the chosen result\")\n        (data, bestqual) = searcher.preprocess(result)\n\n        if data and bestqual:\n          myDB = db.DBConnection()\n          album = myDB.action('SELECT * from albums WHERE AlbumID=?', [AlbumID]).fetchone()\n          searcher.send_to_downloader(data, bestqual, album)\n          return json.dumps({'result':'success'})\n        else:\n          return json.dumps({'result':'failure'})\n\n    @cherrypy.expose\n    def unqueueAlbum(self, AlbumID, ArtistID):\n        logger.info(u\"Marking album: \" + AlbumID + \"as skipped...\")\n        myDB = db.DBConnection()\n        controlValueDict = {'AlbumID': AlbumID}\n        newValueDict = {'Status': 'Skipped'}\n        myDB.upsert(\"albums\", newValueDict, controlValueDict)\n        raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n\n    @cherrypy.expose\n    def deleteAlbum(self, AlbumID, ArtistID=None):\n        logger.info(u\"Deleting all traces of album: \" + AlbumID)\n        myDB = db.DBConnection()\n\n        myDB.action('DELETE from have WHERE Matched=?', [AlbumID])\n        album = myDB.action('SELECT ArtistID, ArtistName, AlbumTitle from albums where AlbumID=?', [AlbumID]).fetchone()\n        if album:\n            ArtistID = album['ArtistID']\n            myDB.action('DELETE from have WHERE ArtistName=? AND AlbumTitle=?', [album['ArtistName'], album['AlbumTitle']])\n\n        myDB.action('DELETE from albums WHERE AlbumID=?', [AlbumID])\n        myDB.action('DELETE from tracks WHERE AlbumID=?', [AlbumID])\n        myDB.action('DELETE from allalbums WHERE AlbumID=?', [AlbumID])\n        myDB.action('DELETE from alltracks WHERE AlbumID=?', [AlbumID])\n        myDB.action('DELETE from releases WHERE ReleaseGroupID=?', [AlbumID])\n        myDB.action('DELETE from descriptions WHERE ReleaseGroupID=?', [AlbumID])\n\n        from headphones import cache\n        c = cache.Cache()\n        c.remove_from_cache(AlbumID=AlbumID)\n\n        if ArtistID:\n            raise cherrypy.HTTPRedirect(\"artistPage?ArtistID=%s\" % ArtistID)\n        else:\n            raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def switchAlbum(self, AlbumID, ReleaseID):\n        \"\"\"\n        Take the values from allalbums/alltracks (based on the ReleaseID) and\n        swap it into the album & track tables\n        \"\"\"\n        from headphones import albumswitcher\n        albumswitcher.switch(AlbumID, ReleaseID)\n        raise cherrypy.HTTPRedirect(\"albumPage?AlbumID=%s\" % AlbumID)\n\n    @cherrypy.expose\n    def editSearchTerm(self, AlbumID, SearchTerm):\n        logger.info(u\"Updating search term for albumid: \" + AlbumID)\n        myDB = db.DBConnection()\n        controlValueDict = {'AlbumID': AlbumID}\n        newValueDict = {'SearchTerm': SearchTerm}\n        myDB.upsert(\"albums\", newValueDict, controlValueDict)\n        raise cherrypy.HTTPRedirect(\"albumPage?AlbumID=%s\" % AlbumID)\n\n    @cherrypy.expose\n    def upcoming(self):\n        myDB = db.DBConnection()\n        upcoming = myDB.select(\"SELECT * from albums WHERE ReleaseDate > date('now') order by ReleaseDate ASC\")\n        wanted = myDB.select(\"SELECT * from albums WHERE Status='Wanted'\")\n        return serve_template(templatename=\"upcoming.html\", title=\"Upcoming\", upcoming=upcoming, wanted=wanted)\n\n    @cherrypy.expose\n    def manage(self):\n        myDB = db.DBConnection()\n        emptyArtists = myDB.select(\"SELECT * FROM artists WHERE LatestAlbum IS NULL\")\n        return serve_template(templatename=\"manage.html\", title=\"Manage\", emptyArtists=emptyArtists)\n\n    @cherrypy.expose\n    def manageArtists(self):\n        myDB = db.DBConnection()\n        artists = myDB.select('SELECT * from artists order by ArtistSortName COLLATE NOCASE')\n        return serve_template(templatename=\"manageartists.html\", title=\"Manage Artists\", artists=artists)\n\n    @cherrypy.expose\n    def manageAlbums(self, Status=None):\n        myDB = db.DBConnection()\n        if Status == \"Upcoming\":\n            albums = myDB.select(\"SELECT * from albums WHERE ReleaseDate > date('now')\")\n        elif Status:\n            albums = myDB.select('SELECT * from albums WHERE Status=?', [Status])\n        else:\n            albums = myDB.select('SELECT * from albums')\n        return serve_template(templatename=\"managealbums.html\", title=\"Manage Albums\", albums=albums)\n\n    @cherrypy.expose\n    def manageNew(self):\n        myDB = db.DBConnection()\n        newartists = myDB.select('SELECT * from newartists')\n        return serve_template(templatename=\"managenew.html\", title=\"Manage New Artists\", newartists=newartists)\n\n    @cherrypy.expose\n    def manageUnmatched(self):\n        myDB = db.DBConnection()\n        have_album_dictionary = []\n        headphones_album_dictionary = []\n        have_albums = myDB.select('SELECT ArtistName, AlbumTitle, TrackTitle, CleanName from have WHERE Matched = \"Failed\" GROUP BY AlbumTitle ORDER BY ArtistName')\n        for albums in have_albums:\n            #Have to skip over manually matched tracks\n            if albums['ArtistName'] and albums['AlbumTitle'] and albums['TrackTitle']:\n                original_clean = helpers.cleanName(albums['ArtistName'] + \" \" + albums['AlbumTitle'] + \" \" + albums['TrackTitle'])\n            # else:\n            #     original_clean = None\n                if original_clean == albums['CleanName']:\n                    have_dict = {'ArtistName': albums['ArtistName'], 'AlbumTitle': albums['AlbumTitle']}\n                    have_album_dictionary.append(have_dict)\n        headphones_albums = myDB.select('SELECT ArtistName, AlbumTitle from albums ORDER BY ArtistName')\n        for albums in headphones_albums:\n            if albums['ArtistName'] and albums['AlbumTitle']:\n                headphones_dict = {'ArtistName': albums['ArtistName'], 'AlbumTitle': albums['AlbumTitle']}\n                headphones_album_dictionary.append(headphones_dict)\n        #unmatchedalbums = [f for f in have_album_dictionary if f not in [x for x in headphones_album_dictionary]]\n\n        check = set([(cleanName(d['ArtistName']).lower(), cleanName(d['AlbumTitle']).lower()) for d in headphones_album_dictionary])\n        unmatchedalbums = [d for d in have_album_dictionary if (cleanName(d['ArtistName']).lower(), cleanName(d['AlbumTitle']).lower()) not in check]\n\n        return serve_template(templatename=\"manageunmatched.html\", title=\"Manage Unmatched Items\", unmatchedalbums=unmatchedalbums)\n\n    @cherrypy.expose\n    def markUnmatched(self, action=None, existing_artist=None, existing_album=None, new_artist=None, new_album=None):\n        myDB = db.DBConnection()\n\n        if action == \"ignoreArtist\":\n            artist = existing_artist\n            myDB.action('UPDATE have SET Matched=\"Ignored\" WHERE ArtistName=? AND Matched = \"Failed\"', [artist])\n\n        elif action == \"ignoreAlbum\":\n            artist = existing_artist\n            album = existing_album\n            myDB.action('UPDATE have SET Matched=\"Ignored\" WHERE ArtistName=? AND AlbumTitle=? AND Matched = \"Failed\"', (artist, album))\n\n        elif action == \"matchArtist\":\n            existing_artist_clean = helpers.cleanName(existing_artist).lower()\n            new_artist_clean = helpers.cleanName(new_artist).lower()\n            if new_artist_clean != existing_artist_clean:\n                have_tracks = myDB.action('SELECT Matched, CleanName, Location, BitRate, Format FROM have WHERE ArtistName=?', [existing_artist])\n                update_count = 0\n                for entry in have_tracks:\n                    old_clean_filename = entry['CleanName']\n                    if old_clean_filename.startswith(existing_artist_clean):\n                        new_clean_filename = old_clean_filename.replace(existing_artist_clean, new_artist_clean, 1)\n                        myDB.action('UPDATE have SET CleanName=? WHERE ArtistName=? AND CleanName=?', [new_clean_filename, existing_artist, old_clean_filename])\n                        controlValueDict = {\"CleanName\": new_clean_filename}\n                        newValueDict = {\"Location\": entry['Location'],\n                                        \"BitRate\": entry['BitRate'],\n                                        \"Format\": entry['Format']\n                                        }\n                        #Attempt to match tracks with new CleanName\n                        match_alltracks = myDB.action('SELECT CleanName from alltracks WHERE CleanName=?', [new_clean_filename]).fetchone()\n                        if match_alltracks:\n                            myDB.upsert(\"alltracks\", newValueDict, controlValueDict)\n                        match_tracks = myDB.action('SELECT CleanName, AlbumID from tracks WHERE CleanName=?', [new_clean_filename]).fetchone()\n                        if match_tracks:\n                            myDB.upsert(\"tracks\", newValueDict, controlValueDict)\n                            myDB.action('UPDATE have SET Matched=\"Manual\" WHERE CleanName=?', [new_clean_filename])\n                            update_count += 1\n                    #This was throwing errors and I don't know why, but it seems to be working fine.\n                    #else:\n                        #logger.info(\"There was an error modifying Artist %s. This should not have happened\" % existing_artist)\n                logger.info(\"Manual matching yielded %s new matches for Artist: %s\" % (update_count, new_artist))\n                if update_count > 0:\n                    librarysync.update_album_status()\n            else:\n                logger.info(\"Artist %s already named appropriately; nothing to modify\" % existing_artist)\n\n        elif action == \"matchAlbum\":\n            existing_artist_clean = helpers.cleanName(existing_artist).lower()\n            new_artist_clean = helpers.cleanName(new_artist).lower()\n            existing_album_clean = helpers.cleanName(existing_album).lower()\n            new_album_clean = helpers.cleanName(new_album).lower()\n            existing_clean_string = existing_artist_clean + \" \" + existing_album_clean\n            new_clean_string = new_artist_clean + \" \" + new_album_clean\n            if existing_clean_string != new_clean_string:\n                have_tracks = myDB.action('SELECT Matched, CleanName, Location, BitRate, Format FROM have WHERE ArtistName=? AND AlbumTitle=?', (existing_artist, existing_album))\n                update_count = 0\n                for entry in have_tracks:\n                    old_clean_filename = entry['CleanName']\n                    if old_clean_filename.startswith(existing_clean_string):\n                        new_clean_filename = old_clean_filename.replace(existing_clean_string, new_clean_string, 1)\n                        myDB.action('UPDATE have SET CleanName=? WHERE ArtistName=? AND AlbumTitle=? AND CleanName=?', [new_clean_filename, existing_artist, existing_album, old_clean_filename])\n                        controlValueDict = {\"CleanName\": new_clean_filename}\n                        newValueDict = {\"Location\": entry['Location'],\n                                        \"BitRate\": entry['BitRate'],\n                                        \"Format\": entry['Format']\n                                        }\n                        #Attempt to match tracks with new CleanName\n                        match_alltracks = myDB.action('SELECT CleanName from alltracks WHERE CleanName=?', [new_clean_filename]).fetchone()\n                        if match_alltracks:\n                            myDB.upsert(\"alltracks\", newValueDict, controlValueDict)\n                        match_tracks = myDB.action('SELECT CleanName, AlbumID from tracks WHERE CleanName=?', [new_clean_filename]).fetchone()\n                        if match_tracks:\n                            myDB.upsert(\"tracks\", newValueDict, controlValueDict)\n                            myDB.action('UPDATE have SET Matched=\"Manual\" WHERE CleanName=?', [new_clean_filename])\n                            album_id = match_tracks['AlbumID']\n                            update_count += 1\n                    #This was throwing errors and I don't know why, but it seems to be working fine.\n                    #else:\n                        #logger.info(\"There was an error modifying Artist %s / Album %s with clean name %s\" % (existing_artist, existing_album, existing_clean_string))\n                logger.info(\"Manual matching yielded %s new matches for Artist: %s / Album: %s\" % (update_count, new_artist, new_album))\n                if update_count > 0:\n                    librarysync.update_album_status(album_id)\n            else:\n                logger.info(\"Artist %s / Album %s already named appropriately; nothing to modify\" % (existing_artist, existing_album))\n\n    @cherrypy.expose\n    def manageManual(self):\n        myDB = db.DBConnection()\n        manual_albums = []\n        manualalbums = myDB.select('SELECT ArtistName, AlbumTitle, TrackTitle, CleanName, Matched from have')\n        for albums in manualalbums:\n            if albums['ArtistName'] and albums['AlbumTitle'] and albums['TrackTitle']:\n                original_clean = helpers.cleanName(albums['ArtistName'] + \" \" + albums['AlbumTitle'] + \" \" + albums['TrackTitle'])\n                if albums['Matched'] == \"Ignored\" or albums['Matched'] == \"Manual\" or albums['CleanName'] != original_clean:\n                    if albums['Matched'] == \"Ignored\":\n                        album_status = \"Ignored\"\n                    elif albums['Matched'] == \"Manual\" or albums['CleanName'] != original_clean:\n                        album_status = \"Matched\"\n                    manual_dict = {'ArtistName': albums['ArtistName'], 'AlbumTitle': albums['AlbumTitle'], 'AlbumStatus': album_status}\n                    if manual_dict not in manual_albums:\n                        manual_albums.append(manual_dict)\n        manual_albums_sorted = sorted(manual_albums, key=itemgetter('ArtistName', 'AlbumTitle'))\n\n        return serve_template(templatename=\"managemanual.html\", title=\"Manage Manual Items\", manualalbums=manual_albums_sorted)\n\n    @cherrypy.expose\n    def markManual(self, action=None, existing_artist=None, existing_album=None):\n        myDB = db.DBConnection()\n        if action == \"unignoreArtist\":\n            artist = existing_artist\n            myDB.action('UPDATE have SET Matched=\"Failed\" WHERE ArtistName=? AND Matched=\"Ignored\"', [artist])\n            logger.info(\"Artist: %s successfully restored to unmatched list\" % artist)\n\n        elif action == \"unignoreAlbum\":\n            artist = existing_artist\n            album = existing_album\n            myDB.action('UPDATE have SET Matched=\"Failed\" WHERE ArtistName=? AND AlbumTitle=? AND Matched=\"Ignored\"', (artist, album))\n            logger.info(\"Album: %s successfully restored to unmatched list\" % album)\n\n        elif action == \"unmatchArtist\":\n            artist = existing_artist\n            update_clean = myDB.select('SELECT ArtistName, AlbumTitle, TrackTitle, CleanName, Matched from have WHERE ArtistName=?', [artist])\n            update_count = 0\n            for tracks in update_clean:\n                original_clean = helpers.cleanName(tracks['ArtistName'] + \" \" + tracks['AlbumTitle'] + \" \" + tracks['TrackTitle']).lower()\n                album = tracks['AlbumTitle']\n                track_title = tracks['TrackTitle']\n                if tracks['CleanName'] != original_clean:\n                    myDB.action('UPDATE tracks SET Location=?, BitRate=?, Format=? WHERE CleanName=?', [None, None, None, tracks['CleanName']])\n                    myDB.action('UPDATE alltracks SET Location=?, BitRate=?, Format=? WHERE CleanName=?', [None, None, None, tracks['CleanName']])\n                    myDB.action('UPDATE have SET CleanName=?, Matched=\"Failed\" WHERE ArtistName=? AND AlbumTitle=? AND TrackTitle=?', (original_clean, artist, album, track_title))\n                    update_count += 1\n            if update_count > 0:\n                librarysync.update_album_status()\n            logger.info(\"Artist: %s successfully restored to unmatched list\" % artist)\n\n        elif action == \"unmatchAlbum\":\n            artist = existing_artist\n            album = existing_album\n            update_clean = myDB.select('SELECT ArtistName, AlbumTitle, TrackTitle, CleanName, Matched from have WHERE ArtistName=? AND AlbumTitle=?', (artist, album))\n            update_count = 0\n            for tracks in update_clean:\n                original_clean = helpers.cleanName(tracks['ArtistName'] + \" \" + tracks['AlbumTitle'] + \" \" + tracks['TrackTitle']).lower()\n                track_title = tracks['TrackTitle']\n                if tracks['CleanName'] != original_clean:\n                    album_id_check = myDB.action('SELECT AlbumID from tracks WHERE CleanName=?', [tracks['CleanName']]).fetchone()\n                    if album_id_check:\n                        album_id = album_id_check[0]\n                    myDB.action('UPDATE tracks SET Location=?, BitRate=?, Format=? WHERE CleanName=?', [None, None, None, tracks['CleanName']])\n                    myDB.action('UPDATE alltracks SET Location=?, BitRate=?, Format=? WHERE CleanName=?', [None, None, None, tracks['CleanName']])\n                    myDB.action('UPDATE have SET CleanName=?, Matched=\"Failed\" WHERE ArtistName=? AND AlbumTitle=? AND TrackTitle=?', (original_clean, artist, album, track_title))\n                    update_count += 1\n            if update_count > 0:\n                librarysync.update_album_status(album_id)\n            logger.info(\"Album: %s successfully restored to unmatched list\" % album)\n\n    @cherrypy.expose\n    def markArtists(self, action=None, **args):\n        myDB = db.DBConnection()\n        artistsToAdd = []\n        for ArtistID in args:\n            if action == 'delete':\n                self.removeArtist(ArtistID)\n            elif action == 'pause':\n                controlValueDict = {'ArtistID': ArtistID}\n                newValueDict = {'Status': 'Paused'}\n                myDB.upsert(\"artists\", newValueDict, controlValueDict)\n            elif action == 'resume':\n                controlValueDict = {'ArtistID': ArtistID}\n                newValueDict = {'Status': 'Active'}\n                myDB.upsert(\"artists\", newValueDict, controlValueDict)\n            else:\n                artistsToAdd.append(ArtistID)\n        if len(artistsToAdd) > 0:\n            logger.debug(\"Refreshing artists: %s\" % artistsToAdd)\n            threading.Thread(target=importer.addArtistIDListToDB, args=[artistsToAdd]).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def importLastFM(self, username):\n        headphones.CONFIG.LASTFM_USERNAME = username\n        headphones.CONFIG.write()\n        threading.Thread(target=lastfm.getArtists).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def importLastFMTag(self, tag, limit):\n        threading.Thread(target=lastfm.getTagTopArtists, args=(tag, limit)).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def importItunes(self, path):\n        headphones.CONFIG.PATH_TO_XML = path\n        headphones.CONFIG.write()\n        thread = threading.Thread(target=importer.itunesImport, args=[path])\n        thread.start()\n        thread.join(10)\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def musicScan(self, path, scan=0, redirect=None, autoadd=0, libraryscan=0):\n        headphones.CONFIG.LIBRARYSCAN = libraryscan\n        headphones.CONFIG.AUTO_ADD_ARTISTS = autoadd\n        headphones.CONFIG.MUSIC_DIR = path\n        headphones.CONFIG.write()\n        if scan:\n            try:\n                threading.Thread(target=librarysync.libraryScan).start()\n            except Exception as e:\n                logger.error('Unable to complete the scan: %s' % e)\n        if redirect:\n            raise cherrypy.HTTPRedirect(redirect)\n        else:\n            raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def forceUpdate(self):\n        from headphones import updater\n        threading.Thread(target=updater.dbUpdate, args=[False]).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def forceFullUpdate(self):\n        from headphones import updater\n        threading.Thread(target=updater.dbUpdate, args=[True]).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def forceSearch(self):\n        from headphones import searcher\n        threading.Thread(target=searcher.searchforalbum).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def forcePostProcess(self, dir=None, album_dir=None, keep_original_folder=False):\n        from headphones import postprocessor\n        threading.Thread(target=postprocessor.forcePostProcess, kwargs={'dir': dir, 'album_dir': album_dir, 'keep_original_folder':keep_original_folder == 'True'}).start()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def checkGithub(self):\n        from headphones import versioncheck\n        versioncheck.checkGithub()\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def history(self):\n        myDB = db.DBConnection()\n        history = myDB.select('''SELECT * from snatched WHERE Status NOT LIKE \"Seed%\" order by DateAdded DESC''')\n        return serve_template(templatename=\"history.html\", title=\"History\", history=history)\n\n    @cherrypy.expose\n    def logs(self):\n        return serve_template(templatename=\"logs.html\", title=\"Log\", lineList=headphones.LOG_LIST)\n\n    @cherrypy.expose\n    def clearLogs(self):\n        headphones.LOG_LIST = []\n        logger.info(\"Web logs cleared\")\n        raise cherrypy.HTTPRedirect(\"logs\")\n\n    @cherrypy.expose\n    def toggleVerbose(self):\n        headphones.VERBOSE = not headphones.VERBOSE\n        logger.initLogger(console=not headphones.QUIET,\n            log_dir=headphones.CONFIG.LOG_DIR, verbose=headphones.VERBOSE)\n        logger.info(\"Verbose toggled, set to %s\", headphones.VERBOSE)\n        logger.debug(\"If you read this message, debug logging is available\")\n        raise cherrypy.HTTPRedirect(\"logs\")\n\n    @cherrypy.expose\n    def getLog(self, iDisplayStart=0, iDisplayLength=100, iSortCol_0=0, sSortDir_0=\"desc\", sSearch=\"\", **kwargs):\n        iDisplayStart = int(iDisplayStart)\n        iDisplayLength = int(iDisplayLength)\n\n        filtered = []\n        if sSearch == \"\":\n            filtered = headphones.LOG_LIST[::]\n        else:\n            filtered = [row for row in headphones.LOG_LIST for column in row if sSearch.lower() in column.lower()]\n\n        sortcolumn = 0\n        if iSortCol_0 == '1':\n            sortcolumn = 2\n        elif iSortCol_0 == '2':\n            sortcolumn = 1\n        filtered.sort(key=lambda x: x[sortcolumn], reverse=sSortDir_0 == \"desc\")\n\n        rows = filtered[iDisplayStart:(iDisplayStart + iDisplayLength)]\n        rows = [[row[0], row[2], row[1]] for row in rows]\n\n        return json.dumps({\n            'iTotalDisplayRecords': len(filtered),\n            'iTotalRecords': len(headphones.LOG_LIST),\n            'aaData': rows,\n        })\n\n    @cherrypy.expose\n    def getArtists_json(self, iDisplayStart=0, iDisplayLength=100, sSearch=\"\", iSortCol_0='0', sSortDir_0='asc', **kwargs):\n        iDisplayStart = int(iDisplayStart)\n        iDisplayLength = int(iDisplayLength)\n        filtered = []\n        totalcount = 0\n        myDB = db.DBConnection()\n\n        sortcolumn = 'ArtistSortName'\n        sortbyhavepercent = False\n        if iSortCol_0 == '2':\n            sortcolumn = 'Status'\n        elif iSortCol_0 == '3':\n            sortcolumn = 'ReleaseDate'\n        elif iSortCol_0 == '4':\n            sortbyhavepercent = True\n\n        if sSearch == \"\":\n            query = 'SELECT * from artists order by %s COLLATE NOCASE %s' % (sortcolumn, sSortDir_0)\n            filtered = myDB.select(query)\n            totalcount = len(filtered)\n        else:\n            query = 'SELECT * from artists WHERE ArtistSortName LIKE \"%' + sSearch + '%\" OR LatestAlbum LIKE \"%' + sSearch + '%\"' + 'ORDER BY %s COLLATE NOCASE %s' % (sortcolumn, sSortDir_0)\n            filtered = myDB.select(query)\n            totalcount = myDB.select('SELECT COUNT(*) from artists')[0][0]\n\n        if sortbyhavepercent:\n            filtered.sort(key=lambda x: (float(x['HaveTracks']) / x['TotalTracks'] if x['TotalTracks'] > 0 else 0.0, x['HaveTracks'] if x['HaveTracks'] else 0.0), reverse=sSortDir_0 == \"asc\")\n\n        #can't figure out how to change the datatables default sorting order when its using an ajax datasource so ill\n        #just reverse it here and the first click on the \"Latest Album\" header will sort by descending release date\n        if sortcolumn == 'ReleaseDate':\n            filtered.reverse()\n\n        artists = filtered[iDisplayStart:(iDisplayStart + iDisplayLength)]\n        rows = []\n        for artist in artists:\n            row = {\"ArtistID\": artist['ArtistID'],\n                      \"ArtistName\": artist[\"ArtistName\"],\n                      \"ArtistSortName\": artist[\"ArtistSortName\"],\n                      \"Status\": artist[\"Status\"],\n                      \"TotalTracks\": artist[\"TotalTracks\"],\n                      \"HaveTracks\": artist[\"HaveTracks\"],\n                      \"LatestAlbum\": \"\",\n                      \"ReleaseDate\": \"\",\n                      \"ReleaseInFuture\": \"False\",\n                      \"AlbumID\": \"\",\n                      }\n\n            if not row['HaveTracks']:\n                row['HaveTracks'] = 0\n            if artist['ReleaseDate'] and artist['LatestAlbum']:\n                row['ReleaseDate'] = artist['ReleaseDate']\n                row['LatestAlbum'] = artist['LatestAlbum']\n                row['AlbumID'] = artist['AlbumID']\n                if artist['ReleaseDate'] > today():\n                    row['ReleaseInFuture'] = \"True\"\n            elif artist['LatestAlbum']:\n                row['ReleaseDate'] = ''\n                row['LatestAlbum'] = artist['LatestAlbum']\n                row['AlbumID'] = artist['AlbumID']\n\n            rows.append(row)\n\n        dict = {'iTotalDisplayRecords': len(filtered),\n                'iTotalRecords': totalcount,\n                'aaData': rows,\n                }\n        s = json.dumps(dict)\n        cherrypy.response.headers['Content-type'] = 'application/json'\n        return s\n\n    @cherrypy.expose\n    def getAlbumsByArtist_json(self, artist=None):\n        myDB = db.DBConnection()\n        album_json = {}\n        counter = 0\n        album_list = myDB.select(\"SELECT AlbumTitle from albums WHERE ArtistName=?\", [artist])\n        for album in album_list:\n            album_json[counter] = album['AlbumTitle']\n            counter += 1\n        json_albums = json.dumps(album_json)\n\n        cherrypy.response.headers['Content-type'] = 'application/json'\n        return json_albums\n\n    @cherrypy.expose\n    def getArtistjson(self, ArtistID, **kwargs):\n        myDB = db.DBConnection()\n        artist = myDB.action('SELECT * FROM artists WHERE ArtistID=?', [ArtistID]).fetchone()\n        artist_json = json.dumps({\n                                    'ArtistName': artist['ArtistName'],\n                                    'Status': artist['Status']\n                                 })\n        return artist_json\n\n    @cherrypy.expose\n    def getAlbumjson(self, AlbumID, **kwargs):\n        myDB = db.DBConnection()\n        album = myDB.action('SELECT * from albums WHERE AlbumID=?', [AlbumID]).fetchone()\n        album_json = json.dumps({\n                                   'AlbumTitle': album['AlbumTitle'],\n                                   'ArtistName': album['ArtistName'],\n                                   'Status': album['Status']\n        })\n        return album_json\n\n    @cherrypy.expose\n    def clearhistory(self, type=None, date_added=None, title=None):\n        myDB = db.DBConnection()\n        if type:\n            if type == 'all':\n                logger.info(u\"Clearing all history\")\n                myDB.action('DELETE from snatched WHERE Status NOT LIKE \"Seed%\"')\n            else:\n                logger.info(u\"Clearing history where status is %s\" % type)\n                myDB.action('DELETE from snatched WHERE Status=?', [type])\n        else:\n            logger.info(u\"Deleting '%s' from history\" % title)\n            myDB.action('DELETE from snatched WHERE Status NOT LIKE \"Seed%\" AND Title=? AND DateAdded=?', [title, date_added])\n        raise cherrypy.HTTPRedirect(\"history\")\n\n    @cherrypy.expose\n    def generateAPI(self):\n        apikey = hashlib.sha224(str(random.getrandbits(256))).hexdigest()[0:32]\n        logger.info(\"New API generated\")\n        return apikey\n\n    @cherrypy.expose\n    def forceScan(self, keepmatched=None):\n        myDB = db.DBConnection()\n        #########################################\n        #NEED TO MOVE THIS INTO A SEPARATE FUNCTION BEFORE RELEASE\n        myDB.select('DELETE from Have')\n        logger.info('Removed all entries in local library database')\n        myDB.select('UPDATE alltracks SET Location=NULL, BitRate=NULL, Format=NULL')\n        myDB.select('UPDATE tracks SET Location=NULL, BitRate=NULL, Format=NULL')\n        logger.info('All tracks in library unmatched')\n        myDB.action('UPDATE artists SET HaveTracks=NULL')\n        logger.info('Reset track counts for all artists')\n        myDB.action('UPDATE albums SET Status=\"Skipped\" WHERE Status=\"Skipped\" OR Status=\"Downloaded\"')\n        logger.info('Marking all unwanted albums as Skipped')\n        try:\n            threading.Thread(target=librarysync.libraryScan).start()\n        except Exception as e:\n            logger.error('Unable to complete the scan: %s' % e)\n        raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def config(self):\n        interface_dir = os.path.join(headphones.PROG_DIR, 'data/interfaces/')\n        interface_list = [name for name in os.listdir(interface_dir) if os.path.isdir(os.path.join(interface_dir, name))]\n\n        config = {\n            \"http_host\": headphones.CONFIG.HTTP_HOST,\n            \"http_username\": headphones.CONFIG.HTTP_USERNAME,\n            \"http_port\": headphones.CONFIG.HTTP_PORT,\n            \"http_password\": headphones.CONFIG.HTTP_PASSWORD,\n            \"launch_browser\": checked(headphones.CONFIG.LAUNCH_BROWSER),\n            \"enable_https\": checked(headphones.CONFIG.ENABLE_HTTPS),\n            \"https_cert\": headphones.CONFIG.HTTPS_CERT,\n            \"https_key\": headphones.CONFIG.HTTPS_KEY,\n            \"api_enabled\": checked(headphones.CONFIG.API_ENABLED),\n            \"api_key\": headphones.CONFIG.API_KEY,\n            \"download_scan_interval\": headphones.CONFIG.DOWNLOAD_SCAN_INTERVAL,\n            \"update_db_interval\": headphones.CONFIG.UPDATE_DB_INTERVAL,\n            \"mb_ignore_age\": headphones.CONFIG.MB_IGNORE_AGE,\n            \"search_interval\": headphones.CONFIG.SEARCH_INTERVAL,\n            \"libraryscan_interval\": headphones.CONFIG.LIBRARYSCAN_INTERVAL,\n            \"sab_host\": headphones.CONFIG.SAB_HOST,\n            \"sab_username\": headphones.CONFIG.SAB_USERNAME,\n            \"sab_apikey\": headphones.CONFIG.SAB_APIKEY,\n            \"sab_password\": headphones.CONFIG.SAB_PASSWORD,\n            \"sab_category\": headphones.CONFIG.SAB_CATEGORY,\n            \"nzbget_host\": headphones.CONFIG.NZBGET_HOST,\n            \"nzbget_username\": headphones.CONFIG.NZBGET_USERNAME,\n            \"nzbget_password\": headphones.CONFIG.NZBGET_PASSWORD,\n            \"nzbget_category\": headphones.CONFIG.NZBGET_CATEGORY,\n            \"nzbget_priority\": headphones.CONFIG.NZBGET_PRIORITY,\n            \"transmission_host\": headphones.CONFIG.TRANSMISSION_HOST,\n            \"transmission_username\": headphones.CONFIG.TRANSMISSION_USERNAME,\n            \"transmission_password\": headphones.CONFIG.TRANSMISSION_PASSWORD,\n            \"utorrent_host\": headphones.CONFIG.UTORRENT_HOST,\n            \"utorrent_username\": headphones.CONFIG.UTORRENT_USERNAME,\n            \"utorrent_password\": headphones.CONFIG.UTORRENT_PASSWORD,\n            \"utorrent_label\": headphones.CONFIG.UTORRENT_LABEL,\n            \"nzb_downloader_sabnzbd\": radio(headphones.CONFIG.NZB_DOWNLOADER, 0),\n            \"nzb_downloader_nzbget\": radio(headphones.CONFIG.NZB_DOWNLOADER, 1),\n            \"nzb_downloader_blackhole\": radio(headphones.CONFIG.NZB_DOWNLOADER, 2),\n            \"torrent_downloader_blackhole\": radio(headphones.CONFIG.TORRENT_DOWNLOADER, 0),\n            \"torrent_downloader_transmission\": radio(headphones.CONFIG.TORRENT_DOWNLOADER, 1),\n            \"torrent_downloader_utorrent\": radio(headphones.CONFIG.TORRENT_DOWNLOADER, 2),\n            \"download_dir\": headphones.CONFIG.DOWNLOAD_DIR,\n            \"use_blackhole\": checked(headphones.CONFIG.BLACKHOLE),\n            \"blackhole_dir\": headphones.CONFIG.BLACKHOLE_DIR,\n            \"usenet_retention\": headphones.CONFIG.USENET_RETENTION,\n            \"headphones_indexer\": checked(headphones.CONFIG.HEADPHONES_INDEXER),\n            \"use_newznab\": checked(headphones.CONFIG.NEWZNAB),\n            \"newznab_host\": headphones.CONFIG.NEWZNAB_HOST,\n            \"newznab_apikey\": headphones.CONFIG.NEWZNAB_APIKEY,\n            \"newznab_enabled\": checked(headphones.CONFIG.NEWZNAB_ENABLED),\n            \"extra_newznabs\": headphones.CONFIG.get_extra_newznabs(),\n            \"use_torznab\": checked(headphones.CONFIG.TORZNAB),\n            \"torznab_host\": headphones.CONFIG.TORZNAB_HOST,\n            \"torznab_apikey\": headphones.CONFIG.TORZNAB_APIKEY,\n            \"torznab_enabled\": checked(headphones.CONFIG.TORZNAB_ENABLED),\n            \"extra_torznabs\": headphones.CONFIG.get_extra_torznabs(),\n            \"use_nzbsorg\": checked(headphones.CONFIG.NZBSORG),\n            \"nzbsorg_uid\": headphones.CONFIG.NZBSORG_UID,\n            \"nzbsorg_hash\": headphones.CONFIG.NZBSORG_HASH,\n            \"use_omgwtfnzbs\": checked(headphones.CONFIG.OMGWTFNZBS),\n            \"omgwtfnzbs_uid\": headphones.CONFIG.OMGWTFNZBS_UID,\n            \"omgwtfnzbs_apikey\": headphones.CONFIG.OMGWTFNZBS_APIKEY,\n            \"preferred_words\": headphones.CONFIG.PREFERRED_WORDS,\n            \"ignored_words\": headphones.CONFIG.IGNORED_WORDS,\n            \"required_words\": headphones.CONFIG.REQUIRED_WORDS,\n            \"ignore_clean_releases\": checked(headphones.CONFIG.IGNORE_CLEAN_RELEASES),\n            \"torrentblackhole_dir\": headphones.CONFIG.TORRENTBLACKHOLE_DIR,\n            \"download_torrent_dir\": headphones.CONFIG.DOWNLOAD_TORRENT_DIR,\n            \"numberofseeders\": headphones.CONFIG.NUMBEROFSEEDERS,\n            \"use_kat\": checked(headphones.CONFIG.KAT),\n            \"kat_proxy_url\": headphones.CONFIG.KAT_PROXY_URL,\n            \"kat_ratio\": headphones.CONFIG.KAT_RATIO,\n            \"use_piratebay\": checked(headphones.CONFIG.PIRATEBAY),\n            \"piratebay_proxy_url\": headphones.CONFIG.PIRATEBAY_PROXY_URL,\n            \"piratebay_ratio\": headphones.CONFIG.PIRATEBAY_RATIO,\n            \"use_oldpiratebay\": checked(headphones.CONFIG.OLDPIRATEBAY),\n            \"oldpiratebay_url\": headphones.CONFIG.OLDPIRATEBAY_URL,\n            \"oldpiratebay_ratio\": headphones.CONFIG.OLDPIRATEBAY_RATIO,\n            \"use_mininova\": checked(headphones.CONFIG.MININOVA),\n            \"mininova_ratio\": headphones.CONFIG.MININOVA_RATIO,\n            \"use_waffles\": checked(headphones.CONFIG.WAFFLES),\n            \"waffles_uid\": headphones.CONFIG.WAFFLES_UID,\n            \"waffles_passkey\": headphones.CONFIG.WAFFLES_PASSKEY,\n            \"waffles_ratio\": headphones.CONFIG.WAFFLES_RATIO,\n            \"use_rutracker\": checked(headphones.CONFIG.RUTRACKER),\n            \"rutracker_user\": headphones.CONFIG.RUTRACKER_USER,\n            \"rutracker_password\": headphones.CONFIG.RUTRACKER_PASSWORD,\n            \"rutracker_ratio\": headphones.CONFIG.RUTRACKER_RATIO,\n            \"use_whatcd\": checked(headphones.CONFIG.WHATCD),\n            \"whatcd_username\": headphones.CONFIG.WHATCD_USERNAME,\n            \"whatcd_password\": headphones.CONFIG.WHATCD_PASSWORD,\n            \"whatcd_ratio\": headphones.CONFIG.WHATCD_RATIO,\n            \"use_strike\": checked(headphones.CONFIG.STRIKE),\n            \"strike_ratio\": headphones.CONFIG.STRIKE_RATIO,\n            \"pref_qual_0\": radio(headphones.CONFIG.PREFERRED_QUALITY, 0),\n            \"pref_qual_1\": radio(headphones.CONFIG.PREFERRED_QUALITY, 1),\n            \"pref_qual_2\": radio(headphones.CONFIG.PREFERRED_QUALITY, 2),\n            \"pref_qual_3\": radio(headphones.CONFIG.PREFERRED_QUALITY, 3),\n            \"preferred_bitrate\": headphones.CONFIG.PREFERRED_BITRATE,\n            \"preferred_bitrate_high\": headphones.CONFIG.PREFERRED_BITRATE_HIGH_BUFFER,\n            \"preferred_bitrate_low\": headphones.CONFIG.PREFERRED_BITRATE_LOW_BUFFER,\n            \"preferred_bitrate_allow_lossless\": checked(headphones.CONFIG.PREFERRED_BITRATE_ALLOW_LOSSLESS),\n            \"detect_bitrate\": checked(headphones.CONFIG.DETECT_BITRATE),\n            \"lossless_bitrate_from\": headphones.CONFIG.LOSSLESS_BITRATE_FROM,\n            \"lossless_bitrate_to\": headphones.CONFIG.LOSSLESS_BITRATE_TO,\n            \"freeze_db\": checked(headphones.CONFIG.FREEZE_DB),\n            \"cue_split\": checked(headphones.CONFIG.CUE_SPLIT),\n            \"cue_split_flac_path\": headphones.CONFIG.CUE_SPLIT_FLAC_PATH,\n            \"cue_split_shntool_path\": headphones.CONFIG.CUE_SPLIT_SHNTOOL_PATH,\n            \"move_files\": checked(headphones.CONFIG.MOVE_FILES),\n            \"rename_files\": checked(headphones.CONFIG.RENAME_FILES),\n            \"correct_metadata\": checked(headphones.CONFIG.CORRECT_METADATA),\n            \"cleanup_files\": checked(headphones.CONFIG.CLEANUP_FILES),\n            \"keep_nfo\": checked(headphones.CONFIG.KEEP_NFO),\n            \"add_album_art\": checked(headphones.CONFIG.ADD_ALBUM_ART),\n            \"album_art_format\": headphones.CONFIG.ALBUM_ART_FORMAT,\n            \"embed_album_art\": checked(headphones.CONFIG.EMBED_ALBUM_ART),\n            \"embed_lyrics\": checked(headphones.CONFIG.EMBED_LYRICS),\n            \"replace_existing_folders\": checked(headphones.CONFIG.REPLACE_EXISTING_FOLDERS),\n            \"keep_original_folder\" : checked(headphones.CONFIG.KEEP_ORIGINAL_FOLDER),\n            \"destination_dir\": headphones.CONFIG.DESTINATION_DIR,\n            \"lossless_destination_dir\": headphones.CONFIG.LOSSLESS_DESTINATION_DIR,\n            \"folder_format\": headphones.CONFIG.FOLDER_FORMAT,\n            \"file_format\": headphones.CONFIG.FILE_FORMAT,\n            \"file_underscores\": checked(headphones.CONFIG.FILE_UNDERSCORES),\n            \"include_extras\": checked(headphones.CONFIG.INCLUDE_EXTRAS),\n            \"official_releases_only\": checked(headphones.CONFIG.OFFICIAL_RELEASES_ONLY),\n            \"wait_until_release_date\": checked(headphones.CONFIG.WAIT_UNTIL_RELEASE_DATE),\n            \"autowant_upcoming\": checked(headphones.CONFIG.AUTOWANT_UPCOMING),\n            \"autowant_all\": checked(headphones.CONFIG.AUTOWANT_ALL),\n            \"autowant_manually_added\": checked(headphones.CONFIG.AUTOWANT_MANUALLY_ADDED),\n            \"do_not_process_unmatched\": checked(headphones.CONFIG.DO_NOT_PROCESS_UNMATCHED),\n            \"keep_torrent_files\": checked(headphones.CONFIG.KEEP_TORRENT_FILES),\n            \"prefer_torrents_0\": radio(headphones.CONFIG.PREFER_TORRENTS, 0),\n            \"prefer_torrents_1\": radio(headphones.CONFIG.PREFER_TORRENTS, 1),\n            \"prefer_torrents_2\": radio(headphones.CONFIG.PREFER_TORRENTS, 2),\n            \"magnet_links_0\": radio(headphones.CONFIG.MAGNET_LINKS, 0),\n            \"magnet_links_1\": radio(headphones.CONFIG.MAGNET_LINKS, 1),\n            \"magnet_links_2\": radio(headphones.CONFIG.MAGNET_LINKS, 2),\n            \"log_dir\": headphones.CONFIG.LOG_DIR,\n            \"cache_dir\": headphones.CONFIG.CACHE_DIR,\n            \"interface_list\": interface_list,\n            \"music_encoder\": checked(headphones.CONFIG.MUSIC_ENCODER),\n            \"encoder\": headphones.CONFIG.ENCODER,\n            \"xldprofile\": headphones.CONFIG.XLDPROFILE,\n            \"bitrate\": int(headphones.CONFIG.BITRATE),\n            \"encoder_path\": headphones.CONFIG.ENCODER_PATH,\n            \"advancedencoder\": headphones.CONFIG.ADVANCEDENCODER,\n            \"encoderoutputformat\": headphones.CONFIG.ENCODEROUTPUTFORMAT,\n            \"samplingfrequency\": headphones.CONFIG.SAMPLINGFREQUENCY,\n            \"encodervbrcbr\": headphones.CONFIG.ENCODERVBRCBR,\n            \"encoderquality\": headphones.CONFIG.ENCODERQUALITY,\n            \"encoderlossless\": checked(headphones.CONFIG.ENCODERLOSSLESS),\n            \"encoder_multicore\": checked(headphones.CONFIG.ENCODER_MULTICORE),\n            \"encoder_multicore_count\": int(headphones.CONFIG.ENCODER_MULTICORE_COUNT),\n            \"delete_lossless_files\": checked(headphones.CONFIG.DELETE_LOSSLESS_FILES),\n            \"growl_enabled\": checked(headphones.CONFIG.GROWL_ENABLED),\n            \"growl_onsnatch\": checked(headphones.CONFIG.GROWL_ONSNATCH),\n            \"growl_host\": headphones.CONFIG.GROWL_HOST,\n            \"growl_password\": headphones.CONFIG.GROWL_PASSWORD,\n            \"prowl_enabled\": checked(headphones.CONFIG.PROWL_ENABLED),\n            \"prowl_onsnatch\": checked(headphones.CONFIG.PROWL_ONSNATCH),\n            \"prowl_keys\": headphones.CONFIG.PROWL_KEYS,\n            \"prowl_priority\": headphones.CONFIG.PROWL_PRIORITY,\n            \"xbmc_enabled\": checked(headphones.CONFIG.XBMC_ENABLED),\n            \"xbmc_host\": headphones.CONFIG.XBMC_HOST,\n            \"xbmc_username\": headphones.CONFIG.XBMC_USERNAME,\n            \"xbmc_password\": headphones.CONFIG.XBMC_PASSWORD,\n            \"xbmc_update\": checked(headphones.CONFIG.XBMC_UPDATE),\n            \"xbmc_notify\": checked(headphones.CONFIG.XBMC_NOTIFY),\n            \"lms_enabled\": checked(headphones.CONFIG.LMS_ENABLED),\n            \"lms_host\": headphones.CONFIG.LMS_HOST,\n            \"plex_enabled\": checked(headphones.CONFIG.PLEX_ENABLED),\n            \"plex_server_host\": headphones.CONFIG.PLEX_SERVER_HOST,\n            \"plex_client_host\": headphones.CONFIG.PLEX_CLIENT_HOST,\n            \"plex_username\": headphones.CONFIG.PLEX_USERNAME,\n            \"plex_password\": headphones.CONFIG.PLEX_PASSWORD,\n            \"plex_token\": headphones.CONFIG.PLEX_TOKEN,\n            \"plex_update\": checked(headphones.CONFIG.PLEX_UPDATE),\n            \"plex_notify\": checked(headphones.CONFIG.PLEX_NOTIFY),\n            \"nma_enabled\": checked(headphones.CONFIG.NMA_ENABLED),\n            \"nma_apikey\": headphones.CONFIG.NMA_APIKEY,\n            \"nma_priority\": int(headphones.CONFIG.NMA_PRIORITY),\n            \"nma_onsnatch\": checked(headphones.CONFIG.NMA_ONSNATCH),\n            \"pushalot_enabled\": checked(headphones.CONFIG.PUSHALOT_ENABLED),\n            \"pushalot_apikey\": headphones.CONFIG.PUSHALOT_APIKEY,\n            \"pushalot_onsnatch\": checked(headphones.CONFIG.PUSHALOT_ONSNATCH),\n            \"synoindex_enabled\": checked(headphones.CONFIG.SYNOINDEX_ENABLED),\n            \"pushover_enabled\": checked(headphones.CONFIG.PUSHOVER_ENABLED),\n            \"pushover_onsnatch\": checked(headphones.CONFIG.PUSHOVER_ONSNATCH),\n            \"pushover_keys\": headphones.CONFIG.PUSHOVER_KEYS,\n            \"pushover_apitoken\": headphones.CONFIG.PUSHOVER_APITOKEN,\n            \"pushover_priority\": headphones.CONFIG.PUSHOVER_PRIORITY,\n            \"pushbullet_enabled\": checked(headphones.CONFIG.PUSHBULLET_ENABLED),\n            \"pushbullet_onsnatch\": checked(headphones.CONFIG.PUSHBULLET_ONSNATCH),\n            \"pushbullet_apikey\": headphones.CONFIG.PUSHBULLET_APIKEY,\n            \"pushbullet_deviceid\": headphones.CONFIG.PUSHBULLET_DEVICEID,\n            \"subsonic_enabled\": checked(headphones.CONFIG.SUBSONIC_ENABLED),\n            \"subsonic_host\": headphones.CONFIG.SUBSONIC_HOST,\n            \"subsonic_username\": headphones.CONFIG.SUBSONIC_USERNAME,\n            \"subsonic_password\": headphones.CONFIG.SUBSONIC_PASSWORD,\n            \"twitter_enabled\": checked(headphones.CONFIG.TWITTER_ENABLED),\n            \"twitter_onsnatch\": checked(headphones.CONFIG.TWITTER_ONSNATCH),\n            \"osx_notify_enabled\": checked(headphones.CONFIG.OSX_NOTIFY_ENABLED),\n            \"osx_notify_onsnatch\": checked(headphones.CONFIG.OSX_NOTIFY_ONSNATCH),\n            \"osx_notify_app\": headphones.CONFIG.OSX_NOTIFY_APP,\n            \"boxcar_enabled\": checked(headphones.CONFIG.BOXCAR_ENABLED),\n            \"boxcar_onsnatch\": checked(headphones.CONFIG.BOXCAR_ONSNATCH),\n            \"boxcar_token\": headphones.CONFIG.BOXCAR_TOKEN,\n            \"mirrorlist\": headphones.MIRRORLIST,\n            \"mirror\": headphones.CONFIG.MIRROR,\n            \"customhost\": headphones.CONFIG.CUSTOMHOST,\n            \"customport\": headphones.CONFIG.CUSTOMPORT,\n            \"customsleep\": headphones.CONFIG.CUSTOMSLEEP,\n            \"customauth\": checked(headphones.CONFIG.CUSTOMAUTH),\n            \"customuser\": headphones.CONFIG.CUSTOMUSER,\n            \"custompass\": headphones.CONFIG.CUSTOMPASS,\n            \"hpuser\": headphones.CONFIG.HPUSER,\n            \"hppass\": headphones.CONFIG.HPPASS,\n            \"songkick_enabled\": checked(headphones.CONFIG.SONGKICK_ENABLED),\n            \"songkick_apikey\": headphones.CONFIG.SONGKICK_APIKEY,\n            \"songkick_location\": headphones.CONFIG.SONGKICK_LOCATION,\n            \"songkick_filter_enabled\": checked(headphones.CONFIG.SONGKICK_FILTER_ENABLED),\n            \"cache_sizemb\": headphones.CONFIG.CACHE_SIZEMB,\n            \"file_permissions\": headphones.CONFIG.FILE_PERMISSIONS,\n            \"folder_permissions\": headphones.CONFIG.FOLDER_PERMISSIONS,\n            \"mpc_enabled\": checked(headphones.CONFIG.MPC_ENABLED),\n            \"email_enabled\": checked(headphones.CONFIG.EMAIL_ENABLED),\n            \"email_from\": headphones.CONFIG.EMAIL_FROM,\n            \"email_to\": headphones.CONFIG.EMAIL_TO,\n            \"email_smtp_server\": headphones.CONFIG.EMAIL_SMTP_SERVER,\n            \"email_smtp_user\": headphones.CONFIG.EMAIL_SMTP_USER,\n            \"email_smtp_password\": headphones.CONFIG.EMAIL_SMTP_PASSWORD,\n            \"email_smtp_port\": int(headphones.CONFIG.EMAIL_SMTP_PORT),\n            \"email_ssl\": checked(headphones.CONFIG.EMAIL_SSL),\n            \"email_tls\": checked(headphones.CONFIG.EMAIL_TLS),\n            \"email_onsnatch\": checked(headphones.CONFIG.EMAIL_ONSNATCH),\n            \"idtag\": checked(headphones.CONFIG.IDTAG)\n        }\n\n        # Need to convert EXTRAS to a dictionary we can pass to the config:\n        # it'll come in as a string like 2,5,6,8\n        extra_munges = {\n            \"dj-mix\": \"dj_mix\",\n            \"mixtape/street\": \"mixtape_street\"\n        }\n\n        extras_list = [extra_munges.get(x, x) for x in headphones.POSSIBLE_EXTRAS]\n        if headphones.CONFIG.EXTRAS:\n            extras = map(int, headphones.CONFIG.EXTRAS.split(','))\n        else:\n            extras = []\n\n        extras_dict = OrderedDict()\n\n        i = 1\n        for extra in extras_list:\n            if i in extras:\n                extras_dict[extra] = \"checked\"\n            else:\n                extras_dict[extra] = \"\"\n            i += 1\n\n        config[\"extras\"] = extras_dict\n\n        return serve_template(templatename=\"config.html\", title=\"Settings\", config=config)\n\n    @cherrypy.expose\n    def configUpdate(self, **kwargs):\n        # Handle the variable config options. Note - keys with False values aren't getting passed\n\n        checked_configs = [\n            \"launch_browser\", \"enable_https\", \"api_enabled\", \"use_blackhole\", \"headphones_indexer\", \"use_newznab\", \"newznab_enabled\", \"use_torznab\", \"torznab_enabled\",\n            \"use_nzbsorg\", \"use_omgwtfnzbs\", \"use_kat\", \"use_piratebay\", \"use_oldpiratebay\", \"use_mininova\", \"use_waffles\", \"use_rutracker\",\n            \"use_whatcd\", \"use_strike\", \"preferred_bitrate_allow_lossless\", \"detect_bitrate\", \"ignore_clean_releases\", \"freeze_db\", \"cue_split\", \"move_files\",\n            \"rename_files\", \"correct_metadata\", \"cleanup_files\", \"keep_nfo\", \"add_album_art\", \"embed_album_art\", \"embed_lyrics\",\n            \"replace_existing_folders\", \"keep_original_folder\", \"file_underscores\", \"include_extras\", \"official_releases_only\",\n            \"wait_until_release_date\", \"autowant_upcoming\", \"autowant_all\", \"autowant_manually_added\", \"do_not_process_unmatched\", \"keep_torrent_files\", \"music_encoder\",\n            \"encoderlossless\", \"encoder_multicore\", \"delete_lossless_files\", \"growl_enabled\", \"growl_onsnatch\", \"prowl_enabled\",\n            \"prowl_onsnatch\", \"xbmc_enabled\", \"xbmc_update\", \"xbmc_notify\", \"lms_enabled\", \"plex_enabled\", \"plex_update\", \"plex_notify\",\n            \"nma_enabled\", \"nma_onsnatch\", \"pushalot_enabled\", \"pushalot_onsnatch\", \"synoindex_enabled\", \"pushover_enabled\",\n            \"pushover_onsnatch\", \"pushbullet_enabled\", \"pushbullet_onsnatch\", \"subsonic_enabled\", \"twitter_enabled\", \"twitter_onsnatch\",\n            \"osx_notify_enabled\", \"osx_notify_onsnatch\", \"boxcar_enabled\", \"boxcar_onsnatch\", \"songkick_enabled\", \"songkick_filter_enabled\",\n            \"mpc_enabled\", \"email_enabled\", \"email_ssl\", \"email_tls\", \"email_onsnatch\", \"customauth\", \"idtag\"\n        ]\n        for checked_config in checked_configs:\n            if checked_config not in kwargs:\n                # checked items should be zero or one. if they were not sent then the item was not checked\n                kwargs[checked_config] = 0\n\n        for plain_config, use_config in [(x[4:], x) for x in kwargs if x.startswith('use_')]:\n            # the use prefix is fairly nice in the html, but does not match the actual config\n            kwargs[plain_config] = kwargs[use_config]\n            del kwargs[use_config]\n\n        extra_newznabs = []\n        for kwarg in [x for x in kwargs if x.startswith('newznab_host')]:\n            newznab_host_key = kwarg\n            newznab_number = kwarg[12:]\n            if len(newznab_number):\n                newznab_api_key = 'newznab_api' + newznab_number\n                newznab_enabled_key = 'newznab_enabled' + newznab_number\n                newznab_host = kwargs.get(newznab_host_key, '')\n                newznab_api = kwargs.get(newznab_api_key, '')\n                newznab_enabled = int(kwargs.get(newznab_enabled_key, 0))\n                for key in [newznab_host_key, newznab_api_key, newznab_enabled_key]:\n                    if key in kwargs:\n                        del kwargs[key]\n                extra_newznabs.append((newznab_host, newznab_api, newznab_enabled))\n\n        extra_torznabs = []\n        for kwarg in [x for x in kwargs if x.startswith('torznab_host')]:\n            torznab_host_key = kwarg\n            torznab_number = kwarg[12:]\n            if len(torznab_number):\n                torznab_api_key = 'torznab_api' + torznab_number\n                torznab_enabled_key = 'torznab_enabled' + torznab_number\n                torznab_host = kwargs.get(torznab_host_key, '')\n                torznab_api = kwargs.get(torznab_api_key, '')\n                torznab_enabled = int(kwargs.get(torznab_enabled_key, 0))\n                for key in [torznab_host_key, torznab_api_key, torznab_enabled_key]:\n                    if key in kwargs:\n                        del kwargs[key]\n                extra_torznabs.append((torznab_host, torznab_api, torznab_enabled))\n\n        # Convert the extras to list then string. Coming in as 0 or 1 (append new extras to the end)\n        temp_extras_list = []\n\n        extra_munges = {\n            \"dj-mix\": \"dj_mix\",\n            \"mixtape/street\": \"mixtape_street\"\n        }\n\n        expected_extras = [extra_munges.get(x, x) for x in headphones.POSSIBLE_EXTRAS]\n        extras_list = [kwargs.get(x, 0) for x in expected_extras]\n\n        i = 1\n        for extra in extras_list:\n            if extra:\n                temp_extras_list.append(i)\n            i += 1\n\n        for extra in expected_extras:\n            temp = '%s_temp' % extra\n            if temp in kwargs:\n                del kwargs[temp]\n            if extra in kwargs:\n                del kwargs[extra]\n\n        headphones.CONFIG.EXTRAS = ','.join(str(n) for n in temp_extras_list)\n\n        headphones.CONFIG.clear_extra_newznabs()\n        headphones.CONFIG.clear_extra_torznabs()\n\n        headphones.CONFIG.process_kwargs(kwargs)\n\n        for extra_newznab in extra_newznabs:\n            headphones.CONFIG.add_extra_newznab(extra_newznab)\n\n        for extra_torznab in extra_torznabs:\n            headphones.CONFIG.add_extra_torznab(extra_torznab)\n\n        # Sanity checking\n        if headphones.CONFIG.SEARCH_INTERVAL and headphones.CONFIG.SEARCH_INTERVAL < 360:\n            logger.info(\"Search interval too low. Resetting to 6 hour minimum\")\n            headphones.CONFIG.SEARCH_INTERVAL = 360\n\n        # Write the config\n        headphones.CONFIG.write()\n\n        # Reconfigure scheduler\n        headphones.initialize_scheduler()\n\n        # Reconfigure musicbrainz database connection with the new values\n        mb.startmb()\n\n        raise cherrypy.HTTPRedirect(\"config\")\n\n    @cherrypy.expose\n    def do_state_change(self, signal, title, timer):\n        headphones.SIGNAL = signal\n        message = title + '...'\n        return serve_template(templatename=\"shutdown.html\", title=title,\n                              message=message, timer=timer)\n\n    @cherrypy.expose\n    def shutdown(self):\n        return self.do_state_change('shutdown', 'Shutting Down', 15)\n\n    @cherrypy.expose\n    def restart(self):\n        return self.do_state_change('restart', 'Restarting', 30)\n\n    @cherrypy.expose\n    def update(self):\n        return self.do_state_change('update', 'Updating', 120)\n\n    @cherrypy.expose\n    def extras(self):\n        myDB = db.DBConnection()\n        cloudlist = myDB.select('SELECT * from lastfmcloud')\n        return serve_template(templatename=\"extras.html\", title=\"Extras\", cloudlist=cloudlist)\n\n    @cherrypy.expose\n    def addReleaseById(self, rid, rgid=None):\n        threading.Thread(target=importer.addReleaseById, args=[rid, rgid]).start()\n        if rgid:\n            raise cherrypy.HTTPRedirect(\"albumPage?AlbumID=%s\" % rgid)\n        else:\n            raise cherrypy.HTTPRedirect(\"home\")\n\n    @cherrypy.expose\n    def updateCloud(self):\n        lastfm.getSimilar()\n        raise cherrypy.HTTPRedirect(\"extras\")\n\n    @cherrypy.expose\n    def api(self, *args, **kwargs):\n        from headphones.api import Api\n\n        a = Api()\n        a.checkParams(*args, **kwargs)\n\n        return a.fetchData()\n\n    @cherrypy.expose\n    def getInfo(self, ArtistID=None, AlbumID=None):\n\n        from headphones import cache\n        info_dict = cache.getInfo(ArtistID, AlbumID)\n\n        return json.dumps(info_dict)\n\n    @cherrypy.expose\n    def getArtwork(self, ArtistID=None, AlbumID=None):\n\n        from headphones import cache\n        return cache.getArtwork(ArtistID, AlbumID)\n\n    @cherrypy.expose\n    def getThumb(self, ArtistID=None, AlbumID=None):\n\n        from headphones import cache\n        return cache.getThumb(ArtistID, AlbumID)\n\n    # If you just want to get the last.fm image links for an album, make sure\n    # to pass a releaseid and not a releasegroupid\n    @cherrypy.expose\n    def getImageLinks(self, ArtistID=None, AlbumID=None):\n        from headphones import cache\n        image_dict = cache.getImageLinks(ArtistID, AlbumID)\n\n        # Return the Cover Art Archive urls if not found on last.fm\n        if AlbumID and not image_dict:\n            image_url = \"http://coverartarchive.org/release/%s/front-500.jpg\" % AlbumID\n            thumb_url = \"http://coverartarchive.org/release/%s/front-250.jpg\" % AlbumID\n            image_dict = {'artwork': image_url, 'thumbnail': thumb_url}\n        elif AlbumID and (not image_dict['artwork'] or not image_dict['thumbnail']):\n            if not image_dict['artwork']:\n                image_dict['artwork'] = \"http://coverartarchive.org/release/%s/front-500.jpg\" % AlbumID\n            if not image_dict['thumbnail']:\n                image_dict['thumbnail'] = \"http://coverartarchive.org/release/%s/front-250.jpg\" % AlbumID\n\n        return json.dumps(image_dict)\n\n    @cherrypy.expose\n    def twitterStep1(self):\n        cherrypy.response.headers['Cache-Control'] = \"max-age=0,no-cache,no-store\"\n        tweet = notifiers.TwitterNotifier()\n        return tweet._get_authorization()\n\n    @cherrypy.expose\n    def twitterStep2(self, key):\n        cherrypy.response.headers['Cache-Control'] = \"max-age=0,no-cache,no-store\"\n        tweet = notifiers.TwitterNotifier()\n        result = tweet._get_credentials(key)\n        logger.info(u\"result: \" + str(result))\n        if result:\n            return \"Key verification successful\"\n        else:\n            return \"Unable to verify key\"\n\n    @cherrypy.expose\n    def testTwitter(self):\n        cherrypy.response.headers['Cache-Control'] = \"max-age=0,no-cache,no-store\"\n        tweet = notifiers.TwitterNotifier()\n        result = tweet.test_notify()\n        if result:\n            return \"Tweet successful, check your twitter to make sure it worked\"\n        else:\n            return \"Error sending tweet\"\n\n    @cherrypy.expose\n    def osxnotifyregister(self, app):\n        cherrypy.response.headers['Cache-Control'] = \"max-age=0,no-cache,no-store\"\n        from osxnotify import registerapp as osxnotify\n        result, msg = osxnotify.registerapp(app)\n        if result:\n            osx_notify = notifiers.OSX_NOTIFY()\n            osx_notify.notify('Registered', result, 'Success :-)')\n            logger.info('Registered %s, to re-register a different app, delete this app first' % result)\n        else:\n            logger.warn(msg)\n        return msg\n\n    @cherrypy.expose\n    def testPushover(self):\n        logger.info(u\"Sending Pushover notification\")\n        pushover = notifiers.PUSHOVER()\n        result = pushover.notify(\"hooray!\", \"This is a test\")\n        return str(result)\n\n    @cherrypy.expose\n    def testPlex(self):\n        logger.info(u\"Testing plex notifications\")\n        plex = notifiers.Plex()\n        plex.notify(\"hellooooo\", \"test album!\", \"\")\n\n    @cherrypy.expose\n    def testPushbullet(self):\n        logger.info(\"Testing Pushbullet notifications\")\n        pushbullet = notifiers.PUSHBULLET()\n        pushbullet.notify(\"it works!\")\n\nclass Artwork(object):\n    @cherrypy.expose\n    def index(self):\n        return \"Artwork\"\n\n    @cherrypy.expose\n    def default(self, ArtistOrAlbum=\"\", ID=None):\n        from headphones import cache\n        ArtistID = None\n        AlbumID = None\n        if ArtistOrAlbum == \"artist\":\n            ArtistID = ID\n        elif ArtistOrAlbum == \"album\":\n            AlbumID = ID\n\n        relpath = cache.getArtwork(ArtistID, AlbumID)\n\n        if not relpath:\n            relpath = \"data/interfaces/default/images/no-cover-art.png\"\n            basedir = os.path.dirname(sys.argv[0])\n            path = os.path.join(basedir, relpath)\n            cherrypy.response.headers['Content-type'] = 'image/png'\n            cherrypy.response.headers['Cache-Control'] = 'no-cache'\n        else:\n            relpath = relpath.replace('cache/', '', 1)\n            path = os.path.join(headphones.CONFIG.CACHE_DIR, relpath)\n            fileext = os.path.splitext(relpath)[1][1::]\n            cherrypy.response.headers['Content-type'] = 'image/' + fileext\n            cherrypy.response.headers['Cache-Control'] = 'max-age=31556926'\n\n        with open(os.path.normpath(path), \"rb\") as fp:\n            return fp.read()\n\n    class Thumbs(object):\n        @cherrypy.expose\n        def index(self):\n            return \"Here be thumbs\"\n\n        @cherrypy.expose\n        def default(self, ArtistOrAlbum=\"\", ID=None):\n            from headphones import cache\n            ArtistID = None\n            AlbumID = None\n            if ArtistOrAlbum == \"artist\":\n                ArtistID = ID\n            elif ArtistOrAlbum == \"album\":\n                AlbumID = ID\n\n            relpath = cache.getThumb(ArtistID, AlbumID)\n\n            if not relpath:\n                relpath = \"data/interfaces/default/images/no-cover-artist.png\"\n                basedir = os.path.dirname(sys.argv[0])\n                path = os.path.join(basedir, relpath)\n                cherrypy.response.headers['Content-type'] = 'image/png'\n                cherrypy.response.headers['Cache-Control'] = 'no-cache'\n            else:\n                relpath = relpath.replace('cache/', '', 1)\n                path = os.path.join(headphones.CONFIG.CACHE_DIR, relpath)\n                fileext = os.path.splitext(relpath)[1][1::]\n                cherrypy.response.headers['Content-type'] = 'image/' + fileext\n                cherrypy.response.headers['Cache-Control'] = 'max-age=31556926'\n\n            with open(os.path.normpath(path), \"rb\") as fp:\n                return fp.read()\n\n    thumbs = Thumbs()\nWebInterface.artwork = Artwork()\n", "license": "gpl-3.0"}
{"id": "3ec3da3c3570fc9140633ee5fe79394895e53c6b", "path": "cosimulation/test/bin2gray.py", "repo_name": "gw0/myhdl", "content": "from myhdl import *\n\ndef bin2gray(B, G, width):\n    \"\"\" Gray encoder.\n\n    B -- input intbv signal, binary encoded\n    G -- output intbv signal, gray encoded\n    width -- bit width\n    \"\"\"\n\n    @always_comb\n    def logic():\n        for i in range(width):\n            G.next[i] = B[i+1] ^ B[i]\n\n    return logic\n\n\n", "license": "lgpl-2.1"}
{"id": "66fef85b04ddc99b9d9d3519c33fca0edec4bad2", "path": "pylib/Twisted/twisted/application/service.py", "repo_name": "smartdata-x/robots", "content": "# Copyright (c) Twisted Matrix Laboratories.\n# See LICENSE for details.\n\n\"\"\"\nService architecture for Twisted.\n\nServices are arranged in a hierarchy. At the leafs of the hierarchy,\nthe services which actually interact with the outside world are started.\nServices can be named or anonymous -- usually, they will be named if\nthere is need to access them through the hierarchy (from a parent or\na sibling).\n\nMaintainer: Moshe Zadka\n\"\"\"\n\nfrom zope.interface import implements, Interface, Attribute\n\nfrom twisted.python.reflect import namedAny\nfrom twisted.python import components\nfrom twisted.internet import defer\nfrom twisted.persisted import sob\nfrom twisted.plugin import IPlugin\n\n\nclass IServiceMaker(Interface):\n    \"\"\"\n    An object which can be used to construct services in a flexible\n    way.\n\n    This interface should most often be implemented along with\n    L{twisted.plugin.IPlugin}, and will most often be used by the\n    'twistd' command.\n    \"\"\"\n    tapname = Attribute(\n        \"A short string naming this Twisted plugin, for example 'web' or \"\n        \"'pencil'. This name will be used as the subcommand of 'twistd'.\")\n\n    description = Attribute(\n        \"A brief summary of the features provided by this \"\n        \"Twisted application plugin.\")\n\n    options = Attribute(\n        \"A C{twisted.python.usage.Options} subclass defining the \"\n        \"configuration options for this application.\")\n\n\n    def makeService(options):\n        \"\"\"\n        Create and return an object providing\n        L{twisted.application.service.IService}.\n\n        @param options: A mapping (typically a C{dict} or\n        L{twisted.python.usage.Options} instance) of configuration\n        options to desired configuration values.\n        \"\"\"\n\n\n\nclass ServiceMaker(object):\n    \"\"\"\n    Utility class to simplify the definition of L{IServiceMaker} plugins.\n    \"\"\"\n    implements(IPlugin, IServiceMaker)\n\n    def __init__(self, name, module, description, tapname):\n        self.name = name\n        self.module = module\n        self.description = description\n        self.tapname = tapname\n\n\n    def options():\n        def get(self):\n            return namedAny(self.module).Options\n        return get,\n    options = property(*options())\n\n\n    def makeService():\n        def get(self):\n            return namedAny(self.module).makeService\n        return get,\n    makeService = property(*makeService())\n\n\n\nclass IService(Interface):\n    \"\"\"\n    A service.\n\n    Run start-up and shut-down code at the appropriate times.\n\n    @type name:            C{string}\n    @ivar name:            The name of the service (or None)\n    @type running:         C{boolean}\n    @ivar running:         Whether the service is running.\n    \"\"\"\n\n    def setName(name):\n        \"\"\"\n        Set the name of the service.\n\n        @type name: C{str}\n        @raise RuntimeError: Raised if the service already has a parent.\n        \"\"\"\n\n    def setServiceParent(parent):\n        \"\"\"\n        Set the parent of the service.  This method is responsible for setting\n        the C{parent} attribute on this service (the child service).\n\n        @type parent: L{IServiceCollection}\n        @raise RuntimeError: Raised if the service already has a parent\n            or if the service has a name and the parent already has a child\n            by that name.\n        \"\"\"\n\n    def disownServiceParent():\n        \"\"\"\n        Use this API to remove an L{IService} from an L{IServiceCollection}.\n\n        This method is used symmetrically with L{setServiceParent} in that it\n        sets the C{parent} attribute on the child.\n\n        @rtype: L{Deferred<defer.Deferred>}\n        @return: a L{Deferred<defer.Deferred>} which is triggered when the\n            service has finished shutting down. If shutting down is immediate,\n            a value can be returned (usually, C{None}).\n        \"\"\"\n\n    def startService():\n        \"\"\"\n        Start the service.\n        \"\"\"\n\n    def stopService():\n        \"\"\"\n        Stop the service.\n\n        @rtype: L{Deferred<defer.Deferred>}\n        @return: a L{Deferred<defer.Deferred>} which is triggered when the\n            service has finished shutting down. If shutting down is immediate,\n            a value can be returned (usually, C{None}).\n        \"\"\"\n\n    def privilegedStartService():\n        \"\"\"\n        Do preparation work for starting the service.\n\n        Here things which should be done before changing directory,\n        root or shedding privileges are done.\n        \"\"\"\n\n\nclass Service:\n    \"\"\"\n    Base class for services.\n\n    Most services should inherit from this class. It handles the\n    book-keeping reponsibilities of starting and stopping, as well\n    as not serializing this book-keeping information.\n    \"\"\"\n\n    implements(IService)\n\n    running = 0\n    name = None\n    parent = None\n\n    def __getstate__(self):\n        dict = self.__dict__.copy()\n        if \"running\" in dict:\n            del dict['running']\n        return dict\n\n    def setName(self, name):\n        if self.parent is not None:\n            raise RuntimeError(\"cannot change name when parent exists\")\n        self.name = name\n\n    def setServiceParent(self, parent):\n        if self.parent is not None:\n            self.disownServiceParent()\n        parent = IServiceCollection(parent, parent)\n        self.parent = parent\n        self.parent.addService(self)\n\n    def disownServiceParent(self):\n        d = self.parent.removeService(self)\n        self.parent = None\n        return d\n\n    def privilegedStartService(self):\n        pass\n\n    def startService(self):\n        self.running = 1\n\n    def stopService(self):\n        self.running = 0\n\n\n\nclass IServiceCollection(Interface):\n    \"\"\"\n    Collection of services.\n\n    Contain several services, and manage their start-up/shut-down.\n    Services can be accessed by name if they have a name, and it\n    is always possible to iterate over them.\n    \"\"\"\n\n    def getServiceNamed(name):\n        \"\"\"\n        Get the child service with a given name.\n\n        @type name: C{str}\n        @rtype: L{IService}\n        @raise KeyError: Raised if the service has no child with the\n            given name.\n        \"\"\"\n\n    def __iter__():\n        \"\"\"\n        Get an iterator over all child services.\n        \"\"\"\n\n    def addService(service):\n        \"\"\"\n        Add a child service.\n\n        Only implementations of L{IService.setServiceParent} should use this\n        method.\n\n        @type service: L{IService}\n        @raise RuntimeError: Raised if the service has a child with\n            the given name.\n        \"\"\"\n\n    def removeService(service):\n        \"\"\"\n        Remove a child service.\n\n        Only implementations of L{IService.disownServiceParent} should\n        use this method.\n\n        @type service: L{IService}\n        @raise ValueError: Raised if the given service is not a child.\n        @rtype: L{Deferred<defer.Deferred>}\n        @return: a L{Deferred<defer.Deferred>} which is triggered when the\n            service has finished shutting down. If shutting down is immediate,\n            a value can be returned (usually, C{None}).\n        \"\"\"\n\n\n\nclass MultiService(Service):\n    \"\"\"\n    Straightforward Service Container.\n\n    Hold a collection of services, and manage them in a simplistic\n    way. No service will wait for another, but this object itself\n    will not finish shutting down until all of its child services\n    will finish.\n    \"\"\"\n\n    implements(IServiceCollection)\n\n    def __init__(self):\n        self.services = []\n        self.namedServices = {}\n        self.parent = None\n\n    def privilegedStartService(self):\n        Service.privilegedStartService(self)\n        for service in self:\n            service.privilegedStartService()\n\n    def startService(self):\n        Service.startService(self)\n        for service in self:\n            service.startService()\n\n    def stopService(self):\n        Service.stopService(self)\n        l = []\n        services = list(self)\n        services.reverse()\n        for service in services:\n            l.append(defer.maybeDeferred(service.stopService))\n        return defer.DeferredList(l)\n\n    def getServiceNamed(self, name):\n        return self.namedServices[name]\n\n    def __iter__(self):\n        return iter(self.services)\n\n    def addService(self, service):\n        if service.name is not None:\n            if service.name in self.namedServices:\n                raise RuntimeError(\"cannot have two services with same name\"\n                                   \" '%s'\" % service.name)\n            self.namedServices[service.name] = service\n        self.services.append(service)\n        if self.running:\n            # It may be too late for that, but we will do our best\n            service.privilegedStartService()\n            service.startService()\n\n    def removeService(self, service):\n        if service.name:\n            del self.namedServices[service.name]\n        self.services.remove(service)\n        if self.running:\n            # Returning this so as not to lose information from the\n            # MultiService.stopService deferred.\n            return service.stopService()\n        else:\n            return None\n\n\n\nclass IProcess(Interface):\n    \"\"\"\n    Process running parameters.\n\n    Represents parameters for how processes should be run.\n    \"\"\"\n    processName = Attribute(\n        \"\"\"\n        A C{str} giving the name the process should have in ps (or C{None}\n        to leave the name alone).\n        \"\"\")\n\n    uid = Attribute(\n        \"\"\"\n        An C{int} giving the user id as which the process should run (or\n        C{None} to leave the UID alone).\n        \"\"\")\n\n    gid = Attribute(\n        \"\"\"\n        An C{int} giving the group id as which the process should run (or\n        C{None} to leave the GID alone).\n        \"\"\")\n\n\n\nclass Process:\n    \"\"\"\n    Process running parameters.\n\n    Sets up uid/gid in the constructor, and has a default\n    of C{None} as C{processName}.\n    \"\"\"\n    implements(IProcess)\n    processName = None\n\n    def __init__(self, uid=None, gid=None):\n        \"\"\"\n        Set uid and gid.\n\n        @param uid: The user ID as whom to execute the process.  If\n            this is C{None}, no attempt will be made to change the UID.\n\n        @param gid: The group ID as whom to execute the process.  If\n            this is C{None}, no attempt will be made to change the GID.\n        \"\"\"\n        self.uid = uid\n        self.gid = gid\n\n\ndef Application(name, uid=None, gid=None):\n    \"\"\"\n    Return a compound class.\n\n    Return an object supporting the L{IService}, L{IServiceCollection},\n    L{IProcess} and L{sob.IPersistable} interfaces, with the given\n    parameters. Always access the return value by explicit casting to\n    one of the interfaces.\n    \"\"\"\n    ret = components.Componentized()\n    for comp in (MultiService(), sob.Persistent(ret, name), Process(uid, gid)):\n        ret.addComponent(comp, ignoreClass=1)\n    IService(ret).setName(name)\n    return ret\n\n\n\ndef loadApplication(filename, kind, passphrase=None):\n    \"\"\"\n    Load Application from a given file.\n\n    The serialization format it was saved in should be given as\n    C{kind}, and is one of C{pickle}, C{source}, C{xml} or C{python}. If\n    C{passphrase} is given, the application was encrypted with the\n    given passphrase.\n\n    @type filename: C{str}\n    @type kind: C{str}\n    @type passphrase: C{str}\n    \"\"\"\n    if kind == 'python':\n        application = sob.loadValueFromFile(filename, 'application', passphrase)\n    else:\n        application = sob.load(filename, kind, passphrase)\n    return application\n\n\n__all__ = ['IServiceMaker', 'IService', 'Service',\n           'IServiceCollection', 'MultiService',\n           'IProcess', 'Process', 'Application', 'loadApplication']\n", "license": "apache-2.0"}
{"id": "dcb519b8fba41727fd9b56ba85cadb6257c89e91", "path": "tests/utils_tests/test_datetime_safe.py", "repo_name": "ryanahall/django", "content": "import unittest\nfrom datetime import (\n    date as original_date, datetime as original_datetime,\n    time as original_time,\n)\n\nfrom django.utils.datetime_safe import date, datetime, time\n\n\nclass DatetimeTests(unittest.TestCase):\n\n    def setUp(self):\n        self.just_safe = (1900, 1, 1)\n        self.just_unsafe = (1899, 12, 31, 23, 59, 59)\n        self.just_time = (11, 30, 59)\n        self.really_old = (20, 1, 1)\n        self.more_recent = (2006, 1, 1)\n\n    def test_compare_datetimes(self):\n        self.assertEqual(original_datetime(*self.more_recent), datetime(*self.more_recent))\n        self.assertEqual(original_datetime(*self.really_old), datetime(*self.really_old))\n        self.assertEqual(original_date(*self.more_recent), date(*self.more_recent))\n        self.assertEqual(original_date(*self.really_old), date(*self.really_old))\n\n        self.assertEqual(original_date(*self.just_safe).strftime('%Y-%m-%d'), date(*self.just_safe).strftime('%Y-%m-%d'))\n        self.assertEqual(original_datetime(*self.just_safe).strftime('%Y-%m-%d'), datetime(*self.just_safe).strftime('%Y-%m-%d'))\n\n        self.assertEqual(original_time(*self.just_time).strftime('%H:%M:%S'), time(*self.just_time).strftime('%H:%M:%S'))\n\n    def test_safe_strftime(self):\n        self.assertEqual(date(*self.just_unsafe[:3]).strftime('%Y-%m-%d (weekday %w)'), '1899-12-31 (weekday 0)')\n        self.assertEqual(date(*self.just_safe).strftime('%Y-%m-%d (weekday %w)'), '1900-01-01 (weekday 1)')\n\n        self.assertEqual(datetime(*self.just_unsafe).strftime('%Y-%m-%d %H:%M:%S (weekday %w)'), '1899-12-31 23:59:59 (weekday 0)')\n        self.assertEqual(datetime(*self.just_safe).strftime('%Y-%m-%d %H:%M:%S (weekday %w)'), '1900-01-01 00:00:00 (weekday 1)')\n\n        self.assertEqual(time(*self.just_time).strftime('%H:%M:%S AM'), '11:30:59 AM')\n\n        # %y will error before this date\n        self.assertEqual(date(*self.just_safe).strftime('%y'), '00')\n        self.assertEqual(datetime(*self.just_safe).strftime('%y'), '00')\n\n        self.assertEqual(date(1850, 8, 2).strftime(\"%Y/%m/%d was a %A\"), '1850/08/02 was a Friday')\n\n    def test_zero_padding(self):\n        \"\"\"\n        Regression for #12524\n\n        Check that pre-1000AD dates are padded with zeros if necessary\n        \"\"\"\n        self.assertEqual(date(1, 1, 1).strftime(\"%Y/%m/%d was a %A\"), '0001/01/01 was a Monday')\n", "license": "bsd-3-clause"}
{"id": "4e49cd29c8e8178cbbc299e83066fc38f9092fa4", "path": "common/test/acceptance/tests/discussion/test_cohort_management.py", "repo_name": "angelapper/edx-platform", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nEnd-to-end tests related to the cohort management on the LMS Instructor Dashboard\n\"\"\"\n\nimport os\nimport uuid\nfrom datetime import datetime\n\nimport unicodecsv\nfrom bok_choy.promise import EmptyPromise\nfrom nose.plugins.attrib import attr\nfrom pytz import UTC, utc\n\nfrom common.test.acceptance.fixtures.course import CourseFixture, XBlockFixtureDesc\nfrom common.test.acceptance.pages.common.auto_auth import AutoAuthPage\nfrom common.test.acceptance.pages.lms.instructor_dashboard import DataDownloadPage, InstructorDashboardPage\nfrom common.test.acceptance.pages.studio.settings_group_configurations import GroupConfigurationsPage\nfrom common.test.acceptance.tests.discussion.helpers import CohortTestMixin\nfrom common.test.acceptance.tests.helpers import EventsTestMixin, UniqueCourseTest, create_user_partition_json\nfrom xmodule.partitions.partitions import Group\n\n\n@attr(shard=8)\nclass CohortConfigurationTest(EventsTestMixin, UniqueCourseTest, CohortTestMixin):\n    \"\"\"\n    Tests for cohort management on the LMS Instructor Dashboard\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Set up a cohorted course\n        \"\"\"\n        super(CohortConfigurationTest, self).setUp()\n\n        # create course with cohorts\n        self.manual_cohort_name = \"ManualCohort1\"\n        self.auto_cohort_name = \"AutoCohort1\"\n        self.course_fixture = CourseFixture(**self.course_info).install()\n        self.setup_cohort_config(self.course_fixture, auto_cohort_groups=[self.auto_cohort_name])\n        self.manual_cohort_id = self.add_manual_cohort(self.course_fixture, self.manual_cohort_name)\n\n        # create a non-instructor who will be registered for the course and in the manual cohort.\n        self.student_name, self.student_email = self._generate_unique_user_data()\n        self.student_id = AutoAuthPage(\n            self.browser, username=self.student_name, email=self.student_email,\n            course_id=self.course_id, staff=False\n        ).visit().get_user_id()\n        self.add_user_to_cohort(self.course_fixture, self.student_name, self.manual_cohort_id)\n\n        # create a second student user\n        self.other_student_name, self.other_student_email = self._generate_unique_user_data()\n        self.other_student_id = AutoAuthPage(\n            self.browser, username=self.other_student_name, email=self.other_student_email,\n            course_id=self.course_id, staff=False\n        ).visit().get_user_id()\n\n        # login as an instructor\n        self.instructor_name, self.instructor_email = self._generate_unique_user_data()\n        self.instructor_id = AutoAuthPage(\n            self.browser, username=self.instructor_name, email=self.instructor_email,\n            course_id=self.course_id, staff=True\n        ).visit().get_user_id()\n\n        # go to the membership page on the instructor dashboard\n        self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id)\n        self.instructor_dashboard_page.visit()\n        self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()\n\n    def verify_cohort_description(self, cohort_name, expected_description):\n        \"\"\"\n        Selects the cohort with the given name and verifies the expected description is presented.\n        \"\"\"\n        self.cohort_management_page.select_cohort(cohort_name)\n        self.assertEquals(self.cohort_management_page.get_selected_cohort(), cohort_name)\n        self.assertIn(expected_description, self.cohort_management_page.get_cohort_group_setup())\n\n    def test_cohort_description(self):\n        \"\"\"\n        Scenario: the cohort configuration management in the instructor dashboard specifies whether\n        students are automatically or manually assigned to specific cohorts.\n\n        Given I have a course with a manual cohort and an automatic cohort defined\n        When I view the manual cohort in the instructor dashboard\n        There is text specifying that students are only added to the cohort manually\n        And when I view the automatic cohort in the instructor dashboard\n        There is text specifying that students are automatically added to the cohort\n        \"\"\"\n        self.verify_cohort_description(\n            self.manual_cohort_name,\n            'Learners are added to this cohort only when you provide '\n            'their email addresses or usernames on this page',\n        )\n        self.verify_cohort_description(\n            self.auto_cohort_name,\n            'Learners are added to this cohort automatically',\n        )\n\n    def test_no_content_groups(self):\n        \"\"\"\n        Scenario: if the course has no content groups defined (user_partitions of type cohort),\n        the settings in the cohort management tab reflect this\n\n        Given I have a course with a cohort defined but no content groups\n        When I view the cohort in the instructor dashboard and select settings\n        Then the cohort is not linked to a content group\n        And there is text stating that no content groups are defined\n        And I cannot select the radio button to enable content group association\n        And there is a link I can select to open Group settings in Studio\n        \"\"\"\n        self.cohort_management_page.select_cohort(self.manual_cohort_name)\n        self.assertIsNone(self.cohort_management_page.get_cohort_associated_content_group())\n        self.assertEqual(\n            \"Warning:\\nNo content groups exist. Create a content group\",\n            self.cohort_management_page.get_cohort_related_content_group_message()\n        )\n        self.assertFalse(self.cohort_management_page.select_content_group_radio_button())\n        self.cohort_management_page.select_studio_group_settings()\n        group_settings_page = GroupConfigurationsPage(\n            self.browser,\n            self.course_info['org'],\n            self.course_info['number'],\n            self.course_info['run']\n        )\n        group_settings_page.wait_for_page()\n\n    def test_add_students_to_cohort_success(self):\n        \"\"\"\n        Scenario: When students are added to a cohort, the appropriate notification is shown.\n\n        Given I have a course with two cohorts\n        And there is a user in one cohort\n        And there is a user in neither cohort\n        When I add the two users to the cohort that initially had no users\n        Then there are 2 users in total in the cohort\n        And I get a notification that 2 users have been added to the cohort\n        And I get a notification that 1 user was moved from the other cohort\n        And the user input field is empty\n        And appropriate events have been emitted\n        \"\"\"\n        start_time = datetime.now(UTC)\n        self.cohort_management_page.select_cohort(self.auto_cohort_name)\n        self.assertEqual(0, self.cohort_management_page.get_selected_cohort_count())\n        self.cohort_management_page.add_students_to_selected_cohort([self.student_name, self.instructor_name])\n        # Wait for the number of users in the cohort to change, indicating that the add operation is complete.\n        EmptyPromise(\n            lambda: 2 == self.cohort_management_page.get_selected_cohort_count(), 'Waiting for added students'\n        ).fulfill()\n        confirmation_messages = self.cohort_management_page.get_cohort_confirmation_messages()\n        self.assertEqual(\n            [\n                \"2 learners have been added to this cohort.\",\n                \"1 learner was moved from \" + self.manual_cohort_name\n            ],\n            confirmation_messages\n        )\n        self.assertEqual(\"\", self.cohort_management_page.get_cohort_student_input_field_value())\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.user_added\",\n                \"time\": {\"$gt\": start_time},\n                \"event.user_id\": {\"$in\": [int(self.instructor_id), int(self.student_id)]},\n                \"event.cohort_name\": self.auto_cohort_name,\n            }).count(),\n            2\n        )\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.user_removed\",\n                \"time\": {\"$gt\": start_time},\n                \"event.user_id\": int(self.student_id),\n                \"event.cohort_name\": self.manual_cohort_name,\n            }).count(),\n            1\n        )\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.user_add_requested\",\n                \"time\": {\"$gt\": start_time},\n                \"event.user_id\": int(self.instructor_id),\n                \"event.cohort_name\": self.auto_cohort_name,\n                \"event.previous_cohort_name\": None,\n            }).count(),\n            1\n        )\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.user_add_requested\",\n                \"time\": {\"$gt\": start_time},\n                \"event.user_id\": int(self.student_id),\n                \"event.cohort_name\": self.auto_cohort_name,\n                \"event.previous_cohort_name\": self.manual_cohort_name,\n            }).count(),\n            1\n        )\n\n    def test_add_students_to_cohort_failure(self):\n        \"\"\"\n        Scenario: When errors occur when adding students to a cohort, the appropriate notification is shown.\n\n        Given I have a course with a cohort and a user already in it\n        When I add the user already in a cohort to that same cohort\n        And I add a non-existing user to that cohort\n        Then there is no change in the number of students in the cohort\n        And I get a notification that one user was already in the cohort\n        And I get a notification that one user is unknown\n        And the user input field still contains the incorrect email addresses\n        \"\"\"\n        self.cohort_management_page.select_cohort(self.manual_cohort_name)\n        self.assertEqual(1, self.cohort_management_page.get_selected_cohort_count())\n        self.cohort_management_page.add_students_to_selected_cohort([self.student_name, \"unknown_user\"])\n        # Wait for notification messages to appear, indicating that the add operation is complete.\n        EmptyPromise(\n            lambda: 2 == len(self.cohort_management_page.get_cohort_confirmation_messages()), 'Waiting for notification'\n        ).fulfill()\n        self.assertEqual(1, self.cohort_management_page.get_selected_cohort_count())\n\n        self.assertEqual(\n            [\n                \"0 learners have been added to this cohort.\",\n                \"1 learner was already in the cohort\"\n            ],\n            self.cohort_management_page.get_cohort_confirmation_messages()\n        )\n\n        self.assertEqual(\n            [\n                \"There was an error when trying to add learners:\",\n                \"Unknown username: unknown_user\"\n            ],\n            self.cohort_management_page.get_cohort_error_messages()\n        )\n        self.assertEqual(\n            self.student_name + \",unknown_user,\",\n            self.cohort_management_page.get_cohort_student_input_field_value()\n        )\n\n    def _verify_cohort_settings(\n            self,\n            cohort_name,\n            assignment_type=None,\n            new_cohort_name=None,\n            new_assignment_type=None,\n            verify_updated=False\n    ):\n\n        \"\"\"\n        Create a new cohort and verify the new and existing settings.\n        \"\"\"\n        start_time = datetime.now(UTC)\n        self.assertNotIn(cohort_name, self.cohort_management_page.get_cohorts())\n        self.cohort_management_page.add_cohort(cohort_name, assignment_type=assignment_type)\n        self.assertEqual(0, self.cohort_management_page.get_selected_cohort_count())\n        # After adding the cohort, it should automatically be selected and its\n        # assignment_type should be \"manual\" as this is the default assignment type\n        _assignment_type = assignment_type or 'manual'\n        msg = \"Waiting for currently selected cohort assignment type\"\n        EmptyPromise(\n            lambda: _assignment_type == self.cohort_management_page.get_cohort_associated_assignment_type(), msg\n        ).fulfill()\n        # Go back to Manage Students Tab\n        self.cohort_management_page.select_manage_settings()\n        self.cohort_management_page.add_students_to_selected_cohort([self.instructor_name])\n        # Wait for the number of users in the cohort to change, indicating that the add operation is complete.\n        EmptyPromise(\n            lambda: 1 == self.cohort_management_page.get_selected_cohort_count(), 'Waiting for student to be added'\n        ).fulfill()\n        self.assertFalse(self.cohort_management_page.is_assignment_settings_disabled)\n        self.assertEqual('', self.cohort_management_page.assignment_settings_message)\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.created\",\n                \"time\": {\"$gt\": start_time},\n                \"event.cohort_name\": cohort_name,\n            }).count(),\n            1\n        )\n        self.assertEqual(\n            self.event_collection.find({\n                \"name\": \"edx.cohort.creation_requested\",\n                \"time\": {\"$gt\": start_time},\n                \"event.cohort_name\": cohort_name,\n            }).count(),\n            1\n        )\n\n        if verify_updated:\n            self.cohort_management_page.select_cohort(cohort_name)\n            self.cohort_management_page.select_cohort_settings()\n            self.cohort_management_page.set_cohort_name(new_cohort_name)\n            self.cohort_management_page.set_assignment_type(new_assignment_type)\n            self.cohort_management_page.save_cohort_settings()\n\n            # If cohort name is empty, then we should get/see an error message.\n            if not new_cohort_name:\n                confirmation_messages = self.cohort_management_page.get_cohort_settings_messages(type='error')\n                self.assertEqual(\n                    [\"The cohort cannot be saved\", \"You must specify a name for the cohort\"],\n                    confirmation_messages\n                )\n            else:\n                confirmation_messages = self.cohort_management_page.get_cohort_settings_messages()\n                self.assertEqual([\"Saved cohort\"], confirmation_messages)\n                self.assertEqual(new_cohort_name, self.cohort_management_page.cohort_name_in_header)\n                self.assertIn(new_cohort_name, self.cohort_management_page.get_cohorts())\n                self.assertEqual(1, self.cohort_management_page.get_selected_cohort_count())\n                self.assertEqual(\n                    new_assignment_type,\n                    self.cohort_management_page.get_cohort_associated_assignment_type()\n                )\n\n    def _create_csv_file(self, filename, csv_text_as_lists):\n        \"\"\"\n        Create a csv file with the provided list of lists.\n\n        :param filename: this is the name that will be used for the csv file. Its location will\n         be under the test upload data directory\n        :param csv_text_as_lists: provide the contents of the csv file int he form of a list of lists\n        \"\"\"\n        filename = self.instructor_dashboard_page.get_asset_path(filename)\n        with open(filename, 'w+') as csv_file:\n            writer = unicodecsv.writer(csv_file)\n            for line in csv_text_as_lists:\n                writer.writerow(line)\n        self.addCleanup(os.remove, filename)\n\n    def _generate_unique_user_data(self):\n        \"\"\"\n        Produce unique username and e-mail.\n        \"\"\"\n        unique_username = 'user' + str(uuid.uuid4().hex)[:12]\n        unique_email = unique_username + \"@example.com\"\n        return unique_username, unique_email\n\n    def test_add_new_cohort(self):\n        \"\"\"\n        Scenario: A new manual cohort can be created, and a student assigned to it.\n\n        Given I have a course with a user in the course\n        When I add a new manual cohort to the course via the LMS instructor dashboard\n        Then the new cohort is displayed and has no users in it\n        And assignment type of displayed cohort to \"manual\" because this is the default\n        And when I add the user to the new cohort\n        Then the cohort has 1 user\n        And appropriate events have been emitted\n        \"\"\"\n        cohort_name = str(uuid.uuid4().get_hex()[0:20])\n        self._verify_cohort_settings(cohort_name=cohort_name, assignment_type=None)\n\n    def test_add_new_cohort_with_manual_assignment_type(self):\n        \"\"\"\n        Scenario: A new cohort with manual assignment type can be created, and a student assigned to it.\n\n        Given I have a course with a user in the course\n        When I add a new manual cohort with manual assignment type to the course via the LMS instructor dashboard\n        Then the new cohort is displayed and has no users in it\n        And assignment type of displayed cohort is \"manual\"\n        And when I add the user to the new cohort\n        Then the cohort has 1 user\n        And appropriate events have been emitted\n        \"\"\"\n        cohort_name = str(uuid.uuid4().get_hex()[0:20])\n        self._verify_cohort_settings(cohort_name=cohort_name, assignment_type='manual')\n\n    def test_add_new_cohort_with_random_assignment_type(self):\n        \"\"\"\n        Scenario: A new cohort with random assignment type can be created, and a student assigned to it.\n\n        Given I have a course with a user in the course\n        When I add a new manual cohort with random assignment type to the course via the LMS instructor dashboard\n        Then the new cohort is displayed and has no users in it\n        And assignment type of displayed cohort is \"random\"\n        And when I add the user to the new cohort\n        Then the cohort has 1 user\n        And appropriate events have been emitted\n        \"\"\"\n        cohort_name = str(uuid.uuid4().get_hex()[0:20])\n        self._verify_cohort_settings(cohort_name=cohort_name, assignment_type='random')\n\n    def test_update_existing_cohort_settings(self):\n        \"\"\"\n        Scenario: Update existing cohort settings(cohort name, assignment type)\n\n        Given I have a course with a user in the course\n        When I add a new cohort with random assignment type to the course via the LMS instructor dashboard\n        Then the new cohort is displayed and has no users in it\n        And assignment type of displayed cohort is \"random\"\n        And when I add the user to the new cohort\n        Then the cohort has 1 user\n        And appropriate events have been emitted\n        Then I select the cohort (that you just created) from existing cohorts\n        Then I change its name and assignment type set to \"manual\"\n        Then I Save the settings\n        And cohort with new name is present in cohorts dropdown list\n        And cohort assignment type should be \"manual\"\n        \"\"\"\n        cohort_name = str(uuid.uuid4().get_hex()[0:20])\n        new_cohort_name = '{old}__NEW'.format(old=cohort_name)\n        self._verify_cohort_settings(\n            cohort_name=cohort_name,\n            assignment_type='random',\n            new_cohort_name=new_cohort_name,\n            new_assignment_type='manual',\n            verify_updated=True\n        )\n\n    def test_update_existing_cohort_settings_with_empty_cohort_name(self):\n        \"\"\"\n        Scenario: Update existing cohort settings(cohort name, assignment type).\n\n        Given I have a course with a user in the course\n        When I add a new cohort with random assignment type to the course via the LMS instructor dashboard\n        Then the new cohort is displayed and has no users in it\n        And assignment type of displayed cohort is \"random\"\n        And when I add the user to the new cohort\n        Then the cohort has 1 user\n        And appropriate events have been emitted\n        Then I select a cohort from existing cohorts\n        Then I set its name as empty string and assignment type set to \"manual\"\n        And I click on Save button\n        Then I should see an error message\n        \"\"\"\n        cohort_name = str(uuid.uuid4().get_hex()[0:20])\n        new_cohort_name = ''\n        self._verify_cohort_settings(\n            cohort_name=cohort_name,\n            assignment_type='random',\n            new_cohort_name=new_cohort_name,\n            new_assignment_type='manual',\n            verify_updated=True\n        )\n\n    def test_default_cohort_assignment_settings(self):\n        \"\"\"\n        Scenario: Cohort assignment settings are disabled for default cohort.\n\n        Given I have a course with a user in the course\n        And I have added a manual cohort\n        And I have added a random cohort\n        When I select the random cohort\n        Then cohort assignment settings are disabled\n        \"\"\"\n        self.cohort_management_page.select_cohort(\"AutoCohort1\")\n        self.cohort_management_page.select_cohort_settings()\n\n        self.assertTrue(self.cohort_management_page.is_assignment_settings_disabled)\n\n        message = \"There must be one cohort to which students can automatically be assigned.\"\n        self.assertEqual(message, self.cohort_management_page.assignment_settings_message)\n\n    def test_cohort_enable_disable(self):\n        \"\"\"\n        Scenario: Cohort Enable/Disable checkbox related functionality is working as intended.\n\n        Given I have a cohorted course with a user.\n        And I can see the `Enable Cohorts` checkbox is checked.\n        And cohort management controls are visible.\n        When I uncheck the `Enable Cohorts` checkbox.\n        Then cohort management controls are not visible.\n        And When I reload the page.\n        Then I can see the `Enable Cohorts` checkbox is unchecked.\n        And cohort management controls are not visible.\n        \"\"\"\n        self.assertTrue(self.cohort_management_page.is_cohorted)\n        self.assertTrue(self.cohort_management_page.cohort_management_controls_visible())\n        self.cohort_management_page.is_cohorted = False\n        self.assertFalse(self.cohort_management_page.cohort_management_controls_visible())\n        self.browser.refresh()\n        self.cohort_management_page.wait_for_page()\n        self.assertFalse(self.cohort_management_page.is_cohorted)\n        self.assertFalse(self.cohort_management_page.cohort_management_controls_visible())\n\n    def test_link_to_data_download(self):\n        \"\"\"\n        Scenario: a link is present from the cohort configuration in\n        the instructor dashboard to the Data Download section.\n\n        Given I have a course with a cohort defined\n        When I view the cohort in the LMS instructor dashboard\n        There is a link to take me to the Data Download section of the Instructor Dashboard.\n        \"\"\"\n        self.cohort_management_page.select_data_download()\n        data_download_page = DataDownloadPage(self.browser)\n        data_download_page.wait_for_page()\n\n    def test_cohort_by_csv_both_columns(self):\n        \"\"\"\n        Scenario: the instructor can upload a file with user and cohort assignments, using both emails and usernames.\n\n        Given I have a course with two cohorts defined\n        When I go to the cohort management section of the instructor dashboard\n        I can upload a CSV file with assignments of users to cohorts via both usernames and emails\n        Then I can download a file with results\n        And appropriate events have been emitted\n        \"\"\"\n        csv_contents = [\n            ['username', 'email', 'ignored_column', 'cohort'],\n            [self.instructor_name, '', 'June', 'ManualCohort1'],\n            ['', self.student_email, 'Spring', 'AutoCohort1'],\n            [self.other_student_name, '', 'Fall', 'ManualCohort1'],\n        ]\n        filename = \"cohort_csv_both_columns_1.csv\"\n        self._create_csv_file(filename, csv_contents)\n        self._verify_csv_upload_acceptable_file(filename)\n\n    def test_cohort_by_csv_only_email(self):\n        \"\"\"\n        Scenario: the instructor can upload a file with user and cohort assignments, using only emails.\n\n        Given I have a course with two cohorts defined\n        When I go to the cohort management section of the instructor dashboard\n        I can upload a CSV file with assignments of users to cohorts via only emails\n        Then I can download a file with results\n        And appropriate events have been emitted\n        \"\"\"\n        csv_contents = [\n            ['email', 'cohort'],\n            [self.instructor_email, 'ManualCohort1'],\n            [self.student_email, 'AutoCohort1'],\n            [self.other_student_email, 'ManualCohort1'],\n        ]\n        filename = \"cohort_csv_emails_only.csv\"\n        self._create_csv_file(filename, csv_contents)\n        self._verify_csv_upload_acceptable_file(filename)\n\n    def test_cohort_by_csv_only_username(self):\n        \"\"\"\n        Scenario: the instructor can upload a file with user and cohort assignments, using only usernames.\n\n        Given I have a course with two cohorts defined\n        When I go to the cohort management section of the instructor dashboard\n        I can upload a CSV file with assignments of users to cohorts via only usernames\n        Then I can download a file with results\n        And appropriate events have been emitted\n        \"\"\"\n        csv_contents = [\n            ['username', 'cohort'],\n            [self.instructor_name, 'ManualCohort1'],\n            [self.student_name, 'AutoCohort1'],\n            [self.other_student_name, 'ManualCohort1'],\n        ]\n        filename = \"cohort_users_only_username1.csv\"\n        self._create_csv_file(filename, csv_contents)\n        self._verify_csv_upload_acceptable_file(filename)\n\n    # TODO: Change unicode_hello_in_korean = u'\u00df\u00df\u00df\u00df\u00df\u00df' to u'\uc548\ub155\ud558\uc138\uc694', after up gradation of Chrome driver. See TNL-3944\n    def test_cohort_by_csv_unicode(self):\n        \"\"\"\n        Scenario: the instructor can upload a file with user and cohort assignments, using both emails and usernames.\n\n        Given I have a course with two cohorts defined\n        And I add another cohort with a unicode name\n        When I go to the cohort management section of the instructor dashboard\n        I can upload a CSV file with assignments of users to the unicode cohort via both usernames and emails\n        Then I can download a file with results\n\n        TODO: refactor events verification to handle this scenario. Events verification assumes movements\n        between other cohorts (manual and auto).\n        \"\"\"\n        unicode_hello_in_korean = u'\u00df\u00df\u00df\u00df\u00df\u00df'\n        self._verify_cohort_settings(cohort_name=unicode_hello_in_korean, assignment_type=None)\n        csv_contents = [\n            ['username', 'email', 'cohort'],\n            [self.instructor_name, '', unicode_hello_in_korean],\n            ['', self.student_email, unicode_hello_in_korean],\n            [self.other_student_name, '', unicode_hello_in_korean]\n        ]\n        filename = \"cohort_unicode_name.csv\"\n        self._create_csv_file(filename, csv_contents)\n        self._verify_csv_upload_acceptable_file(filename, skip_events=True)\n\n    def _verify_csv_upload_acceptable_file(self, filename, skip_events=None):\n        \"\"\"\n        Helper method to verify cohort assignments after a successful CSV upload.\n\n        When skip_events is specified, no assertions are made on events.\n        \"\"\"\n        start_time = datetime.now(UTC)\n        self.cohort_management_page.upload_cohort_file(filename)\n        self._verify_cohort_by_csv_notification(\n            \"Your file '{}' has been uploaded. Allow a few minutes for processing.\".format(filename)\n        )\n\n        if not skip_events:\n            # student_user is moved from manual cohort to auto cohort\n            self.assertEqual(\n                self.event_collection.find({\n                    \"name\": \"edx.cohort.user_added\",\n                    \"time\": {\"$gt\": start_time},\n                    \"event.user_id\": {\"$in\": [int(self.student_id)]},\n                    \"event.cohort_name\": self.auto_cohort_name,\n                }).count(),\n                1\n            )\n            self.assertEqual(\n                self.event_collection.find({\n                    \"name\": \"edx.cohort.user_removed\",\n                    \"time\": {\"$gt\": start_time},\n                    \"event.user_id\": int(self.student_id),\n                    \"event.cohort_name\": self.manual_cohort_name,\n                }).count(),\n                1\n            )\n            # instructor_user (previously unassigned) is added to manual cohort\n            self.assertEqual(\n                self.event_collection.find({\n                    \"name\": \"edx.cohort.user_added\",\n                    \"time\": {\"$gt\": start_time},\n                    \"event.user_id\": {\"$in\": [int(self.instructor_id)]},\n                    \"event.cohort_name\": self.manual_cohort_name,\n                }).count(),\n                1\n            )\n            # other_student_user (previously unassigned) is added to manual cohort\n            self.assertEqual(\n                self.event_collection.find({\n                    \"name\": \"edx.cohort.user_added\",\n                    \"time\": {\"$gt\": start_time},\n                    \"event.user_id\": {\"$in\": [int(self.other_student_id)]},\n                    \"event.cohort_name\": self.manual_cohort_name,\n                }).count(),\n                1\n            )\n\n        # Verify the results can be downloaded.\n        data_download = self.instructor_dashboard_page.select_data_download()\n        data_download.wait_for_available_report()\n        report = data_download.get_available_reports_for_download()[0]\n        base_file_name = \"cohort_results_\"\n        self.assertIn(\"{}_{}\".format(\n            '_'.join([self.course_info['org'], self.course_info['number'], self.course_info['run']]), base_file_name\n        ), report)\n        report_datetime = datetime.strptime(\n            report[report.index(base_file_name) + len(base_file_name):-len(\".csv\")],\n            \"%Y-%m-%d-%H%M\"\n        )\n        self.assertLessEqual(start_time.replace(second=0, microsecond=0), utc.localize(report_datetime))\n\n    def test_cohort_by_csv_wrong_file_type(self):\n        \"\"\"\n        Scenario: if the instructor uploads a non-csv file, an error message is presented.\n\n        Given I have a course with cohorting enabled\n        When I go to the cohort management section of the instructor dashboard\n        And I upload a file without the CSV extension\n        Then I get an error message stating that the file must have a CSV extension\n        \"\"\"\n        self.cohort_management_page.upload_cohort_file(\"image.jpg\")\n        self._verify_cohort_by_csv_notification(\"The file must end with the extension '.csv'.\")\n\n    def test_cohort_by_csv_missing_cohort(self):\n        \"\"\"\n        Scenario: if the instructor uploads a csv file with no cohort column, an error message is presented.\n\n        Given I have a course with cohorting enabled\n        When I go to the cohort management section of the instructor dashboard\n        And I upload a CSV file that is missing the cohort column\n        Then I get an error message stating that the file must have a cohort column\n        \"\"\"\n        self.cohort_management_page.upload_cohort_file(\"cohort_users_missing_cohort_column.csv\")\n        self._verify_cohort_by_csv_notification(\"The file must contain a 'cohort' column containing cohort names.\")\n\n    def test_cohort_by_csv_missing_user(self):\n        \"\"\"\n        Scenario: if the instructor uploads a csv file with no username or email column, an error message is presented.\n\n        Given I have a course with cohorting enabled\n        When I go to the cohort management section of the instructor dashboard\n        And I upload a CSV file that is missing both the username and email columns\n        Then I get an error message stating that the file must have either a username or email column\n        \"\"\"\n        self.cohort_management_page.upload_cohort_file(\"cohort_users_missing_user_columns.csv\")\n        self._verify_cohort_by_csv_notification(\n            \"The file must contain a 'username' column, an 'email' column, or both.\"\n        )\n\n    def _verify_cohort_by_csv_notification(self, expected_message):\n        \"\"\"\n        Helper method to check the CSV file upload notification message.\n        \"\"\"\n        # Wait for notification message to appear, indicating file has been uploaded.\n        EmptyPromise(\n            lambda: 1 == len(self.cohort_management_page.get_csv_messages()), 'Waiting for notification'\n        ).fulfill()\n        messages = self.cohort_management_page.get_csv_messages()\n        self.assertEquals(expected_message, messages[0])\n\n    @attr('a11y')\n    def test_cohorts_management_a11y(self):\n        \"\"\"\n        Run accessibility audit for cohort management.\n        \"\"\"\n        self.cohort_management_page.a11y_audit.check_for_accessibility_errors()\n\n\n@attr(shard=6)\nclass CohortContentGroupAssociationTest(UniqueCourseTest, CohortTestMixin):\n    \"\"\"\n    Tests for linking between content groups and cohort in the instructor dashboard.\n    \"\"\"\n\n    def setUp(self):\n        \"\"\"\n        Set up a cohorted course with a user_partition of scheme \"cohort\".\n        \"\"\"\n        super(CohortContentGroupAssociationTest, self).setUp()\n\n        # create course with single cohort and two content groups (user_partition of type \"cohort\")\n        self.cohort_name = \"OnlyCohort\"\n        self.course_fixture = CourseFixture(**self.course_info).install()\n        self.setup_cohort_config(self.course_fixture)\n        self.cohort_id = self.add_manual_cohort(self.course_fixture, self.cohort_name)\n\n        self.course_fixture._update_xblock(self.course_fixture._course_location, {\n            \"metadata\": {\n                u\"user_partitions\": [\n                    create_user_partition_json(\n                        0,\n                        'Apples, Bananas',\n                        'Content Group Partition',\n                        [Group(\"0\", 'Apples'), Group(\"1\", 'Bananas')],\n                        scheme=\"cohort\"\n                    )\n                ],\n            },\n        })\n\n        # login as an instructor\n        self.instructor_name = \"instructor_user\"\n        self.instructor_id = AutoAuthPage(\n            self.browser, username=self.instructor_name, email=\"instructor_user@example.com\",\n            course_id=self.course_id, staff=True\n        ).visit().get_user_id()\n\n        # go to the membership page on the instructor dashboard\n        self.instructor_dashboard_page = InstructorDashboardPage(self.browser, self.course_id)\n        self.instructor_dashboard_page.visit()\n        self.cohort_management_page = self.instructor_dashboard_page.select_cohort_management()\n\n    def test_no_content_group_linked(self):\n        \"\"\"\n        Scenario: In a course with content groups, cohorts are initially not linked to a content group\n\n        Given I have a course with a cohort defined and content groups defined\n        When I view the cohort in the instructor dashboard and select settings\n        Then the cohort is not linked to a content group\n        And there is no text stating that content groups are undefined\n        And the content groups are listed in the selector\n        \"\"\"\n        self.cohort_management_page.select_cohort(self.cohort_name)\n        self.assertIsNone(self.cohort_management_page.get_cohort_associated_content_group())\n        self.assertIsNone(self.cohort_management_page.get_cohort_related_content_group_message())\n        self.assertEquals([\"Apples\", \"Bananas\"], self.cohort_management_page.get_all_content_groups())\n\n    def test_link_to_content_group(self):\n        \"\"\"\n        Scenario: In a course with content groups, cohorts can be linked to content groups\n\n        Given I have a course with a cohort defined and content groups defined\n        When I view the cohort in the instructor dashboard and select settings\n        And I link the cohort to one of the content groups and save\n        Then there is a notification that my cohort has been saved\n        And when I reload the page\n        And I view the cohort in the instructor dashboard and select settings\n        Then the cohort is still linked to the content group\n        \"\"\"\n        self._link_cohort_to_content_group(self.cohort_name, \"Bananas\")\n        self.assertEqual(\"Bananas\", self.cohort_management_page.get_cohort_associated_content_group())\n\n    def test_unlink_from_content_group(self):\n        \"\"\"\n        Scenario: In a course with content groups, cohorts can be unlinked from content groups\n\n        Given I have a course with a cohort defined and content groups defined\n        When I view the cohort in the instructor dashboard and select settings\n        And I link the cohort to one of the content groups and save\n        Then there is a notification that my cohort has been saved\n        And I reload the page\n        And I view the cohort in the instructor dashboard and select settings\n        And I unlink the cohort from any content group and save\n        Then there is a notification that my cohort has been saved\n        And when I reload the page\n        And I view the cohort in the instructor dashboard and select settings\n        Then the cohort is not linked to any content group\n        \"\"\"\n        self._link_cohort_to_content_group(self.cohort_name, \"Bananas\")\n        self.cohort_management_page.set_cohort_associated_content_group(None)\n        self._verify_settings_saved_and_reload(self.cohort_name)\n        self.assertEqual(None, self.cohort_management_page.get_cohort_associated_content_group())\n\n    def test_create_new_cohort_linked_to_content_group(self):\n        \"\"\"\n        Scenario: In a course with content groups, a new cohort can be linked to a content group\n            at time of creation.\n\n        Given I have a course with a cohort defined and content groups defined\n        When I create a new cohort and link it to a content group\n        Then when I select settings I see that the cohort is linked to the content group\n        And when I reload the page\n        And I view the cohort in the instructor dashboard and select settings\n        Then the cohort is still linked to the content group\n        \"\"\"\n        new_cohort = \"correctly linked cohort\"\n        self._create_new_cohort_linked_to_content_group(new_cohort, \"Apples\")\n        self.browser.refresh()\n        self.cohort_management_page.wait_for_page()\n        self.cohort_management_page.select_cohort(new_cohort)\n        self.assertEqual(\"Apples\", self.cohort_management_page.get_cohort_associated_content_group())\n\n    def test_missing_content_group(self):\n        \"\"\"\n        Scenario: In a course with content groups, if a cohort is associated with a content group that no longer\n            exists, a warning message is shown\n\n        Given I have a course with a cohort defined and content groups defined\n        When I create a new cohort and link it to a content group\n        And I delete that content group from the course\n        And I reload the page\n        And I view the cohort in the instructor dashboard and select settings\n        Then the settings display a message that the content group no longer exists\n        And when I select a different content group and save\n        Then the error message goes away\n        \"\"\"\n        new_cohort = \"linked to missing content group\"\n        self._create_new_cohort_linked_to_content_group(new_cohort, \"Apples\")\n        self.course_fixture._update_xblock(self.course_fixture._course_location, {\n            \"metadata\": {\n                u\"user_partitions\": [\n                    create_user_partition_json(\n                        0,\n                        'Apples, Bananas',\n                        'Content Group Partition',\n                        [Group(\"2\", 'Pears'), Group(\"1\", 'Bananas')],\n                        scheme=\"cohort\"\n                    )\n                ],\n            },\n        })\n        self.browser.refresh()\n        self.cohort_management_page.wait_for_page()\n        self.cohort_management_page.select_cohort(new_cohort)\n        self.assertEqual(\"Deleted Content Group\", self.cohort_management_page.get_cohort_associated_content_group())\n        self.assertEquals(\n            [\"Bananas\", \"Pears\", \"Deleted Content Group\"],\n            self.cohort_management_page.get_all_content_groups()\n        )\n        self.assertEqual(\n            \"Warning:\\nThe previously selected content group was deleted. Select another content group.\",\n            self.cohort_management_page.get_cohort_related_content_group_message()\n        )\n        self.cohort_management_page.set_cohort_associated_content_group(\"Pears\")\n        confirmation_messages = self.cohort_management_page.get_cohort_settings_messages()\n        self.assertEqual([\"Saved cohort\"], confirmation_messages)\n        self.assertIsNone(self.cohort_management_page.get_cohort_related_content_group_message())\n        self.assertEquals([\"Bananas\", \"Pears\"], self.cohort_management_page.get_all_content_groups())\n\n    def _create_new_cohort_linked_to_content_group(self, new_cohort, cohort_group):\n        \"\"\"\n        Creates a new cohort linked to a content group.\n        \"\"\"\n        self.cohort_management_page.add_cohort(new_cohort, content_group=cohort_group)\n        self.assertEqual(cohort_group, self.cohort_management_page.get_cohort_associated_content_group())\n\n    def _link_cohort_to_content_group(self, cohort_name, content_group):\n        \"\"\"\n        Links a cohort to a content group. Saves the changes and verifies the cohort updated properly.\n        Then refreshes the page and selects the cohort.\n        \"\"\"\n        self.cohort_management_page.select_cohort(cohort_name)\n        self.cohort_management_page.set_cohort_associated_content_group(content_group)\n        self._verify_settings_saved_and_reload(cohort_name)\n\n    def _verify_settings_saved_and_reload(self, cohort_name):\n        \"\"\"\n        Verifies the confirmation message indicating that a cohort's settings have been updated.\n        Then refreshes the page and selects the cohort.\n        \"\"\"\n        confirmation_messages = self.cohort_management_page.get_cohort_settings_messages()\n        self.assertEqual([\"Saved cohort\"], confirmation_messages)\n        self.browser.refresh()\n        self.cohort_management_page.wait_for_page()\n        self.cohort_management_page.select_cohort(cohort_name)\n", "license": "agpl-3.0"}
{"id": "0325a2d8614a1a3c30e3da3716b4bef8e96b3216", "path": "pip/_vendor/requests/packages/chardet/gb2312prober.py", "repo_name": "jythontools/pip", "content": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n# \n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################\n\nfrom .mbcharsetprober import MultiByteCharSetProber\nfrom .codingstatemachine import CodingStateMachine\nfrom .chardistribution import GB2312DistributionAnalysis\nfrom .mbcssm import GB2312SMModel\n\nclass GB2312Prober(MultiByteCharSetProber):\n    def __init__(self):\n        MultiByteCharSetProber.__init__(self)\n        self._mCodingSM = CodingStateMachine(GB2312SMModel)\n        self._mDistributionAnalyzer = GB2312DistributionAnalysis()\n        self.reset()\n\n    def get_charset_name(self):\n        return \"GB2312\"\n", "license": "mit"}
{"id": "0325a2d8614a1a3c30e3da3716b4bef8e96b3216", "path": "web/env/lib/python2.7/site-packages/pip/_vendor/requests/packages/chardet/gb2312prober.py", "repo_name": "nodice73/hspipeline", "content": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is mozilla.org code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n# \n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n# \n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################\n\nfrom .mbcharsetprober import MultiByteCharSetProber\nfrom .codingstatemachine import CodingStateMachine\nfrom .chardistribution import GB2312DistributionAnalysis\nfrom .mbcssm import GB2312SMModel\n\nclass GB2312Prober(MultiByteCharSetProber):\n    def __init__(self):\n        MultiByteCharSetProber.__init__(self)\n        self._mCodingSM = CodingStateMachine(GB2312SMModel)\n        self._mDistributionAnalyzer = GB2312DistributionAnalysis()\n        self.reset()\n\n    def get_charset_name(self):\n        return \"GB2312\"\n", "license": "gpl-2.0"}
{"id": "33ebc60779d2b7b030aec217729938a2acde18b9", "path": "cloud/rackspace/rax_scaling_policy.py", "repo_name": "jasonzzz/ansible-modules-core", "content": "#!/usr/bin/python\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# This is a DOCUMENTATION stub specific to this module, it extends\n# a documentation fragment located in ansible.utils.module_docs_fragments\nDOCUMENTATION = '''\n---\nmodule: rax_scaling_policy\nshort_description: Manipulate Rackspace Cloud Autoscale Scaling Policy\ndescription:\n    - Manipulate Rackspace Cloud Autoscale Scaling Policy\nversion_added: 1.7\noptions:\n  at:\n    description:\n      - The UTC time when this policy will be executed. The time must be\n        formatted according to C(yyyy-MM-dd'T'HH:mm:ss.SSS) such as\n        C(2013-05-19T08:07:08Z)\n  change:\n    description:\n      - The change, either as a number of servers or as a percentage, to make\n        in the scaling group. If this is a percentage, you must set\n        I(is_percent) to C(true) also.\n  cron:\n    description:\n      - The time when the policy will be executed, as a cron entry. For\n        example, if this is parameter is set to C(1 0 * * *)\n  cooldown:\n    description:\n      - The period of time, in seconds, that must pass before any scaling can\n        occur after the previous scaling. Must be an integer between 0 and\n        86400 (24 hrs).\n  desired_capacity:\n    description:\n      - The desired server capacity of the scaling the group; that is, how\n        many servers should be in the scaling group.\n  is_percent:\n    description:\n      - Whether the value in I(change) is a percent value\n    default: false\n  name:\n    description:\n      - Name to give the policy\n    required: true\n  policy_type:\n    description:\n      - The type of policy that will be executed for the current release.\n    choices:\n      - webhook\n      - schedule\n    required: true\n  scaling_group:\n    description:\n      - Name of the scaling group that this policy will be added to\n    required: true\n  state:\n    description:\n      - Indicate desired state of the resource\n    choices:\n      - present\n      - absent\n    default: present\nauthor: \"Matt Martz (@sivel)\"\nextends_documentation_fragment: rackspace\n'''\n\nEXAMPLES = '''\n---\n- hosts: localhost\n  gather_facts: false\n  connection: local\n  tasks:\n    - rax_scaling_policy:\n        credentials: ~/.raxpub\n        region: ORD\n        at: '2013-05-19T08:07:08Z'\n        change: 25\n        cooldown: 300\n        is_percent: true\n        name: ASG Test Policy - at\n        policy_type: schedule\n        scaling_group: ASG Test\n      register: asps_at\n\n    - rax_scaling_policy:\n        credentials: ~/.raxpub\n        region: ORD\n        cron: '1 0 * * *'\n        change: 25\n        cooldown: 300\n        is_percent: true\n        name: ASG Test Policy - cron\n        policy_type: schedule\n        scaling_group: ASG Test\n      register: asp_cron\n\n    - rax_scaling_policy:\n        credentials: ~/.raxpub\n        region: ORD\n        cooldown: 300\n        desired_capacity: 5\n        name: ASG Test Policy - webhook\n        policy_type: webhook\n        scaling_group: ASG Test\n      register: asp_webhook\n'''\n\ntry:\n    import pyrax\n    HAS_PYRAX = True\nexcept ImportError:\n    HAS_PYRAX = False\n\n\ndef rax_asp(module, at=None, change=0, cron=None, cooldown=300,\n            desired_capacity=0, is_percent=False, name=None,\n            policy_type=None, scaling_group=None, state='present'):\n    changed = False\n\n    au = pyrax.autoscale\n    if not au:\n        module.fail_json(msg='Failed to instantiate client. This '\n                             'typically indicates an invalid region or an '\n                             'incorrectly capitalized region name.')\n\n    try:\n        UUID(scaling_group)\n    except ValueError:\n        try:\n            sg = au.find(name=scaling_group)\n        except Exception as e:\n            module.fail_json(msg='%s' % e.message)\n    else:\n        try:\n            sg = au.get(scaling_group)\n        except Exception as e:\n            module.fail_json(msg='%s' % e.message)\n\n    if state == 'present':\n        policies = filter(lambda p: name == p.name, sg.list_policies())\n        if len(policies) > 1:\n            module.fail_json(msg='No unique policy match found by name')\n        if at:\n            args = dict(at=at)\n        elif cron:\n            args = dict(cron=cron)\n        else:\n            args = None\n\n        if not policies:\n            try:\n                policy = sg.add_policy(name, policy_type=policy_type,\n                                       cooldown=cooldown, change=change,\n                                       is_percent=is_percent,\n                                       desired_capacity=desired_capacity,\n                                       args=args)\n                changed = True\n            except Exception as e:\n                module.fail_json(msg='%s' % e.message)\n\n        else:\n            policy = policies[0]\n            kwargs = {}\n            if policy_type != policy.type:\n                kwargs['policy_type'] = policy_type\n\n            if cooldown != policy.cooldown:\n                kwargs['cooldown'] = cooldown\n\n            if hasattr(policy, 'change') and change != policy.change:\n                kwargs['change'] = change\n\n            if hasattr(policy, 'changePercent') and is_percent is False:\n                kwargs['change'] = change\n                kwargs['is_percent'] = False\n            elif hasattr(policy, 'change') and is_percent is True:\n                kwargs['change'] = change\n                kwargs['is_percent'] = True\n\n            if hasattr(policy, 'desiredCapacity') and change:\n                kwargs['change'] = change\n            elif ((hasattr(policy, 'change') or\n                    hasattr(policy, 'changePercent')) and desired_capacity):\n                kwargs['desired_capacity'] = desired_capacity\n\n            if hasattr(policy, 'args') and args != policy.args:\n                kwargs['args'] = args\n\n            if kwargs:\n                policy.update(**kwargs)\n                changed = True\n\n        policy.get()\n\n        module.exit_json(changed=changed, autoscale_policy=rax_to_dict(policy))\n\n    else:\n        try:\n            policies = filter(lambda p: name == p.name, sg.list_policies())\n            if len(policies) > 1:\n                module.fail_json(msg='No unique policy match found by name')\n            elif not policies:\n                policy = {}\n            else:\n                policy.delete()\n                changed = True\n        except Exception as e:\n            module.fail_json(msg='%s' % e.message)\n\n        module.exit_json(changed=changed, autoscale_policy=rax_to_dict(policy))\n\n\ndef main():\n    argument_spec = rax_argument_spec()\n    argument_spec.update(\n        dict(\n            at=dict(),\n            change=dict(type='int'),\n            cron=dict(),\n            cooldown=dict(type='int', default=300),\n            desired_capacity=dict(type='int'),\n            is_percent=dict(type='bool', default=False),\n            name=dict(required=True),\n            policy_type=dict(required=True, choices=['webhook', 'schedule']),\n            scaling_group=dict(required=True),\n            state=dict(default='present', choices=['present', 'absent']),\n        )\n    )\n\n    module = AnsibleModule(\n        argument_spec=argument_spec,\n        required_together=rax_required_together(),\n        mutually_exclusive=[\n            ['cron', 'at'],\n            ['change', 'desired_capacity'],\n        ]\n    )\n\n    if not HAS_PYRAX:\n        module.fail_json(msg='pyrax is required for this module')\n\n    at = module.params.get('at')\n    change = module.params.get('change')\n    cron = module.params.get('cron')\n    cooldown = module.params.get('cooldown')\n    desired_capacity = module.params.get('desired_capacity')\n    is_percent = module.params.get('is_percent')\n    name = module.params.get('name')\n    policy_type = module.params.get('policy_type')\n    scaling_group = module.params.get('scaling_group')\n    state = module.params.get('state')\n\n    if (at or cron) and policy_type == 'webhook':\n        module.fail_json(msg='policy_type=schedule is required for a time '\n                             'based policy')\n\n    setup_rax_module(module, pyrax)\n\n    rax_asp(module, at=at, change=change, cron=cron, cooldown=cooldown,\n            desired_capacity=desired_capacity, is_percent=is_percent,\n            name=name, policy_type=policy_type, scaling_group=scaling_group,\n            state=state)\n\n\n# import module snippets\nfrom ansible.module_utils.basic import *\nfrom ansible.module_utils.rax import *\n\n# invoke the module\nmain()\n", "license": "gpl-3.0"}
{"id": "78433fc7da602c350a9fb545bfd23b8df7e4297c", "path": "pylinguistics/resources/syllable/silva2011.py", "repo_name": "vwoloszyn/pylinguistics", "content": "#!/usr/bin/env python3\n#-*- encoding:utf-8 -*-\n\n# silva2011.py - Syllable separation using the algorithm described in Silva\n# [2011].\n# Copyright (C) 2014  Alessandro Bokan\n#\n# This program is free software: you can redistribute it and/or modify it\n# under the terms of the GNU General Public License as published by the Free\n# Software Foundation, either version 3 of the License, or (at your option)\n# any later version.\n#\n# This program is distributed in the hope that it will be useful, but WITHOUT\n# ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or\n# FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for\n# more details.\n#\n# You should have received a copy of the GNU General Public License along with\n# this program.  If not, see <http://www.gnu.org/licenses/>.\n#\n# Authors:  Alessandro Bokan <alessandro.bokan@gmail.com>\n#           Andre Cunha      <andre.lv.cunha@gmail.com> (minor modifications)\n\n\nfrom .cases import case1, case2, case3, case4, case5, case6, case7, case8\nfrom .tonic import tonic_vowel\nfrom .api import SyllableSeparator\n\nimport re\nfrom sys import argv\n\n\n# Vowels\nV = ['a', 'e', 'o', '\u00e1', '\u00e9', '\u00ed', '\u00f3', '\u00fa', '\u00e3', '\u00f5', '\u00e2', '\u00ea', '\u00f4', '\u00e0', '\u00fc']\n\n# Semivowels\nG = ['i', 'u']\n\n# Stop consonants\nCOc = ['ca', 'co', 'cu', 'que', 'qui', 'ga', 'go', 'gu', 'gue', 'gui']\nCO = ['p', 't',  'b', 'd', 'c', 'g', 'q'] + COc\n\n# Fricative consonants\nCFc = ['ce', 'ci', 'ss', 'ch', 'ge', 'gi']\nCF = ['f', 'v', 's', '\u00e7', 'z', 'j', 'x'] + CFc\n\n# Liquid consonants\nCL = ['l', 'r', 'rr']\n\n# Nasal consonants\nCN = ['m', 'n']\n\n# Consonants\nC = ['lh', 'nh'] + CO + CF + CL + CN\n\n\nclass Silva2011SyllableSeparator(SyllableSeparator):\n    \"\"\"This class implements the syllabic separation algorithm presented in\n    the fourth chapther of the PhD thesis:\n\n        Silva, D.C. (2011) Algoritmos de Processamento da Linguagem e S\u00edntese\n        de Voz com Emo\u00e7\u00f5es Aplicados a um Conversor Text-Fala Baseado\n        em HMM. PhD dissertation, COPPE, UFRJ.\n    \"\"\"\n\n    def separate(self, w):\n        \"\"\"Separate the syllables of a word.\n\n        Required arguments:\n        w -- the word that will be separated in syllables\n\n        Returns:\n        A list of strings, containing each syllable of the word.\n        \"\"\"\n        vowels = 'a|e|o|i|u|\u00e1|\u00e9|\u00ed|\u00f3|\u00fa|\u00e3|\u00f5|\u00e2|\u00ea|\u00f4|\u00e0|\u00fc'\n        p = [match.start() for match in re.finditer(vowels, w, re.UNICODE)]\n        p0 = 0  # syllable start position\n        pVt = tonic_vowel(w)  # tonic vowel position\n        k = 0\n        c = 0  # Count hyfens\n\n        # Just to pass the Biderman test.\n        if len(w) == 1:\n            return [w]\n\n        while p0 <= (len(w) - 1):\n            # Rule 1:\n            if p[k] + 1 < len(w)\\\n                    and w[p0] in V\\\n                    and not w[p[k]] in ['\u00e3', '\u00f5']\\\n                    and w[p[k] + 1] in V\\\n                    and not w[p[k] + 1] in G:\n                # print \"RULE 1\"\n                if p[k] + 3 < len(w)\\\n                        and w[p[k] + 2] == 's'\\\n                        and p[k] + 3 == len(w):\n                    # print \"RULE 1.1\"\n                    return w\n                else:\n                    # print \"RULE 1.2\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 2:\n            elif p[k] + 3 < len(w)\\\n                    and w[p0] in V\\\n                    and w[p[k] + 1] in C\\\n                    and w[p[k] + 2] in C\\\n                    and w[p[k] + 3] in CO:\n                # print \"RULE 2\"\n                w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n\n            # Rule 3:\n            elif p[k] + 2 < len(w)\\\n                    and w[p0] in V\\\n                    and w[p[k] + 1] in G + CN + ['s', 'r', 'l', 'x']\\\n                    and w[p[k] + 2] in C:\n                # TODO Problema \"arr\".\n                # Exemplo: arrendar -> a-rre-dar (N) | ar-ren-dar (Y)\n                # print \"RULE 3\"\n                if w[p[k] + 1] == 'i'\\\n                        and w[p[k] + 2] in CN:  # NOVA REGRA, p.ex: \"ainda\"\n                    # print \"RULE 3.0\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                elif not w[p[k] + 2] in ['s', 'h']\\\n                        and w[p[k] + 1] != w[p[k] + 2]:\n                    # print \"RULE 3.1\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                elif p[k] + 3 < len(w)\\\n                        and w[p[k] + 1] in CN\\\n                        and w[p[k] + 2] == 's'\\\n                        and not w[p[k] + 3] in V:\n                    # print \"RULE 3.2\"\n                    w, p0, k, c, p, pVt = case7(w, p, p0, pVt, k, c)\n                elif w[p[k] + 1] == w[p[k] + 2]\\\n                        or w[p[k] + 2] == 'h':\n                    # print \"RULE 3.3\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                elif p[k] + 3 < len(w)\\\n                        and w[p[k] + 2] == 's'\\\n                        and ((w[p[k] + 3] in C and w[p[k] + 3] != 's')\n                             or not w[p[k] + 3] in C + V):\n                    # print \"RULE 3.4\"\n                    w, p0, k, c, p, pVt = case7(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 3.5\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n\n            # Rule 4:\n            elif p[k] + 3 < len(w)\\\n                    and w[p0] in V\\\n                    and w[p[k] + 1] in CO + CF + ['g', 'p']\\\n                    and w[p[k] + 2] in CO + CF + CN + ['\u00e7']\\\n                    and w[p[k] + 3] in V + G:\n\n                # print \"RULE 4\"\n                # TODO adicionando um G ao w[p[k] + 3], p.ex: ab-di-car\n                if w[p[k] + 1] == w[p[k] + 2]:\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                else:\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n\n            # Rule 5:\n            elif p[k] + 2 < len(w)\\\n                    and w[p0] in V\\\n                    and w[p[k] + 1] in C\\\n                    and w[p[k] + 2] in V + G + CL + ['h']:\n                # print \"RULE 5\"\n                w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 6:\n            elif p[k] + 3 < len(w)\\\n                    and w[p0] in V\\\n                    and w[p[k] + 1] in G\\\n                    and w[p[k] + 2] == 's'\\\n                    and w[p[k] + 3] in CO:\n                # TODO Regra 6 esta dentro da regra 3\n                # print \"RULE 6\"\n                w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n\n            # Rule 7:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C + ['u', '\u00fc', 'q']\\\n                    and w[p[k] + 1] in C\\\n                    and w[p[k] + 2] in V:\n                # print \"RULE 7\"\n                w, p0, k, c, p, pVt = case3(w, p, p0, pVt, k, c)\n\n            # Rule 8:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in G\\\n                    and w[p[k] + 2] == 'r'\\\n                    and w[p[k] + 3] in C:\n                # print \"RULE 8\"\n                # if p[k] == pVt:\n                #    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                # else:\n                w, p0, k, c, p, pVt = case3(w, p, p0, pVt, k, c)\n\n            # Rule 9:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in G + CN\\\n                    and w[p[k] + 2] == 's'\\\n                    and w[p[k] + 3] in CO:\n                # print \"RULE 9\"\n                w, p0, k, c, p, pVt = case7(w, p, p0, pVt, k, c)\n\n            # Rule 10:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C + G\\\n                    and w[p[k] + 1] in ['i', 'u', 'e', 'o']\\\n                    and p[k] + 1 != pVt\\\n                    and w[p[k]] != w[p[k] + 1]\\\n                    and w[p[k] + 2] in C\\\n                    and w[p[k] + 3] in C + V\\\n                    and w[p[k] + 2] != 's':\n                # print \"RULE 10\"\n                # a-juizado\n                if p[k] == pVt\\\n                        and w[p[k] + 2] != 'n'\\\n                        and not w[p[k] + 3] in C:\n                    # print \"RULE 10.1\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                elif not w[p[k] - 1] in ['q', 'g']\\\n                        and w[p[k]] == 'u'\\\n                        and w[p[k] + 1] == 'i'\\\n                        and w[p[k] + 2] != 'n':\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                elif p[k] != pVt\\\n                        and w[p[k] + 1] == 'i'\\\n                        and w[p[k] + 2] != 'n':\n                    # print \"RULE 10.2\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                elif (w[p[k] + 1] != 'i'\n                      and w[p[k] + 2] in CN + ['r']\n                      and not w[p[k] + 3] in ['h', w[pVt]])\\\n                        or (w[p[k]] in ['a', 'e', 'o']\n                            and w[p[k] + 1] in ['a', 'e', 'o']\n                            and w[p[k] + 2] in CN\n                            and not w[p[k] + 3] in ['h', 's']\n                            and w[p[k] + 4] in V + C):\n                    # print \"RULE 10.3\"\n                    if w[p[k] - 1:p[k] + 1] == \"gu\"\\\n                            and w[p[k] + 1] in V\\\n                            and w[p[k] + 2] in CN:\n                        w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n                    elif w[p[k] - 1:p[k] + 1] == \"gu\"\\\n                            and w[p[k] + 1] in V\\\n                            and w[p[k] + 2] in CL:\n                        w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                    else:\n                        w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                elif w[p[k]] in G\\\n                        and w[p[k] + 1] in ['a', 'e', 'o']\\\n                        and w[p[k] + 2] in CN:\n                    # print \"RULE 10.4\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 10.5\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n\n            # Rule 11:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in G\\\n                    and w[p[k] + 2] in V:\n                # print \"RULE 11\"\n                w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n\n            # Rule 12:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k]] in G\\\n                    and w[p[k] + 1] in V + ['i']\\\n                    and w[p[k]] != w[p[k] + 1]\\\n                    and w[p[k] + 2] in C\\\n                    and w[p[k] + 3] in V:\n                # TODO Agregue un \"i\" as vogais\n                #   porque sino no entra ao exemplo.\n                # print \"RULE 12\"\n                if w[p[k] - 1] in ['q', 'g']\\\n                   and ((w[p[k] + 2] == '\u00e7'\n                         and w[p[k] + 3] in ['\u00e3', '\u00f5'])\n                        or (w[p[k] - 1] == 'q'\n                            and w[p[k] + 1] in V)):\n                    # print \"RULE 12.1\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                elif p[k] + 1 == pVt\\\n                        or w[p[k] - 1] == 'r' and p[k] + 3 == pVt:\n                    # print \"RULE 12.2\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 12.3\"\n                    w, p0, k, c, p, pVt = case8(w, p, p0, pVt, k, c)\n\n            # Rule 13:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and (w[p[k] - 1] in C\n                         or (w[p[k] - 1:p[k] + 1]\n                             in ['qu', 'q\u00fc', 'gu', 'g\u00fc']))\\\n                    and w[p[k] + 1] in V + CL + CN + ['c', 'x']\\\n                    and w[p[k] + 2] in ['h', 'l', 'r']\\\n                    and w[p[k] + 3] in V + ['h', 'l', 'r']:\n                # TODO Arrumando regra para \"guerra\" -> gue-rra\n                # print \"RULE 13\"\n                if w[p[k] + 1] == w[p[k] + 2]\\\n                        or w[p[k] + 1] in ['c', 'l']\\\n                        or w[p[k] + 1:p[k] + 3] == 'nh':\n                    # print \"RULE 13.1\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 13.2\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n\n            # Rule 14:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in CL + CN + ['i']\\\n                    and w[p[k] + 2] == 's':\n                # print \"RULE 14\"\n                if p[k] + 3 == len(w):\n                    p0 = case6(w, p0)\n                elif p[k] == pVt or (p[k] + 3 < len(w) and w[p[k] + 3] in V):\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                else:\n                    w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n\n            # Rule 15:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] + 1] in V\\\n                    and w[p[k] + 2] in V + G\\\n                    and not w[p[k] - 1:p[k] + 1] in ['qu', 'gu']:\n                # print \"RULE 15\", w[p0]\n                if p[k] + 3 < len(w)\\\n                        and p[k] == pVt\\\n                        and w[p[k] + 1] in G\\\n                        and w[p[k] + 3] in C:\n                    # print \"RULE 15.1\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 15.2\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 16:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k]] != 'u'\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in V\\\n                    and w[p[k] + 2] in CN:\n                # print \"RULE 16\"\n                w, p0, k, c, p, pVt = case3(w, p, p0, pVt, k, c)\n\n            # Rule 17:\n            elif p[k] + 1 < len(w)\\\n                    and p[k] - 2 >= 0\\\n                    and not w[p0] in V\\\n                    and w[p[k]] == 'i'\\\n                    and (w[p[k] - 2] in ['\u00e1', '\u00e9', '\u00ed', '\u00f3', '\u00fa']\n                         or w[p[k] - 3] in ['\u00e1', '\u00e9', '\u00ed', '\u00f3', '\u00fa'])\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in ['a', 'o']:\n                # TODO trocar caso 6 por caso 1.\n                # car\u00edcia -> ca-r\u00ed-cia (N) | ca-r\u00ed-ci-a (Y)\n                # print \"RULE 17\"\n                w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 18:\n            elif p[k] + 1 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k]] in ['\u00e3', '\u00f5']\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in ['e', 'o']:\n                # print \"RULE 18\"\n                p0 = case6(w, p0)\n\n            # -------------------- Change rule 19 by 20 --------------------\n            # Rule 20:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] + 1] in V\\\n                    and w[p[k] + 2] in CN\\\n                    and w[p[k] + 3] in C:\n                # print \"RULE 20\"\n                w, p0, k, c, p, pVt = case7(w, p, p0, pVt, k, c)\n\n            # Rule 19:\n            elif p[k] + 1 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and p[k] + 1 == pVt\\\n                    and not w[p[k] + 1] in ['i', 'u']\\\n                    and not w[p[k] - 1:p[k] + 1] in ['gu', 'qu']:\n                # print \"RULE 19\"\n                if p[k] + 3 == len(w)\\\n                        and w[p[k] - 1:p[k] + 1] in ['gu', 'qu']\\\n                        and w[p[k] + 1] in V\\\n                        and w[p[k] + 2] in C:\n                    # print \"RULE 19.1\"\n                    p0 = case6(w, p0)\n                elif p[k] + 2 < len(w)\\\n                        and w[p[k] - 1:p[k] + 1] in ['gu', 'qu']\\\n                        and w[p[k] + 1] in V\\\n                        and w[p[k] + 2] in C + G:\n                    # print \"RULE 19.2\"\n                    w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 19.3\"\n                    w, p0, k, c, p, pVt = case3(w, p, p0, pVt, k, c)\n\n            # Rule 21:\n            elif p[k] + 3 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] + 1] in CO + ['f', 'v', 'g']\\\n                    and w[p[k] + 2] in CL + CO\\\n                    and w[p[k] + 3] in V + G:\n                # print \"RULE 21\"\n                if w[p[k] + 1] in ['f', 'p']\\\n                        and w[p[k] + 2] in ['t', '\u00e7']:\n                    # print \"RULE 21.1\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                else:\n                    # print \"RULE 21.2\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 22:\n            elif p[k] + 1 < len(w)\\\n                    and p[k] - 2 >= 0\\\n                    and not w[p0] in V\\\n                    and (w[p[k] - 1] in C\n                         or w[p[k] - 1:p[k] + 1] in ['qu', 'gu'])\\\n                    and w[p[k] + 1] in V\\\n                    and (p[k] + 2 == len(w)\n                         or w[p[k] + 2] in C):\n                # print \"RULE 22\"\n                if (w[p[k]] in ['i', 'u', '\u00ed', '\u00fa', '\u00e9', '\u00ea']\n                    and p[k] == pVt\n                    and w[p[k] + 1] != 'u')\\\n                        or (p[k] + 3 < len(w)\n                            and not w[p[k]] in G\n                            and w[p[k] + 2] == 's'\n                            and not w[p[k] + 3] in C + V):\n                    # print \"RULE 22.1\"\n                    w, p0, k, c, p, pVt = case3(w, p, p0, pVt, k, c)\n                elif p[k] + 2 == len(w)\\\n                        and w[p[k]] == 'i'\\\n                        and p[k] == pVt\\\n                        and w[p[k] + 1] == 'u':\n                    # print \"RULE 22.2\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                elif p[k] + 3 < len(w)\\\n                        and ((w[p[k]] in G\n                              and p[k] + 1 != pVt\n                              and not w[p[k] + 2] in C + V)\n                             or (w[p[k] + 2] == 's'\n                                 and not w[p[k] + 3] in C + V)\n                             or (p[k] != pVt\n                             and p[k] + 1 != pVt\n                             and w[p[k] + 2] == 's'\n                             and p[k] + 3 == len(w))):\n                    # print \"RULE 22.3\"\n                    p0 = case6(w, p0)\n                elif p[k] + 3 < len(w)\\\n                        and w[p[k] - 1:p[k] + 1] in ['qu', 'gu']\\\n                        and w[p[k] + 2] in C\\\n                        and w[p[k] + 3] in V + G:\n                    # print \"RULE 22.4\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                elif p[k] + 2 == len(w)\\\n                        and w[p[k] - 1:p[k] + 1] in ['qu', 'gu']\\\n                        and w[p[k] + 1] in V + G:\n                    # print \"RULE 22.4.5\"\n                    p0 = case6(w, p0)\n                elif p[k] + 3 == len(w)\\\n                        and w[p[k] + 1] in ['o', 'u']\\\n                        and p[k] + 1 != pVt\\\n                        and w[p[k] + 2] == 's':\n                    # print \"RULE 22.5\"\n                    w, p0, k, c, p, pVt = case7(w, p, p0, pVt, k, c)\n                else:\n                    # TODO Trocar case2 por case 1\n                    # print \"RULE 22.6\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 23:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and (w[p[k] - 1] in C\n                         or w[p[k] - 2:p[k] - 1] == \"qu\")\\\n                    and w[p[k] + 1] in C\\\n                    and w[p[k] + 2] in C:\n                # print \"RULE 23\"\n                if w[p[k] + 1] == w[p[k] + 2]:\n                    # print \"RULE 23.1\"\n                    w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n                elif w[p[k] + 1] == 's'\\\n                        and w[p[k] + 2] != 's':\n                    # print \"RULE 23.2\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n                elif p[k] + 3 < len(w)\\\n                        and w[p[k] + 2] == 's'\\\n                        and w[p[k] + 3] in CO:\n                    # print \"RULE 23.3\"\n                    w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n                else:  # Adicionando ELSE\n                    # print \"RULE 23.4\"\n                    w, p0, k, c, p, pVt = case2(w, p, p0, pVt, k, c)\n\n            # Rule 24:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] + 1] in C\\\n                    and w[p[k] + 2] in G:\n                # Regra 24 igual a 23. Arrumar regra, p.ex: di-sen-\"teria\"\n                # print \"RULE 24\"\n                w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 25: Already aplicated\n\n            # Rule 26:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and (w[p[k] - 1] in C\n                         or (w[p[k] - 1:p[k] + 1]\n                             in ['qu', 'q\u00fc', 'gu', 'g\u00fc']))\\\n                    and w[p[k] + 1] in G\\\n                    and w[p[k] + 2] in CN:\n                # Manual: a-mi-gui-nho | Automatic: a-mi-gu-i-nho\n                # print \"RULE 26\"\n                w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n\n            # Rule 27:\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1] in C\\\n                    and w[p[k] - 2] in C\\\n                    and w[p[k] + 1] in G\\\n                    and w[p[k] + 2] in C:\n                # print \"RULE 27\"\n                w, p0, k, c, p, pVt = case1(w, p, p0, pVt, k, c)\n\n            # Rule 28\n            elif p[k] + 2 < len(w)\\\n                    and not w[p0] in V\\\n                    and w[p[k] - 1:p[k] + 1] in ['qu', 'q\u00fc', 'gu', 'g\u00fc']\\\n                    and w[p[k] + 1] in V:\n                # print \"RULE 28\"\n                if p[k] + 3 < len(w)\\\n                        and w[p[k] + 2] in C\\\n                        and w[p[k] + 3] in C:\n                    # print \"RULE 28.1\"\n                    w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n                elif p[k] + 3 < len(w)\\\n                        and w[p[k] + 2] in C\\\n                        and w[p[k] + 3] in V + G:\n                    # print \"RULE 28.2\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                elif p[k] + 2 < len(w)\\\n                        and w[p[k] + 2] in V:\n                    # print \"RULE 28.3\"\n                    w, p0, k, c, p, pVt = case4(w, p, p0, pVt, k, c)\n                elif p[k] + 2 < len(w)\\\n                        and w[p[k] + 2] in G:\n                    # print \"RULE 28.4\"\n                    w, p0, k, c, p, pVt = case5(w, p, p0, pVt, k, c)\n\n            p0 += 1\n\n        return w.split('-')\n\n    def _test(self):\n        import codecs\n        with codecs.open('./test/biderman-UTF-8.txt',\n                         encoding='utf-8', mode='r') as content_file,\\\n            codecs.open('./test/biderman_output.txt',\n                        encoding='utf-8', mode='w') as output_file:\n\n            content = content_file.read()\n            lines = content.split('\\n')\n            for line in lines:\n                word = line.split(',')[0]\n                print('Testing word ' + word)\n                result = self.separate_syllables(word)\n                output_file.write(' '.join(result) + ' - '\n                                  + str(len(result)) + ' silabas\\n')\n\n\n\nif __name__ == '__main__':\n    separator = Silva2011SyllableSeparator()\n    if len(argv) == 1:\n        separator._test()\n    else:\n        print(separator.separate(argv[1]))\n\nsyllable_separator = Silva2011SyllableSeparator()\n\n", "license": "gpl-2.0"}
{"id": "4ecc4c4c06ccf4033c1caeca84154d8531709514", "path": "django/contrib/comments/forms.py", "repo_name": "sunils34/buffer-django-nonrel", "content": "import time\nimport datetime\n\nfrom django import forms\nfrom django.forms.util import ErrorDict\nfrom django.conf import settings\nfrom django.contrib.contenttypes.models import ContentType\nfrom models import Comment\nfrom django.utils.crypto import salted_hmac, constant_time_compare\nfrom django.utils.encoding import force_unicode\nfrom django.utils.hashcompat import sha_constructor\nfrom django.utils.text import get_text_list\nfrom django.utils.translation import ungettext, ugettext_lazy as _\n\nCOMMENT_MAX_LENGTH = getattr(settings,'COMMENT_MAX_LENGTH', 3000)\n\nclass CommentSecurityForm(forms.Form):\n    \"\"\"\n    Handles the security aspects (anti-spoofing) for comment forms.\n    \"\"\"\n    content_type  = forms.CharField(widget=forms.HiddenInput)\n    object_pk     = forms.CharField(widget=forms.HiddenInput)\n    timestamp     = forms.IntegerField(widget=forms.HiddenInput)\n    security_hash = forms.CharField(min_length=40, max_length=40, widget=forms.HiddenInput)\n\n    def __init__(self, target_object, data=None, initial=None):\n        self.target_object = target_object\n        if initial is None:\n            initial = {}\n        initial.update(self.generate_security_data())\n        super(CommentSecurityForm, self).__init__(data=data, initial=initial)\n\n    def security_errors(self):\n        \"\"\"Return just those errors associated with security\"\"\"\n        errors = ErrorDict()\n        for f in [\"honeypot\", \"timestamp\", \"security_hash\"]:\n            if f in self.errors:\n                errors[f] = self.errors[f]\n        return errors\n\n    def clean_security_hash(self):\n        \"\"\"Check the security hash.\"\"\"\n        security_hash_dict = {\n            'content_type' : self.data.get(\"content_type\", \"\"),\n            'object_pk' : self.data.get(\"object_pk\", \"\"),\n            'timestamp' : self.data.get(\"timestamp\", \"\"),\n        }\n        expected_hash = self.generate_security_hash(**security_hash_dict)\n        actual_hash = self.cleaned_data[\"security_hash\"]\n        if not constant_time_compare(expected_hash, actual_hash):\n            # Fallback to Django 1.2 method for compatibility\n            # PendingDeprecationWarning <- here to remind us to remove this\n            # fallback in Django 1.5\n            expected_hash_old = self._generate_security_hash_old(**security_hash_dict)\n            if not constant_time_compare(expected_hash_old, actual_hash):\n                raise forms.ValidationError(\"Security hash check failed.\")\n        return actual_hash\n\n    def clean_timestamp(self):\n        \"\"\"Make sure the timestamp isn't too far (> 2 hours) in the past.\"\"\"\n        ts = self.cleaned_data[\"timestamp\"]\n        if time.time() - ts > (2 * 60 * 60):\n            raise forms.ValidationError(\"Timestamp check failed\")\n        return ts\n\n    def generate_security_data(self):\n        \"\"\"Generate a dict of security data for \"initial\" data.\"\"\"\n        timestamp = int(time.time())\n        security_dict =   {\n            'content_type'  : str(self.target_object._meta),\n            'object_pk'     : str(self.target_object._get_pk_val()),\n            'timestamp'     : str(timestamp),\n            'security_hash' : self.initial_security_hash(timestamp),\n        }\n        return security_dict\n\n    def initial_security_hash(self, timestamp):\n        \"\"\"\n        Generate the initial security hash from self.content_object\n        and a (unix) timestamp.\n        \"\"\"\n\n        initial_security_dict = {\n            'content_type' : str(self.target_object._meta),\n            'object_pk' : str(self.target_object._get_pk_val()),\n            'timestamp' : str(timestamp),\n          }\n        return self.generate_security_hash(**initial_security_dict)\n\n    def generate_security_hash(self, content_type, object_pk, timestamp):\n        \"\"\"\n        Generate a HMAC security hash from the provided info.\n        \"\"\"\n        info = (content_type, object_pk, timestamp)\n        key_salt = \"django.contrib.forms.CommentSecurityForm\"\n        value = \"-\".join(info)\n        return salted_hmac(key_salt, value).hexdigest()\n\n    def _generate_security_hash_old(self, content_type, object_pk, timestamp):\n        \"\"\"Generate a (SHA1) security hash from the provided info.\"\"\"\n        # Django 1.2 compatibility\n        info = (content_type, object_pk, timestamp, settings.SECRET_KEY)\n        return sha_constructor(\"\".join(info)).hexdigest()\n\nclass CommentDetailsForm(CommentSecurityForm):\n    \"\"\"\n    Handles the specific details of the comment (name, comment, etc.).\n    \"\"\"\n    name          = forms.CharField(label=_(\"Name\"), max_length=50)\n    email         = forms.EmailField(label=_(\"Email address\"))\n    url           = forms.URLField(label=_(\"URL\"), required=False)\n    comment       = forms.CharField(label=_('Comment'), widget=forms.Textarea,\n                                    max_length=COMMENT_MAX_LENGTH)\n\n    def get_comment_object(self):\n        \"\"\"\n        Return a new (unsaved) comment object based on the information in this\n        form. Assumes that the form is already validated and will throw a\n        ValueError if not.\n\n        Does not set any of the fields that would come from a Request object\n        (i.e. ``user`` or ``ip_address``).\n        \"\"\"\n        if not self.is_valid():\n            raise ValueError(\"get_comment_object may only be called on valid forms\")\n\n        CommentModel = self.get_comment_model()\n        new = CommentModel(**self.get_comment_create_data())\n        new = self.check_for_duplicate_comment(new)\n\n        return new\n\n    def get_comment_model(self):\n        \"\"\"\n        Get the comment model to create with this form. Subclasses in custom\n        comment apps should override this, get_comment_create_data, and perhaps\n        check_for_duplicate_comment to provide custom comment models.\n        \"\"\"\n        return Comment\n\n    def get_comment_create_data(self):\n        \"\"\"\n        Returns the dict of data to be used to create a comment. Subclasses in\n        custom comment apps that override get_comment_model can override this\n        method to add extra fields onto a custom comment model.\n        \"\"\"\n        return dict(\n            content_type = ContentType.objects.get_for_model(self.target_object),\n            object_pk    = force_unicode(self.target_object._get_pk_val()),\n            user_name    = self.cleaned_data[\"name\"],\n            user_email   = self.cleaned_data[\"email\"],\n            user_url     = self.cleaned_data[\"url\"],\n            comment      = self.cleaned_data[\"comment\"],\n            submit_date  = datetime.datetime.now(),\n            site_id      = settings.SITE_ID,\n            is_public    = True,\n            is_removed   = False,\n        )\n\n    def check_for_duplicate_comment(self, new):\n        \"\"\"\n        Check that a submitted comment isn't a duplicate. This might be caused\n        by someone posting a comment twice. If it is a dup, silently return the *previous* comment.\n        \"\"\"\n        possible_duplicates = self.get_comment_model()._default_manager.using(\n            self.target_object._state.db\n        ).filter(\n            content_type = new.content_type,\n            object_pk = new.object_pk,\n            user_name = new.user_name,\n            user_email = new.user_email,\n            user_url = new.user_url,\n        )\n        for old in possible_duplicates:\n            if old.submit_date.date() == new.submit_date.date() and old.comment == new.comment:\n                return old\n\n        return new\n\n    def clean_comment(self):\n        \"\"\"\n        If COMMENTS_ALLOW_PROFANITIES is False, check that the comment doesn't\n        contain anything in PROFANITIES_LIST.\n        \"\"\"\n        comment = self.cleaned_data[\"comment\"]\n        if settings.COMMENTS_ALLOW_PROFANITIES == False:\n            bad_words = [w for w in settings.PROFANITIES_LIST if w in comment.lower()]\n            if bad_words:\n                plural = len(bad_words) > 1\n                raise forms.ValidationError(ungettext(\n                    \"Watch your mouth! The word %s is not allowed here.\",\n                    \"Watch your mouth! The words %s are not allowed here.\", plural) % \\\n                    get_text_list(['\"%s%s%s\"' % (i[0], '-'*(len(i)-2), i[-1]) for i in bad_words], 'and'))\n        return comment\n\nclass CommentForm(CommentDetailsForm):\n    honeypot      = forms.CharField(required=False,\n                                    label=_('If you enter anything in this field '\\\n                                            'your comment will be treated as spam'))\n\n    def clean_honeypot(self):\n        \"\"\"Check that nothing's been entered into the honeypot.\"\"\"\n        value = self.cleaned_data[\"honeypot\"]\n        if value:\n            raise forms.ValidationError(self.fields[\"honeypot\"].label)\n        return value\n", "license": "bsd-3-clause"}
{"id": "d28728b94090ffd3f596f195426764fa552e9634", "path": "sdk/google_appengine/lib/django-1.4/django/contrib/localflavor/ec/forms.py", "repo_name": "ProfessionalIT/professionalit-webiste", "content": "\"\"\"\nEcuador-specific form helpers.\n\"\"\"\n\nfrom __future__ import absolute_import\n\nfrom django.contrib.localflavor.ec.ec_provinces import PROVINCE_CHOICES\nfrom django.forms.fields import Select\n\nclass ECProvinceSelect(Select):\n    \"\"\"\n    A Select widget that uses a list of Ecuador provinces as its choices.\n    \"\"\"\n    def __init__(self, attrs=None):\n        super(ECProvinceSelect, self).__init__(attrs, choices=PROVINCE_CHOICES)\n", "license": "lgpl-3.0"}
{"id": "e98b58921c5c4f8f5e3d3c657b4770e119bd0d84", "path": "pyparticleest/models/ltv.py", "repo_name": "jerkern/pyParticleEst", "content": "\"\"\"Model definition for base class for Linear Time-varying systems\n@author: Jerker Nordh\n\"\"\"\nfrom pyparticleest.interfaces import FFBSi, ParticleFiltering\n\ntry:\n    import pyparticleest.utils.ckalman as kalman\n    import pyparticleest.utils.cmlnlg_compute as mlnlg_compute\nexcept ImportError:\n    print(\"Falling back to pure python implementaton, expect horrible performance\")\n    import pyparticleest.utils.kalman as kalman\n    import pyparticleest.utils.mlnlg_compute as mlnlg_compute\n\nimport numpy\nimport scipy.linalg\n\nfrom builtins import range\n\n\nclass LTV(FFBSi, ParticleFiltering):\n    \"\"\"\n    Base class for particles of the type linear time varying with additive gaussian noise.\n\n    Implement this type of system by extending this class and provide the methods for returning\n    the system matrices at each time instant\n\n    z_{t+1} = A*z_t + f + v, v ~ N(0, Q)\n    y_t = C*z_t + h + e, e ~ N(0,R)\n\n    Args:\n     - z0: Initial mean value of the state estimate\n     - P0: Coviariance of initial z estimate\n     - A (array-like): A matrix (if constant)\n     - C (array-like): C matrix (if constant)\n     - Q (array-like): Q matrix (if constant)\n     - R (array-like): R matrix (if constant)\n     - f (array-like): f vector (if constant)\n     - h (array-like): h vector (if constant)\n     - params (array-like): model parameters (if any)\n    \"\"\"\n\n    def __init__(self, z0, P0, A=None, C=None, Q=None,\n                 R=None, f=None, h=None, params=None, **kwargs):\n        self.z0 = numpy.copy(z0).reshape((-1, 1))\n        self.P0 = numpy.copy(P0)\n        if (f is None):\n            f = numpy.zeros_like(self.z0)\n        self.kf = kalman.KalmanSmoother(lz=len(self.z0),\n                                        A=A, C=C,\n                                        Q=Q, R=R,\n                                        f_k=f, h_k=h)\n        super(LTV, self).__init__(**kwargs)\n\n    def create_initial_estimate(self, N):\n        \"\"\"Sample particles from initial distribution\n\n        Args:\n         - N (int): Number of particles to sample, since the estimate is\n           deterministic there is no reason for N > 1\n\n        Returns:\n         (array-like) with first dimension = N, model specific representation\n         of all particles \"\"\"\n\n        if (N > 1):\n            print(\"N > 1 redundamt for LTV system (N={0})\".format(N),)\n        lz = len(self.z0)\n        dim = lz + lz * lz\n        particles = numpy.empty((N, dim))\n\n        for i in range(N):\n            particles[i, :lz] = numpy.copy(self.z0).ravel()\n            particles[i, lz:] = numpy.copy(self.P0).ravel()\n        return particles\n\n    def set_states(self, particles, z_list, P_list):\n        \"\"\"\n        Set the estimate of the states\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - z_list (list): list of mean values for z for each particle\n         - P_list (list): list of covariance matrices for z for each particle\n        \"\"\"\n        lz = len(self.z0)\n        N = len(particles)\n        for i in range(N):\n            particles[i, :lz] = z_list[i].ravel()\n            lzP = lz + lz * lz\n            particles[i, lz:lzP] = P_list[i].ravel()\n\n    def get_states(self, particles):\n        \"\"\"\n        Return the estimates contained in the particles array\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n\n        Returns\n            (zl, Pl):\n             - zl: list of mean values for z\n             - Pl: list of covariance matrices for z\n        \"\"\"\n        N = len(particles)\n        zl = list()\n        Pl = list()\n        lz = len(self.z0)\n        for i in range(N):\n            zl.append(particles[i, :lz].reshape(-1, 1))\n            lzP = lz + lz * lz\n            Pl.append(particles[i, lz:lzP].reshape(self.P0.shape))\n\n        return (zl, Pl)\n\n    def get_pred_dynamics(self, u, t):\n        \"\"\"\n        Return matrices describing affine relation of next\n        nonlinear state conditioned on the current time and input signal\n\n        z_{t+1} = A*z_t + f + v, v ~ N(0, Q)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - u (array-like): input signal\n         - t (float): time stamp\n\n        Returns:\n         (A, f, Q) where each element is a list\n         with the corresponding matrix for each particle. None indicates\n         that the matrix is identical for all particles and the value stored\n         in this class should be used instead\n        \"\"\"\n        return (None, None, None)\n\n    def update(self, particles, u, t, noise):\n        \"\"\" Propagate estimate forward in time\n\n        Args:\n\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - u (array-like):  input signal\n         - t (float): time-stamp\n         - noise: Unused for this type of model\n\n        Returns:\n         (array-like) with first dimension = N, particle estimate at time t+1\n        \"\"\"\n        # Update linear estimate with data from measurement of next non-linear\n        # state\n        (zl, Pl) = self.get_states(particles)\n        (A, f, Q) = self.get_pred_dynamics(u=u, t=t)\n        self.kf.set_dynamics(A=A, Q=Q, f_k=f)\n        for i in range(len(zl)):\n            # Predict z_{t+1}\n            (zl[i], Pl[i]) = self.kf.predict(zl[i], Pl[i])\n\n        # Predict next states conditioned on eta_next\n        self.set_states(particles, zl, Pl)\n        return particles\n\n    def get_meas_dynamics(self, y, t):\n        \"\"\"\n        Return matrices describing affine relation of measurement and current\n        state estimates\n\n        y_t = C*z_t + h + e, e ~ N(0,R)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - y (array-like): measurement\n         - t (float): time stamp\n\n        Returns:\n         (y, C, h, R): y is a preprocessed measurement, the rest are lists\n         with the corresponding matrix for each particle. None indicates\n         that the matrix is identical for all particles and the value stored\n         in this class should be used instead\n        \"\"\"\n        return (y, None, None, None)\n\n    def measure(self, particles, y, t):\n        \"\"\"\n        Return the log-pdf value of the measurement and update the statistics\n        for the states\n\n        Args:\n\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - y (array-like):  measurement\n         - t (float): time-stamp\n\n        Returns:\n         (array-like) with first dimension = N, logp(y|x^i)\n        \"\"\"\n\n        (zl, Pl) = self.get_states(particles)\n        (y, C, h, R) = self.get_meas_dynamics(y=y, t=t)\n        self.kf.set_dynamics(C=C, R=R, h_k=h)\n        lyz = numpy.empty((len(particles)))\n        for i in range(len(zl)):\n            # Predict z_{t+1}\n            lyz[i] = self.kf.measure(y, zl[i], Pl[i])\n\n        self.set_states(particles, zl, Pl)\n        return lyz\n\n    def logp_xnext(self, particles, next_part, u, t):\n        \"\"\"\n        Return the log-pdf value for the possible future state 'next'\n        given input u.\n\n        Always returns zeros since all particles are always equivalent for this\n        type of model\n\n        Args:\n\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - next_part: Unused\n         - u: Unused\n         - t: Unused\n\n        Returns:\n         (array-like) with first dimension = N, numpu.zeros((N,))\n        \"\"\"\n        # Not needed for Linear Gaussian models, always return 0 (all particles will be identical anyhow)\n        N = len(particles)\n        return numpy.zeros((N,))\n\n    def sample_process_noise(self, particles, u, t):\n        \"\"\"\n        There is no need to sample noise for this type of model\n\n        Args:\n\n         - particles: Unused\n         - next_part: Unused\n         - u: Unused\n         - t: Unused\n\n        Returns:\n         None\n        \"\"\"\n        return None\n\n    def sample_smooth(self, part, ptraj, anc, future_trajs, find, ut, yt, tt, cur_ind):\n        \"\"\"\n        Update sufficient statistics based on the future states\n\n        Args:\n         - part  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - ptraj: array of trajectory step objects from previous time-steps,\n           last index is step just before the current\n         - anc (array-like): index of the ancestor of each particle in part\n         - future_trajs (array-like): particle estimate for {t+1:T}\n         - find (array-like): index in future_trajs corresponding to each\n           particle in part\n         - ut (array-like): input signals for {0:T}\n         - yt (array-like): measurements for {0:T}\n         - tt (array-like): time stamps for {0:T}\n         - cur_ind (int): index of current timestep (in ut, yt and tt)\n\n        Returns:\n         (array-like) with first dimension = N\n        \"\"\"\n\n        (zl, Pl) = self.get_states(part)\n        M = len(part)\n        lz = len(self.z0)\n        lzP = lz + lz * lz\n        res = numpy.empty((M, lz + 2 * lz ** 2))\n        for j in range(M):\n            if (future_trajs is not None):\n                zn = future_trajs[0].pa.part[j, :lz].reshape((lz, 1))\n                Pn = future_trajs[0].pa.part[j, lz:lzP].reshape((lz, lz))\n                (A, f, Q) = self.get_pred_dynamics(u=ut[0], t=tt[0])\n                self.kf.set_dynamics(A=A, Q=Q, f_k=f)\n                (zs, Ps, Ms) = self.kf.smooth(zl[0], Pl[0], zn, Pn, self.kf.A, self.kf.f_k, self.kf.Q)\n            else:\n                zs = zl[j]\n                Ps = Pl[j]\n                Ms = numpy.zeros_like(Ps)\n            res[j] = numpy.hstack((zs.ravel(), Ps.ravel(), Ms.ravel()))\n\n        return res\n\n    def fwd_peak_density(self, u, t):\n        \"\"\"\n        No need for rejections sampling for this type of model, always returns\n        0.0 since all particles are equivalent\n\n        Args:\n         - u: Unused\n         - t: Unused\n\n        Returns\n         (float) 0.0\n        \"\"\"\n        return 0.0\n\n    def eval_logp_x0(self, particles, t):\n        \"\"\"\n        Evaluate sum log p(x_0)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - t (float): time stamp\n        \"\"\"\n        # Calculate l1 according to (19a)\n        N = len(particles)\n        (zl, Pl) = self.get_states(particles)\n        lpz0 = numpy.empty(N)\n        for i in range(N):\n            l1 = self.calc_l1(zl[i], Pl[i], self.z0, self.P0)\n            (_tmp, ld) = numpy.linalg.slogdet(self.P0)\n            tmp = numpy.linalg.solve(self.P0, l1)\n            lpz0[i] = -0.5 * (ld + numpy.trace(tmp))\n        return lpz0\n\n    def eval_logp_x0_val_grad(self, particles, t):\n        \"\"\"\n        Evaluate gradient of sum log p(x_0)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - t (float): time stamp\n        \"\"\"\n        # Calculate l1 according to (19a)\n        N = len(particles)\n        lparam = len(self.params)\n        lpz0_grad = numpy.zeros(lparam)\n        (zl, Pl) = self.get_states(particles)\n        (z0_grad, P0_grad) = self.get_initial_grad()\n        if (z0_grad is None and P0_grad is None):\n            lpz0 = self.eval_logp_x0(particles, t)\n        else:\n            lpz0 = 0.0\n            P0cho = scipy.linalg.cho_factor(self.P0)\n            ld = numpy.sum(numpy.log(numpy.diagonal(P0cho[0]))) * 2\n            for i in range(N):\n                (l1, l1_grad) = self.calc_l1_grad(zl[i], Pl[i], self.z0, self.P0, z0_grad)\n                tmp = scipy.linalg.cho_solve(P0cho, l1)\n                lpz0 += -0.5 * (ld + numpy.trace(tmp))\n                for j in range(len(self.params)):\n                    lpz0_grad[j] -= 0.5 * mlnlg_compute.compute_logprod_derivative(P0cho, P0_grad[j], l1, l1_grad[j])\n        return (lpz0, lpz0_grad)\n\n    def eval_logp_xnext(self, particles, x_next, u, t):\n        \"\"\"\n        Evaluate log p(x_{t+1}|x_t)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - x_next (array-like): future states\n         - t (float): time stamp\n\n        Returns: (array-like)\n        \"\"\"\n        # Calculate l2 according to (16)\n        N = len(particles)\n        (zl, Pl) = self.get_states(particles)\n        (zn, Pn) = self.get_states(x_next)\n        (A, f, Q) = self.get_pred_dynamics(u=u, t=t)\n        self.kf.set_dynamics(A=A, Q=Q, f_k=f)\n        self.t = t\n        lpxn = numpy.empty(N)\n\n        for k in range(N):\n            lz = len(self.z0)\n            lzP = lz + lz * lz\n            Mz = particles[k][lzP:].reshape((lz, lz))\n            (l2, _A, _M_ext, _predict_err) = self.calc_l2(zn[k], Pn[k], zl[k], Pl[k], self.kf.A, self.kf.f_k, Mz)\n            (_tmp, ld) = numpy.linalg.slogdet(self.kf.Q)\n            tmp = numpy.linalg.solve(self.kf.Q, l2)\n            lpxn[k] = -0.5 * (ld + numpy.trace(tmp))\n\n        return lpxn\n\n    def eval_logp_xnext_val_grad(self, particles, x_next, u, t):\n        \"\"\"\n        Evaluate value and gradient of log p(x_{t+1}|x_t)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - x_next (array-like): future states\n         - t (float): time stamp\n\n        Returns: ((array-like), (array-like))\n        \"\"\"\n        # Calculate l2 according to (16)\n        N = len(particles)\n        lparam = len(self.params)\n        (zl, Pl) = self.get_states(particles)\n        (zn, Pn) = self.get_states(x_next)\n        (A, f, Q) = self.get_pred_dynamics(u=u, t=t)\n        (A_grad, f_grad, Q_grad) = self.get_pred_dynamics_grad(u=u, t=t)\n        lpxn_grad = numpy.zeros(lparam)\n        if (A_grad is None and f_grad is None and Q_grad is None):\n            lpxn = self.eval_logp_xnext(particles, x_next, u, t)\n        else:\n            self.kf.set_dynamics(A=A, Q=Q, f_k=f)\n            lpxn = 0.0\n            Qcho = scipy.linalg.cho_factor(self.kf.Q, check_finite=False)\n            ld = numpy.sum(numpy.log(numpy.diagonal(Qcho[0]))) * 2\n\n            if (Q_grad is None):\n                Q_grad = numpy.zeros(\n                    (len(self.params), self.kf.lz, self.kf.lz))\n\n            for k in range(N):\n                lz = len(self.z0)\n                lzP = lz + lz * lz\n                Mz = particles[k][lzP:].reshape((lz, lz))\n                (l2, l2_grad) = self.calc_l2_grad(zn[k], Pn[k], zl[k], Pl[k], self.kf.A, self.kf.f_k, Mz, A_grad, f_grad)\n                tmp = scipy.linalg.cho_solve(Qcho, l2)\n                lpxn += -0.5 * (ld + numpy.trace(tmp))\n\n                for j in range(len(self.params)):\n                    lpxn_grad[j] -= 0.5 * mlnlg_compute.compute_logprod_derivative(Qcho, Q_grad[j], l2, l2_grad[j])\n\n        return (lpxn, lpxn_grad)\n\n    def eval_logp_y(self, particles, y, t):\n        \"\"\"\n        Evaluate value of log p(y_t|x_t)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - y (array-like): measurement\n         - t (float): time stamp\n\n        Returns: (array-like)\n        \"\"\"\n        N = len(particles)\n        self.t = t\n        (y, C, h, R) = self.get_meas_dynamics(y=y, t=t)\n        self.kf.set_dynamics(C=C, R=R, h_k=h)\n        (zl, Pl) = self.get_states(particles)\n        logpy = numpy.empty(N)\n        for i in range(N):\n            # Calculate l3 according to (19b)\n            l3 = self.calc_l3(y, zl[i], Pl[i])\n            (_tmp, ld) = numpy.linalg.slogdet(self.kf.R)\n            tmp = numpy.linalg.solve(self.kf.R, l3)\n            logpy[i] = -0.5 * (ld + numpy.trace(tmp))\n\n        return logpy\n\n    def eval_logp_y_val_grad(self, particles, y, t):\n        \"\"\"\n        Evaluate value and gradient of log p(y_t|x_t)\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - y (array-like): measurement\n         - t (float): time stamp\n\n        Returns: ((array-like), (array-like))\n        \"\"\"\n        N = len(particles)\n        lparam = len(self.params)\n        (y, C, h, R) = self.get_meas_dynamics(y=y, t=t)\n        (C_grad, h_grad, R_grad) = self.get_meas_dynamics_grad(y=y, t=t)\n        logpy_grad = numpy.zeros(lparam)\n        if (C_grad is None and h_grad is None and R_grad is None):\n            logpy = self.eval_logp_y(particles, y, t)\n        else:\n\n            self.kf.set_dynamics(C=C, R=R, h_k=h)\n            Rcho = scipy.linalg.cho_factor(self.kf.R, check_finite=False)\n            ld = numpy.sum(numpy.log(numpy.diagonal(Rcho[0]))) * 2\n            (zl, Pl) = self.get_states(particles)\n            logpy = 0.0\n\n            if (R_grad is None):\n                R_grad = numpy.zeros((len(self.params), len(y), len(y)))\n\n            for i in range(N):\n                # Calculate l3 according to (19b)\n                (l3, l3_grad) = self.calc_l3_grad(y, zl[i], Pl[i])\n                tmp = scipy.linalg.cho_solve(Rcho, l3)\n                logpy += -0.5 * (ld + numpy.trace(tmp))\n\n                for j in range(len(self.params)):\n                    logpy_grad[j] -= 0.5 * mlnlg_compute.compute_logprod_derivative(\n                        Rcho, R_grad[j], l3, l3_grad[j])\n\n        return (logpy, logpy_grad)\n\n    def get_pred_dynamics_grad(self, u, t):\n        \"\"\"\n        Override this method if (A, f, Q) depends on the parameters\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - u (array-like): input signal\n         - t (float): time stamps\n\n        Returns:\n         (A_grad, f_grad, Q_grad): Element-wise gradients with respect to all\n         the parameters for the system matrices\n        \"\"\"\n        return (None, None, None)\n\n    def get_meas_dynamics_grad(self, y, t):\n        \"\"\"\n        Override this method if (C, h, R) depends on the parameters\n\n        Args:\n         - particles  (array-like): Model specific representation\n           of all particles, with first dimension = N (number of particles)\n         - y (array-like): measurment\n         - t (float): time stamps\n\n        Returns:\n         (C_grad, h_grad, R_grad): Element-wise gradients with respect to all\n         the parameters for the system matrices\n        \"\"\"\n        return (None, None, None)\n\n    def get_initial_grad(self):\n        \"\"\"\n        Default implementation has no dependence on xi, override if needed\n\n        Calculate gradient estimate of initial state for linear state condition on the\n        nonlinear estimate\n\n        Args:\n         - xi0 (array-like): Initial xi states\n\n        Returns:\n         (z,P): z is a list of element-wise gradients for the inital mean values,\n         P is a list of element-wise gradients for the covariance matrices\n        \"\"\"\n        lparam = len(self.params)\n        return (numpy.zeros((lparam, self.kf.lz, 1)),\n                numpy.zeros((lparam, self.kf.lz, self.kf.lz)))\n\n    def calc_l1(self, z, P, z0, P0):\n        \"\"\" internal helper function \"\"\"\n        z0_diff = z - z0\n        l1 = z0_diff.dot(z0_diff.T) + P\n        return l1\n\n    def calc_l1_grad(self, z, P, z0, P0, z0_grad):\n        \"\"\" internal helper function \"\"\"\n        lparams = len(self.params)\n        z0_diff = z - z0\n        l1 = z0_diff.dot(z0_diff.T) + P\n        l1_diff = numpy.zeros((lparams, self.kf.lz, self.kf.lz))\n        if (z0_grad is not None):\n            for j in range(lparams):\n                tmp = -z0_grad[j].dot(z0_diff.T)\n                l1_diff[j] += tmp + tmp.T\n        return (l1, l1_diff)\n\n    def calc_l2(self, zn, Pn, z, P, A, f, M):\n        \"\"\" internal helper function \"\"\"\n        predict_err = zn - f - A.dot(z)\n        AM = A.dot(M)\n        l2 = predict_err.dot(predict_err.T)\n        l2 += Pn + A.dot(P).dot(A.T) - AM.T - AM\n        return (l2, A, M, predict_err)\n\n    def calc_l2_grad(self, zn, Pn, z, P, A, f, M, A_grad, f_grad):\n        \"\"\" internal helper function \"\"\"\n        lparam = len(self.params)\n        predict_err = zn - f - A.dot(z)\n        AM = A.dot(M)\n        l2 = predict_err.dot(predict_err.T)\n        l2 += Pn + A.dot(P).dot(A.T) - AM.T - AM\n        l2_grad = numpy.zeros((lparam, self.kf.lz, self.kf.lz))\n        if (f_grad is not None):\n            for j in range(lparam):\n                tmp = -f_grad[j].dot(predict_err.T)\n                l2_grad[j] += tmp + tmp.T\n        if (A_grad is not None):\n            for j in range(lparam):\n                tmp = -A_grad[j].dot(z).dot(predict_err.T)\n                l2_grad[j] += tmp + tmp.T\n                tmp = A_grad[j].dot(P).dot(A.T)\n                l2_grad[j] += tmp + tmp.T\n                tmp = -A_grad[j].dot(M)\n                l2_grad[j] += tmp + tmp.T\n        return (l2, l2_grad)\n\n    def calc_l3(self, y, z, P):\n        \"\"\" internal helper function \"\"\"\n        meas_diff = self.kf.measurement_diff(y.reshape((-1, 1)),\n                                             z,\n                                             C=self.kf.C,\n                                             h_k=self.kf.h_k)\n        l3 = meas_diff.dot(meas_diff.T)\n        l3 += self.kf.C.dot(P).dot(self.kf.C.T)\n        return l3\n\n    def calc_l3_grad(self, y, z, P, C_grad, h_grad):\n        \"\"\" internal helper function \"\"\"\n        lparam = len(self.params)\n        meas_diff = self.kf.measurement_diff(y.reshape((-1, 1)),\n                                             z,\n                                             C=self.kf.C,\n                                             h_k=self.kf.h_k)\n        l3 = meas_diff.dot(meas_diff.T)\n        l3 += self.kf.C.dot(P).dot(self.kf.C.T)\n        l3_grad = numpy.zeros((lparam, len(y), len(y)))\n        if (h_grad is not None):\n            for j in range(lparam):\n                tmp = -h_grad[j].dot(meas_diff)\n                l3_grad[j] += tmp + tmp.T\n        if (C_grad is not None):\n            for j in range(lparam):\n                tmp = -C_grad[j].dot(z).dot(meas_diff)\n                l3_grad[j] += tmp + tmp.T\n                tmp = C_grad[j].dot(P).dot(self.kf.C)\n                l3_grad[j] += tmp + tmp.T\n        return (l3, l3_grad)\n", "license": "lgpl-3.0"}
{"id": "21e02adbf3ea78ea03fc9daed63352093009afec", "path": "contrib/oe-stylize.py", "repo_name": "JrCs/opendreambox", "content": "#!/usr/bin/env python\n\n\"\"\"\\\nSanitize a bitbake file following the OpenEmbedded style guidelines,\nsee http://openembedded.org/wiki/StyleGuide \n\n(C) 2006 Cyril Romain <cyril.romain@gmail.com>\nMIT license\n\nTODO: \n - add the others OpenEmbedded variables commonly used:\n - parse command arguments and print usage on misuse\n    . prevent giving more than one .bb file in arguments\n - write result to a file\n - backup the original .bb file\n - make a diff and ask confirmation for patching ?\n - do not use startswith only:\n    /!\\ startswith('SOMETHING') is not taken into account due to the previous startswith('S').\n - count rule breaks and displays them in the order frequence\n\"\"\"\n\nimport fileinput\nimport string\nimport re\n\n__author__ = \"Cyril Romain <cyril.romain@gmail.com>\"\n__version__ = \"$Revision: 0.5 $\"\n\n# The standard set of variables often found in .bb files in the preferred order\nOE_vars = [\n    'DESCRIPTION',\n    'AUTHOR',\n    'HOMEPAGE',\n    'SECTION',\n    'PRIORITY',\n    'LICENSE',\n    'DEPENDS',\n    'RDEPENDS',\n    'RRECOMMENDS',\n    'RSUGGESTS',\n    'PROVIDES',\n    'RPROVIDES',\n    'RCONFLICTS',\n    'SRCDATE',\n    'PE',\n    'PV',\n    'PR',\n    'SRC_URI',\n    'S',\n    'GPE_TARBALL_SUFFIX',\n    'inherit',\n    'EXTRA_',\n    'do_fetch',\n    'do_unpack',\n    'do_patch',\n    'do_configure',\n    'do_compile',\n    'do_install',\n    'do_package',\n    'do_stage',\n    'PACKAGE_ARCH',\n    'PACKAGES',\n    'FILES',\n    'WORKDIR',\n    'acpaths',\n    'addhandler',\n    'addtask',\n    'bindir',\n    'export',\n    'headers',\n    'include',\n    'includedir',\n    'python',\n    'qtopiadir',\n    'pkg_preins',\n    'pkg_prerm',\n    'pkg_postins',\n    'pkg_postrm',\n    'require',\n    'sbindir',\n    'basesysconfdir',\n    'sysconfdir',\n    'ALLOW_EMPTY',\n    'ALTERNATIVE_NAME',\n    'ALTERNATIVE_PATH',\n    'ALTERNATIVE_LINK',\n    'ALTERNATIVE_PRIORITY',\n    'ALTNAME',\n    'AMD_DRIVER_LABEL',\n    'AMD_DRIVER_VERSION',\n    'ANGSTROM_EXTRA_INSTALL',\n    'APPDESKTOP',\n    'APPIMAGE',\n    'APPNAME',\n    'APPTYPE',\n    'APPWEB_BUILD',\n    'APPWEB_HOST',\n    'AR',\n    'ARCH',\n    'ARM_INSTRUCTION_SET',\n    'ARM_MUTEX',\n    'ART_CONFIG',\n    'B',\n    'BJAM_OPTS',\n    'BJAM_TOOLS',\n    'BONOBO_HEADERS',\n    'BOOTSCRIPTS',\n    'BROKEN',\n    'BUILD_CPPFLAGS',\n    'CFLAGS',\n    'CCFLAGS',\n    'CMDLINE',\n    'COLLIE_MEMORY_SIZE',\n    'COMPATIBLE_HOST',\n    'COMPATIBLE_MACHINE',\n    'COMPILE_HERMES',\n    'CONFFILES',\n    'CONFLICTS',\n    'CORE_EXTRA_D',\n    'CORE_PACKAGES_D',\n    'CORE_PACKAGES_RD',\n    'CPPFLAGS',\n    'CVSDATE',\n    'CXXFLAGS',\n    'DEBIAN_NOAUTONAME',\n    'DEBUG_APPS',\n    'DEFAULT_PREFERENCE',\n    'DB4_CONFIG',\n    'EXCLUDE_FROM_SHLIBS',\n    'EXCLUDE_FROM_WORLD',\n    'FIXEDSRCDATE',\n    'GLIBC_ADDONS',\n    'GLIBC_EXTRA_OECONF',\n    'GNOME_VFS_HEADERS',\n    'HEADERS',\n    'INHIBIT_DEFAULT_DEPS',\n    'INITSCRIPT_PACKAGES',\n    'INITSCRIPT_NAME',\n    'INITSCRIPT_PARAMS',\n    'PACKAGE_INSTALL',\n    'KERNEL_IMAGETYPE',\n    'KERNEL_IMAGEDEST',\n    'KERNEL_OUTPUT',\n    'KERNEL_RELEASE',\n    'KERNEL_PRIORITY',\n    'KERNEL_SOURCE',\n    'KERNEL_SUFFIX',\n    'KERNEL_VERSION',\n    'K_MAJOR',\n    'K_MICRO',\n    'K_MINOR',\n    'HHV',\n    'KV',\n    'LDFLAGS',\n    'LD',\n    'LD_SO',\n    'LDLIBS',\n    'LEAD_SONAME',\n    'LIBTOOL',\n    'LIBBDB_EXTRA',\n    'LIBV',\n    'MACHINE_ESSENTIAL_EXTRA_RDEPENDS',\n    'MACHINE_ESSENTIAL_EXTRA_RRECOMMENDS',\n    'MACHINE_EXTRA_RDEPENDS',\n    'MACHINE_EXTRA_RRECOMMENDS',\n    'MACHINE_FEATURES',\n    'MACHINE_TASKS',\n    'MACHINE',\n    'MACHTYPE',\n    'MAKE_TARGETS',\n    'MESSAGEUSER',\n    'MESSAGEHOME',\n    'MIRRORS',\n    'MUTEX',\n    'OE_QMAKE_INCDIR_QT',\n    'OE_QMAKE_CXXFLAGS',\n    'ORBIT_IDL_SRC',\n    'PARALLEL_MAKE',\n    'PAKCAGE_ARCH',\n    'PCMCIA_MANAGER',\n    'PKG_BASENAME',\n    'PKG',\n    'QEMU',\n    'QMAKE_PROFILES',\n    'QPEDIR',\n    'QPF_DESCRIPTION',\n    'QPF_PKGPATTERN',\n    'QT_CONFIG_FLAGS',\n    'QT_LIBRARY',\n    'ROOTFS_POSTPROCESS_COMMAND',\n    'RREPLACES',\n    'TARGET_CFLAGS',\n    'TARGET_CPPFLAGS',\n    'TARGET_LDFLAGS',\n    'UBOOT_MACHINE',\n    'UCLIBC_BASE',\n    'UCLIBC_PATCHES',\n    'VIRTUAL_NAME',\n    'XORG_PN',\n    'XSERVER',\n    'others'\n]\n\nvarRegexp = r'^([a-zA-Z_0-9${}-]*)([ \\t]*)([+.:]?=[+.]?)([ \\t]*)([^\\t]+)'\nroutineRegexp = r'^([a-zA-Z0-9_ ${}-]+?)\\('\n\n# Variables seen in the processed .bb\nseen_vars = {}\nfor v in OE_vars: \n    seen_vars[v] = []\n\n# _Format guideline #0_: \n#   No spaces are allowed at the beginning of lines that define a variable or \n#   a do_ routine\ndef respect_rule0(line): \n    return line.lstrip()==line\ndef conformTo_rule0(line): \n    return line.lstrip()\n\n# _Format guideline #1_: \n#   No spaces are allowed behind the line continuation symbol '\\'\ndef respect_rule1(line):\n    if line.rstrip().endswith('\\\\'):\n        return line.endswith('\\\\')\n    else: \n        return True\ndef conformTo_rule1(line):\n    return line.rstrip()\n\n# _Format guideline #2_: \n#   Tabs should not be used (use spaces instead).\ndef respect_rule2(line):\n    return line.count('\\t')==0\ndef conformTo_rule2(line):\n    return line.expandtabs()\n\n# _Format guideline #3_:\n#   Comments inside bb files are allowed using the '#' character at the \n#   beginning of a line.\ndef respect_rule3(line):\n    if line.lstrip().startswith('#'):\n        return line.startswith('#')\n    else: \n        return True\ndef conformTo_rule3(line):\n    return line.lstrip()\n\n# _Format guideline #4_:\n#   Use quotes on the right hand side of assignments FOO = \"BAR\"\ndef respect_rule4(line):\n    r = re.search(varRegexp, line)\n    if r is not None:\n        r2 = re.search(r'(\"?)([^\"\\\\]*)([\"\\\\]?)', r.group(5))\n        # do not test for None it because always match\n        return r2.group(1)=='\"' and r2.group(3)!=''\n    return False\ndef conformTo_rule4(line):\n    r = re.search(varRegexp, line)\n    return ''.join([r.group(1), ' ', r.group(3), ' \"', r.group(5), r.group(5).endswith('\"') and '' or '\"'])\n\n# _Format guideline #5_:\n#   The correct spacing for a variable is FOO = \"BAR\".\ndef respect_rule5(line):\n    r = re.search(varRegexp, line)\n    return r is not None and r.group(2)==\" \" and r.group(4)==\" \"\ndef conformTo_rule5(line):\n    r = re.search(varRegexp, line)\n    return ''.join([r.group(1), ' ', r.group(3), ' ', r.group(5)])\n\n# _Format guideline #6_:\n#   Don't use spaces or tabs on empty lines\ndef respect_rule6(line):\n    return not line.isspace() or line==\"\\n\"\ndef conformTo_rule6(line):\n    return \"\"\n\n# _Format guideline #7_:\n#   Indentation of multiline variables such as SRC_URI is desireable.\ndef respect_rule7(line):\n    return True\ndef conformTo_rule7(line):\n    return line\n\nrules = (\n    (respect_rule0, conformTo_rule0, \"No spaces are allowed at the beginning of lines that define a variable or a do_ routine\"),\n    (respect_rule1, conformTo_rule1, \"No spaces are allowed behind the line continuation symbol '\\\\'\"),\n    (respect_rule2, conformTo_rule2, \"Tabs should not be used (use spaces instead)\"),\n    (respect_rule3, conformTo_rule3, \"Comments inside bb files are allowed using the '#' character at the beginning of a line\"),\n    (respect_rule4, conformTo_rule4, \"Use quotes on the right hand side of assignments FOO = \\\"BAR\\\"\"),\n    (respect_rule5, conformTo_rule5, \"The correct spacing for a variable is FOO = \\\"BAR\\\"\"),\n    (respect_rule6, conformTo_rule6, \"Don't use spaces or tabs on empty lines\"),\n    (respect_rule7, conformTo_rule7, \"Indentation of multiline variables such as SRC_URI is desireable\"),\n)\n\n# Function to check that a line respects a rule. If not, it tries to conform\n# the line to the rule. Reminder or Disgression message are dump accordingly.\ndef follow_rule(i, line):\n    oldline = line\n    # if the line does not respect the rule\n    if not rules[i][0](line):\n        # try to conform it to the rule\n        line = rules[i][1](line)\n        # if the line still does not respect the rule\n        if not rules[i][0](line):\n            # this is a rule disgression\n            print \"## Disgression: \", rules[i][2], \" in:\", oldline\n        else:\n            # just remind user about his/her errors\n            print \"## Reminder: \", rules[i][2], \" in :\", oldline\n    return line\n\n\nif __name__ == \"__main__\":\n\n    # -- retrieves the lines of the .bb file --\n    lines = []\n    for line in fileinput.input():\n        # use 'if True' to warn user about all the rule he/she breaks\n        # use 'if False' to conform to rules{2,1,6} without warnings\n        if True:\n            lines.append(line)\n        else:\n            # expandtabs on each line so that rule2 is always respected \n            # rstrip each line so that rule1 is always respected \n            line = line.expandtabs().rstrip()\n            # ignore empty lines (or line filled with spaces or tabs only)\n            # so that rule6 is always respected\n            if line is not '':\n                lines.append(line)\n\n    # -- parse the file --\n    var = \"\"\n    in_routine = False\n    commentBloc = []\n    olines = []\n    for line in lines: \n        originalLine = line\n        # rstrip line to remove line breaks characters\n        line = line.rstrip()\n        line = follow_rule(2, line)\n        line = follow_rule(1, line)\n        line = follow_rule(6, line)\n\n        # ignore empty lines\n        if line.isspace() or line is '':\n            # flush comments into the olines\n            for c in commentBloc: olines.append(c)\n            commentBloc = []\n            continue\n\n        if line.startswith('}'): \n            in_routine=False\n        keep = line.endswith('\\\\') or in_routine\n\n        # handles commented lines\n        if line.lstrip().startswith('#'):\n            # check and follow rule3 if not in a variables or routines\n            if not in_routine:\n                line = follow_rule(3, line)\n            commentBloc.append(line)\n            continue\n\n        if seen_vars.has_key(var):\n            for c in commentBloc: seen_vars[var].append(c)\n            commentBloc = []\n            seen_vars[var].append(line)\n        else:\n            for k in OE_vars:\n                if line.startswith(k):\n                    var = k\n                    break\n            if re.match(routineRegexp, line) is not None: \n                in_routine=True\n                line = follow_rule(0, line)\n            elif re.match(varRegexp, line) is not None:\n                line = follow_rule(0, line)\n                line = follow_rule(4, line)\n                line = follow_rule(5, line)\n            if var == \"\":\n                if not in_routine:\n                    print \"## Warning: unknown variable/routine \\\"%s\\\"\" % originalLine\n                var = 'others'\n            for c in commentBloc: seen_vars[var].append(c)\n            commentBloc = []\n            seen_vars[var].append(line)\n        if not keep and not in_routine: var = \"\"\n\n    # -- dump the sanitized .bb file --\n    addEmptyLine = False\n    # write comments that are not related to variables nor routines\n    for l in commentBloc: olines.append(l)\n    # write variables and routines\n    previourVarPrefix = \"unknown\"\n    for k in OE_vars:\n        if k=='SRC_URI': addEmptyLine = True\n        if seen_vars[k] != []: \n            if addEmptyLine and not k.startswith(previourVarPrefix):\n                olines.append(\"\")\n            for l in seen_vars[k]: \n                olines.append(l)\n            previourVarPrefix = k.split('_')[0]=='' and \"unknown\" or k.split('_')[0]\n    for line in olines: print line\n\n", "license": "mit"}
{"id": "13b5f5954f54268bd0c1a6b1988583a47bb2f0de", "path": "nova/api/openstack/compute/deferred_delete.py", "repo_name": "edulramirez/nova", "content": "# Copyright 2011 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"The deferred instance delete extension.\"\"\"\n\nimport webob\n\nfrom nova.api.openstack import common\nfrom nova.api.openstack import extensions\nfrom nova.api.openstack import wsgi\nfrom nova import compute\nfrom nova import exception\n\nALIAS = 'os-deferred-delete'\nauthorize = extensions.os_compute_authorizer(ALIAS)\n\n\nclass DeferredDeleteController(wsgi.Controller):\n    def __init__(self, *args, **kwargs):\n        super(DeferredDeleteController, self).__init__(*args, **kwargs)\n        self.compute_api = compute.API(skip_policy_check=True)\n\n    @wsgi.response(202)\n    @extensions.expected_errors((404, 409, 403))\n    @wsgi.action('restore')\n    def _restore(self, req, id, body):\n        \"\"\"Restore a previously deleted instance.\"\"\"\n        context = req.environ[\"nova.context\"]\n        authorize(context)\n        instance = common.get_instance(self.compute_api, context, id)\n        try:\n            self.compute_api.restore(context, instance)\n        except exception.InstanceUnknownCell as error:\n            raise webob.exc.HTTPNotFound(explanation=error.format_message())\n        except exception.QuotaError as error:\n            raise webob.exc.HTTPForbidden(explanation=error.format_message())\n        except exception.InstanceInvalidState as state_error:\n            common.raise_http_conflict_for_instance_invalid_state(state_error,\n                    'restore', id)\n\n    @wsgi.response(202)\n    @extensions.expected_errors((404, 409))\n    @wsgi.action('forceDelete')\n    def _force_delete(self, req, id, body):\n        \"\"\"Force delete of instance before deferred cleanup.\"\"\"\n        context = req.environ[\"nova.context\"]\n        authorize(context)\n        instance = common.get_instance(self.compute_api, context, id)\n        try:\n            self.compute_api.force_delete(context, instance)\n        except exception.InstanceIsLocked as e:\n            raise webob.exc.HTTPConflict(explanation=e.format_message())\n\n\nclass DeferredDelete(extensions.V21APIExtensionBase):\n    \"\"\"Instance deferred delete.\"\"\"\n\n    name = \"DeferredDelete\"\n    alias = \"os-deferred-delete\"\n    version = 1\n\n    def get_controller_extensions(self):\n        controller = DeferredDeleteController()\n        extension = extensions.ControllerExtension(self, 'servers', controller)\n        return [extension]\n\n    def get_resources(self):\n        return []\n", "license": "apache-2.0"}
{"id": "ca49ed2e5eae0bca8d0813d385407e3ced5aefd0", "path": "examples/asyncio/wamp/rpc/arguments/backend.py", "repo_name": "RyanHope/AutobahnPython", "content": "###############################################################################\n#\n# The MIT License (MIT)\n#\n# Copyright (c) Tavendo GmbH\n#\n# Permission is hereby granted, free of charge, to any person obtaining a copy\n# of this software and associated documentation files (the \"Software\"), to deal\n# in the Software without restriction, including without limitation the rights\n# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n# copies of the Software, and to permit persons to whom the Software is\n# furnished to do so, subject to the following conditions:\n#\n# The above copyright notice and this permission notice shall be included in\n# all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n# THE SOFTWARE.\n#\n###############################################################################\n\ntry:\n    import asyncio\nexcept ImportError:\n    # Trollius >= 0.3 was renamed\n    import trollius as asyncio\n\nfrom os import environ\nfrom autobahn.asyncio.wamp import ApplicationSession, ApplicationRunner\n\n\nclass Component(ApplicationSession):\n    \"\"\"\n    An application component providing procedures with different kinds\n    of arguments.\n    \"\"\"\n\n    @asyncio.coroutine\n    def onJoin(self, details):\n\n        def ping():\n            return\n\n        def add2(a, b):\n            return a + b\n\n        def stars(nick=\"somebody\", stars=0):\n            return u\"{} starred {}x\".format(nick, stars)\n\n        # noinspection PyUnusedLocal\n        def orders(product, limit=5):\n            return [u\"Product {}\".format(i) for i in range(50)][:limit]\n\n        def arglen(*args, **kwargs):\n            return [len(args), len(kwargs)]\n\n        yield from self.register(ping, u'com.arguments.ping')\n        yield from self.register(add2, u'com.arguments.add2')\n        yield from self.register(stars, u'com.arguments.stars')\n        yield from self.register(orders, u'com.arguments.orders')\n        yield from self.register(arglen, u'com.arguments.arglen')\n        print(\"Registered methods; ready for frontend.\")\n\n\nif __name__ == '__main__':\n    runner = ApplicationRunner(\n        environ.get(\"AUTOBAHN_DEMO_ROUTER\", u\"ws://127.0.0.1:8080/ws\"),\n        u\"crossbardemo\",\n\n        debug=False,  # optional; log even more details\n    )\n    runner.run(Component)\n", "license": "mit"}
{"id": "0a8c9abd9bf10fd89830000fe940cafaedc1e285", "path": "evolutionary/hpcjobs/Minesweeper13-3/context.py", "repo_name": "jakejhansen/minesweeper_solver", "content": "# -*- coding: utf-8 -*-\n\nimport os\nimport sys\nsys.path.insert(0, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nimport core\n", "license": "mit"}
{"id": "fea4c59aed878d07e31a215d3c0ada4a1a48b728", "path": "lemur/tests/factories.py", "repo_name": "Netflix/lemur", "content": "from datetime import date\n\nfrom factory import Sequence, post_generation, SubFactory\nfrom factory.alchemy import SQLAlchemyModelFactory\nfrom factory.fuzzy import FuzzyChoice, FuzzyText, FuzzyDate, FuzzyInteger\n\n\nfrom lemur.database import db\nfrom lemur.authorities.models import Authority\nfrom lemur.certificates.models import Certificate\nfrom lemur.destinations.models import Destination\nfrom lemur.sources.models import Source\nfrom lemur.notifications.models import Notification\nfrom lemur.pending_certificates.models import PendingCertificate\nfrom lemur.users.models import User\nfrom lemur.roles.models import Role\nfrom lemur.endpoints.models import Policy, Endpoint\nfrom lemur.policies.models import RotationPolicy\nfrom lemur.api_keys.models import ApiKey\n\nfrom .vectors import (\n    SAN_CERT_STR,\n    SAN_CERT_KEY,\n    CSR_STR,\n    INTERMEDIATE_CERT_STR,\n    ROOTCA_CERT_STR,\n    INTERMEDIATE_KEY,\n    WILDCARD_CERT_KEY,\n    INVALID_CERT_STR,\n)\n\n\nclass BaseFactory(SQLAlchemyModelFactory):\n    \"\"\"Base factory.\"\"\"\n\n    class Meta:\n        \"\"\"Factory configuration.\"\"\"\n\n        abstract = True\n        sqlalchemy_session = db.session\n\n\nclass RotationPolicyFactory(BaseFactory):\n    \"\"\"Rotation Factory.\"\"\"\n\n    name = Sequence(lambda n: \"policy{0}\".format(n))\n    days = 30\n\n    class Meta:\n        \"\"\"Factory configuration.\"\"\"\n\n        model = RotationPolicy\n\n\nclass CertificateFactory(BaseFactory):\n    \"\"\"Certificate factory.\"\"\"\n\n    name = Sequence(lambda n: \"certificate{0}\".format(n))\n    chain = INTERMEDIATE_CERT_STR\n    body = SAN_CERT_STR\n    private_key = SAN_CERT_KEY\n    owner = \"joe@example.com\"\n    status = FuzzyChoice([\"valid\", \"revoked\", \"unknown\"])\n    deleted = False\n    description = FuzzyText(length=128)\n    active = True\n    date_created = FuzzyDate(date(2016, 1, 1), date(2020, 1, 1))\n    rotation_policy = SubFactory(RotationPolicyFactory)\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Certificate\n\n    @post_generation\n    def user(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            self.user_id = extracted.id\n\n    @post_generation\n    def authority(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            self.authority_id = extracted.id\n\n    @post_generation\n    def notifications(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for notification in extracted:\n                self.notifications.append(notification)\n\n    @post_generation\n    def destinations(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for destination in extracted:\n                self.destintations.append(destination)\n\n    @post_generation\n    def replaces(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for replace in extracted:\n                self.replaces.append(replace)\n\n    @post_generation\n    def sources(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for source in extracted:\n                self.sources.append(source)\n\n    @post_generation\n    def domains(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for domain in extracted:\n                self.domains.append(domain)\n\n    @post_generation\n    def roles(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for domain in extracted:\n                self.roles.append(domain)\n\n\nclass CACertificateFactory(CertificateFactory):\n    chain = ROOTCA_CERT_STR\n    body = INTERMEDIATE_CERT_STR\n    private_key = INTERMEDIATE_KEY\n\n\nclass InvalidCertificateFactory(CertificateFactory):\n    body = INVALID_CERT_STR\n    private_key = \"\"\n    chain = \"\"\n\n\nclass AuthorityFactory(BaseFactory):\n    \"\"\"Authority factory.\"\"\"\n\n    name = Sequence(lambda n: \"authority{0}\".format(n))\n    owner = \"joe@example.com\"\n    plugin = {\"slug\": \"test-issuer\"}\n    description = FuzzyText(length=128)\n    authority_certificate = SubFactory(CACertificateFactory)\n\n    class Meta:\n        \"\"\"Factory configuration.\"\"\"\n\n        model = Authority\n\n    @post_generation\n    def roles(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for role in extracted:\n                self.roles.append(role)\n\n\nclass AsyncAuthorityFactory(AuthorityFactory):\n    \"\"\"Async Authority factory.\"\"\"\n\n    name = Sequence(lambda n: \"authority{0}\".format(n))\n    owner = \"joe@example.com\"\n    plugin = {\"slug\": \"test-issuer-async\"}\n    description = FuzzyText(length=128)\n    authority_certificate = SubFactory(CertificateFactory)\n\n\nclass CryptoAuthorityFactory(AuthorityFactory):\n    \"\"\"Authority factory based on 'cryptography' plugin.\"\"\"\n\n    plugin = {\"slug\": \"cryptography-issuer\"}\n\n\nclass DestinationFactory(BaseFactory):\n    \"\"\"Destination factory.\"\"\"\n\n    plugin_name = \"test-destination\"\n    label = Sequence(lambda n: \"destination{0}\".format(n))\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Destination\n\n\nclass SourceFactory(BaseFactory):\n    \"\"\"Source factory.\"\"\"\n\n    plugin_name = \"test-source\"\n    label = Sequence(lambda n: \"source{0}\".format(n))\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Source\n\n\nclass NotificationFactory(BaseFactory):\n    \"\"\"Notification factory.\"\"\"\n\n    plugin_name = \"test-notification\"\n    label = Sequence(lambda n: \"notification{0}\".format(n))\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Notification\n\n\nclass RoleFactory(BaseFactory):\n    \"\"\"Role factory.\"\"\"\n\n    name = Sequence(lambda n: \"role{0}\".format(n))\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Role\n\n    @post_generation\n    def users(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for user in extracted:\n                self.users.append(user)\n\n\nclass UserFactory(BaseFactory):\n    \"\"\"User Factory.\"\"\"\n\n    username = Sequence(lambda n: \"user{0}\".format(n))\n    email = Sequence(lambda n: \"user{0}@example.com\".format(n))\n    active = True\n    password = FuzzyText(length=24)\n    certificates = []\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = User\n\n    @post_generation\n    def roles(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for role in extracted:\n                self.roles.append(role)\n\n    @post_generation\n    def certificates(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for cert in extracted:\n                self.certificates.append(cert)\n\n    @post_generation\n    def authorities(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for authority in extracted:\n                self.authorities.append(authority)\n\n\nclass PolicyFactory(BaseFactory):\n    \"\"\"Policy Factory.\"\"\"\n\n    name = Sequence(lambda n: \"endpoint{0}\".format(n))\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Policy\n\n\nclass EndpointFactory(BaseFactory):\n    \"\"\"Endpoint Factory.\"\"\"\n\n    owner = \"joe@example.com\"\n    name = Sequence(lambda n: \"endpoint{0}\".format(n))\n    type = FuzzyChoice([\"elb\"])\n    active = True\n    port = FuzzyInteger(0, high=65535)\n    dnsname = \"endpoint.example.com\"\n    policy = SubFactory(PolicyFactory)\n    certificate = SubFactory(CertificateFactory)\n    source = SubFactory(SourceFactory)\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = Endpoint\n\n\nclass ApiKeyFactory(BaseFactory):\n    \"\"\"Api Key Factory.\"\"\"\n\n    name = Sequence(lambda n: \"api_key_{0}\".format(n))\n    revoked = False\n    ttl = -1\n    issued_at = 1\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = ApiKey\n\n    @post_generation\n    def user(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            self.userId = extracted.id\n\n\nclass PendingCertificateFactory(BaseFactory):\n    \"\"\"PendingCertificate factory.\"\"\"\n\n    name = Sequence(lambda n: \"pending_certificate{0}\".format(n))\n    external_id = 12345\n    csr = CSR_STR\n    chain = INTERMEDIATE_CERT_STR\n    private_key = WILDCARD_CERT_KEY\n    owner = \"joe@example.com\"\n    status = FuzzyChoice([\"valid\", \"revoked\", \"unknown\"])\n    deleted = False\n    description = FuzzyText(length=128)\n    date_created = FuzzyDate(date(2016, 1, 1), date(2020, 1, 1))\n    number_attempts = 0\n    rename = False\n\n    class Meta:\n        \"\"\"Factory Configuration.\"\"\"\n\n        model = PendingCertificate\n\n    @post_generation\n    def user(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            self.user_id = extracted.id\n\n    @post_generation\n    def authority(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            self.authority_id = extracted.id\n\n    @post_generation\n    def notifications(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for notification in extracted:\n                self.notifications.append(notification)\n\n    @post_generation\n    def destinations(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for destination in extracted:\n                self.destintations.append(destination)\n\n    @post_generation\n    def replaces(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for replace in extracted:\n                self.replaces.append(replace)\n\n    @post_generation\n    def sources(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for source in extracted:\n                self.sources.append(source)\n\n    @post_generation\n    def domains(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for domain in extracted:\n                self.domains.append(domain)\n\n    @post_generation\n    def roles(self, create, extracted, **kwargs):\n        if not create:\n            return\n\n        if extracted:\n            for domain in extracted:\n                self.roles.append(domain)\n", "license": "apache-2.0"}
{"id": "8e4e2d3e5b190caac587ecb0dcc5d7a226caa4ec", "path": "tests/components/no_ip/test_init.py", "repo_name": "balloob/home-assistant", "content": "\"\"\"Test the NO-IP component.\"\"\"\nimport asyncio\nfrom datetime import timedelta\n\nimport pytest\n\nfrom homeassistant.setup import async_setup_component\nfrom homeassistant.components import no_ip\nfrom homeassistant.util.dt import utcnow\n\nfrom tests.common import async_fire_time_changed\n\nDOMAIN = 'test.example.com'\n\nPASSWORD = 'xyz789'\n\nUPDATE_URL = no_ip.UPDATE_URL\n\nUSERNAME = 'abc@123.com'\n\n\n@pytest.fixture\ndef setup_no_ip(hass, aioclient_mock):\n    \"\"\"Fixture that sets up NO-IP.\"\"\"\n    aioclient_mock.get(\n        UPDATE_URL, params={'hostname': DOMAIN}, text='good 0.0.0.0')\n\n    hass.loop.run_until_complete(async_setup_component(hass, no_ip.DOMAIN, {\n            no_ip.DOMAIN: {\n                'domain': DOMAIN,\n                'username': USERNAME,\n                'password': PASSWORD,\n            }\n        }))\n\n\n@asyncio.coroutine\ndef test_setup(hass, aioclient_mock):\n    \"\"\"Test setup works if update passes.\"\"\"\n    aioclient_mock.get(\n        UPDATE_URL, params={'hostname': DOMAIN}, text='nochg 0.0.0.0')\n\n    result = yield from async_setup_component(hass, no_ip.DOMAIN, {\n        no_ip.DOMAIN: {\n            'domain': DOMAIN,\n            'username': USERNAME,\n            'password': PASSWORD,\n        }\n    })\n    assert result\n    assert aioclient_mock.call_count == 1\n\n    async_fire_time_changed(hass, utcnow() + timedelta(minutes=5))\n    yield from hass.async_block_till_done()\n    assert aioclient_mock.call_count == 2\n\n\n@asyncio.coroutine\ndef test_setup_fails_if_update_fails(hass, aioclient_mock):\n    \"\"\"Test setup fails if first update fails.\"\"\"\n    aioclient_mock.get(UPDATE_URL, params={'hostname': DOMAIN}, text='nohost')\n\n    result = yield from async_setup_component(hass, no_ip.DOMAIN, {\n        no_ip.DOMAIN: {\n            'domain': DOMAIN,\n            'username': USERNAME,\n            'password': PASSWORD,\n        }\n    })\n    assert not result\n    assert aioclient_mock.call_count == 1\n\n\n@asyncio.coroutine\ndef test_setup_fails_if_wrong_auth(hass, aioclient_mock):\n    \"\"\"Test setup fails if first update fails through wrong authentication.\"\"\"\n    aioclient_mock.get(UPDATE_URL, params={'hostname': DOMAIN}, text='badauth')\n\n    result = yield from async_setup_component(hass, no_ip.DOMAIN, {\n        no_ip.DOMAIN: {\n            'domain': DOMAIN,\n            'username': USERNAME,\n            'password': PASSWORD,\n        }\n    })\n    assert not result\n    assert aioclient_mock.call_count == 1\n", "license": "apache-2.0"}
{"id": "c4200a5e9925fabc8ed98b6cc2cc4db0b30c5e96", "path": "account_security_modifications/__init__.py", "repo_name": "adhoc-dev/odoo-addons", "content": "# -*- encoding: utf-8 -*-\n\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n", "license": "agpl-3.0"}
{"id": "4e5fa4aa1286a30149bdb83907e24d7a05913278", "path": "lwapp/lwf/apps/__init__.py", "repo_name": "Aladdin-Unimi/Learning-Week-2012-Software", "content": "# Copyright (C) 2012 Massimo Santini <massimo.santini@unimi.it>\n# \n# This file is part of Learning-Week-2012-Software.\n# \n# Learning-Week-2012-Software is free software: you can redistribute it and/or\n# modify it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or (at your\n# option) any later version.\n# \n# Learning-Week-2012-Software is distributed in the hope that it will be\n# useful, but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.\tSee the GNU General\n# Public License for more details.\n# \n# You should have received a copy of the GNU General Public License along with\n# Learning-Week-2012-Software If not, see <http://www.gnu.org/licenses/>.\n\nfrom .usr import usr\nfrom .img import img\nfrom .gby import gby\nfrom .xml import xml\n\nAPPLICATIONS = {\n\t'usr': usr,\n\t'img': img,\n\t'gby': gby,\n\t'xml': xml,\n}", "license": "gpl-3.0"}
{"id": "7094a0a2723c7188021d823bd3c4051b87243cd5", "path": "tests/src/python/test_authsettingswidget.py", "repo_name": "blazek/QGIS", "content": "# -*- coding: utf-8 -*-\n\"\"\"\nTests for authentication widget\n\nFrom build dir, run from test directory:\nLC_ALL=en_US.UTF-8 ctest -R PyQgsAuthSettingsWidget -V\n\n.. note:: This program is free software; you can redistribute it and/or modify\nit under the terms of the GNU General Public License as published by\nthe Free Software Foundation; either version 2 of the License, or\n(at your option) any later version.\n\"\"\"\nimport os\nimport re\nimport string\nimport sys\nfrom shutil import rmtree\nimport tempfile\nimport random\n\nfrom qgis.core import QgsAuthMethodConfig, QgsNetworkAccessManager, QgsSettings, QgsApplication\nfrom qgis.gui import QgsAuthSettingsWidget\nfrom qgis.testing import start_app, unittest\n\nfrom utilities import unitTestDataPath\n\n__author__ = 'Alessandro Pasotti'\n__date__ = '27/09/2017'\n__copyright__ = 'Copyright 2017, The QGIS Project'\n\n\nQGIS_AUTH_DB_DIR_PATH = tempfile.mkdtemp()\n\nos.environ['QGIS_AUTH_DB_DIR_PATH'] = QGIS_AUTH_DB_DIR_PATH\n\nqgis_app = start_app()\n\n\nclass TestAuthenticationWidget(unittest.TestCase):\n\n    @classmethod\n    def setUpClass(cls):\n        \"\"\"Run before all tests:\n        Creates an auth configuration\"\"\"\n        # Enable auth\n        # os.environ['QGIS_AUTH_PASSWORD_FILE'] = QGIS_AUTH_PASSWORD_FILE\n        authm = QgsApplication.authManager()\n        assert (authm.setMasterPassword('masterpassword', True))\n        cls.auth_config = QgsAuthMethodConfig('Basic')\n        cls.auth_config.setName('test_auth_config')\n        cls.username = ''.join(random.choice(string.ascii_uppercase + string.digits) for _ in range(6))\n        cls.password = cls.username[::-1]  # reversed\n        cls.auth_config.setConfig('username', cls.username)\n        cls.auth_config.setConfig('password', cls.password)\n        assert (authm.storeAuthenticationConfig(cls.auth_config)[0])\n\n    @classmethod\n    def tearDownClass(cls):\n        \"\"\"Run after all tests\"\"\"\n        rmtree(QGIS_AUTH_DB_DIR_PATH)\n\n    def setUp(self):\n        \"\"\"Run before each test.\"\"\"\n        pass\n\n    def tearDown(self):\n        \"\"\"Run after each test.\"\"\"\n        pass\n\n    def testWidgetNoArgs(self):\n        \"\"\"\n        Test the widget with no args\n        \"\"\"\n        w = QgsAuthSettingsWidget()\n        self.assertEqual(w.username(), '')\n        self.assertEqual(w.password(), '')\n        self.assertEqual(w.configId(), '')\n        self.assertTrue(w.configurationTabIsSelected())\n        self.assertFalse(w.btnConvertToEncryptedIsEnabled())\n\n    def testWidgetConfigId(self):\n        \"\"\"\n        Test the widget with configId\n        \"\"\"\n        w = QgsAuthSettingsWidget(None, self.auth_config.id())\n        self.assertEqual(w.username(), '')\n        self.assertEqual(w.password(), '')\n        self.assertEqual(w.configId(), self.auth_config.id())\n        self.assertTrue(w.configurationTabIsSelected())\n        self.assertFalse(w.btnConvertToEncryptedIsEnabled())\n\n    def testWidgetUsername(self):\n        \"\"\"\n        Test the widget with username only\n        \"\"\"\n        w = QgsAuthSettingsWidget(None, None, 'username')\n        self.assertEqual(w.username(), 'username')\n        self.assertEqual(w.password(), '')\n        self.assertEqual(w.configId(), '')\n        self.assertFalse(w.configurationTabIsSelected())\n\n    def testWidgetPassword(self):\n        \"\"\"\n        Test the widget with password only\n        \"\"\"\n        w = QgsAuthSettingsWidget(None, None, None, 'password')\n        self.assertEqual(w.username(), '')\n        self.assertEqual(w.password(), 'password')\n        self.assertEqual(w.configId(), '')\n        self.assertFalse(w.configurationTabIsSelected())\n\n    def testWidgetUsernameAndPassword(self):\n        \"\"\"\n        Test the widget with username and password\n        \"\"\"\n        w = QgsAuthSettingsWidget(None, None, 'username', 'password')\n        self.assertEqual(w.username(), 'username')\n        self.assertEqual(w.password(), 'password')\n        self.assertEqual(w.configId(), '')\n        self.assertFalse(w.configurationTabIsSelected())\n        self.assertTrue(w.btnConvertToEncryptedIsEnabled())\n\n    def testConvertToEncrypted(self):\n        \"\"\"\n        Test the widget to encrypted conversion\n        \"\"\"\n        w = QgsAuthSettingsWidget(None, None, 'username', 'password')\n        self.assertEqual(w.username(), 'username')\n        self.assertEqual(w.password(), 'password')\n        self.assertEqual(w.configId(), '')\n        self.assertFalse(w.configurationTabIsSelected())\n        self.assertTrue(w.btnConvertToEncryptedIsEnabled())\n        self.assertTrue(w.convertToEncrypted())\n        self.assertNotEqual(w.configId(), '')\n        self.assertEqual(w.username(), '')\n        self.assertEqual(w.password(), '')\n        self.assertTrue(w.configurationTabIsSelected())\n        self.assertFalse(w.btnConvertToEncryptedIsEnabled())\n\n    def test_setters(self):\n        \"\"\"\n        Test setters\n        \"\"\"\n        w = QgsAuthSettingsWidget()\n        w.setUsername('username')\n        self.assertFalse(w.configurationTabIsSelected())\n        self.assertEqual(w.username(), 'username')\n\n        w = QgsAuthSettingsWidget()\n        w.setPassword('password')\n        self.assertEqual(w.password(), 'password')\n        self.assertFalse(w.configurationTabIsSelected())\n\n        w = QgsAuthSettingsWidget()\n        w.setConfigId(self.auth_config.id())\n        self.assertEqual(w.configId(), self.auth_config.id())\n        self.assertTrue(w.configurationTabIsSelected())\n\n        w = QgsAuthSettingsWidget()\n        w.setUsername('username')\n        w.setPassword('password')\n        w.setConfigId(self.auth_config.id())\n        self.assertEqual(w.configId(), self.auth_config.id())\n        self.assertTrue(w.configurationTabIsSelected())\n\n        w = QgsAuthSettingsWidget()\n        w.setDataprovider('db2')\n        self.assertEqual(w.dataprovider(), 'db2')\n\n    def test_storeCheckBoxes(self):\n        \"\"\"\n        Test store cb setters and getters\n        \"\"\"\n        w = QgsAuthSettingsWidget()\n        self.assertFalse(w.storePasswordIsChecked())\n        self.assertFalse(w.storeUsernameIsChecked())\n\n        w = QgsAuthSettingsWidget()\n        w.setStorePasswordChecked(True)\n        self.assertTrue(w.storePasswordIsChecked())\n        self.assertFalse(w.storeUsernameIsChecked())\n\n        w = QgsAuthSettingsWidget()\n        w.setStoreUsernameChecked(True)\n        self.assertFalse(w.storePasswordIsChecked())\n        self.assertTrue(w.storeUsernameIsChecked())\n\n        w = QgsAuthSettingsWidget()\n        w.setStoreUsernameChecked(True)\n        w.setStorePasswordChecked(True)\n        self.assertTrue(w.storePasswordIsChecked())\n        self.assertTrue(w.storeUsernameIsChecked())\n\n\nif __name__ == '__main__':\n    unittest.main()\n", "license": "gpl-2.0"}
{"id": "1564afabe22ac859f20a62f5765f17d71611957f", "path": "tests/artificial/transf_Integration/trend_PolyTrend/cycle_30/ar_/test_artificial_32_Integration_PolyTrend_30__100.py", "repo_name": "antoinecarme/pyaf", "content": "import pyaf.Bench.TS_datasets as tsds\nimport tests.artificial.process_artificial_dataset as art\n\n\n\n\nart.process_dataset(N = 32 , FREQ = 'D', seed = 0, trendtype = \"PolyTrend\", cycle_length = 30, transform = \"Integration\", sigma = 0.0, exog_count = 100, ar_order = 0);", "license": "bsd-3-clause"}
{"id": "716f22840414e604ab38661f91eb07f094309833", "path": "Python-3.5.0-Amiga/Lib/xml/sax/xmlreader.py", "repo_name": "Belxjander/Kirito", "content": "\"\"\"An XML Reader is the SAX 2 name for an XML parser. XML Parsers\nshould be based on this code. \"\"\"\n\nfrom . import handler\n\nfrom ._exceptions import SAXNotSupportedException, SAXNotRecognizedException\n\n\n# ===== XMLREADER =====\n\nclass XMLReader:\n    \"\"\"Interface for reading an XML document using callbacks.\n\n    XMLReader is the interface that an XML parser's SAX2 driver must\n    implement. This interface allows an application to set and query\n    features and properties in the parser, to register event handlers\n    for document processing, and to initiate a document parse.\n\n    All SAX interfaces are assumed to be synchronous: the parse\n    methods must not return until parsing is complete, and readers\n    must wait for an event-handler callback to return before reporting\n    the next event.\"\"\"\n\n    def __init__(self):\n        self._cont_handler = handler.ContentHandler()\n        self._dtd_handler = handler.DTDHandler()\n        self._ent_handler = handler.EntityResolver()\n        self._err_handler = handler.ErrorHandler()\n\n    def parse(self, source):\n        \"Parse an XML document from a system identifier or an InputSource.\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def getContentHandler(self):\n        \"Returns the current ContentHandler.\"\n        return self._cont_handler\n\n    def setContentHandler(self, handler):\n        \"Registers a new object to receive document content events.\"\n        self._cont_handler = handler\n\n    def getDTDHandler(self):\n        \"Returns the current DTD handler.\"\n        return self._dtd_handler\n\n    def setDTDHandler(self, handler):\n        \"Register an object to receive basic DTD-related events.\"\n        self._dtd_handler = handler\n\n    def getEntityResolver(self):\n        \"Returns the current EntityResolver.\"\n        return self._ent_handler\n\n    def setEntityResolver(self, resolver):\n        \"Register an object to resolve external entities.\"\n        self._ent_handler = resolver\n\n    def getErrorHandler(self):\n        \"Returns the current ErrorHandler.\"\n        return self._err_handler\n\n    def setErrorHandler(self, handler):\n        \"Register an object to receive error-message events.\"\n        self._err_handler = handler\n\n    def setLocale(self, locale):\n        \"\"\"Allow an application to set the locale for errors and warnings.\n\n        SAX parsers are not required to provide localization for errors\n        and warnings; if they cannot support the requested locale,\n        however, they must raise a SAX exception. Applications may\n        request a locale change in the middle of a parse.\"\"\"\n        raise SAXNotSupportedException(\"Locale support not implemented\")\n\n    def getFeature(self, name):\n        \"Looks up and returns the state of a SAX2 feature.\"\n        raise SAXNotRecognizedException(\"Feature '%s' not recognized\" % name)\n\n    def setFeature(self, name, state):\n        \"Sets the state of a SAX2 feature.\"\n        raise SAXNotRecognizedException(\"Feature '%s' not recognized\" % name)\n\n    def getProperty(self, name):\n        \"Looks up and returns the value of a SAX2 property.\"\n        raise SAXNotRecognizedException(\"Property '%s' not recognized\" % name)\n\n    def setProperty(self, name, value):\n        \"Sets the value of a SAX2 property.\"\n        raise SAXNotRecognizedException(\"Property '%s' not recognized\" % name)\n\nclass IncrementalParser(XMLReader):\n    \"\"\"This interface adds three extra methods to the XMLReader\n    interface that allow XML parsers to support incremental\n    parsing. Support for this interface is optional, since not all\n    underlying XML parsers support this functionality.\n\n    When the parser is instantiated it is ready to begin accepting\n    data from the feed method immediately. After parsing has been\n    finished with a call to close the reset method must be called to\n    make the parser ready to accept new data, either from feed or\n    using the parse method.\n\n    Note that these methods must _not_ be called during parsing, that\n    is, after parse has been called and before it returns.\n\n    By default, the class also implements the parse method of the XMLReader\n    interface using the feed, close and reset methods of the\n    IncrementalParser interface as a convenience to SAX 2.0 driver\n    writers.\"\"\"\n\n    def __init__(self, bufsize=2**16):\n        self._bufsize = bufsize\n        XMLReader.__init__(self)\n\n    def parse(self, source):\n        from . import saxutils\n        source = saxutils.prepare_input_source(source)\n\n        self.prepareParser(source)\n        file = source.getCharacterStream()\n        if file is None:\n            file = source.getByteStream()\n        buffer = file.read(self._bufsize)\n        while buffer:\n            self.feed(buffer)\n            buffer = file.read(self._bufsize)\n        self.close()\n\n    def feed(self, data):\n        \"\"\"This method gives the raw XML data in the data parameter to\n        the parser and makes it parse the data, emitting the\n        corresponding events. It is allowed for XML constructs to be\n        split across several calls to feed.\n\n        feed may raise SAXException.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def prepareParser(self, source):\n        \"\"\"This method is called by the parse implementation to allow\n        the SAX 2.0 driver to prepare itself for parsing.\"\"\"\n        raise NotImplementedError(\"prepareParser must be overridden!\")\n\n    def close(self):\n        \"\"\"This method is called when the entire XML document has been\n        passed to the parser through the feed method, to notify the\n        parser that there are no more data. This allows the parser to\n        do the final checks on the document and empty the internal\n        data buffer.\n\n        The parser will not be ready to parse another document until\n        the reset method has been called.\n\n        close may raise SAXException.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n    def reset(self):\n        \"\"\"This method is called after close has been called to reset\n        the parser so that it is ready to parse new documents. The\n        results of calling parse or feed after close without calling\n        reset are undefined.\"\"\"\n        raise NotImplementedError(\"This method must be implemented!\")\n\n# ===== LOCATOR =====\n\nclass Locator:\n    \"\"\"Interface for associating a SAX event with a document\n    location. A locator object will return valid results only during\n    calls to DocumentHandler methods; at any other time, the\n    results are unpredictable.\"\"\"\n\n    def getColumnNumber(self):\n        \"Return the column number where the current event ends.\"\n        return -1\n\n    def getLineNumber(self):\n        \"Return the line number where the current event ends.\"\n        return -1\n\n    def getPublicId(self):\n        \"Return the public identifier for the current event.\"\n        return None\n\n    def getSystemId(self):\n        \"Return the system identifier for the current event.\"\n        return None\n\n# ===== INPUTSOURCE =====\n\nclass InputSource:\n    \"\"\"Encapsulation of the information needed by the XMLReader to\n    read entities.\n\n    This class may include information about the public identifier,\n    system identifier, byte stream (possibly with character encoding\n    information) and/or the character stream of an entity.\n\n    Applications will create objects of this class for use in the\n    XMLReader.parse method and for returning from\n    EntityResolver.resolveEntity.\n\n    An InputSource belongs to the application, the XMLReader is not\n    allowed to modify InputSource objects passed to it from the\n    application, although it may make copies and modify those.\"\"\"\n\n    def __init__(self, system_id = None):\n        self.__system_id = system_id\n        self.__public_id = None\n        self.__encoding  = None\n        self.__bytefile  = None\n        self.__charfile  = None\n\n    def setPublicId(self, public_id):\n        \"Sets the public identifier of this InputSource.\"\n        self.__public_id = public_id\n\n    def getPublicId(self):\n        \"Returns the public identifier of this InputSource.\"\n        return self.__public_id\n\n    def setSystemId(self, system_id):\n        \"Sets the system identifier of this InputSource.\"\n        self.__system_id = system_id\n\n    def getSystemId(self):\n        \"Returns the system identifier of this InputSource.\"\n        return self.__system_id\n\n    def setEncoding(self, encoding):\n        \"\"\"Sets the character encoding of this InputSource.\n\n        The encoding must be a string acceptable for an XML encoding\n        declaration (see section 4.3.3 of the XML recommendation).\n\n        The encoding attribute of the InputSource is ignored if the\n        InputSource also contains a character stream.\"\"\"\n        self.__encoding = encoding\n\n    def getEncoding(self):\n        \"Get the character encoding of this InputSource.\"\n        return self.__encoding\n\n    def setByteStream(self, bytefile):\n        \"\"\"Set the byte stream (a Python file-like object which does\n        not perform byte-to-character conversion) for this input\n        source.\n\n        The SAX parser will ignore this if there is also a character\n        stream specified, but it will use a byte stream in preference\n        to opening a URI connection itself.\n\n        If the application knows the character encoding of the byte\n        stream, it should set it with the setEncoding method.\"\"\"\n        self.__bytefile = bytefile\n\n    def getByteStream(self):\n        \"\"\"Get the byte stream for this input source.\n\n        The getEncoding method will return the character encoding for\n        this byte stream, or None if unknown.\"\"\"\n        return self.__bytefile\n\n    def setCharacterStream(self, charfile):\n        \"\"\"Set the character stream for this input source. (The stream\n        must be a Python 2.0 Unicode-wrapped file-like that performs\n        conversion to Unicode strings.)\n\n        If there is a character stream specified, the SAX parser will\n        ignore any byte stream and will not attempt to open a URI\n        connection to the system identifier.\"\"\"\n        self.__charfile = charfile\n\n    def getCharacterStream(self):\n        \"Get the character stream for this input source.\"\n        return self.__charfile\n\n# ===== ATTRIBUTESIMPL =====\n\nclass AttributesImpl:\n\n    def __init__(self, attrs):\n        \"\"\"Non-NS-aware implementation.\n\n        attrs should be of the form {name : value}.\"\"\"\n        self._attrs = attrs\n\n    def getLength(self):\n        return len(self._attrs)\n\n    def getType(self, name):\n        return \"CDATA\"\n\n    def getValue(self, name):\n        return self._attrs[name]\n\n    def getValueByQName(self, name):\n        return self._attrs[name]\n\n    def getNameByQName(self, name):\n        if name not in self._attrs:\n            raise KeyError(name)\n        return name\n\n    def getQNameByName(self, name):\n        if name not in self._attrs:\n            raise KeyError(name)\n        return name\n\n    def getNames(self):\n        return list(self._attrs.keys())\n\n    def getQNames(self):\n        return list(self._attrs.keys())\n\n    def __len__(self):\n        return len(self._attrs)\n\n    def __getitem__(self, name):\n        return self._attrs[name]\n\n    def keys(self):\n        return list(self._attrs.keys())\n\n    def __contains__(self, name):\n        return name in self._attrs\n\n    def get(self, name, alternative=None):\n        return self._attrs.get(name, alternative)\n\n    def copy(self):\n        return self.__class__(self._attrs)\n\n    def items(self):\n        return list(self._attrs.items())\n\n    def values(self):\n        return list(self._attrs.values())\n\n# ===== ATTRIBUTESNSIMPL =====\n\nclass AttributesNSImpl(AttributesImpl):\n\n    def __init__(self, attrs, qnames):\n        \"\"\"NS-aware implementation.\n\n        attrs should be of the form {(ns_uri, lname): value, ...}.\n        qnames of the form {(ns_uri, lname): qname, ...}.\"\"\"\n        self._attrs = attrs\n        self._qnames = qnames\n\n    def getValueByQName(self, name):\n        for (nsname, qname) in self._qnames.items():\n            if qname == name:\n                return self._attrs[nsname]\n\n        raise KeyError(name)\n\n    def getNameByQName(self, name):\n        for (nsname, qname) in self._qnames.items():\n            if qname == name:\n                return nsname\n\n        raise KeyError(name)\n\n    def getQNameByName(self, name):\n        return self._qnames[name]\n\n    def getQNames(self):\n        return list(self._qnames.values())\n\n    def copy(self):\n        return self.__class__(self._attrs, self._qnames)\n\n\ndef _test():\n    XMLReader()\n    IncrementalParser()\n    Locator()\n\nif __name__ == \"__main__\":\n    _test()\n", "license": "gpl-3.0"}
{"id": "c98514ee440374c244375e7a4dfd7b49835ddb03", "path": "src/VBox/Devices/EFI/Firmware/BaseTools/Source/Python/UPT/Xml/GuidProtocolPpiXml.py", "repo_name": "mirror/vbox", "content": "## @file\n# This file is used to parse a xml file of .PKG file\n#\n# Copyright (c) 2011, Intel Corporation. All rights reserved.<BR>\n#\n# This program and the accompanying materials are licensed and made available \n# under the terms and conditions of the BSD License which accompanies this \n# distribution. The full text of the license may be found at \n# http://opensource.org/licenses/bsd-license.php\n#\n# THE PROGRAM IS DISTRIBUTED UNDER THE BSD LICENSE ON AN \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR REPRESENTATIONS OF ANY KIND, EITHER EXPRESS OR IMPLIED.\n#\n\n'''\nGuidProtocolPpiXml\n'''\nfrom Library.String import ConvertNEToNOTEQ\nfrom Library.String import ConvertNOTEQToNE\nfrom Library.String import GetStringOfList\nfrom Library.Xml.XmlRoutines import XmlElement\nfrom Library.Xml.XmlRoutines import XmlAttribute\nfrom Library.Xml.XmlRoutines import XmlNode\nfrom Library.Xml.XmlRoutines import XmlList\nfrom Library.Xml.XmlRoutines import CreateXmlElement\n\nfrom Object.POM.CommonObject import GuidObject\nfrom Object.POM.CommonObject import ProtocolObject\nfrom Object.POM.CommonObject import PpiObject\n\nfrom Xml.CommonXml import CommonDefinesXml\nfrom Xml.CommonXml import HelpTextXml\n\nfrom Xml.XmlParserMisc import GetHelpTextList\n\n##\n#GUID/Protocol/Ppi Common\n#\nclass GuidProtocolPpiXml(object):\n    def __init__(self, Mode):\n        self.UiName = ''\n        self.GuidTypes = ''\n        self.Notify = ''\n        self.CName = ''\n        self.GuidValue = ''\n        self.CommonDefines = CommonDefinesXml()\n        self.HelpText = []\n        #\n        # Guid/Ppi/Library, internal used for indicate return object for \n        # FromXml\n        #\n        self.Type = '' \n        #\n        # there are slightly different field between package and module\n        #\n        self.Mode = Mode\n        self.GuidType = ''\n        self.VariableName = ''\n        \n    def FromXml(self, Item, Key):\n        self.UiName = XmlAttribute(XmlNode(Item, '%s' % Key), 'UiName')\n        self.GuidType = XmlAttribute(XmlNode(Item, '%s' % Key), 'GuidType')\n        self.Notify = XmlAttribute(XmlNode(Item, '%s' % Key), 'Notify')\n        self.CName = XmlElement(Item, '%s/CName' % Key)\n        self.GuidValue = XmlElement(Item, '%s/GuidValue' % Key)\n        self.VariableName = XmlElement(Item, '%s/VariableName' % Key)\n        self.CommonDefines.FromXml(XmlNode(Item, '%s' % Key), Key)\n        for HelpTextItem in XmlList(Item, '%s/HelpText' % Key):\n            HelpTextObj = HelpTextXml()\n            HelpTextObj.FromXml(HelpTextItem, '%s/HelpText' % Key)\n            self.HelpText.append(HelpTextObj)\n        \n        if self.Type == 'Guid':    \n            GuidProtocolPpi = GuidObject()\n        elif self.Type == 'Protocol':\n            GuidProtocolPpi = ProtocolObject()\n        else:\n            GuidProtocolPpi = PpiObject()\n        GuidProtocolPpi.SetHelpTextList(GetHelpTextList(self.HelpText))\n\n        return GuidProtocolPpi\n\n    def ToXml(self, GuidProtocolPpi, Key):\n        if self.GuidValue:\n            pass\n        AttributeList = \\\n        [['Usage', GetStringOfList(GuidProtocolPpi.GetUsage())], \\\n         ['UiName', GuidProtocolPpi.GetName()], \\\n         ['GuidType', GetStringOfList(GuidProtocolPpi.GetGuidTypeList())], \\\n         ['Notify', str(GuidProtocolPpi.GetNotify()).lower()], \\\n         ['SupArchList', GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n         ['SupModList', GetStringOfList(GuidProtocolPpi.GetSupModuleList())], \\\n         ['FeatureFlag', ConvertNEToNOTEQ(GuidProtocolPpi.GetFeatureFlag())]\n        ]\n        NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                    ['GuidValue', GuidProtocolPpi.GetGuid()],\n                    ['VariableName', GuidProtocolPpi.VariableName]\n                   ]\n        for Item in GuidProtocolPpi.GetHelpTextList():\n            Tmp = HelpTextXml()\n            NodeList.append(Tmp.ToXml(Item))\n        Root = CreateXmlElement('%s' % Key, '', NodeList, AttributeList)\n        \n        return Root\n\n    def __str__(self):\n        Str = \\\n        \"UiName = %s Notify = %s GuidTypes = %s CName = %s GuidValue = %s %s\" \\\n        % (self.UiName, self.Notify, self.GuidTypes, self.CName, \\\n           self.GuidValue, self.CommonDefines)\n        for Item in self.HelpText:\n            Str = Str + \"\\n\\t\" + str(Item)\n        return Str\n##\n#GUID Xml\n#\nclass GuidXml(GuidProtocolPpiXml):\n    def __init__(self, Mode):\n        GuidProtocolPpiXml.__init__(self, Mode)\n        self.Type = 'Guid'\n        \n    def FromXml(self, Item, Key):         \n        GuidProtocolPpi = GuidProtocolPpiXml.FromXml(self, Item, Key)\n\n        if self.Mode == 'Package':\n            \n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetSupModuleList(self.CommonDefines.SupModList)\n            GuidProtocolPpi.SetCName(self.CName)\n            GuidProtocolPpi.SetGuid(self.GuidValue)\n        else:\n            GuidProtocolPpi.SetUsage(self.CommonDefines.Usage)\n            if self.GuidType:\n                GuidProtocolPpi.SetGuidTypeList([self.GuidType])\n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetFeatureFlag(ConvertNOTEQToNE(self.CommonDefines.FeatureFlag))\n            GuidProtocolPpi.SetCName(self.CName)\n            GuidProtocolPpi.SetVariableName(self.VariableName)\n        return GuidProtocolPpi\n\n    def ToXml(self, GuidProtocolPpi, Key):\n        if self.Mode == 'Package': \n            AttributeList = \\\n            [['GuidType', \\\n              GetStringOfList(GuidProtocolPpi.GetGuidTypeList())], \\\n              ['SupArchList', \\\n               GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n               ['SupModList', \\\n                GetStringOfList(GuidProtocolPpi.GetSupModuleList())], \n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                        ['GuidValue', GuidProtocolPpi.GetGuid()],\n                       ]\n        else:\n            AttributeList = \\\n            [['Usage', GetStringOfList(GuidProtocolPpi.GetUsage())], \\\n             ['GuidType', GetStringOfList(GuidProtocolPpi.GetGuidTypeList())],\\\n              ['SupArchList', \\\n               GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n               ['FeatureFlag', ConvertNEToNOTEQ(GuidProtocolPpi.GetFeatureFlag())]\n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                        ['VariableName', GuidProtocolPpi.GetVariableName()]\n                       ]\n\n        for Item in GuidProtocolPpi.GetHelpTextList():\n            Tmp = HelpTextXml()\n            NodeList.append(Tmp.ToXml(Item))\n        Root = CreateXmlElement('%s' % Key, '', NodeList, AttributeList)\n        \n        return Root\n##\n#Protocol Xml\n#\nclass ProtocolXml(GuidProtocolPpiXml):\n    def __init__(self, Mode):\n        GuidProtocolPpiXml.__init__(self, Mode)\n        self.Type = 'Protocol'\n        \n    def FromXml(self, Item, Key):\n        GuidProtocolPpi = GuidProtocolPpiXml.FromXml(self, Item, Key)\n        if self.Mode == 'Package':\n            GuidProtocolPpi.SetFeatureFlag(self.CommonDefines.FeatureFlag) \n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetSupModuleList(self.CommonDefines.SupModList)\n            GuidProtocolPpi.SetCName(self.CName)\n            GuidProtocolPpi.SetGuid(self.GuidValue)\n        else:\n            GuidProtocolPpi.SetUsage(self.CommonDefines.Usage)\n            if self.Notify.upper() == \"TRUE\":\n                GuidProtocolPpi.SetNotify(True)\n            elif self.Notify.upper() == \"FALSE\":\n                GuidProtocolPpi.SetNotify(False)\n            else:\n                GuidProtocolPpi.SetNotify('')\n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetFeatureFlag(ConvertNOTEQToNE(self.CommonDefines.FeatureFlag))\n            GuidProtocolPpi.SetCName(self.CName)\n \n        return GuidProtocolPpi\n\n    def ToXml(self, GuidProtocolPpi, Key):\n        if self.Mode == 'Package':        \n            AttributeList = \\\n            [['SupArchList', \\\n              GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n              ['SupModList', \\\n               GetStringOfList(GuidProtocolPpi.GetSupModuleList())], \\\n               ['FeatureFlag', GuidProtocolPpi.GetFeatureFlag()]\n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                        ['GuidValue', GuidProtocolPpi.GetGuid()],\n                       ]\n        else:\n            AttributeList = \\\n            [['Usage', GetStringOfList(GuidProtocolPpi.GetUsage())], \\\n             ['Notify', str(GuidProtocolPpi.GetNotify()).lower()], \\\n             ['SupArchList', \\\n              GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n              ['FeatureFlag', ConvertNEToNOTEQ(GuidProtocolPpi.GetFeatureFlag())]\n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                       ]\n            \n        for Item in GuidProtocolPpi.GetHelpTextList():\n            Tmp = HelpTextXml()\n            NodeList.append(Tmp.ToXml(Item))\n        Root = CreateXmlElement('%s' % Key, '', NodeList, AttributeList)\n        \n        return Root\n##\n#Ppi Xml\n#\nclass PpiXml(GuidProtocolPpiXml):\n    def __init__(self, Mode):\n        GuidProtocolPpiXml.__init__(self, Mode)\n        self.Type = 'Ppi'\n        \n    def FromXml(self, Item, Key):\n        GuidProtocolPpi = GuidProtocolPpiXml.FromXml(self, Item, Key)\n        if self.Mode == 'Package':\n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetSupModuleList(self.CommonDefines.SupModList)\n            GuidProtocolPpi.SetCName(self.CName)\n            GuidProtocolPpi.SetGuid(self.GuidValue)\n        else:\n            GuidProtocolPpi.SetUsage(self.CommonDefines.Usage)\n            if self.Notify.upper() == \"TRUE\":\n                GuidProtocolPpi.SetNotify(True)\n            elif self.Notify.upper() == \"FALSE\":\n                GuidProtocolPpi.SetNotify(False)\n            else:\n                GuidProtocolPpi.SetNotify('')\n            GuidProtocolPpi.SetSupArchList(self.CommonDefines.SupArchList)\n            GuidProtocolPpi.SetFeatureFlag(ConvertNOTEQToNE(self.CommonDefines.FeatureFlag))\n            GuidProtocolPpi.SetCName(self.CName)\n \n        return GuidProtocolPpi\n\n    def ToXml(self, GuidProtocolPpi, Key):\n        if self.Mode == 'Package':\n            AttributeList = \\\n            [['SupArchList', \\\n              GetStringOfList(GuidProtocolPpi.GetSupArchList())], \n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                        ['GuidValue', GuidProtocolPpi.GetGuid()],\n                       ]\n        else:\n            AttributeList = \\\n            [['Usage', GetStringOfList(GuidProtocolPpi.GetUsage())], \\\n             ['Notify', str(GuidProtocolPpi.GetNotify()).lower()], \\\n             ['SupArchList', \\\n              GetStringOfList(GuidProtocolPpi.GetSupArchList())], \\\n              ['FeatureFlag', ConvertNEToNOTEQ(GuidProtocolPpi.GetFeatureFlag())]\n            ]\n            NodeList = [['CName', GuidProtocolPpi.GetCName()], \n                       ]\n        \n        for Item in GuidProtocolPpi.GetHelpTextList():\n            Tmp = HelpTextXml()\n            NodeList.append(Tmp.ToXml(Item))\n        Root = CreateXmlElement('%s' % Key, '', NodeList, AttributeList)\n        return Root\n", "license": "gpl-2.0"}
{"id": "e8a3775c0d80d8392b27154e5f8d56ed4e3dd813", "path": "test/integer/mad_sat_short8short8short8/compile.py", "repo_name": "xianggong/m2c_unit_test", "content": "#!/usr/bin/python\n\nimport os\nimport subprocess\nimport re\n\n\ndef runCommand(command):\n        p = subprocess.Popen(command,\n                             stdout=subprocess.PIPE,\n                             stderr=subprocess.STDOUT)\n        p.wait()\n        return iter(p.stdout.readline, b'')\n\n\ndef dumpRunCommand(command, dump_file_name, postfix):\n        dumpFile = open(dump_file_name + postfix, \"w+\")\n        dumpFile.write(command + \"\\n\")\n        for line in runCommand(command.split()):\n                dumpFile.write(line)\n\n\ndef rmFile(file_name):\n        cmd = \"rm -rf \" + file_name\n        runCommand(cmd.split())\n\n\ndef rnm_ir(file_name):\n        # Append all unnamed variable with prefix 'tmp_'\n        ir_file_name = file_name + \".ll\"\n        if os.path.isfile(ir_file_name):\n                fo = open(ir_file_name, \"rw+\")\n                lines = fo.readlines()\n                fo.seek(0)\n                fo.truncate()\n                for line in lines:\n                        # Add entry block identifier\n                        if \"define\" in line:\n                            line += \"entry:\\n\"\n                        # Rename all unnamed variables\n                        line = re.sub('\\%([0-9]+)',\n                                      r'%tmp_\\1',\n                                      line.rstrip())\n                        # Also rename branch name\n                        line = re.sub('(\\;\\ \\<label\\>\\:)([0-9]+)',\n                                      r'tmp_\\2:',\n                                      line.rstrip())\n                        fo.write(line + '\\n')\n\n\ndef gen_ir(file_name):\n        # Directories\n        root_dir = '../../../'\n        header_dir = root_dir + \"inc/\"\n\n        # Headers\n        header = \" -I \" + header_dir\n        header += \" -include \" + header_dir + \"m2c_buildin_fix.h \"\n        header += \" -include \" + header_dir + \"clc/clc.h \"\n        header += \" -D cl_clang_storage_class_specifiers \"\n\n        gen_ir = \"clang -S -emit-llvm -O0 -target r600-- -mcpu=verde \"\n        cmd_gen_ir = gen_ir + header + file_name + \".cl\"\n        dumpRunCommand(cmd_gen_ir, file_name, \".clang.log\")\n\n\ndef asm_ir(file_name):\n    if os.path.isfile(file_name + \".ll\"):\n        # Command to assemble IR to bitcode\n        gen_bc = \"llvm-as \"\n        gen_bc_src = file_name + \".ll\"\n        gen_bc_dst = file_name + \".bc\"\n        cmd_gen_bc = gen_bc + gen_bc_src + \" -o \" + gen_bc_dst\n        runCommand(cmd_gen_bc.split())\n\n\ndef opt_bc(file_name):\n    if os.path.isfile(file_name + \".bc\"):\n        # Command to optmize bitcode\n        opt_bc = \"opt --mem2reg \"\n        opt_ir_src = file_name + \".bc\"\n        opt_ir_dst = file_name + \".opt.bc\"\n        cmd_opt_bc = opt_bc + opt_ir_src + \" -o \" + opt_ir_dst\n        runCommand(cmd_opt_bc.split())\n\n\ndef dis_bc(file_name):\n    if os.path.isfile(file_name + \".bc\"):\n        # Command to disassemble bitcode\n        dis_bc = \"llvm-dis \"\n        dis_ir_src = file_name + \".opt.bc\"\n        dis_ir_dst = file_name + \".opt.ll\"\n        cmd_dis_bc = dis_bc + dis_ir_src + \" -o \" + dis_ir_dst\n        runCommand(cmd_dis_bc.split())\n\n\ndef m2c_gen(file_name):\n    if os.path.isfile(file_name + \".opt.bc\"):\n        # Command to disassemble bitcode\n        m2c_gen = \"m2c --llvm2si \"\n        m2c_gen_src = file_name + \".opt.bc\"\n        cmd_m2c_gen = m2c_gen + m2c_gen_src\n        dumpRunCommand(cmd_m2c_gen, file_name, \".m2c.llvm2si.log\")\n\n    # Remove file if size is 0\n    if os.path.isfile(file_name + \".opt.s\"):\n        if os.path.getsize(file_name + \".opt.s\") == 0:\n            rmFile(file_name + \".opt.s\")\n\n\ndef m2c_bin(file_name):\n    if os.path.isfile(file_name + \".opt.s\"):\n        # Command to disassemble bitcode\n        m2c_bin = \"m2c --si2bin \"\n        m2c_bin_src = file_name + \".opt.s\"\n        cmd_m2c_bin = m2c_bin + m2c_bin_src\n        dumpRunCommand(cmd_m2c_bin, file_name, \".m2c.si2bin.log\")\n\n\ndef main():\n\n        # Commands\n\n        for file in os.listdir(\"./\"):\n                if file.endswith(\".cl\"):\n                        file_name = os.path.splitext(file)[0]\n                        # Execute commands\n                        gen_ir(file_name)\n                        rnm_ir(file_name)\n                        asm_ir(file_name)\n                        opt_bc(file_name)\n                        dis_bc(file_name)\n                        m2c_gen(file_name)\n                        m2c_bin(file_name)\n\nif __name__ == \"__main__\":\n        main()\n", "license": "gpl-2.0"}
{"id": "7e9d5a6986f79e6df07171a490768ee0b4178621", "path": "django/contrib/gis/gdal/feature.py", "repo_name": "h4r5h1t/django-hauthy", "content": "from django.contrib.gis.gdal.base import GDALBase\nfrom django.contrib.gis.gdal.error import GDALException, OGRIndexError\nfrom django.contrib.gis.gdal.field import Field\nfrom django.contrib.gis.gdal.geometries import OGRGeometry, OGRGeomType\nfrom django.contrib.gis.gdal.prototypes import ds as capi, geom as geom_api\nfrom django.utils import six\nfrom django.utils.encoding import force_bytes, force_text\nfrom django.utils.six.moves import range\n\n\n# For more information, see the OGR C API source code:\n#  http://www.gdal.org/ogr/ogr__api_8h.html\n#\n# The OGR_F_* routines are relevant here.\nclass Feature(GDALBase):\n    \"\"\"\n    This class that wraps an OGR Feature, needs to be instantiated\n    from a Layer object.\n    \"\"\"\n\n    def __init__(self, feat, layer):\n        \"\"\"\n        Initializes Feature from a pointer and its Layer object.\n        \"\"\"\n        if not feat:\n            raise GDALException('Cannot create OGR Feature, invalid pointer given.')\n        self.ptr = feat\n        self._layer = layer\n\n    def __del__(self):\n        \"Releases a reference to this object.\"\n        if self._ptr and capi:\n            capi.destroy_feature(self._ptr)\n\n    def __getitem__(self, index):\n        \"\"\"\n        Gets the Field object at the specified index, which may be either\n        an integer or the Field's string label.  Note that the Field object\n        is not the field's _value_ -- use the `get` method instead to\n        retrieve the value (e.g. an integer) instead of a Field instance.\n        \"\"\"\n        if isinstance(index, six.string_types):\n            i = self.index(index)\n        else:\n            if index < 0 or index > self.num_fields:\n                raise OGRIndexError('index out of range')\n            i = index\n        return Field(self, i)\n\n    def __iter__(self):\n        \"Iterates over each field in the Feature.\"\n        for i in range(self.num_fields):\n            yield self[i]\n\n    def __len__(self):\n        \"Returns the count of fields in this feature.\"\n        return self.num_fields\n\n    def __str__(self):\n        \"The string name of the feature.\"\n        return 'Feature FID %d in Layer<%s>' % (self.fid, self.layer_name)\n\n    def __eq__(self, other):\n        \"Does equivalence testing on the features.\"\n        return bool(capi.feature_equal(self.ptr, other._ptr))\n\n    # #### Feature Properties ####\n    @property\n    def encoding(self):\n        return self._layer._ds.encoding\n\n    @property\n    def fid(self):\n        \"Returns the feature identifier.\"\n        return capi.get_fid(self.ptr)\n\n    @property\n    def layer_name(self):\n        \"Returns the name of the layer for the feature.\"\n        name = capi.get_feat_name(self._layer._ldefn)\n        return force_text(name, self.encoding, strings_only=True)\n\n    @property\n    def num_fields(self):\n        \"Returns the number of fields in the Feature.\"\n        return capi.get_feat_field_count(self.ptr)\n\n    @property\n    def fields(self):\n        \"Returns a list of fields in the Feature.\"\n        return [capi.get_field_name(capi.get_field_defn(self._layer._ldefn, i))\n                for i in range(self.num_fields)]\n\n    @property\n    def geom(self):\n        \"Returns the OGR Geometry for this Feature.\"\n        # Retrieving the geometry pointer for the feature.\n        geom_ptr = capi.get_feat_geom_ref(self.ptr)\n        return OGRGeometry(geom_api.clone_geom(geom_ptr))\n\n    @property\n    def geom_type(self):\n        \"Returns the OGR Geometry Type for this Feture.\"\n        return OGRGeomType(capi.get_fd_geom_type(self._layer._ldefn))\n\n    # #### Feature Methods ####\n    def get(self, field):\n        \"\"\"\n        Returns the value of the field, instead of an instance of the Field\n        object.  May take a string of the field name or a Field object as\n        parameters.\n        \"\"\"\n        field_name = getattr(field, 'name', field)\n        return self[field_name].value\n\n    def index(self, field_name):\n        \"Returns the index of the given field name.\"\n        i = capi.get_field_index(self.ptr, force_bytes(field_name))\n        if i < 0:\n            raise OGRIndexError('invalid OFT field name given: \"%s\"' % field_name)\n        return i\n", "license": "bsd-3-clause"}
{"id": "7f2e9b0c027417faa3d5f6258a38d1be9d9a252b", "path": "tests/messages_tests/test_middleware.py", "repo_name": "georgemarshall/django", "content": "import unittest\n\nfrom django.contrib.messages.middleware import MessageMiddleware\nfrom django.http import HttpRequest, HttpResponse\n\n\nclass MiddlewareTests(unittest.TestCase):\n\n    def setUp(self):\n        self.middleware = MessageMiddleware()\n\n    def test_response_without_messages(self):\n        \"\"\"\n        MessageMiddleware is tolerant of messages not existing on request.\n        \"\"\"\n        request = HttpRequest()\n        response = HttpResponse()\n        self.middleware.process_response(request, response)\n", "license": "bsd-3-clause"}
{"id": "f4c1f45ac7005faf6b4c6ce5acbc0a85d1b5b1c5", "path": "tests/archive_tests.py", "repo_name": "tbohn/VIC", "content": "#!/usr/bin/env python\n\"\"\"VIC test archiving command line interface\"\"\"\n\nimport argparse\nfrom collections import OrderedDict\nfrom datetime import datetime\n\nfrom .run_tests import epilog, description, CustomFormatter\n\n\n# -------------------------------------------------------------------- #\ndef main():\n    \"\"\"\n    Archive VIC tests\n    \"\"\"\n\n    # Parse arguments\n    test_results = OrderedDict()\n\n    parser = argparse.ArgumentParser(description=description, epilog=epilog,\n                                     formatter_class=CustomFormatter)\n\n    parser.add_argument(\"tests\", type=str,\n                        help=\"Test sets to run\",\n                        choices=['all', 'unit', 'system', 'science',\n                                 'examples', 'release'],\n                        default=['unit', 'system', 'science'], nargs='+')\n    parser.add_argument(\"--output_dir\", type=str,\n                        help=\"directory to get test output\",\n                        default=\"$WORKDIR/VIC_tests_{0}\".format(\n                            datetime.datetime.now().strftime(\"%Y%m%d\")))\n    parser.add_argument(\"--data_dir\", type=str,\n                        help=\"directory to put test data\",\n                        default='test_data/VIC.4.1.2')\n    args = parser.parse_args()\n\n    print(test_results, args)\n\n    return\n# -------------------------------------------------------------------- #\n\n\n# -------------------------------------------------------------------- #\nif __name__ == \"__main__\":\n    main()\n# -------------------------------------------------------------------- #\n", "license": "gpl-2.0"}
{"id": "4b04c8fd0c665a423e482f40de898fdcaa4ee305", "path": "Assets/IronPythonConsole/Plugins/Lib/lib2to3/fixes/fix_isinstance.py", "repo_name": "bob-white/UnityIronPythonConsole", "content": "# Copyright 2008 Armin Ronacher.\n# Licensed to PSF under a Contributor Agreement.\n\n\"\"\"Fixer that cleans up a tuple argument to isinstance after the tokens\nin it were fixed.  This is mainly used to remove double occurrences of\ntokens as a leftover of the long -> int / unicode -> str conversion.\n\neg.  isinstance(x, (int, long)) -> isinstance(x, (int, int))\n       -> isinstance(x, int)\n\"\"\"\n\nfrom .. import fixer_base\nfrom ..fixer_util import token\n\n\nclass FixIsinstance(fixer_base.BaseFix):\n    BM_compatible = True\n    PATTERN = \"\"\"\n    power<\n        'isinstance'\n        trailer< '(' arglist< any ',' atom< '('\n            args=testlist_gexp< any+ >\n        ')' > > ')' >\n    >\n    \"\"\"\n\n    run_order = 6\n\n    def transform(self, node, results):\n        names_inserted = set()\n        testlist = results[\"args\"]\n        args = testlist.children\n        new_args = []\n        iterator = enumerate(args)\n        for idx, arg in iterator:\n            if arg.type == token.NAME and arg.value in names_inserted:\n                if idx < len(args) - 1 and args[idx + 1].type == token.COMMA:\n                    iterator.next()\n                    continue\n            else:\n                new_args.append(arg)\n                if arg.type == token.NAME:\n                    names_inserted.add(arg.value)\n        if new_args and new_args[-1].type == token.COMMA:\n            del new_args[-1]\n        if len(new_args) == 1:\n            atom = testlist.parent\n            new_args[0].prefix = atom.prefix\n            atom.replace(new_args[0])\n        else:\n            args[:] = new_args\n            node.changed()\n", "license": "mpl-2.0"}
{"id": "a07ecfa6110673ea8ea6f20f44bb5baecaec2046", "path": "hwrt/serve.py", "repo_name": "MartinThoma/hwrt", "content": "#!/usr/bin/env python\n\n\"\"\"Start a webserver which can record the data and work as a classifier.\"\"\"\n\n# Core Library modules\nimport json\nimport logging\nimport os\nimport uuid\nfrom typing import Any, Dict, List, Optional\n\n# Third party modules\nimport pkg_resources\nimport requests\nfrom flask import Flask, render_template, request\nfrom flask_bootstrap import Bootstrap\nfrom six.moves.urllib.request import urlopen\n\n# First party modules\nimport hwrt\n\n# Local modules\nfrom . import classify\nfrom . import segmentation as se\nfrom . import utils\n\nlogger = logging.getLogger(__name__)\nlogging.getLogger(\"requests\").setLevel(logging.WARNING)\n\n\n# Global variables\nn = 10\nuse_segmenter_flag = False\n\n\ndef submit_recording(raw_data_json):\n    \"\"\"Submit a recording to the database on write-math.com.\n\n    Parameters\n    ----------\n    raw_data_json : str\n        Raw data in JSON format\n\n    Raises\n    ------\n    requests.exceptions.ConnectionError\n        If the internet connection is lost.\n    \"\"\"\n    url = \"http://www.martin-thoma.de/write-math/classify/index.php\"\n    headers = {\n        \"User-Agent\": \"Mozilla/5.0\",\n        \"Content-Type\": \"application/x-www-form-urlencoded\",\n    }\n    payload = {\"drawnJSON\": raw_data_json}\n\n    s = requests.Session()\n    req = requests.Request(\"POST\", url, headers=headers, data=payload)\n    prepared = req.prepare()\n    s.send(prepared)\n\n\ndef show_results(results: List[Dict[str, float]], n: int = 10) -> str:\n    r\"\"\"Show the TOP n results of a classification.\n    >>> results = [{'\\\\alpha': 0.67, 'semantics': '\\\\alpha', 'probability': 0.67},\n    ... {'\\\\propto': 0.25, 'semantics': '\\\\propto', 'probability': 0.33}]\n    >>> result_str = show_results(results)\n    Class              Prob\n    ##################################################\n    \\alpha             67.0000%\n    \\propto            33.0000%\n    ##################################################\n\n    \"\"\"\n    import nntoolkit.evaluate\n\n    classification = nntoolkit.evaluate.show_results(results, n)\n    return \"<pre>\" + classification.replace(\"\\n\", \"<br/>\") + \"</pre>\"\n\n\n# configuration\nDEBUG = True\n\ntemplate_path = utils.get_template_folder()\n\napp = Flask(__name__, template_folder=template_path)\nBootstrap(app)\napp.config.from_object(__name__)\n\n\n@app.route(\"/\", methods=[\"GET\"])\ndef interactive():\n    \"\"\"Interactive classifier.\"\"\"\n    if request.method == \"GET\" and request.args.get(\"heartbeat\", \"\") != \"\":\n        return request.args.get(\"heartbeat\", \"\")\n    return render_template(\"canvas.html\")\n\n\ndef get_json_result(results: List[Dict[str, Any]], n: int = 10) -> str:\n    \"\"\"Return the top `n` results as a JSON list.\n\n    Examples\n    --------\n    >>> results = [{'probability': 0.65,\n    ...             'whatever': 'bar'},\n    ...            {'probability': 0.21,\n    ...             'whatever': 'bar'},\n    ...            {'probability': 0.05,\n    ...             'whatever': 'bar'},]\n    >>> get_json_result(results, n=2)\n    '[{\"probability\": 0.65, \"whatever\": \"bar\"}, {\"probability\": 0.21, \"whatever\": \"bar\"}]'\n    \"\"\"\n    s = []\n    last = -1\n    for res in results[: min(len(results), n)]:\n        if res[\"probability\"] < last * 0.5 and res[\"probability\"] < 0.05:\n            break\n        if res[\"probability\"] < 0.01:\n            break\n        s.append(res)\n        last = res[\"probability\"]\n    return json.dumps(s)\n\n\n@app.route(\"/worker\", methods=[\"POST\", \"GET\"])\ndef worker():\n    \"\"\"Implement a worker for write-math.com.\"\"\"\n    global n\n    global use_segmenter_flag\n    if request.method == \"POST\":\n        raw_data_json = request.form[\"classify\"]\n        secret_uuid = request.form.get(\"secret\", None)\n        if secret_uuid is None:\n            logger.info(\"No secret uuid given. Create one.\")\n            secret_uuid = str(uuid.uuid4())\n\n        # Check recording\n        try:\n            json.loads(raw_data_json)\n        except ValueError:\n            return \"Invalid JSON string: %s\" % raw_data_json\n\n        # Classify\n        if use_segmenter_flag:\n            strokelist = json.loads(raw_data_json)\n            beam = utils.get_beam(secret_uuid)\n\n            if beam is None:\n                beam = se.Beam()\n                for stroke in strokelist:\n                    beam.add_stroke(stroke)\n                results = beam.get_results()\n                utils.store_beam(beam, secret_uuid)\n            else:\n                stroke = strokelist[-1]\n                beam.add_stroke(stroke)\n                results = beam.get_results()\n                utils.store_beam(beam, secret_uuid)\n        else:\n            results = classify.classify_segmented_recording(raw_data_json)\n        return get_json_result(results, n=n)\n    else:\n        # Page where the user can enter a recording\n        return \"Classification Worker (Version %s)\" % hwrt.__version__\n\n\ndef _get_part(pointlist, strokes):\n    \"\"\"Get some strokes of pointlist\n\n    Parameters\n    ----------\n    pointlist : list of lists of dicts\n    strokes : list of integers\n\n    Returns\n    -------\n    list of lists of dicts\n    \"\"\"\n    result = []\n    strokes = sorted(strokes)\n    for stroke_index in strokes:\n        result.append(pointlist[stroke_index])\n    return result\n\n\ndef _get_translate() -> Dict[str, str]:\n    \"\"\"\n    Get a dictionary which translates from a neural network output to\n    semantics.\n    \"\"\"\n    translate = {}\n    model_path = pkg_resources.resource_filename(\"hwrt\", \"misc/\")\n    translation_csv = os.path.join(model_path, \"latex2writemathindex.csv\")\n    with open(translation_csv, newline=\"\", encoding=\"utf8\") as csvfile:\n        contents = csvfile.read()\n    lines = contents.split(\"\\n\")\n    for csvrow_str in lines:\n        csvrow = csvrow_str.split(\",\")\n        if len(csvrow) == 1:\n            writemathid = csvrow[0]\n            latex = \"\"\n        else:\n            writemathid, latex_list = csvrow[0], csvrow[1:]\n            latex = \",\".join(latex_list)\n        translate[latex] = writemathid\n    return translate\n\n\ndef get_writemath_id(el: Dict[Any, Any], translate) -> Optional[int]:\n    \"\"\"\n    Parameters\n    ----------\n    el : Dict\n        with key 'semantics'\n        results element\n\n    Returns\n    -------\n    writemathid: Optional[int]\n        ID of the symbol on write-math.com\n    \"\"\"\n    semantics = el[\"semantics\"].split(\";\")[1]\n    if semantics not in translate:\n        logger.debug(f\"Could not find '{semantics}' in translate. el={el}\")\n        return None\n    else:\n        writemathid = translate[semantics]\n    return writemathid\n\n\ndef fix_writemath_answer(results: List[Dict[str, Any]]):\n    \"\"\"\n    Bring ``results`` into a format that is accepted by write-math.com. This\n    means using the ID for the formula that is used by the write-math server.\n\n    Examples\n    --------\n    >>> results = [{'symbolnr': 214,\n    ...             'semantics': 'foobar;A',\n    ...             'probability': 0.03}]\n    >>> fix_writemath_answer(results)\n    [{'symbolnr': 214, 'semantics': '31', 'probability': 0.03}]\n    \"\"\"\n    new_results = []\n    # Read csv\n    translate = _get_translate()\n\n    for i, el in enumerate(results):\n        writemathid = get_writemath_id(el, translate)\n        if writemathid is None:\n            continue\n        new_results.append(\n            {\n                \"symbolnr\": el[\"symbolnr\"],\n                \"semantics\": writemathid,\n                \"probability\": el[\"probability\"],\n            }\n        )\n        if i >= 10 or (i > 0 and el[\"probability\"] < 0.20):\n            break\n    return new_results\n\n\n@app.route(\"/work\", methods=[\"POST\", \"GET\"])\ndef work():\n    \"\"\"Implement a worker for write-math.com.\"\"\"\n    global n\n\n    cmd = utils.get_project_configuration()\n    if \"worker_api_key\" not in cmd:\n        return \"You need to define a 'worker_api_key' in your ~/\"\n\n    chunk_size = 1000\n\n    logger.info(\"Start working with n=%i\", n)\n    for _ in range(chunk_size):\n        # contact the write-math server and get something to classify\n        url = \"http://www.martin-thoma.de/write-math/api/get_unclassified.php\"\n        response = urlopen(url)\n        page_source = response.read()\n        parsed_json = json.loads(page_source)\n        if parsed_json is False:\n            return \"Nothing left to classify\"\n        raw_data_json = parsed_json[\"recording\"]\n\n        # Classify\n        # Check recording\n        try:\n            json.loads(raw_data_json)\n        except ValueError:\n            return \"Raw Data ID {}; Invalid JSON string: {}\".format(\n                parsed_json[\"id\"], raw_data_json,\n            )\n\n        # Classify\n        if use_segmenter_flag:\n            strokelist = json.loads(raw_data_json)\n            beam = se.Beam()\n            for stroke in strokelist:\n                beam.add_stroke(stroke)\n            results = beam.get_writemath_results()\n        else:\n            results_sym = classify.classify_segmented_recording(raw_data_json)\n            results = []\n            strokelist = json.loads(raw_data_json)\n            segmentation = [list(range(len(strokelist)))]\n            translate = _get_translate()\n            for symbol in results_sym:\n                s = {\n                    \"id\": get_writemath_id(symbol, translate),\n                    \"probability\": symbol[\"probability\"],\n                }\n                results.append(\n                    {\n                        \"probability\": symbol[\"probability\"],\n                        \"segmentation\": segmentation,\n                        \"symbols\": [s],\n                    }\n                )\n\n        print(\"\\thttp://write-math.com/view/?raw_data_id=%s\" % str(parsed_json[\"id\"]))\n\n        # Submit classification to write-math.com server\n        results_json = get_json_result(results, n=n)\n        headers = {\n            \"User-Agent\": \"Mozilla/5.0\",\n            \"Content-Type\": \"application/x-www-form-urlencoded\",\n        }\n        payload = {\n            \"recording_id\": parsed_json[\"id\"],\n            \"results\": results_json,\n            \"api_key\": cmd[\"worker_api_key\"],\n        }\n\n        s = requests.Session()\n        req = requests.Request(\"POST\", url, headers=headers, data=payload)\n        prepared = req.prepare()\n        response = s.send(prepared)\n        try:\n            response = json.loads(response.text)\n        except ValueError:\n            return \"Invalid JSON response: %s\" % response.text\n\n        if \"error\" in response:\n            logger.info(response)\n            return str(response)\n    return \"Done - Classified %i recordings\" % chunk_size\n\n\ndef main(port=8000, n_output=10, use_segmenter=False):\n    \"\"\"Main function starting the webserver.\"\"\"\n    global n\n    global use_segmenter_flag\n    n = n_output\n    use_segmenter_flag = use_segmenter\n    logger.info(\"Start webserver...\")\n    app.run(port=port)\n", "license": "mit"}
{"id": "da7887586f7a3192bd6729101a4c3c9e73e7051d", "path": "libs/pyutil/test/current/test_mathutil.py", "repo_name": "duramato/CouchPotatoServer", "content": "#!/usr/bin/env python\n\nimport unittest\n\nfrom pyutil import mathutil\nfrom pyutil.assertutil import _assert\n\nclass MathUtilTestCase(unittest.TestCase):\n    def _help_test_is_power_of_k(self, k):\n        for i in range(2, 40):\n            _assert(mathutil.is_power_of_k(k**i, k), k, i)\n\n    def test_is_power_of_k(self):\n        for i in range(2, 5):\n            self._help_test_is_power_of_k(i)\n\n    def test_log_ceil(self):\n        f = mathutil.log_ceil\n        self.failUnlessEqual(f(1, 2), 0)\n        self.failUnlessEqual(f(1, 3), 0)\n        self.failUnlessEqual(f(2, 2), 1)\n        self.failUnlessEqual(f(2, 3), 1)\n        self.failUnlessEqual(f(3, 2), 2)\n\n    def test_log_floor(self):\n        f = mathutil.log_floor\n        self.failUnlessEqual(f(1, 2), 0)\n        self.failUnlessEqual(f(1, 3), 0)\n        self.failUnlessEqual(f(2, 2), 1)\n        self.failUnlessEqual(f(2, 3), 0)\n        self.failUnlessEqual(f(3, 2), 1)\n\n    def test_div_ceil(self):\n        f = mathutil.div_ceil\n        self.failUnlessEqual(f(0, 1), 0)\n        self.failUnlessEqual(f(0, 2), 0)\n        self.failUnlessEqual(f(0, 3), 0)\n        self.failUnlessEqual(f(1, 3), 1)\n        self.failUnlessEqual(f(2, 3), 1)\n        self.failUnlessEqual(f(3, 3), 1)\n        self.failUnlessEqual(f(4, 3), 2)\n        self.failUnlessEqual(f(5, 3), 2)\n        self.failUnlessEqual(f(6, 3), 2)\n        self.failUnlessEqual(f(7, 3), 3)\n        self.failUnless(isinstance(f(0.0, 1), int))\n        self.failUnlessEqual(f(7.0, 3.0), 3)\n        self.failUnlessEqual(f(7, 3.0), 3)\n        self.failUnlessEqual(f(7.0, 3), 3)\n        self.failUnlessEqual(f(6.0, 3.0), 2)\n        self.failUnlessEqual(f(6.0, 3), 2)\n        self.failUnlessEqual(f(6, 3.0), 2)\n\n    def test_next_multiple(self):\n        f = mathutil.next_multiple\n        self.failUnlessEqual(f(5, 1), 5)\n        self.failUnlessEqual(f(5, 2), 6)\n        self.failUnlessEqual(f(5, 3), 6)\n        self.failUnlessEqual(f(5, 4), 8)\n        self.failUnlessEqual(f(5, 5), 5)\n        self.failUnlessEqual(f(5, 6), 6)\n        self.failUnlessEqual(f(32, 1), 32)\n        self.failUnlessEqual(f(32, 2), 32)\n        self.failUnlessEqual(f(32, 3), 33)\n        self.failUnlessEqual(f(32, 4), 32)\n        self.failUnlessEqual(f(32, 5), 35)\n        self.failUnlessEqual(f(32, 6), 36)\n        self.failUnlessEqual(f(32, 7), 35)\n        self.failUnlessEqual(f(32, 8), 32)\n        self.failUnlessEqual(f(32, 9), 36)\n        self.failUnlessEqual(f(32, 10), 40)\n        self.failUnlessEqual(f(32, 11), 33)\n        self.failUnlessEqual(f(32, 12), 36)\n        self.failUnlessEqual(f(32, 13), 39)\n        self.failUnlessEqual(f(32, 14), 42)\n        self.failUnlessEqual(f(32, 15), 45)\n        self.failUnlessEqual(f(32, 16), 32)\n        self.failUnlessEqual(f(32, 17), 34)\n        self.failUnlessEqual(f(32, 18), 36)\n        self.failUnlessEqual(f(32, 589), 589)\n\n    def test_pad_size(self):\n        f = mathutil.pad_size\n        self.failUnlessEqual(f(0, 4), 0)\n        self.failUnlessEqual(f(1, 4), 3)\n        self.failUnlessEqual(f(2, 4), 2)\n        self.failUnlessEqual(f(3, 4), 1)\n        self.failUnlessEqual(f(4, 4), 0)\n        self.failUnlessEqual(f(5, 4), 3)\n\n    def test_is_power_of_k_part_2(self):\n        f = mathutil.is_power_of_k\n        for i in range(1, 100):\n            if i in (1, 2, 4, 8, 16, 32, 64):\n                self.failUnless(f(i, 2), \"but %d *is* a power of 2\" % i)\n            else:\n                self.failIf(f(i, 2), \"but %d is *not* a power of 2\" % i)\n        for i in range(1, 100):\n            if i in (1, 3, 9, 27, 81):\n                self.failUnless(f(i, 3), \"but %d *is* a power of 3\" % i)\n            else:\n                self.failIf(f(i, 3), \"but %d is *not* a power of 3\" % i)\n\n    def test_next_power_of_k(self):\n        f = mathutil.next_power_of_k\n        self.failUnlessEqual(f(0,2), 1)\n        self.failUnlessEqual(f(1,2), 1)\n        self.failUnlessEqual(f(2,2), 2)\n        self.failUnlessEqual(f(3,2), 4)\n        self.failUnlessEqual(f(4,2), 4)\n        for i in range(5, 8): self.failUnlessEqual(f(i,2), 8, \"%d\" % i)\n        for i in range(9, 16): self.failUnlessEqual(f(i,2), 16, \"%d\" % i)\n        for i in range(17, 32): self.failUnlessEqual(f(i,2), 32, \"%d\" % i)\n        for i in range(33, 64): self.failUnlessEqual(f(i,2), 64, \"%d\" % i)\n        for i in range(65, 100): self.failUnlessEqual(f(i,2), 128, \"%d\" % i)\n\n        self.failUnlessEqual(f(0,3), 1)\n        self.failUnlessEqual(f(1,3), 1)\n        self.failUnlessEqual(f(2,3), 3)\n        self.failUnlessEqual(f(3,3), 3)\n        for i in range(4, 9): self.failUnlessEqual(f(i,3), 9, \"%d\" % i)\n        for i in range(10, 27): self.failUnlessEqual(f(i,3), 27, \"%d\" % i)\n        for i in range(28, 81): self.failUnlessEqual(f(i,3), 81, \"%d\" % i)\n        for i in range(82, 200): self.failUnlessEqual(f(i,3), 243, \"%d\" % i)\n\n    def test_ave(self):\n        f = mathutil.ave\n        self.failUnlessEqual(f([1,2,3]), 2)\n        self.failUnlessEqual(f([0,0,0,4]), 1)\n        self.failUnlessAlmostEqual(f([0.0, 1.0, 1.0]), .666666666666)\n\n    def failUnlessEqualContents(self, a, b):\n        self.failUnlessEqual(sorted(a), sorted(b))\n\n    def test_permute(self):\n        f = mathutil.permute\n        self.failUnlessEqualContents(f([]), [])\n        self.failUnlessEqualContents(f([1]), [[1]])\n        self.failUnlessEqualContents(f([1,2]), [[1,2], [2,1]])\n        self.failUnlessEqualContents(f([1,2,3]),\n                                     [[1,2,3], [1,3,2],\n                                      [2,1,3], [2,3,1],\n                                      [3,1,2], [3,2,1]])\n", "license": "gpl-3.0"}
{"id": "c49f8b88ea128d5e8fd12ba1ea65dcc278b78d07", "path": "kbe/res/scripts/common/Lib/turtledemo/fractalcurves.py", "repo_name": "vikatory/kbengine", "content": "#!/usr/bin/env python3\n\"\"\"      turtle-example-suite:\n\n        tdemo_fractalCurves.py\n\nThis program draws two fractal-curve-designs:\n(1) A hilbert curve (in a box)\n(2) A combination of Koch-curves.\n\nThe CurvesTurtle class and the fractal-curve-\nmethods are taken from the PythonCard example\nscripts for turtle-graphics.\n\"\"\"\nfrom turtle import *\nfrom time import sleep, clock\n\nclass CurvesTurtle(Pen):\n    # example derived from\n    # Turtle Geometry: The Computer as a Medium for Exploring Mathematics\n    # by Harold Abelson and Andrea diSessa\n    # p. 96-98\n    def hilbert(self, size, level, parity):\n        if level == 0:\n            return\n        # rotate and draw first subcurve with opposite parity to big curve\n        self.left(parity * 90)\n        self.hilbert(size, level - 1, -parity)\n        # interface to and draw second subcurve with same parity as big curve\n        self.forward(size)\n        self.right(parity * 90)\n        self.hilbert(size, level - 1, parity)\n        # third subcurve\n        self.forward(size)\n        self.hilbert(size, level - 1, parity)\n        # fourth subcurve\n        self.right(parity * 90)\n        self.forward(size)\n        self.hilbert(size, level - 1, -parity)\n        # a final turn is needed to make the turtle\n        # end up facing outward from the large square\n        self.left(parity * 90)\n\n    # Visual Modeling with Logo: A Structural Approach to Seeing\n    # by James Clayson\n    # Koch curve, after Helge von Koch who introduced this geometric figure in 1904\n    # p. 146\n    def fractalgon(self, n, rad, lev, dir):\n        import math\n\n        # if dir = 1 turn outward\n        # if dir = -1 turn inward\n        edge = 2 * rad * math.sin(math.pi / n)\n        self.pu()\n        self.fd(rad)\n        self.pd()\n        self.rt(180 - (90 * (n - 2) / n))\n        for i in range(n):\n            self.fractal(edge, lev, dir)\n            self.rt(360 / n)\n        self.lt(180 - (90 * (n - 2) / n))\n        self.pu()\n        self.bk(rad)\n        self.pd()\n\n    # p. 146\n    def fractal(self, dist, depth, dir):\n        if depth < 1:\n            self.fd(dist)\n            return\n        self.fractal(dist / 3, depth - 1, dir)\n        self.lt(60 * dir)\n        self.fractal(dist / 3, depth - 1, dir)\n        self.rt(120 * dir)\n        self.fractal(dist / 3, depth - 1, dir)\n        self.lt(60 * dir)\n        self.fractal(dist / 3, depth - 1, dir)\n\ndef main():\n    ft = CurvesTurtle()\n\n    ft.reset()\n    ft.speed(0)\n    ft.ht()\n    ft.getscreen().tracer(1,0)\n    ft.pu()\n\n    size = 6\n    ft.setpos(-33*size, -32*size)\n    ft.pd()\n\n    ta=clock()\n    ft.fillcolor(\"red\")\n    ft.begin_fill()\n    ft.fd(size)\n\n    ft.hilbert(size, 6, 1)\n\n    # frame\n    ft.fd(size)\n    for i in range(3):\n        ft.lt(90)\n        ft.fd(size*(64+i%2))\n    ft.pu()\n    for i in range(2):\n        ft.fd(size)\n        ft.rt(90)\n    ft.pd()\n    for i in range(4):\n        ft.fd(size*(66+i%2))\n        ft.rt(90)\n    ft.end_fill()\n    tb=clock()\n    res =  \"Hilbert: %.2fsec. \" % (tb-ta)\n\n    sleep(3)\n\n    ft.reset()\n    ft.speed(0)\n    ft.ht()\n    ft.getscreen().tracer(1,0)\n\n    ta=clock()\n    ft.color(\"black\", \"blue\")\n    ft.begin_fill()\n    ft.fractalgon(3, 250, 4, 1)\n    ft.end_fill()\n    ft.begin_fill()\n    ft.color(\"red\")\n    ft.fractalgon(3, 200, 4, -1)\n    ft.end_fill()\n    tb=clock()\n    res +=  \"Koch: %.2fsec.\" % (tb-ta)\n    return res\n\nif __name__  == '__main__':\n    msg = main()\n    print(msg)\n    mainloop()\n", "license": "lgpl-3.0"}
{"id": "2496d16c4683cfa18e4a93a25048c4f861866398", "path": "bin/daemon/nfssync.py", "repo_name": "itay-moav/talis", "content": "#!/usr/local/bin/python2.7\n\nimport sys\nimport os\nimport time\nimport logging\nimport smtplib\nimport socket\n\nfrom sitel.environment import ActiveMQ\nfrom sitel.environment import proxy_email\n\nfrom stompest.config import StompConfig\nfrom stompest.protocol import StompSpec\nfrom stompest.sync import Stomp\nfrom subprocess import call\nfrom daemon import runner\nfrom logging.handlers import TimedRotatingFileHandler\n\n# HOST = \"localhost\"\n# PORT = 61613\n# TOPIC = \"filesync\"\n# SRCDIR = \"/mnt/nfs_files\"\n# DSTDIR = \"/var/www/files\"\n\nLOGFILE = '/var/log/lms2/daemon_nfssync.log'\nPIDFILE = '/tmp/nfssync.pid'\nSTDIN = '/dev/null'\nSTDOUT = '/dev/tty'\nSTDERR = '/dev/tty'\nTIMEOUT = 5\n\nprocessgid=0 # 1048 apache, run as root for now\nprocessuid=0 # 1048 apache, run as root for now\n\nhostname=socket.gethostname()\n\nemail_subject_warning = \"NFSSYNC warning on %s\" % hostname\nemail_subject_error = \"NFSSYNC error on %s\" % hostname\n\nclass LmsSync():\n    def __init__(self):\n        self.stdin_path = STDIN\n        self.stdout_path = STDOUT\n        self.stderr_path = STDERR\n        self.pidfile_path =  PIDFILE\n        self.pidfile_timeout = TIMEOUT\n\n    def run(self):\n        if(processgid > 0): os.setgid(processgid)\n        if(processuid > 0): os.setuid(processuid)\n        config = StompConfig('tcp://%(HOST)s:%(PORT)s' % ActiveMQ)\n        topic = \"/topic/%(FILESYNCTOPIC)s\" % ActiveMQ\n        client = Stomp(config)\n        self.logger = logging.getLogger('nfssync') \n        self.logger.setLevel(logging.DEBUG)\n        handler = TimedRotatingFileHandler(LOGFILE, when='midnight', interval=1, backupCount=30)\n        formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')\n        handler.setFormatter(formatter)\n        self.logger.addHandler(handler)\n\n        try:\n            client.connect()\n            client.subscribe(topic, {StompSpec.ACK_HEADER: StompSpec.ACK_CLIENT_INDIVIDUAL})\n\n            while True:\n                frame = client.receiveFrame()\n                subdir = frame.body\n\n                srcpath = os.path.join(ActiveMQ['SRCDIR'], subdir)\n                dstpath = os.path.dirname(os.path.join(ActiveMQ['DSTDIR'], subdir))\n                self.logger.info(\"Syncing %s to %s\" % (srcpath,dstpath))\n                if(not os.path.exists(srcpath)):\n                    msg = \"Source %s does not exist\" % srcpath\n                    self.logger.error(msg)\n                    proxy_email(email_subject_error, msg)\n                    client.ack(frame)\n                    continue\n                elif(not os.path.isdir(srcpath)):\n                    msg = \"Source %s is not a directory\" % srcpath\n                    self.logger.error(msg)\n                    proxy_email(email_subject_error, msg)\n                    client.ack(frame)\n                    continue\n                elif(not os.path.exists(dstpath)):\n                    msg = \"Destination %s does not exist\" % dstpath\n                    self.logger.warning(msg)\n                    proxy_email(email_subject_warning, msg)\n                    os.umask(0)\n                    os.makedirs(dstpath, 0777)\n                cmd = \"rsync -avzq --delete %s %s\" % (srcpath,dstpath)\n                if(call(cmd, shell=True) > 0):\n                    msg = \"Sync %s failed\" % cmd\n                    self.logger.error(msg)\n                    proxy_email(email_subject, msg)\n                client.ack(frame)\n                    \n        except Exception, e:\n            msg = \"Exception in %s: %s\" % (sys.argv[0], str(e))\n            self.logger.error(msg)\n            proxy_email(email_subject_error, msg)\n            exit(1)\n            \nif(sys.argv[1]==\"status\"):\n    if(os.path.exists(PIDFILE)):\n        pid = int(open(PIDFILE,\"r\").read())\n        try:\n            os.kill(pid,0)\n            exit(0)\n        except Exception, e:\n            exit(1)\n    else:\n        exit(1)\n        \napp = LmsSync()\ndaemon_runner = runner.DaemonRunner(app)\ndaemon_runner.do_action()\n", "license": "gpl-3.0"}
{"id": "99ff4ba3033e37bafcf2329b8efb976dfe6395b0", "path": "bddtests/steps/docgen.py", "repo_name": "tkuhrt/fabric", "content": "# Copyright IBM Corp. 2016 All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#      http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n#\n\nfrom StringIO import StringIO\nfrom itertools import chain\nfrom google.protobuf.message import Message\n\nfrom b3j0f.aop import weave, unweave, is_intercepted, weave_on\n\nfrom jinja2 import Environment, PackageLoader, select_autoescape, FileSystemLoader, Template\nenv = Environment(\n    loader=FileSystemLoader(searchpath=\"templates\"),\n    autoescape=select_autoescape(['html', 'xml']),\n    trim_blocks=True,\n    lstrip_blocks=True\n)\n\nfrom bootstrap_util import getDirectory\n\nclass DocumentGenerator:\n\n\n    def __init__(self, contextHelper, scenario):\n        self.contextHelper = contextHelper\n        self.directory = getDirectory(contextHelper.context)\n        self.output = StringIO()\n        self.currentStep = 0\n        self.composition = None\n\n        #Weave advices into contextHelper\n        weave(target=self.contextHelper.before_step, advices=self.beforeStepAdvice)\n        weave(target=self.contextHelper.after_step, advices=self.afterStepAdvice)\n        weave(target=self.contextHelper.after_scenario, advices=self.afterScenarioAdvice)\n        weave(target=self.contextHelper.getBootrapHelper, advices=self.getBootstrapHelperAdvice)\n        weave(target=self.contextHelper.registerComposition, advices=self.registerCompositionAdvice)\n\n        # Weave advices into Directory\n        weave(target=self.directory._registerOrg, advices=self.registerOrgAdvice)\n        weave(target=self.directory._registerUser, advices=self.registerUserAdvice)\n        weave(target=self.directory.registerOrdererAdminTuple, advices=self.registerNamedNodeAdminTupleAdvice)\n\n    def beforeStepAdvice(self, joinpoint):\n        self.currentStep += 1\n        step = joinpoint.kwargs['step']\n        # Now the jinja template\n        self.output.write(env.get_template(\"html/step.html\").render(step_id=\"Step {0}\".format(self.currentStep), step=step))\n        return joinpoint.proceed()\n\n    def afterStepAdvice(self, joinpoint):\n        step = joinpoint.kwargs['step']\n        # Now the jinja template\n        if step.status==\"failed\":\n            self.output.write(env.get_template(\"html/error.html\").render(err=step.error_message))\n        return joinpoint.proceed()\n\n\n    def compositionCallCLIAdvice(self, joinpoint):\n        'This advice is called around the compositions usage of the cli'\n        result = joinpoint.proceed()\n        # Create table for environment\n        composition = joinpoint.kwargs['self']\n        envAdditions = composition.getEnvAdditions()\n        keys = envAdditions.keys()\n        keys.sort()\n        envPreamble = \" \".join([\"{0}={1}\".format(key,envAdditions[key]) for key in keys])\n        args= \" \".join(joinpoint.kwargs['argList'])\n        self.output.write(env.get_template(\"html/cli.html\").render(command=\"{0} {1}\".format(envPreamble, args)))\n        return result\n\n    def _getNetworkGroup(self, serviceName):\n        groups = {\"peer\" : 1, \"orderer\" : 2, \"kafka\" : 7, \"zookeeper\" : 8, \"couchdb\" : 9}\n        groupId = 0\n        for group, id in groups.iteritems():\n            if serviceName.lower().startswith(group):\n                groupId = id\n        return groupId\n\n    def _getNetworkForConfig(self, configAsYaml):\n        import yaml\n        config = yaml.load(configAsYaml)\n        assert \"services\" in config, \"Expected config from docker-compose config to have services key at top level:  \\n{0}\".format(config)\n        network = {\"nodes\": [], \"links\" : []}\n        for serviceName in config['services'].keys():\n            network['nodes'].append({\"id\" : serviceName, \"group\" : self._getNetworkGroup(serviceName), \"type\" : \"node\"})\n            # Now get links\n            if \"depends_on\" in config['services'][serviceName]:\n                for dependedOnServiceName in config['services'][serviceName]['depends_on']:\n                    network['links'].append({\"source\": serviceName, \"target\": dependedOnServiceName, \"value\" : 1})\n        return network\n\n    def _getNetworkForDirectory(self):\n        network = {\"nodes\":[], \"links\": []}\n        for orgName, org in self.directory.getOrganizations().iteritems():\n            network['nodes'].append({\"id\" : orgName, \"group\" : 3, \"type\" : \"org\"})\n        for userName, user in self.directory.getUsers().iteritems():\n            network['nodes'].append({\"id\" : userName, \"group\" : 4, \"type\" : \"user\"})\n        # Now get links\n        for nct, cert in self.directory.getNamedCtxTuples().iteritems():\n            nctId = \"{0}-{1}-{2}\".format(nct.user, nct.nodeName, nct.organization)\n            network['nodes'].append({\"id\" : nctId, \"group\" : 5, \"type\" : \"cert\"})\n            network['links'].append({\"source\": nctId, \"target\": nct.organization, \"value\" : 1})\n            network['links'].append({\"source\": nctId, \"target\": nct.user, \"value\" : 1})\n            # Only add the context link if it is a compose service, else the target may not exist.\n            if nct.nodeName in self.composition.getServiceNames():\n                network['links'].append({\"source\": nctId, \"target\": nct.nodeName, \"value\" : 1})\n        return network\n\n    def _writeNetworkJson(self):\n        if self.composition:\n            import json\n            configNetwork = self._getNetworkForConfig(configAsYaml=self.composition.getConfig())\n            directoryNetwork = self._getNetworkForDirectory()\n            # Join the network info together\n            fullNetwork = dict(chain([(key, configNetwork[key] + directoryNetwork[key]) for key in configNetwork.keys()]))\n            (fileName, fileExists) = self.contextHelper.getTmpPathForName(\"network\", extension=\"json\")\n            with open(fileName, \"w\") as f:\n                f.write(json.dumps(fullNetwork))\n\n\n    def registerCompositionAdvice(self, joinpoint):\n        composition = joinpoint.kwargs['composition']\n        weave(target=composition._callCLI, advices=self.compositionCallCLIAdvice)\n        result = joinpoint.proceed()\n        if composition:\n            #Now get the config for the composition and dump out.\n            self.composition = composition\n            configAsYaml = composition.getConfig()\n            (dokerComposeYmlFileName, fileExists) = self.contextHelper.getTmpPathForName(name=\"docker-compose\", extension=\"yml\")\n            with open(dokerComposeYmlFileName, 'w') as f:\n                f.write(configAsYaml)\n            self.output.write(env.get_template(\"html/composition-py.html\").render(compose_project_name= self.composition.projectName,docker_compose_yml_file=dokerComposeYmlFileName))\n            self.output.write(env.get_template(\"html/header.html\").render(text=\"Configuration\", level=4))\n            self.output.write(env.get_template(\"html/cli.html\").render(command=configAsYaml))\n            #Inject the graph\n            self.output.write(env.get_template(\"html/header.html\").render(text=\"Network Graph\", level=4))\n            self.output.write(env.get_template(\"html/graph.html\").render())\n        return result\n\n    def _addLinkToFile(self, fileName ,linkText):\n        import ntpath\n        baseName = ntpath.basename(fileName)\n        # self.markdownWriter.addLink(linkUrl=\"./{0}\".format(baseName), linkText=linkText, linkTitle=baseName)\n\n    def _getLinkInfoForFile(self, fileName):\n        import ntpath\n        return \"./{0}\".format(ntpath.basename(fileName))\n\n    def registerOrgAdvice(self, joinpoint):\n        orgName = joinpoint.kwargs['orgName']\n        newlyRegisteredOrg = joinpoint.proceed()\n        orgCert = newlyRegisteredOrg.getCertAsPEM()\n        #Write out key material\n        (fileName, fileExists) = self.contextHelper.getTmpPathForName(name=\"dir-org-{0}-cert\".format(orgName), extension=\"pem\")\n        with open(fileName, 'w') as f:\n            f.write(orgCert)\n        self._addLinkToFile(fileName=fileName, linkText=\"Public cert for Organization\")\n        #Now the jinja output\n        self.output.write(env.get_template(\"html/org.html\").render(org=newlyRegisteredOrg, cert_href=self._getLinkInfoForFile(fileName), path_to_cert=fileName))\n        return newlyRegisteredOrg\n\n    def registerUserAdvice(self, joinpoint):\n        userName = joinpoint.kwargs['userName']\n        newlyRegisteredUser = joinpoint.proceed()\n        #Write out key material\n        privateKeyAsPem = newlyRegisteredUser.getPrivateKeyAsPEM()\n        (fileName, fileExists) = self.contextHelper.getTmpPathForName(name=\"dir-user-{0}-privatekey\".format(userName), extension=\"pem\")\n        with open(fileName, 'w') as f:\n            f.write(privateKeyAsPem)\n        #Weave into user tags setting\n        weave(target=newlyRegisteredUser.setTagValue, advices=self.userSetTagValueAdvice)\n        #Now the jinja output\n        self.output.write(env.get_template(\"html/user.html\").render(user=newlyRegisteredUser, private_key_href=self._getLinkInfoForFile(fileName)))\n        return newlyRegisteredUser\n\n    def _dump_context(self):\n        (dirPickleFileName, fileExists) = self.contextHelper.getTmpPathForName(\"dir\", extension=\"pickle\")\n        with open(dirPickleFileName, 'w') as f:\n            self.directory.dump(f)\n        #Now the jinja output\n        self.output.write(env.get_template(\"html/directory.html\").render(directory=self.directory, path_to_pickle=dirPickleFileName))\n        if self.composition:\n            (dokerComposeYmlFileName, fileExists) = self.contextHelper.getTmpPathForName(name=\"docker-compose\", extension=\"yml\")\n            self.output.write(env.get_template(\"html/appendix-py.html\").render(directory=self.directory,\n                                                                               path_to_pickle=dirPickleFileName,\n                                                                               compose_project_name=self.composition.projectName,\n                                                                               docker_compose_yml_file=dokerComposeYmlFileName))\n\n\n    def afterScenarioAdvice(self, joinpoint):\n        scenario = joinpoint.kwargs['scenario']\n        self._dump_context()\n        #Render with jinja\n        header = env.get_template(\"html/scenario.html\").render(scenario=scenario, steps=scenario.steps)\n        main = env.get_template(\"html/main.html\").render(header=header, body=self.output.getvalue())\n        (fileName, fileExists) = self.contextHelper.getTmpPathForName(\"scenario\", extension=\"html\")\n        with open(fileName, 'w') as f:\n            f.write(main.encode(\"utf-8\"))\n        self._writeNetworkJson()\n        return joinpoint.proceed()\n\n    def registerNamedNodeAdminTupleAdvice(self, joinpoint):\n        namedNodeAdminTuple = joinpoint.proceed()\n        directory = joinpoint.kwargs['self']\n        #jinja\n        newCertAsPEM = directory.getCertAsPEM(namedNodeAdminTuple)\n        self.output.write(env.get_template(\"html/header.html\").render(text=\"Created new named node admin tuple: {0}\".format(namedNodeAdminTuple), level=4))\n        self.output.write(env.get_template(\"html/cli.html\").render(command=newCertAsPEM))\n        #Write cert out\n        fileNameTocheck = \"dir-user-{0}-cert-{1}-{2}\".format(namedNodeAdminTuple.user, namedNodeAdminTuple.nodeName, namedNodeAdminTuple.organization)\n        (fileName, fileExists) = self.contextHelper.getTmpPathForName(fileNameTocheck, extension=\"pem\")\n        with open(fileName, 'w') as f:\n            f.write(newCertAsPEM)\n        return namedNodeAdminTuple\n\n    def bootstrapHelperSignConfigItemAdvice(self, joinpoint):\n        configItem = joinpoint.kwargs['configItem']\n        #jinja\n        self.output.write(env.get_template(\"html/header.html\").render(text=\"Dumping signed config item...\", level=4))\n        self.output.write(env.get_template(\"html/protobuf.html\").render(msg=configItem, msgLength=len(str(configItem))))\n\n        signedConfigItem = joinpoint.proceed()\n        return signedConfigItem\n\n    def getBootstrapHelperAdvice(self, joinpoint):\n        bootstrapHelper = joinpoint.proceed()\n        weave(target=bootstrapHelper.signConfigItem, advices=self.bootstrapHelperSignConfigItemAdvice)\n        return bootstrapHelper\n\n    def _isProtobufMessage(self, target):\n        return isinstance(target, Message)\n\n    def _isListOfProtobufMessages(self, target):\n        result = False\n        if isinstance(target, list):\n            messageList = [item for item in target if self._isProtobufMessage(item)]\n            result = len(messageList) == len(target)\n        return result\n\n    def _isDictOfProtobufMessages(self, target):\n        result = False\n        if isinstance(target, dict):\n            messageList = [item for item in target.values() if self._isProtobufMessage(item)]\n            result = len(messageList) == len(target)\n        return result\n\n    def _writeProtobuf(self, fileName, msg):\n        import ntpath\n        baseName = ntpath.basename(fileName)\n        dataToWrite = msg.SerializeToString()\n        with open(\"{0}\".format(fileName), 'wb') as f:\n            f.write(dataToWrite)\n        self.output.write(env.get_template(\"html/protobuf.html\").render(id=baseName, msg=msg, path_to_protobuf=fileName, msgLength=len(dataToWrite),linkUrl=\"./{0}\".format(baseName), linkText=\"Protobuf message in binary form\", linkTitle=baseName))\n\n\n    def userSetTagValueAdvice(self, joinpoint):\n        result = joinpoint.proceed()\n        user = joinpoint.kwargs['self']\n        tagKey = joinpoint.kwargs['tagKey']\n        tagValue = joinpoint.kwargs['tagValue']\n\n        #jinja invoke\n        self.output.write(env.get_template(\"html/tag.html\").render(user=user, tag_key=tagKey))\n\n        # If protobuf message, write out in binary form\n        if self._isProtobufMessage(tagValue):\n            import ntpath\n            (fileName, fileExists) = self.contextHelper.getTmpPathForName(\"{0}-{1}\".format(user.getUserName(), tagKey), extension=\"protobuf\")\n            self._writeProtobuf(fileName=fileName, msg=tagValue)\n        # If protobuf message, write out in binary form\n        elif self._isListOfProtobufMessages(tagValue):\n            index = 0\n            for msg in tagValue:\n                (fileName, fileExists) = self.contextHelper.getTmpPathForName(\"{0}-{1}-{2:0>4}\".format(user.getUserName(), tagKey, index), extension=\"protobuf\")\n                self._writeProtobuf(fileName=fileName, msg=msg)\n                index += 1\n        elif self._isDictOfProtobufMessages(tagValue):\n            for key,msg in tagValue.iteritems():\n                (fileName, fileExists) = self.contextHelper.getTmpPathForName(\"{0}-{1}-{2}\".format(user.getUserName(), tagKey, key), extension=\"protobuf\")\n                self._writeProtobuf(fileName=fileName, msg=msg)\n        else:\n            self.output.write(env.get_template(\"html/cli.html\").render(command=str(tagValue)))\n        return result", "license": "apache-2.0"}
{"id": "341c2aa2d8c7c60f937fadac5f2ce2c0c8b441db", "path": "lib/ansible/plugins/filter/mathstuff.py", "repo_name": "firmanm/ansible", "content": "# (c) 2014, Brian Coca <bcoca@ansible.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\nfrom __future__ import absolute_import\n\nimport math\nimport collections\nfrom ansible import errors\n\ndef unique(a):\n    if isinstance(a,collections.Hashable):\n        c = set(a)\n    else:\n        c = []\n        for x in a:\n            if x not in c:\n                c.append(x)\n    return c\n\ndef intersect(a, b):\n    if isinstance(a,collections.Hashable) and isinstance(b,collections.Hashable):\n        c = set(a) & set(b)\n    else:\n        c = unique(filter(lambda x: x in b, a))\n    return c\n\ndef difference(a, b):\n    if isinstance(a,collections.Hashable) and isinstance(b,collections.Hashable):\n        c = set(a) - set(b)\n    else:\n        c = unique(filter(lambda x: x not in b, a))\n    return c\n\ndef symmetric_difference(a, b):\n    if isinstance(a,collections.Hashable) and isinstance(b,collections.Hashable):\n        c = set(a) ^ set(b)\n    else:\n        c = unique(filter(lambda x: x not in intersect(a,b), union(a,b)))\n    return c\n\ndef union(a, b):\n    if isinstance(a,collections.Hashable) and isinstance(b,collections.Hashable):\n        c = set(a) | set(b)\n    else:\n        c = unique(a + b)\n    return c\n\ndef min(a):\n    _min = __builtins__.get('min')\n    return _min(a);\n\ndef max(a):\n    _max = __builtins__.get('max')\n    return _max(a);\n\ndef isnotanumber(x):\n    try:\n        return math.isnan(x)\n    except TypeError:\n        return False\n\n\ndef logarithm(x, base=math.e):\n    try:\n        if base == 10:\n            return math.log10(x)\n        else:\n            return math.log(x, base)\n    except TypeError as e:\n        raise errors.AnsibleFilterError('log() can only be used on numbers: %s' % str(e))\n\n\ndef power(x, y):\n    try:\n        return math.pow(x, y)\n    except TypeError as e:\n        raise errors.AnsibleFilterError('pow() can only be used on numbers: %s' % str(e))\n\n\ndef inversepower(x, base=2):\n    try:\n        if base == 2:\n            return math.sqrt(x)\n        else:\n            return math.pow(x, 1.0/float(base))\n    except TypeError as e:\n        raise errors.AnsibleFilterError('root() can only be used on numbers: %s' % str(e))\n\n\ndef human_readable(size, isbits=False, unit=None):\n\n    base = 'bits' if isbits else 'Bytes'\n    suffix = ''\n\n    ranges = (\n            (1<<70, 'Z'),\n            (1<<60, 'E'),\n            (1<<50, 'P'),\n            (1<<40, 'T'),\n            (1<<30, 'G'),\n            (1<<20, 'M'),\n            (1<<10, 'K'),\n            (1, base)\n        )\n\n    for limit, suffix in ranges:\n        if (unit is None and size >= limit) or \\\n            unit is not None and unit.upper() == suffix:\n            break\n\n    if limit != 1:\n        suffix += base[0]\n\n    return '%.2f %s' % (float(size)/ limit, suffix)\n\nclass FilterModule(object):\n    ''' Ansible math jinja2 filters '''\n\n    def filters(self):\n        return {\n            # general math\n            'isnan': isnotanumber,\n            'min' : min,\n            'max' : max,\n\n            # exponents and logarithms\n            'log': logarithm,\n            'pow': power,\n            'root': inversepower,\n\n            # set theory\n            'unique' : unique,\n            'intersect': intersect,\n            'difference': difference,\n            'symmetric_difference': symmetric_difference,\n            'union': union,\n\n            # computer theory\n            'human_readable' : human_readable,\n\n        }\n", "license": "gpl-3.0"}
{"id": "36eab3379671c09104eb5facd70af291f6dadebe", "path": "src/qt/qtwebkit/Tools/QueueStatusServer/handlers/svnrevision.py", "repo_name": "RobertoMalatesta/phantomjs", "content": "# Copyright (C) 2009 Google Inc. All rights reserved.\n#\n# Redistribution and use in source and binary forms, with or without\n# modification, are permitted provided that the following conditions are\n# met:\n# \n#     * Redistributions of source code must retain the above copyright\n# notice, this list of conditions and the following disclaimer.\n#     * Redistributions in binary form must reproduce the above\n# copyright notice, this list of conditions and the following disclaimer\n# in the documentation and/or other materials provided with the\n# distribution.\n#     * Neither the name of Google Inc. nor the names of its\n# contributors may be used to endorse or promote products derived from\n# this software without specific prior written permission.\n# \n# THIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS\n# \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT\n# LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR\n# A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT\n# OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL,\n# SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT\n# LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE,\n# DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY\n# THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n# (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE\n# OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\nfrom google.appengine.ext import webapp\n\nimport model\n\n\nclass SVNRevision(webapp.RequestHandler):\n    def get(self, svn_revision_number):\n        svn_revisions = model.SVNRevision.all().filter('number =', int(svn_revision_number)).order('-date').fetch(1)\n        if not svn_revisions:\n            self.error(404)\n            return\n        self.response.out.write(svn_revisions[0].to_xml())\n", "license": "bsd-3-clause"}
{"id": "412cbcbc6a11c5cf69f5dbba888beb8997dd83ec", "path": "tests/issue_submitter_tests.py", "repo_name": "ArthurGarnier/SickRage", "content": "# coding=UTF-8\n# Author: Dennis Lutter <lad1337@gmail.com>\n# URL: https://sickrage.github.io\n#\n# This file is part of SickRage.\n#\n# SickRage is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# SickRage is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with SickRage. If not, see <http://www.gnu.org/licenses/>.\n\n\"\"\"\nTest exception logging\n\"\"\"\n\nfrom __future__ import print_function, unicode_literals\nimport os.path\nimport sys\nimport unittest\n\nsys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname(__file__), '../lib')))\nsys.path.insert(1, os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n\nfrom sickbeard import logger, ex\n\n\ndef exception_generator():\n    \"\"\"\n    Dummy function to raise a fake exception and log it\n    \"\"\"\n    try:\n        raise Exception('FAKE EXCEPTION')\n    except Exception as error:\n        logger.log(\"FAKE ERROR: \" + ex(error), logger.ERROR)  # pylint: disable=no-member\n        logger.submit_errors()  # pylint: disable=no-member\n        raise\n\n\nclass IssueSubmitterBasicTests(unittest.TestCase):\n    \"\"\"\n    Tests logging of exceptions\n    \"\"\"\n    def test_submitter(self):\n        \"\"\"\n        Test that an exception is raised\n        \"\"\"\n        self.assertRaises(Exception, exception_generator)\n\n\nif __name__ == \"__main__\":\n    print(\"==================\")\n    print(\"STARTING - ISSUE SUBMITTER TESTS\")\n    print(\"==================\")\n    print(\"######################################################################\")\n\n    SUITE = unittest.TestLoader().loadTestsFromTestCase(IssueSubmitterBasicTests)\n    unittest.TextTestRunner(verbosity=2).run(SUITE)\n", "license": "gpl-3.0"}
{"id": "95c9c38c009ab835a3c0c49c9e04f317e7e3e53f", "path": "serverfiles/usr/local/lib/networkx-1.6/networkx/algorithms/tests/test_vitality.py", "repo_name": "ChristianKniep/QNIB", "content": "#!/usr/bin/env python\n\nfrom nose.tools import *\nimport networkx as nx\n\nclass TestVitality:\n\n    def test_closeness_vitality_unweighted(self):\n        G=nx.cycle_graph(3)\n        v=nx.closeness_vitality(G)\n        assert_equal(v,{0:4.0, 1:4.0, 2:4.0})\n\n    def test_closeness_vitality_weighted(self):\n        G=nx.Graph()\n        G.add_cycle([0,1,2],weight=2)\n        v=nx.closeness_vitality(G,weight='weight')\n        assert_equal(v,{0:8.0, 1:8.0, 2:8.0})\n\n    def test_closeness_vitality_unweighted_digraph(self):\n        G=nx.DiGraph()\n        G.add_cycle([0,1,2])\n        v=nx.closeness_vitality(G)\n        assert_equal(v,{0:8.0, 1:8.0, 2:8.0})\n\n    def test_closeness_vitality_weighted_digraph(self):\n        G=nx.DiGraph()\n        G.add_cycle([0,1,2],weight=2)\n        v=nx.closeness_vitality(G,weight='weight')\n        assert_equal(v,{0:16.0, 1:16.0, 2:16.0})\n\n\n", "license": "gpl-2.0"}
{"id": "1ddb2447feb26ebb8d175a3b1c24f408fe23b90e", "path": "tests/get_or_create/tests.py", "repo_name": "deployed/django", "content": "from __future__ import unicode_literals\n\nfrom datetime import date\nimport traceback\nimport warnings\n\nfrom django.db import IntegrityError, DatabaseError\nfrom django.utils.encoding import DjangoUnicodeDecodeError\nfrom django.test import TestCase, TransactionTestCase\n\nfrom .models import DefaultPerson, Person, ManualPrimaryKeyTest, Profile, Tag, Thing\n\n\nclass GetOrCreateTests(TestCase):\n\n    def test_get_or_create(self):\n        p = Person.objects.create(\n            first_name='John', last_name='Lennon', birthday=date(1940, 10, 9)\n        )\n\n        p, created = Person.objects.get_or_create(\n            first_name=\"John\", last_name=\"Lennon\", defaults={\n                \"birthday\": date(1940, 10, 9)\n            }\n        )\n        self.assertFalse(created)\n        self.assertEqual(Person.objects.count(), 1)\n\n        p, created = Person.objects.get_or_create(\n            first_name='George', last_name='Harrison', defaults={\n                'birthday': date(1943, 2, 25)\n            }\n        )\n        self.assertTrue(created)\n        self.assertEqual(Person.objects.count(), 2)\n\n        # If we execute the exact same statement, it won't create a Person.\n        p, created = Person.objects.get_or_create(\n            first_name='George', last_name='Harrison', defaults={\n                'birthday': date(1943, 2, 25)\n            }\n        )\n        self.assertFalse(created)\n        self.assertEqual(Person.objects.count(), 2)\n\n        # If you don't specify a value or default value for all required\n        # fields, you will get an error.\n        self.assertRaises(\n            IntegrityError,\n            Person.objects.get_or_create, first_name=\"Tom\", last_name=\"Smith\"\n        )\n\n        # If you specify an existing primary key, but different other fields,\n        # then you will get an error and data will not be updated.\n        ManualPrimaryKeyTest.objects.create(id=1, data=\"Original\")\n        self.assertRaises(\n            IntegrityError,\n            ManualPrimaryKeyTest.objects.get_or_create, id=1, data=\"Different\"\n        )\n        self.assertEqual(ManualPrimaryKeyTest.objects.get(id=1).data, \"Original\")\n\n        # get_or_create should raise IntegrityErrors with the full traceback.\n        # This is tested by checking that a known method call is in the traceback.\n        # We cannot use assertRaises/assertRaises here because we need to inspect\n        # the actual traceback. Refs #16340.\n        try:\n            ManualPrimaryKeyTest.objects.get_or_create(id=1, data=\"Different\")\n        except IntegrityError:\n            formatted_traceback = traceback.format_exc()\n            self.assertIn(str('obj.save'), formatted_traceback)\n\n    def test_savepoint_rollback(self):\n        # Regression test for #20463: the database connection should still be\n        # usable after a DataError or ProgrammingError in .get_or_create().\n        try:\n            # Hide warnings when broken data is saved with a warning (MySQL).\n            with warnings.catch_warnings():\n                warnings.simplefilter('ignore')\n                Person.objects.get_or_create(\n                    birthday=date(1970, 1, 1),\n                    defaults={'first_name': b\"\\xff\", 'last_name': b\"\\xff\"})\n        except (DatabaseError, DjangoUnicodeDecodeError):\n            Person.objects.create(\n                first_name=\"Bob\", last_name=\"Ross\", birthday=date(1950, 1, 1))\n        else:\n            self.skipTest(\"This backend accepts broken utf-8.\")\n\n    def test_get_or_create_empty(self):\n        # Regression test for #16137: get_or_create does not require kwargs.\n        try:\n            DefaultPerson.objects.get_or_create()\n        except AssertionError:\n            self.fail(\"If all the attributes on a model have defaults, we \"\n                      \"shouldn't need to pass any arguments.\")\n\n\nclass GetOrCreateTransactionTests(TransactionTestCase):\n\n    available_apps = ['get_or_create']\n\n    def test_get_or_create_integrityerror(self):\n        # Regression test for #15117. Requires a TransactionTestCase on\n        # databases that delay integrity checks until the end of transactions,\n        # otherwise the exception is never raised.\n        try:\n            Profile.objects.get_or_create(person=Person(id=1))\n        except IntegrityError:\n            pass\n        else:\n            self.skipTest(\"This backend does not support integrity checks.\")\n\n\nclass GetOrCreateThroughManyToMany(TestCase):\n\n    def test_get_get_or_create(self):\n        tag = Tag.objects.create(text='foo')\n        a_thing = Thing.objects.create(name='a')\n        a_thing.tags.add(tag)\n        obj, created = a_thing.tags.get_or_create(text='foo')\n\n        self.assertFalse(created)\n        self.assertEqual(obj.pk, tag.pk)\n\n    def test_create_get_or_create(self):\n        a_thing = Thing.objects.create(name='a')\n        obj, created = a_thing.tags.get_or_create(text='foo')\n\n        self.assertTrue(created)\n        self.assertEqual(obj.text, 'foo')\n        self.assertIn(obj, a_thing.tags.all())\n\n    def test_something(self):\n        Tag.objects.create(text='foo')\n        a_thing = Thing.objects.create(name='a')\n        self.assertRaises(IntegrityError, a_thing.tags.get_or_create, text='foo')\n\n\nclass UpdateOrCreateTests(TestCase):\n\n    def test_update(self):\n        Person.objects.create(\n            first_name='John', last_name='Lennon', birthday=date(1940, 10, 9)\n        )\n        p, created = Person.objects.update_or_create(\n            first_name='John', last_name='Lennon', defaults={\n                'birthday': date(1940, 10, 10)\n            }\n        )\n        self.assertFalse(created)\n        self.assertEqual(p.first_name, 'John')\n        self.assertEqual(p.last_name, 'Lennon')\n        self.assertEqual(p.birthday, date(1940, 10, 10))\n\n    def test_create(self):\n        p, created = Person.objects.update_or_create(\n            first_name='John', last_name='Lennon', defaults={\n                'birthday': date(1940, 10, 10)\n            }\n        )\n        self.assertTrue(created)\n        self.assertEqual(p.first_name, 'John')\n        self.assertEqual(p.last_name, 'Lennon')\n        self.assertEqual(p.birthday, date(1940, 10, 10))\n\n    def test_create_twice(self):\n        params = {\n            'first_name': 'John',\n            'last_name': 'Lennon',\n            'birthday': date(1940, 10, 10),\n        }\n        Person.objects.update_or_create(**params)\n        # If we execute the exact same statement, it won't create a Person.\n        p, created = Person.objects.update_or_create(**params)\n        self.assertFalse(created)\n\n    def test_integrity(self):\n        # If you don't specify a value or default value for all required\n        # fields, you will get an error.\n        self.assertRaises(IntegrityError,\n            Person.objects.update_or_create, first_name=\"Tom\", last_name=\"Smith\")\n\n    def test_manual_primary_key_test(self):\n        # If you specify an existing primary key, but different other fields,\n        # then you will get an error and data will not be updated.\n        ManualPrimaryKeyTest.objects.create(id=1, data=\"Original\")\n        self.assertRaises(\n            IntegrityError,\n            ManualPrimaryKeyTest.objects.update_or_create, id=1, data=\"Different\"\n        )\n        self.assertEqual(ManualPrimaryKeyTest.objects.get(id=1).data, \"Original\")\n\n    def test_error_contains_full_traceback(self):\n        # update_or_create should raise IntegrityErrors with the full traceback.\n        # This is tested by checking that a known method call is in the traceback.\n        # We cannot use assertRaises/assertRaises here because we need to inspect\n        # the actual traceback. Refs #16340.\n        try:\n            ManualPrimaryKeyTest.objects.update_or_create(id=1, data=\"Different\")\n        except IntegrityError:\n            formatted_traceback = traceback.format_exc()\n            self.assertIn('obj.save', formatted_traceback)\n", "license": "bsd-3-clause"}
{"id": "7e36d8da1cad077ffe0f1cf8d4d1dc806deb0cba", "path": "murano_tempest_tests/tests/api/application_catalog/artifacts/base.py", "repo_name": "DavidPurcell/murano_temp", "content": "# Copyright (c) 2016 Mirantis, Inc.\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom tempest.common import credentials_factory as common_creds\nfrom tempest.common import dynamic_creds\nfrom tempest import config\nfrom tempest.lib import base\n\nfrom murano_tempest_tests import clients\nfrom murano_tempest_tests import utils\n\nCONF = config.CONF\n\n\nclass BaseArtifactsTest(base.BaseTestCase):\n    \"\"\"Base test class for Murano Glare tests.\"\"\"\n\n    @classmethod\n    def setUpClass(cls):\n        super(BaseArtifactsTest, cls).setUpClass()\n        cls.resource_setup()\n\n    @classmethod\n    def tearDownClass(cls):\n        cls.resource_cleanup()\n        super(BaseArtifactsTest, cls).tearDownClass()\n\n    @classmethod\n    def get_client_with_isolated_creds(cls, type_of_creds=\"admin\"):\n\n        creds = cls.get_configured_isolated_creds(type_of_creds=type_of_creds)\n\n        os = clients.Manager(credentials=creds)\n        client = os.artifacts_client\n\n        return client\n\n    @classmethod\n    def get_configured_isolated_creds(cls, type_of_creds='admin'):\n        identity_version = CONF.identity.auth_version\n        if identity_version == 'v3':\n            cls.admin_role = CONF.identity.admin_role\n        else:\n            cls.admin_role = 'admin'\n        cls.dynamic_cred = dynamic_creds.DynamicCredentialProvider(\n            identity_version=CONF.identity.auth_version,\n            name=cls.__name__, admin_role=cls.admin_role,\n            admin_creds=common_creds.get_configured_admin_credentials(\n                'identity_admin'))\n        if type_of_creds == 'primary':\n            creds = cls.dynamic_cred.get_primary_creds()\n        elif type_of_creds == 'admin':\n            creds = cls.dynamic_cred.get_admin_creds()\n        elif type_of_creds == 'alt':\n            creds = cls.dynamic_cred.get_alt_creds()\n        else:\n            creds = cls.dynamic_cred.get_credentials(type_of_creds)\n        cls.dynamic_cred.type_of_creds = type_of_creds\n\n        return creds.credentials\n\n    @classmethod\n    def verify_nonempty(cls, *args):\n        if not all(args):\n            msg = \"Missing API credentials in configuration.\"\n            raise cls.skipException(msg)\n\n    @classmethod\n    def resource_setup(cls):\n        if not CONF.service_available.murano:\n            skip_msg = \"Murano is disabled\"\n            raise cls.skipException(skip_msg)\n        if not hasattr(cls, \"os\"):\n            creds = cls.get_configured_isolated_creds(type_of_creds='primary')\n            cls.os = clients.Manager(credentials=creds)\n        cls.artifacts_client = cls.os.artifacts_client\n        cls.application_catalog_client = cls.os.application_catalog_client\n\n    @classmethod\n    def resource_cleanup(cls):\n        cls.clear_isolated_creds()\n\n    @classmethod\n    def clear_isolated_creds(cls):\n        if hasattr(cls, \"dynamic_cred\"):\n            cls.dynamic_cred.clear_creds()\n\n    @classmethod\n    def upload_package(cls, application_name, version=None, require=None):\n        abs_archive_path, dir_with_archive, archive_name = \\\n            utils.prepare_package(application_name, version=version,\n                                  add_class_name=True, require=require)\n        package = cls.artifacts_client.upload_package(\n            application_name, archive_name, dir_with_archive,\n            {\"categories\": [], \"tags\": [], 'is_public': False})\n        return package, abs_archive_path\n\n    @staticmethod\n    def create_obj_model(package):\n        return {\n            \"name\": package['display_name'],\n            \"?\": {\n                \"type\": package['name'],\n                \"id\": utils.generate_uuid(),\n                \"classVersion\": package['version']\n            }\n        }\n", "license": "apache-2.0"}
{"id": "d8bd2faf5a83d8dee9f1ed8a2a3d8d64584077f0", "path": "sccp/network/tests/test_ipAddress.py", "repo_name": "mwicat/sccp", "content": "'''\nCreated on Jun 14, 2011\n\n@author: lebleu1\n'''\nimport unittest\nfrom network.ipAddress import IpAddress\n\nclass Test(unittest.TestCase):\n    \n    \n    def testPack(self):\n        address = IpAddress(\"192.168.1.12\")\n        self.assertEquals(\"\\xC0\\xA8\\x01\\x0C\",address.pack())\n", "license": "gpl-3.0"}
{"id": "2312439a8d67d715bacae6d2cc4b76c54c1bb13a", "path": "scons/scons-local-2.2.0/SCons/Platform/hpux.py", "repo_name": "ake-koomsin/mapnik_nvpr", "content": "\"\"\"engine.SCons.Platform.hpux\n\nPlatform-specific initialization for HP-UX systems.\n\nThere normally shouldn't be any need to import this module directly.  It\nwill usually be imported through the generic SCons.Platform.Platform()\nselection method.\n\"\"\"\n\n#\n# Copyright (c) 2001, 2002, 2003, 2004, 2005, 2006, 2007, 2008, 2009, 2010, 2011, 2012 The SCons Foundation\n#\n# Permission is hereby granted, free of charge, to any person obtaining\n# a copy of this software and associated documentation files (the\n# \"Software\"), to deal in the Software without restriction, including\n# without limitation the rights to use, copy, modify, merge, publish,\n# distribute, sublicense, and/or sell copies of the Software, and to\n# permit persons to whom the Software is furnished to do so, subject to\n# the following conditions:\n#\n# The above copyright notice and this permission notice shall be included\n# in all copies or substantial portions of the Software.\n#\n# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY\n# KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE\n# WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND\n# NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE\n# LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION\n# OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION\n# WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n#\n\n__revision__ = \"src/engine/SCons/Platform/hpux.py issue-2856:2676:d23b7a2f45e8 2012/08/05 15:38:28 garyo\"\n\nimport posix\n\ndef generate(env):\n    posix.generate(env)\n    #Based on HP-UX11i: ARG_MAX=2048000 - 3000 for environment expansion\n    env['MAXLINELENGTH']  = 2045000\n\n# Local Variables:\n# tab-width:4\n# indent-tabs-mode:nil\n# End:\n# vim: set expandtab tabstop=4 shiftwidth=4:\n", "license": "lgpl-2.1"}
{"id": "3775b6c437c0572bebfd535e90802eee76d7c076", "path": "shadowsocks/crypto/openssl.py", "repo_name": "chitanda/shadowsocks", "content": "#!/usr/bin/env python\n#\n# Copyright 2015 clowwindy\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n# not use this file except in compliance with the License. You may obtain\n# a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n# WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n# License for the specific language governing permissions and limitations\n# under the License.\n\nfrom __future__ import absolute_import, division, print_function, \\\n    with_statement\n\nfrom ctypes import c_char_p, c_int, c_long, byref,\\\n    create_string_buffer, c_void_p\n\nfrom shadowsocks import common\nfrom shadowsocks.crypto import util\n\n__all__ = ['ciphers']\n\nlibcrypto = None\nloaded = False\n\nbuf_size = 2048\n\n\ndef load_openssl():\n    global loaded, libcrypto, buf\n\n    libcrypto = util.find_library(('crypto', 'eay32'),\n                                  'EVP_get_cipherbyname',\n                                  'libcrypto')\n    if libcrypto is None:\n        raise Exception('libcrypto(OpenSSL) not found')\n\n    libcrypto.EVP_get_cipherbyname.restype = c_void_p\n    libcrypto.EVP_CIPHER_CTX_new.restype = c_void_p\n\n    libcrypto.EVP_CipherInit_ex.argtypes = (c_void_p, c_void_p, c_char_p,\n                                            c_char_p, c_char_p, c_int)\n\n    libcrypto.EVP_CipherUpdate.argtypes = (c_void_p, c_void_p, c_void_p,\n                                           c_char_p, c_int)\n\n    libcrypto.EVP_CIPHER_CTX_cleanup.argtypes = (c_void_p,)\n    libcrypto.EVP_CIPHER_CTX_free.argtypes = (c_void_p,)\n    if hasattr(libcrypto, 'OpenSSL_add_all_ciphers'):\n        libcrypto.OpenSSL_add_all_ciphers()\n\n    buf = create_string_buffer(buf_size)\n    loaded = True\n\n\ndef load_cipher(cipher_name):\n    func_name = 'EVP_' + cipher_name.replace('-', '_')\n    if bytes != str:\n        func_name = str(func_name, 'utf-8')\n    cipher = getattr(libcrypto, func_name, None)\n    if cipher:\n        cipher.restype = c_void_p\n        return cipher()\n    return None\n\n\nclass OpenSSLCrypto(object):\n    def __init__(self, cipher_name, key, iv, op):\n        self._ctx = None\n        if not loaded:\n            load_openssl()\n        cipher_name = common.to_bytes(cipher_name)\n        cipher = libcrypto.EVP_get_cipherbyname(cipher_name)\n        if not cipher:\n            cipher = load_cipher(cipher_name)\n        if not cipher:\n            raise Exception('cipher %s not found in libcrypto' % cipher_name)\n        key_ptr = c_char_p(key)\n        iv_ptr = c_char_p(iv)\n        self._ctx = libcrypto.EVP_CIPHER_CTX_new()\n        if not self._ctx:\n            raise Exception('can not create cipher context')\n        r = libcrypto.EVP_CipherInit_ex(self._ctx, cipher, None,\n                                        key_ptr, iv_ptr, c_int(op))\n        if not r:\n            self.clean()\n            raise Exception('can not initialize cipher context')\n\n    def update(self, data):\n        global buf_size, buf\n        cipher_out_len = c_long(0)\n        l = len(data)\n        if buf_size < l:\n            buf_size = l * 2\n            buf = create_string_buffer(buf_size)\n        libcrypto.EVP_CipherUpdate(self._ctx, byref(buf),\n                                   byref(cipher_out_len), c_char_p(data), l)\n        # buf is copied to a str object when we access buf.raw\n        return buf.raw[:cipher_out_len.value]\n\n    def __del__(self):\n        self.clean()\n\n    def clean(self):\n        if self._ctx:\n            libcrypto.EVP_CIPHER_CTX_cleanup(self._ctx)\n            libcrypto.EVP_CIPHER_CTX_free(self._ctx)\n\n\nciphers = {\n    'aes-128-cfb': (16, 16, OpenSSLCrypto),\n    'aes-192-cfb': (24, 16, OpenSSLCrypto),\n    'aes-256-cfb': (32, 16, OpenSSLCrypto),\n    'aes-128-ofb': (16, 16, OpenSSLCrypto),\n    'aes-192-ofb': (24, 16, OpenSSLCrypto),\n    'aes-256-ofb': (32, 16, OpenSSLCrypto),\n    'aes-128-ctr': (16, 16, OpenSSLCrypto),\n    'aes-192-ctr': (24, 16, OpenSSLCrypto),\n    'aes-256-ctr': (32, 16, OpenSSLCrypto),\n    'aes-128-cfb8': (16, 16, OpenSSLCrypto),\n    'aes-192-cfb8': (24, 16, OpenSSLCrypto),\n    'aes-256-cfb8': (32, 16, OpenSSLCrypto),\n    'aes-128-cfb1': (16, 16, OpenSSLCrypto),\n    'aes-192-cfb1': (24, 16, OpenSSLCrypto),\n    'aes-256-cfb1': (32, 16, OpenSSLCrypto),\n    'bf-cfb': (16, 8, OpenSSLCrypto),\n    'camellia-128-cfb': (16, 16, OpenSSLCrypto),\n    'camellia-192-cfb': (24, 16, OpenSSLCrypto),\n    'camellia-256-cfb': (32, 16, OpenSSLCrypto),\n    'cast5-cfb': (16, 8, OpenSSLCrypto),\n    'des-cfb': (8, 8, OpenSSLCrypto),\n    'idea-cfb': (16, 8, OpenSSLCrypto),\n    'rc2-cfb': (16, 8, OpenSSLCrypto),\n    'rc4': (16, 0, OpenSSLCrypto),\n    'seed-cfb': (16, 16, OpenSSLCrypto),\n}\n\n\ndef run_method(method):\n\n    cipher = OpenSSLCrypto(method, b'k' * 32, b'i' * 16, 1)\n    decipher = OpenSSLCrypto(method, b'k' * 32, b'i' * 16, 0)\n\n    util.run_cipher(cipher, decipher)\n\n\ndef test_aes_128_cfb():\n    run_method('aes-128-cfb')\n\n\ndef test_aes_256_cfb():\n    run_method('aes-256-cfb')\n\n\ndef test_aes_128_cfb8():\n    run_method('aes-128-cfb8')\n\n\ndef test_aes_256_ofb():\n    run_method('aes-256-ofb')\n\n\ndef test_aes_256_ctr():\n    run_method('aes-256-ctr')\n\n\ndef test_bf_cfb():\n    run_method('bf-cfb')\n\n\ndef test_rc4():\n    run_method('rc4')\n\n\nif __name__ == '__main__':\n    test_aes_128_cfb()\n", "license": "apache-2.0"}
{"id": "a2ea892c48ea1837ab9222ff43f0841fa5edc952", "path": "lib/python2.7/site-packages/nbformat/converter.py", "repo_name": "fzheng/codejam", "content": "\"\"\"API for converting notebooks between versions.\"\"\"\n\n# Copyright (c) IPython Development Team.\n# Distributed under the terms of the Modified BSD License.\n\nfrom . import versions\nfrom .reader import get_version\n\n\ndef convert(nb, to_version):\n    \"\"\"Convert a notebook node object to a specific version.  Assumes that\n    all the versions starting from 1 to the latest major X are implemented.\n    In other words, there should never be a case where v1 v2 v3 v5 exist without\n    a v4.  Also assumes that all conversions can be made in one step increments\n    between major versions and ignores minor revisions.\n\n    Parameters\n    ----------\n    nb : NotebookNode\n    to_version : int\n        Major revision to convert the notebook to.  Can either be an upgrade or\n        a downgrade.\n    \"\"\"\n\n    # Get input notebook version.\n    (version, version_minor) = get_version(nb)\n\n    # Check if destination is target version, if so return contents\n    if version == to_version:\n        return nb\n\n    # If the version exist, try to convert to it one step at a time.\n    elif to_version in versions:\n\n        # Get the the version that this recursion will convert to as a step \n        # closer to the final revision.  Make sure the newer of the conversion\n        # functions is used to perform the conversion.\n        if to_version > version:\n            step_version = version + 1\n            convert_function = versions[step_version].upgrade\n        else:\n            step_version = version - 1\n            convert_function = versions[version].downgrade\n\n        # Convert and make sure version changed during conversion.\n        converted = convert_function(nb)\n        if converted.get('nbformat', 1) == version:\n            raise ValueError(\"Failed to convert notebook from v%d to v%d.\" % (version, step_version))\n\n        # Recursively convert until target version is reached.\n        return convert(converted, to_version)\n    else:\n        raise ValueError(\"Cannot convert notebook to v%d because that \" \\\n                        \"version doesn't exist\" % (to_version))\n", "license": "mit"}
{"id": "0cc261591bc0879fe5ad74d48336b993f0283788", "path": "tensorflow/tools/test/system_info_lib.py", "repo_name": "jostep/tensorflow", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"Library for getting system information during TensorFlow tests.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport glob\nimport multiprocessing\nimport platform\nimport re\nimport socket\n\n# pylint: disable=g-bad-import-order\n# Note: cpuinfo and psutil are not installed for you in the TensorFlow\n# OSS tree.  They are installable via pip.\nimport cpuinfo\nimport psutil\n# pylint: enable=g-bad-import-order\n\nfrom tensorflow.core.util import test_log_pb2\nfrom tensorflow.python.client import device_lib\nfrom tensorflow.python.framework import errors\nfrom tensorflow.python.platform import gfile\nfrom tensorflow.tools.test import gpu_info_lib\n\n\ndef gather_machine_configuration():\n  \"\"\"Gather Machine Configuration.  This is the top level fn of this library.\"\"\"\n  config = test_log_pb2.MachineConfiguration()\n\n  config.cpu_info.CopyFrom(gather_cpu_info())\n  config.platform_info.CopyFrom(gather_platform_info())\n\n  # gather_available_device_info must come before gather_gpu_devices\n  # because the latter may access libcudart directly, which confuses\n  # TensorFlow StreamExecutor.\n  for d in gather_available_device_info():\n    config.available_device_info.add().CopyFrom(d)\n  for gpu in gpu_info_lib.gather_gpu_devices():\n    config.device_info.add().Pack(gpu)\n\n  config.memory_info.CopyFrom(gather_memory_info())\n\n  config.hostname = gather_hostname()\n\n  return config\n\n\ndef gather_hostname():\n  return socket.gethostname()\n\n\ndef gather_memory_info():\n  \"\"\"Gather memory info.\"\"\"\n  mem_info = test_log_pb2.MemoryInfo()\n  vmem = psutil.virtual_memory()\n  mem_info.total = vmem.total\n  mem_info.available = vmem.available\n  return mem_info\n\n\ndef gather_cpu_info():\n  \"\"\"Gather CPU Information.  Assumes all CPUs are the same.\"\"\"\n  cpu_info = test_log_pb2.CPUInfo()\n  cpu_info.num_cores = multiprocessing.cpu_count()\n\n  # Gather num_cores_allowed\n  try:\n    with gfile.GFile('/proc/self/status', 'rb') as fh:\n      nc = re.search(r'(?m)^Cpus_allowed:\\s*(.*)$', fh.read())\n    if nc:  # e.g. 'ff' => 8, 'fff' => 12\n      cpu_info.num_cores_allowed = (\n          bin(int(nc.group(1).replace(',', ''), 16)).count('1'))\n  except errors.OpError:\n    pass\n  finally:\n    if cpu_info.num_cores_allowed == 0:\n      cpu_info.num_cores_allowed = cpu_info.num_cores\n\n  # Gather the rest\n  info = cpuinfo.get_cpu_info()\n  cpu_info.cpu_info = info['brand']\n  cpu_info.num_cores = info['count']\n  cpu_info.mhz_per_cpu = info['hz_advertised_raw'][0] / 1.0e6\n  l2_cache_size = re.match(r'(\\d+)', str(info.get('l2_cache_size', '')))\n  if l2_cache_size:\n    # If a value is returned, it's in KB\n    cpu_info.cache_size['L2'] = int(l2_cache_size.group(0)) * 1024\n\n  # Try to get the CPU governor\n  try:\n    cpu_governors = set([\n        gfile.GFile(f, 'r').readline().rstrip()\n        for f in glob.glob(\n            '/sys/devices/system/cpu/cpu*/cpufreq/scaling_governor')\n    ])\n    if cpu_governors:\n      if len(cpu_governors) > 1:\n        cpu_info.cpu_governor = 'mixed'\n      else:\n        cpu_info.cpu_governor = list(cpu_governors)[0]\n  except errors.OpError:\n    pass\n\n  return cpu_info\n\n\ndef gather_available_device_info():\n  \"\"\"Gather list of devices available to TensorFlow.\n\n  Returns:\n    A list of test_log_pb2.AvailableDeviceInfo messages.\n  \"\"\"\n  device_info_list = []\n  devices = device_lib.list_local_devices()\n\n  for d in devices:\n    device_info = test_log_pb2.AvailableDeviceInfo()\n    device_info.name = d.name\n    device_info.type = d.device_type\n    device_info.memory_limit = d.memory_limit\n    device_info.physical_description = d.physical_device_desc\n    device_info_list.append(device_info)\n\n  return device_info_list\n\n\ndef gather_platform_info():\n  \"\"\"Gather platform info.\"\"\"\n  platform_info = test_log_pb2.PlatformInfo()\n  (platform_info.bits, platform_info.linkage) = platform.architecture()\n  platform_info.machine = platform.machine()\n  platform_info.release = platform.release()\n  platform_info.system = platform.system()\n  platform_info.version = platform.version()\n  return platform_info\n", "license": "apache-2.0"}
{"id": "ef1e77ee5723b1a1561de47fc7c87ab2cb710f9f", "path": "octoprint_systemcommandeditor/__init__.py", "repo_name": "Salandora/OctoPrint-SystemCommandEditor", "content": "# coding=utf-8\nfrom __future__ import absolute_import\n\n__author__ = \"Marc Hannappel <salandora@gmail.com>\"\n__license__ = 'GNU Affero General Public License http://www.gnu.org/licenses/agpl.html'\n\nimport octoprint.plugin\n\nclass SystemCommandEditorPlugin(octoprint.plugin.SettingsPlugin,\n\t\t\t\t\t\t\t\toctoprint.plugin.TemplatePlugin,\n\t\t\t\t\t\t\t\toctoprint.plugin.BlueprintPlugin,\n\t\t\t\t\t\t\t\toctoprint.plugin.AssetPlugin):\n\tdef get_settings_defaults(self):\n\t\treturn dict(actions=[])\n\n\tdef get_template_configs(self):\n\t\tif \"editorcollection\" in self._plugin_manager.enabled_plugins:\n\t\t\treturn [\n\t\t\t\tdict(type=\"plugin_editorcollection_EditorCollection\", template=\"systemcommandeditor_hookedsettings.jinja2\", custom_bindings=True)\n\t\t\t]\n\t\telse:\n\t\t\treturn [\n\t\t\t\tdict(type=\"settings\", template=\"systemcommandeditor_hookedsettings.jinja2\", custom_bindings=True)\n\t\t\t]\n\n\tdef on_settings_save(self, data):\n\t\tpass\n\n\tdef get_assets(self):\n\t\treturn dict(\n\t\t\tjs=[\"js/jquery.ui.sortable.js\",\n\t\t\t\t\"js/systemcommandeditor.js\",\n\t\t\t\t\"js/systemcommandeditorDialog.js\"],\n\t\t\tcss=[\"css/systemcommandeditor.css\"]\n\t\t)\n\n\tdef get_update_information(self):\n\t\treturn dict(\n\t\t\tsystemcommandeditor=dict(\n\t\t\t\tdisplayName=\"System Command Editor Plugin\",\n\t\t\t\tdisplayVersion=self._plugin_version,\n\n\t\t\t\t# version check: github repository\n\t\t\t\ttype=\"github_release\",\n\t\t\t\tuser=\"Salandora\",\n\t\t\t\trepo=\"OctoPrint-SystemCommandEditor\",\n\t\t\t\tcurrent=self._plugin_version,\n\n\t\t\t\t# update method: pip\n\t\t\t\tpip=\"https://github.com/Salandora/OctoPrint-SystemCommandEditor/archive/{target_version}.zip\"\n\t\t\t)\n\t\t)\n\n\n__plugin_name__ = \"System Command Editor\"\n__plugin_pythoncompat__ = \">=2.7,<4\"\n\n\ndef __plugin_load__():\n\tglobal __plugin_implementation__\n\t__plugin_implementation__ = SystemCommandEditorPlugin()\n\n\tglobal __plugin_hooks__\n\t__plugin_hooks__ = {\n\t\t\"octoprint.plugin.softwareupdate.check_config\": __plugin_implementation__.get_update_information\n\t}\n\n\tglobal __plugin_license__\n\t__plugin_license__ = \"AGPLv3\"\n", "license": "agpl-3.0"}
{"id": "095f4d5e7ae246dccb2a38797f51091a62e742ad", "path": "build/lib/npyscreen/fmFormMultiPage.py", "repo_name": "C-Blu/npyscreen", "content": "## Very, very experimental. Do NOT USE.\nimport curses\nfrom .         import fmForm\nfrom .wgwidget import NotEnoughSpaceForWidget\nfrom .         import wgNMenuDisplay\n\n\nclass FormMultiPage(fmForm.FormBaseNew):\n    page_info_pre_pages_display = '[ '\n    page_info_post_pages_display = ' ]'\n    page_info_pages_name = 'Page'\n    page_info_out_of     = 'of'\n    def __init__(self, display_pages=True, pages_label_color='NORMAL', *args, **keywords):\n        self.display_pages = display_pages\n        self.pages_label_color = pages_label_color\n        super(FormMultiPage, self).__init__(*args, **keywords)\n        self.switch_page(0)\n    \n    def draw_form(self, *args, **keywords):\n        super(FormMultiPage, self).draw_form(*args, **keywords)\n        self.display_page_number()\n    \n    def _resize(self, *args):\n        if not self.ALLOW_RESIZE:\n            return False\n\n        if hasattr(self, 'parentApp'):\n            self.parentApp.resize()\n            \n        self._create_screen()\n        self.resize()\n        for page in self._pages__:\n            for w in page:\n                w._resize()\n        self.DISPLAY()\n    \n    \n    def display_page_number(self):\n        if not self.display_pages:\n            return False\n            \n        if len(self._pages__) > 1:\n            display_text = \"%s%s %s %s %s%s\" % (\n                self.page_info_pre_pages_display,\n                self.page_info_pages_name,\n                self._active_page + 1,\n                self.page_info_out_of,\n                len(self._pages__),\n                self.page_info_post_pages_display,\n            )\n        # for python2\n            if isinstance(display_text, bytes):\n                display_text = display_text.decode('utf-8', 'replace')\n        \n            maxy,maxx = self.curses_pad.getmaxyx()\n        \n            if (maxx-5) <= len(display_text):\n                # then give up.\n                return False\n        \n            self.add_line(\n                maxy - 1,\n                maxx - len(display_text) - 2,\n                display_text,\n                self.make_attributes_list(display_text, \n                     curses.A_NORMAL | self.theme_manager.findPair(self, \n                                                                  self.pages_label_color)),\n                maxx - len(display_text) - 2,\n            )\n        \n    \n    def add_widget_intelligent(self, *args, **keywords):\n        try:\n            return self.add_widget(*args, **keywords)\n        except NotEnoughSpaceForWidget:\n            self.add_page()\n            return self.add_widget(*args, **keywords)\n            \n    \n    def _clear_all_widgets(self,):\n        super(FormMultiPage, self)._clear_all_widgets()\n        self._pages__     = [ [],]\n        self._active_page = 0\n        self.switch_page(self._active_page, display=False)\n    \n    def switch_page(self, page, display=True):\n        self._widgets__ = self._pages__[page]\n        self._active_page = page\n        self.editw = 0\n        if display:\n            self.display(clear=True)\n    \n    def add_page(self):\n        self._pages__.append([])\n        page_number   = len(self._pages__)-1\n        self.nextrely = self.DEFAULT_NEXTRELY\n        self.nextrelx = self.DEFAULT_X_OFFSET\n        self.switch_page(page_number, display=False)\n        return page_number\n    \n    def find_next_editable(self, *args):\n        if not self.editw == len(self._widgets__):\n            value_changed = False\n            if not self.cycle_widgets:\n                r = list(range(self.editw+1, len(self._widgets__)))\n            else:\n                r = list(range(self.editw+1, len(self._widgets__))) + list(range(0, self.editw))\n            for n in r:\n                if self._widgets__[n].editable and not self._widgets__[n].hidden: \n                    self.editw = n\n                    value_changed = True\n                    break\n            if not value_changed:\n                if self._active_page < len(self._pages__)-1:\n                    self.switch_page(self._active_page + 1)\n        self.display()\n    \n    \n    def find_previous_editable(self, *args):\n        if self.editw == 0:\n            if self._active_page > 0:\n                self.switch_page(self._active_page-1)\n        \n        if not self.editw == 0:     \n            # remember that xrange does not return the 'last' value,\n            # so go to -1, not 0! (fence post error in reverse)\n            for n in range(self.editw-1, -1, -1 ):\n                if self._widgets__[n].editable and not self._widgets__[n].hidden: \n                    self.editw = n\n                    break\n                    \n                    \nclass FormMultiPageAction(FormMultiPage):\n    CANCEL_BUTTON_BR_OFFSET = (2, 12)\n    OK_BUTTON_TEXT          = \"OK\"\n    CANCEL_BUTTON_TEXT      = \"Cancel\"\n    \n    def on_ok(self):\n        pass\n    \n    def on_cancel(self):\n        pass\n    \n    def pre_edit_loop(self):\n        self._page_for_buttons = len(self._pages__)-1\n        self.switch_page(self._page_for_buttons)\n        \n        # Add ok and cancel buttons. Will remove later\n        tmp_rely, tmp_relx = self.nextrely, self.nextrelx\n        \n        c_button_text = self.CANCEL_BUTTON_TEXT\n        cmy, cmx = self.curses_pad.getmaxyx()\n        cmy -= self.__class__.CANCEL_BUTTON_BR_OFFSET[0]\n        cmx -= len(c_button_text)+self.__class__.CANCEL_BUTTON_BR_OFFSET[1]\n        self.c_button = self.add_widget(self.__class__.OKBUTTON_TYPE, name=c_button_text, rely=cmy, relx=cmx, use_max_space=True)\n        self._c_button_postion = len(self._widgets__)-1\n        self.c_button.update()\n        \n        my, mx = self.curses_pad.getmaxyx()\n        ok_button_text = self.OK_BUTTON_TEXT\n        my -= self.__class__.OK_BUTTON_BR_OFFSET[0]\n        mx -= len(ok_button_text)+self.__class__.OK_BUTTON_BR_OFFSET[1]\n        self.ok_button = self.add_widget(self.__class__.OKBUTTON_TYPE, name=ok_button_text, rely=my, relx=mx, use_max_space=True)\n        self._ok_button_postion = len(self._widgets__)-1\n        # End add buttons\n        self.nextrely, self.nextrelx = tmp_rely, tmp_relx\n        self.switch_page(0)\n        \n    def _during_edit_loop(self):\n        if self.ok_button.value or self.c_button.value:\n            self.editing = False\n    \n        if self.ok_button.value:\n            self.ok_button.value = False\n            self.edit_return_value = self.on_ok()\n        elif self.c_button.value:\n            self.c_button.value = False\n            self.edit_return_value = self.on_cancel()\n    \n    def resize(self):\n        super(FormMultiPageAction, self).resize()\n        self.move_ok_button()\n          \n    def move_ok_button(self):\n        if hasattr(self, 'ok_button'):\n            my, mx = self.curses_pad.getmaxyx()\n            my -= self.__class__.OK_BUTTON_BR_OFFSET[0]\n            mx -= len(self.__class__.OK_BUTTON_TEXT)+self.__class__.OK_BUTTON_BR_OFFSET[1]\n            self.ok_button.relx = mx\n            self.ok_button.rely = my\n        if hasattr(self, 'c_button'):\n            c_button_text = self.CANCEL_BUTTON_TEXT\n            cmy, cmx = self.curses_pad.getmaxyx()\n            cmy -= self.__class__.CANCEL_BUTTON_BR_OFFSET[0]\n            cmx -= len(c_button_text)+self.__class__.CANCEL_BUTTON_BR_OFFSET[1]\n            self.c_button.rely = cmy\n            self.c_button.relx = cmx\n    \n    \n    def post_edit_loop(self):\n        self.switch_page(self._page_for_buttons)\n        self.ok_button.destroy()\n        self.c_button.destroy()\n        del self._widgets__[self._ok_button_postion]\n        del self.ok_button\n        del self._widgets__[self._c_button_postion]\n        del self.c_button\n        #self.nextrely, self.nextrelx = tmp_rely, tmp_relx\n        self.display()\n        self.editing = False\n        \n        return self.edit_return_value\n\n\nclass FormMultiPageWithMenus(fmForm.FormBaseNew):\n    def __init__(self, *args, **keywords):\n        super(FormMultiPageWithMenus, self).__init__(*args, **keywords)\n        self.initialize_menus()\n\nclass FormMultiPageActionWithMenus(FormMultiPageAction, wgNMenuDisplay.HasMenus):\n    def __init__(self, *args, **keywords):\n        super(FormMultiPageActionWithMenus, self).__init__(*args, **keywords)\n        self.initialize_menus()\n", "license": "bsd-2-clause"}
{"id": "017e78a863996abfd23b0eebaaa8e4c7e1655df8", "path": "tests/template_tests/test_parser.py", "repo_name": "YYWen0o0/python-frame-django", "content": "\"\"\"\nTesting some internals of the template processing. These are *not* examples to be copied in user code.\n\"\"\"\nfrom __future__ import unicode_literals\n\nfrom unittest import TestCase\n\nfrom django.template import (TokenParser, FilterExpression, Parser, Variable,\n    Template, TemplateSyntaxError, Library)\nfrom django.test import override_settings\nfrom django.utils import six\n\n\nclass ParserTests(TestCase):\n    def test_token_parsing(self):\n        # Tests for TokenParser behavior in the face of quoted strings with\n        # spaces.\n\n        p = TokenParser(\"tag thevar|filter sometag\")\n        self.assertEqual(p.tagname, \"tag\")\n        self.assertEqual(p.value(), \"thevar|filter\")\n        self.assertTrue(p.more())\n        self.assertEqual(p.tag(), \"sometag\")\n        self.assertFalse(p.more())\n\n        p = TokenParser('tag \"a value\"|filter sometag')\n        self.assertEqual(p.tagname, \"tag\")\n        self.assertEqual(p.value(), '\"a value\"|filter')\n        self.assertTrue(p.more())\n        self.assertEqual(p.tag(), \"sometag\")\n        self.assertFalse(p.more())\n\n        p = TokenParser(\"tag 'a value'|filter sometag\")\n        self.assertEqual(p.tagname, \"tag\")\n        self.assertEqual(p.value(), \"'a value'|filter\")\n        self.assertTrue(p.more())\n        self.assertEqual(p.tag(), \"sometag\")\n        self.assertFalse(p.more())\n\n    def test_filter_parsing(self):\n        c = {\"article\": {\"section\": \"News\"}}\n        p = Parser(\"\")\n\n        def fe_test(s, val):\n            self.assertEqual(FilterExpression(s, p).resolve(c), val)\n\n        fe_test(\"article.section\", \"News\")\n        fe_test(\"article.section|upper\", \"NEWS\")\n        fe_test('\"News\"', \"News\")\n        fe_test(\"'News'\", \"News\")\n        fe_test(r'\"Some \\\"Good\\\" News\"', 'Some \"Good\" News')\n        fe_test(r'\"Some \\\"Good\\\" News\"', 'Some \"Good\" News')\n        fe_test(r\"'Some \\'Bad\\' News'\", \"Some 'Bad' News\")\n\n        fe = FilterExpression(r'\"Some \\\"Good\\\" News\"', p)\n        self.assertEqual(fe.filters, [])\n        self.assertEqual(fe.var, 'Some \"Good\" News')\n\n        # Filtered variables should reject access of attributes beginning with\n        # underscores.\n        self.assertRaises(TemplateSyntaxError, FilterExpression, \"article._hidden|upper\", p)\n\n    def test_variable_parsing(self):\n        c = {\"article\": {\"section\": \"News\"}}\n        self.assertEqual(Variable(\"article.section\").resolve(c), \"News\")\n        self.assertEqual(Variable('\"News\"').resolve(c), \"News\")\n        self.assertEqual(Variable(\"'News'\").resolve(c), \"News\")\n\n        # Translated strings are handled correctly.\n        self.assertEqual(Variable(\"_(article.section)\").resolve(c), \"News\")\n        self.assertEqual(Variable('_(\"Good News\")').resolve(c), \"Good News\")\n        self.assertEqual(Variable(\"_('Better News')\").resolve(c), \"Better News\")\n\n        # Escaped quotes work correctly as well.\n        self.assertEqual(\n            Variable(r'\"Some \\\"Good\\\" News\"').resolve(c), 'Some \"Good\" News'\n        )\n        self.assertEqual(\n            Variable(r\"'Some \\'Better\\' News'\").resolve(c), \"Some 'Better' News\"\n        )\n\n        # Variables should reject access of attributes beginning with\n        # underscores.\n        self.assertRaises(TemplateSyntaxError, Variable, \"article._hidden\")\n\n        # Variables should raise on non string type\n        with six.assertRaisesRegex(self, TypeError, \"Variable must be a string or number, got <(class|type) 'dict'>\"):\n            Variable({})\n\n    @override_settings(DEBUG=True, TEMPLATE_DEBUG=True)\n    def test_compile_filter_error(self):\n        # regression test for #19819\n        msg = \"Could not parse the remainder: '@bar' from 'foo@bar'\"\n        with six.assertRaisesRegex(self, TemplateSyntaxError, msg) as cm:\n            Template(\"{% if 1 %}{{ foo@bar }}{% endif %}\")\n        self.assertEqual(cm.exception.django_template_source[1], (10, 23))\n\n    def test_filter_args_count(self):\n        p = Parser(\"\")\n        l = Library()\n\n        @l.filter\n        def no_arguments(value):\n            pass\n\n        @l.filter\n        def one_argument(value, arg):\n            pass\n\n        @l.filter\n        def one_opt_argument(value, arg=False):\n            pass\n\n        @l.filter\n        def two_arguments(value, arg, arg2):\n            pass\n\n        @l.filter\n        def two_one_opt_arg(value, arg, arg2=False):\n            pass\n        p.add_library(l)\n        for expr in (\n                '1|no_arguments:\"1\"',\n                '1|two_arguments',\n                '1|two_arguments:\"1\"',\n                '1|two_one_opt_arg',\n        ):\n            with self.assertRaises(TemplateSyntaxError):\n                FilterExpression(expr, p)\n        for expr in (\n                # Correct number of arguments\n                '1|no_arguments',\n                '1|one_argument:\"1\"',\n                # One optional\n                '1|one_opt_argument',\n                '1|one_opt_argument:\"1\"',\n                # Not supplying all\n                '1|two_one_opt_arg:\"1\"',\n        ):\n            FilterExpression(expr, p)\n", "license": "bsd-3-clause"}
{"id": "0f0a37116a06e7824e372dd5cfd0324bee412e3e", "path": "nova/tests/unit/virt/xenapi/image/test_bittorrent.py", "repo_name": "nikesh-mahalka/nova", "content": "# Copyright 2013 OpenStack Foundation\n# All Rights Reserved.\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\nfrom mox3 import mox\nimport six\n\nfrom nova import context\nfrom nova import test\nfrom nova.tests.unit.virt.xenapi import stubs\nfrom nova.virt.xenapi import driver as xenapi_conn\nfrom nova.virt.xenapi import fake\nfrom nova.virt.xenapi.image import bittorrent\nfrom nova.virt.xenapi import vm_utils\n\n\nclass TestBittorrentStore(stubs.XenAPITestBaseNoDB):\n    def setUp(self):\n        super(TestBittorrentStore, self).setUp()\n        self.store = bittorrent.BittorrentStore()\n        self.mox = mox.Mox()\n\n        self.flags(torrent_base_url='http://foo',\n                   connection_url='test_url',\n                   connection_password='test_pass',\n                   group='xenserver')\n\n        self.context = context.RequestContext(\n                'user', 'project', auth_token='foobar')\n\n        fake.reset()\n        stubs.stubout_session(self.stubs, fake.SessionBase)\n\n        driver = xenapi_conn.XenAPIDriver(False)\n        self.session = driver._session\n\n        self.stubs.Set(\n                vm_utils, 'get_sr_path', lambda *a, **kw: '/fake/sr/path')\n\n    def test_download_image(self):\n\n        instance = {'uuid': '00000000-0000-0000-0000-000000007357'}\n        params = {'image_id': 'fake_image_uuid',\n                  'sr_path': '/fake/sr/path',\n                  'torrent_download_stall_cutoff': 600,\n                  'torrent_listen_port_end': 6891,\n                  'torrent_listen_port_start': 6881,\n                  'torrent_max_last_accessed': 86400,\n                  'torrent_max_seeder_processes_per_host': 1,\n                  'torrent_seed_chance': 1.0,\n                  'torrent_seed_duration': 3600,\n                  'torrent_url': 'http://foo/fake_image_uuid.torrent',\n                  'uuid_stack': ['uuid1']}\n\n        self.stubs.Set(vm_utils, '_make_uuid_stack',\n                       lambda *a, **kw: ['uuid1'])\n\n        self.mox.StubOutWithMock(self.session, 'call_plugin_serialized')\n        self.session.call_plugin_serialized(\n                'bittorrent', 'download_vhd', **params)\n        self.mox.ReplayAll()\n\n        self.store.download_image(self.context, self.session,\n                                  instance, 'fake_image_uuid')\n\n        self.mox.VerifyAll()\n\n    def test_upload_image(self):\n        self.assertRaises(NotImplementedError, self.store.upload_image,\n                self.context, self.session, mox.IgnoreArg, 'fake_image_uuid',\n                ['fake_vdi_uuid'])\n\n\nclass LookupTorrentURLTestCase(test.NoDBTestCase):\n    def setUp(self):\n        super(LookupTorrentURLTestCase, self).setUp()\n        self.store = bittorrent.BittorrentStore()\n        self.image_id = 'fakeimageid'\n\n    def test_default_fetch_url_no_base_url_set(self):\n        self.flags(torrent_base_url=None,\n                   group='xenserver')\n\n        exc = self.assertRaises(\n                RuntimeError, self.store._lookup_torrent_url_fn)\n        self.assertEqual('Cannot create default bittorrent URL without'\n                         ' xenserver.torrent_base_url configuration option'\n                         ' set.',\n                         six.text_type(exc))\n\n    def test_default_fetch_url_base_url_is_set(self):\n        self.flags(torrent_base_url='http://foo',\n                   group='xenserver')\n\n        lookup_fn = self.store._lookup_torrent_url_fn()\n        self.assertEqual('http://foo/fakeimageid.torrent',\n                         lookup_fn(self.image_id))\n\n    def test_invalid_base_url_warning_logged(self):\n        self.flags(torrent_base_url='www.foo.com',\n                   group='xenserver')\n\n        # Make sure a warning is logged when an invalid base URL is set,\n        # where invalid means it does not contain any slash characters\n        warnings = []\n\n        def fake_warn(msg):\n            warnings.append(msg)\n\n        self.stubs.Set(bittorrent.LOG, 'warn', fake_warn)\n\n        lookup_fn = self.store._lookup_torrent_url_fn()\n        self.assertEqual('fakeimageid.torrent',\n                         lookup_fn(self.image_id))\n\n        self.assertTrue(any('does not contain a slash character' in msg for\n                            msg in warnings),\n                        '_lookup_torrent_url_fn() did not log a warning '\n                        'message when the torrent_base_url did not contain a '\n                        'slash character.')\n", "license": "apache-2.0"}
{"id": "aeb9ca13bdaec49396b213f2e42a75182c59889c", "path": "test/units/executor/module_common/test_recursive_finder.py", "repo_name": "halberom/ansible", "content": "# (c) 2017, Toshio Kuratomi <tkuratomi@ansible.com>\n#\n# This file is part of Ansible\n#\n# Ansible is free software: you can redistribute it and/or modify\n# it under the terms of the GNU General Public License as published by\n# the Free Software Foundation, either version 3 of the License, or\n# (at your option) any later version.\n#\n# Ansible is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU General Public License for more details.\n#\n# You should have received a copy of the GNU General Public License\n# along with Ansible.  If not, see <http://www.gnu.org/licenses/>.\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\nimport imp\nimport zipfile\nfrom collections import namedtuple\nfrom functools import partial\nfrom io import BytesIO, StringIO\n\nimport pytest\n\nimport ansible.errors\nfrom ansible.compat.six import PY2\nfrom ansible.compat.six.moves import builtins\n\nfrom ansible.executor.module_common import recursive_finder\n\n\noriginal_find_module = imp.find_module\n\n\n@pytest.fixture\ndef finder_containers():\n    FinderContainers = namedtuple('FinderContainers', ['py_module_names', 'py_module_cache', 'zf'])\n\n    py_module_names = set()\n    #py_module_cache = {('__init__',): b''}\n    py_module_cache = {}\n\n    zipoutput = BytesIO()\n    zf = zipfile.ZipFile(zipoutput, mode='w', compression=zipfile.ZIP_STORED)\n    #zf.writestr('ansible/__init__.py', b'')\n\n    return FinderContainers(py_module_names, py_module_cache, zf)\n\n\ndef find_module_foo(module_utils_data, *args, **kwargs):\n    if args[0] == 'foo':\n        return (module_utils_data, '/usr/lib/python2.7/site-packages/ansible/module_utils/foo.py', ('.py', 'r', imp.PY_SOURCE))\n    return original_find_module(*args, **kwargs)\n\n\ndef find_package_foo(module_utils_data, *args, **kwargs):\n    if args[0] == 'foo':\n        return (module_utils_data, '/usr/lib/python2.7/site-packages/ansible/module_utils/foo', ('', '', imp.PKG_DIRECTORY))\n    return original_find_module(*args, **kwargs)\n\n\nclass TestRecursiveFinder(object):\n    def test_no_module_utils(self, finder_containers):\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nreturn \\'{\\\"changed\\\": false}\\''\n        recursive_finder(name, data, *finder_containers)\n        assert finder_containers.py_module_names == set(())\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset()\n\n    def test_from_import_toplevel_package(self, finder_containers, mocker):\n        if PY2:\n            module_utils_data = BytesIO(b'# License\\ndef do_something():\\n    pass\\n')\n        else:\n            module_utils_data = StringIO(u'# License\\ndef do_something():\\n    pass\\n')\n        mocker.patch('imp.find_module', side_effect=partial(find_package_foo, module_utils_data))\n        mocker.patch('ansible.executor.module_common._slurp', side_effect= lambda x: b'# License\\ndef do_something():\\n    pass\\n')\n\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nfrom ansible.module_utils import foo'\n        recursive_finder(name, data, *finder_containers)\n        mocker.stopall()\n\n        assert finder_containers.py_module_names == set((('foo', '__init__'),))\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset(('ansible/module_utils/foo/__init__.py',))\n\n    def test_from_import_toplevel_module(self, finder_containers, mocker):\n        if PY2:\n            module_utils_data = BytesIO(b'# License\\ndef do_something():\\n    pass\\n')\n        else:\n            module_utils_data = StringIO(u'# License\\ndef do_something():\\n    pass\\n')\n        mocker.patch('imp.find_module', side_effect=partial(find_module_foo, module_utils_data))\n\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nfrom ansible.module_utils import foo'\n        recursive_finder(name, data, *finder_containers)\n        mocker.stopall()\n\n        assert finder_containers.py_module_names == set((('foo',),))\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset(('ansible/module_utils/foo.py',))\n\n    #\n    # Test importing six with many permutations because it is not a normal module\n    #\n    def test_from_import_six(self, finder_containers):\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nfrom ansible.module_utils import six'\n        recursive_finder(name, data, *finder_containers)\n        assert finder_containers.py_module_names == set((('six',),))\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset(('ansible/module_utils/six.py',))\n\n    def test_import_six(self, finder_containers):\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nimport ansible.module_utils.six'\n        recursive_finder(name, data, *finder_containers)\n        assert finder_containers.py_module_names == set((('six',),))\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset(('ansible/module_utils/six.py',))\n\n    def test_import_six_from_many_submodules(self, finder_containers):\n        name = 'ping'\n        data = b'#!/usr/bin/python\\nfrom ansible.module_utils.six.moves.urllib.parse import urlparse'\n        recursive_finder(name, data, *finder_containers)\n        assert finder_containers.py_module_names == set((('six',),))\n        assert finder_containers.py_module_cache == {}\n        assert frozenset(finder_containers.zf.namelist()) == frozenset(('ansible/module_utils/six.py',))\n", "license": "gpl-3.0"}
{"id": "9f144996c6269d68e7f9c93feb405934d4b8449c", "path": "tests/handlers/test_base.py", "repo_name": "Hironsan/Brain_Hacker", "content": "import tornado.testing\nimport tornado.web\nimport tornado.escape\n\nfrom tornado.options import options\n\nfrom urls import url_patterns\nfrom urllib import parse\n\nclass TestBase(tornado.testing.AsyncHTTPTestCase):\n\n    def setUp(self):\n        super(TestBase, self).setUp()\n\n    def get_app(self):\n        self.app = tornado.web.Application(handlers=url_patterns, xsrf_cookies=False)\n\n    def get_http_port(self):\n        return options.port\n\n    def user_login(self):\n        login_url = '/auth/login/'\n        post_args = {'email': 'test@test.co.jp',\n                     'password': 'test'\n                     }\n        response = self.fetch(login_url, method='POST',\n                              body=parse.urlencode(post_args),\n                              follow_redirects=False)\n        return response\n\n    def user_logout(self):\n        test_url = '/auth/logout/'\n        response = self.fetch(test_url,  method='GET', follow_redirects=False)\n        return response\n", "license": "mit"}
{"id": "20bb039d38dd9c62ff6c38135f67f9aa41f484aa", "path": "youtube_dl/extractor/weibo.py", "repo_name": "pridemusvaire/youtube-dl", "content": "# coding: utf-8\nfrom __future__ import unicode_literals\n\nimport re\n\nfrom .common import InfoExtractor\n\n\nclass WeiboIE(InfoExtractor):\n    \"\"\"\n    The videos in Weibo come from different sites, this IE just finds the link\n    to the external video and returns it.\n    \"\"\"\n    _VALID_URL = r'https?://video\\.weibo\\.com/v/weishipin/t_(?P<id>.+?)\\.htm'\n\n    _TEST = {\n        'url': 'http://video.weibo.com/v/weishipin/t_zjUw2kZ.htm',\n        'info_dict': {\n            'id': '98322879',\n            'ext': 'flv',\n            'title': '\u9b54\u58f0\u8033\u673a\u6700\u65b0\u5e7f\u544a\u201cAll Eyes On Us\u201d',\n        },\n        'params': {\n            'skip_download': True,\n        },\n        'add_ie': ['Sina'],\n    }\n\n    # Additional example videos from different sites\n    # Youku: http://video.weibo.com/v/weishipin/t_zQGDWQ8.htm\n    # 56.com: http://video.weibo.com/v/weishipin/t_zQ44HxN.htm\n\n    def _real_extract(self, url):\n        mobj = re.match(self._VALID_URL, url, flags=re.VERBOSE)\n        video_id = mobj.group('id')\n        info_url = 'http://video.weibo.com/?s=v&a=play_list&format=json&mix_video_id=t_%s' % video_id\n        info = self._download_json(info_url, video_id)\n\n        videos_urls = map(lambda v: v['play_page_url'], info['result']['data'])\n        # Prefer sina video since they have thumbnails\n        videos_urls = sorted(videos_urls, key=lambda u: 'video.sina.com' in u)\n        player_url = videos_urls[-1]\n        m_sina = re.match(r'https?://video\\.sina\\.com\\.cn/v/b/(\\d+)-\\d+\\.html',\n                          player_url)\n        if m_sina is not None:\n            self.to_screen('Sina video detected')\n            sina_id = m_sina.group(1)\n            player_url = 'http://you.video.sina.com.cn/swf/quotePlayer.swf?vid=%s' % sina_id\n        return self.url_result(player_url)\n", "license": "unlicense"}
{"id": "c0395f4a45aaa5c4ba1824a81d8ef8f69b46dc60", "path": "env/lib/python2.7/site-packages/chardet/chardistribution.py", "repo_name": "nichung/wwwflaskBlogrevA", "content": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################\n\nfrom .euctwfreq import (EUCTW_CHAR_TO_FREQ_ORDER, EUCTW_TABLE_SIZE,\n                        EUCTW_TYPICAL_DISTRIBUTION_RATIO)\nfrom .euckrfreq import (EUCKR_CHAR_TO_FREQ_ORDER, EUCKR_TABLE_SIZE,\n                        EUCKR_TYPICAL_DISTRIBUTION_RATIO)\nfrom .gb2312freq import (GB2312_CHAR_TO_FREQ_ORDER, GB2312_TABLE_SIZE,\n                         GB2312_TYPICAL_DISTRIBUTION_RATIO)\nfrom .big5freq import (BIG5_CHAR_TO_FREQ_ORDER, BIG5_TABLE_SIZE,\n                       BIG5_TYPICAL_DISTRIBUTION_RATIO)\nfrom .jisfreq import (JIS_CHAR_TO_FREQ_ORDER, JIS_TABLE_SIZE,\n                      JIS_TYPICAL_DISTRIBUTION_RATIO)\n\n\nclass CharDistributionAnalysis(object):\n    ENOUGH_DATA_THRESHOLD = 1024\n    SURE_YES = 0.99\n    SURE_NO = 0.01\n    MINIMUM_DATA_THRESHOLD = 3\n\n    def __init__(self):\n        # Mapping table to get frequency order from char order (get from\n        # GetOrder())\n        self._char_to_freq_order = None\n        self._table_size = None  # Size of above table\n        # This is a constant value which varies from language to language,\n        # used in calculating confidence.  See\n        # http://www.mozilla.org/projects/intl/UniversalCharsetDetection.html\n        # for further detail.\n        self.typical_distribution_ratio = None\n        self._done = None\n        self._total_chars = None\n        self._freq_chars = None\n        self.reset()\n\n    def reset(self):\n        \"\"\"reset analyser, clear any state\"\"\"\n        # If this flag is set to True, detection is done and conclusion has\n        # been made\n        self._done = False\n        self._total_chars = 0  # Total characters encountered\n        # The number of characters whose frequency order is less than 512\n        self._freq_chars = 0\n\n    def feed(self, char, char_len):\n        \"\"\"feed a character with known length\"\"\"\n        if char_len == 2:\n            # we only care about 2-bytes character in our distribution analysis\n            order = self.get_order(char)\n        else:\n            order = -1\n        if order >= 0:\n            self._total_chars += 1\n            # order is valid\n            if order < self._table_size:\n                if 512 > self._char_to_freq_order[order]:\n                    self._freq_chars += 1\n\n    def get_confidence(self):\n        \"\"\"return confidence based on existing data\"\"\"\n        # if we didn't receive any character in our consideration range,\n        # return negative answer\n        if self._total_chars <= 0 or self._freq_chars <= self.MINIMUM_DATA_THRESHOLD:\n            return self.SURE_NO\n\n        if self._total_chars != self._freq_chars:\n            r = (self._freq_chars / ((self._total_chars - self._freq_chars)\n                 * self.typical_distribution_ratio))\n            if r < self.SURE_YES:\n                return r\n\n        # normalize confidence (we don't want to be 100% sure)\n        return self.SURE_YES\n\n    def got_enough_data(self):\n        # It is not necessary to receive all data to draw conclusion.\n        # For charset detection, certain amount of data is enough\n        return self._total_chars > self.ENOUGH_DATA_THRESHOLD\n\n    def get_order(self, byte_str):\n        # We do not handle characters based on the original encoding string,\n        # but convert this encoding string to a number, here called order.\n        # This allows multiple encodings of a language to share one frequency\n        # table.\n        return -1\n\n\nclass EUCTWDistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(EUCTWDistributionAnalysis, self).__init__()\n        self._char_to_freq_order = EUCTW_CHAR_TO_FREQ_ORDER\n        self._table_size = EUCTW_TABLE_SIZE\n        self.typical_distribution_ratio = EUCTW_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for euc-TW encoding, we are interested\n        #   first  byte range: 0xc4 -- 0xfe\n        #   second byte range: 0xa1 -- 0xfe\n        # no validation needed here. State machine has done that\n        first_char = byte_str[0]\n        if first_char >= 0xC4:\n            return 94 * (first_char - 0xC4) + byte_str[1] - 0xA1\n        else:\n            return -1\n\n\nclass EUCKRDistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(EUCKRDistributionAnalysis, self).__init__()\n        self._char_to_freq_order = EUCKR_CHAR_TO_FREQ_ORDER\n        self._table_size = EUCKR_TABLE_SIZE\n        self.typical_distribution_ratio = EUCKR_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for euc-KR encoding, we are interested\n        #   first  byte range: 0xb0 -- 0xfe\n        #   second byte range: 0xa1 -- 0xfe\n        # no validation needed here. State machine has done that\n        first_char = byte_str[0]\n        if first_char >= 0xB0:\n            return 94 * (first_char - 0xB0) + byte_str[1] - 0xA1\n        else:\n            return -1\n\n\nclass GB2312DistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(GB2312DistributionAnalysis, self).__init__()\n        self._char_to_freq_order = GB2312_CHAR_TO_FREQ_ORDER\n        self._table_size = GB2312_TABLE_SIZE\n        self.typical_distribution_ratio = GB2312_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for GB2312 encoding, we are interested\n        #  first  byte range: 0xb0 -- 0xfe\n        #  second byte range: 0xa1 -- 0xfe\n        # no validation needed here. State machine has done that\n        first_char, second_char = byte_str[0], byte_str[1]\n        if (first_char >= 0xB0) and (second_char >= 0xA1):\n            return 94 * (first_char - 0xB0) + second_char - 0xA1\n        else:\n            return -1\n\n\nclass Big5DistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(Big5DistributionAnalysis, self).__init__()\n        self._char_to_freq_order = BIG5_CHAR_TO_FREQ_ORDER\n        self._table_size = BIG5_TABLE_SIZE\n        self.typical_distribution_ratio = BIG5_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for big5 encoding, we are interested\n        #   first  byte range: 0xa4 -- 0xfe\n        #   second byte range: 0x40 -- 0x7e , 0xa1 -- 0xfe\n        # no validation needed here. State machine has done that\n        first_char, second_char = byte_str[0], byte_str[1]\n        if first_char >= 0xA4:\n            if second_char >= 0xA1:\n                return 157 * (first_char - 0xA4) + second_char - 0xA1 + 63\n            else:\n                return 157 * (first_char - 0xA4) + second_char - 0x40\n        else:\n            return -1\n\n\nclass SJISDistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(SJISDistributionAnalysis, self).__init__()\n        self._char_to_freq_order = JIS_CHAR_TO_FREQ_ORDER\n        self._table_size = JIS_TABLE_SIZE\n        self.typical_distribution_ratio = JIS_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for sjis encoding, we are interested\n        #   first  byte range: 0x81 -- 0x9f , 0xe0 -- 0xfe\n        #   second byte range: 0x40 -- 0x7e,  0x81 -- oxfe\n        # no validation needed here. State machine has done that\n        first_char, second_char = byte_str[0], byte_str[1]\n        if (first_char >= 0x81) and (first_char <= 0x9F):\n            order = 188 * (first_char - 0x81)\n        elif (first_char >= 0xE0) and (first_char <= 0xEF):\n            order = 188 * (first_char - 0xE0 + 31)\n        else:\n            return -1\n        order = order + second_char - 0x40\n        if second_char > 0x7F:\n            order = -1\n        return order\n\n\nclass EUCJPDistributionAnalysis(CharDistributionAnalysis):\n    def __init__(self):\n        super(EUCJPDistributionAnalysis, self).__init__()\n        self._char_to_freq_order = JIS_CHAR_TO_FREQ_ORDER\n        self._table_size = JIS_TABLE_SIZE\n        self.typical_distribution_ratio = JIS_TYPICAL_DISTRIBUTION_RATIO\n\n    def get_order(self, byte_str):\n        # for euc-JP encoding, we are interested\n        #   first  byte range: 0xa0 -- 0xfe\n        #   second byte range: 0xa1 -- 0xfe\n        # no validation needed here. State machine has done that\n        char = byte_str[0]\n        if char >= 0xA0:\n            return 94 * (char - 0xA1) + byte_str[1] - 0xa1\n        else:\n            return -1\n", "license": "mit"}
{"id": "04e219ec5e57ffae4aa9e887cee688e964933876", "path": "pollsite/polls/tests.py", "repo_name": "philmui/django-poll", "content": "import datetime\n\nfrom django.test import TestCase\nfrom django.utils import timezone\n\nfrom polls.models import Question, Choice\n\nclass QuestionModelTest(TestCase):\n\n    def setUp(self):\n        self.q1 = Question()\n        self.q1.question_text = \"What is your question?\"\n        self.q1.pub_date = timezone.now()\n        self.q1.save()\n\n        self.q2 = Question()\n        self.q2.question_text = \"What is your next question?\"\n        self.q2.pub_date = timezone.now()\n        self.q2.save()\n\n    def tearDown(self):\n        pass\n\n    def test_save_and_retrieve_Question(self):\n        saved_items = Question.objects.all()\n        self.assertEqual(saved_items.count(), 2)\n        self.assertEqual(saved_items[0].question_text, self.q1.question_text)\n        self.assertEqual(saved_items[1].question_text, self.q2.question_text)\n\n    def test_save_and_retrieve_Choice(self):\n        self.q1.choice_set.create(choice_text = 'Choice 1', votes=0)\n        self.q1.choice_set.create(choice_text = \"Choice 2\", votes=1)\n\n        saved_items = Choice.objects.all()\n        self.assertEqual(saved_items.count(), 2)\n        self.assertEqual(saved_items[0].choice_text, \"Choice 1\")\n        self.assertEqual(saved_items[1].choice_text, \"Choice 2\")\n", "license": "mit"}
{"id": "561b13572c767a55b30ee3d2cb7945eea8e7fdfc", "path": "server/sqlmap/plugins/dbms/postgresql/__init__.py", "repo_name": "V11/volcano", "content": "#!/usr/bin/env python\n\n\"\"\"\nCopyright (c) 2006-2015 sqlmap developers (http://sqlmap.org/)\nSee the file 'doc/COPYING' for copying permission\n\"\"\"\n\nfrom lib.core.enums import DBMS\nfrom lib.core.settings import PGSQL_SYSTEM_DBS\nfrom lib.core.unescaper import unescaper\nfrom plugins.dbms.postgresql.enumeration import Enumeration\nfrom plugins.dbms.postgresql.filesystem import Filesystem\nfrom plugins.dbms.postgresql.fingerprint import Fingerprint\nfrom plugins.dbms.postgresql.syntax import Syntax\nfrom plugins.dbms.postgresql.takeover import Takeover\nfrom plugins.generic.misc import Miscellaneous\n\nclass PostgreSQLMap(Syntax, Fingerprint, Enumeration, Filesystem, Miscellaneous, Takeover):\n    \"\"\"\n    This class defines PostgreSQL methods\n    \"\"\"\n\n    def __init__(self):\n        self.excludeDbsList = PGSQL_SYSTEM_DBS\n        self.sysUdfs = {\n                         # UDF name:     UDF parameters' input data-type and return data-type\n                         \"sys_exec\":     { \"input\":  [\"text\"], \"return\": \"int4\" },\n                         \"sys_eval\":     { \"input\":  [\"text\"], \"return\": \"text\" },\n                         \"sys_bineval\":  { \"input\":  [\"text\"], \"return\": \"int4\" },\n                         \"sys_fileread\": { \"input\":  [\"text\"], \"return\": \"text\" }\n                       }\n\n        Syntax.__init__(self)\n        Fingerprint.__init__(self)\n        Enumeration.__init__(self)\n        Filesystem.__init__(self)\n        Miscellaneous.__init__(self)\n        Takeover.__init__(self)\n\n    unescaper[DBMS.PGSQL] = Syntax.escape\n", "license": "mit"}
{"id": "00e04b83ac45a83e54eee7a6e4e146fb683c3d98", "path": "tensorflow/contrib/framework/python/framework/experimental_test.py", "repo_name": "kevin-coder/tensorflow-fork", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\"\"\"@experimental tests.\"\"\"\n\n# pylint: disable=unused-import\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nfrom tensorflow.contrib.framework.python.framework import experimental\nfrom tensorflow.python.platform import test\nfrom tensorflow.python.platform import tf_logging as logging\n\n\nclass ExperimentalTest(test.TestCase):\n\n  @test.mock.patch.object(logging, \"warning\", autospec=True)\n  def test_warning(self, mock_warning):\n\n    @experimental\n    def _fn(arg0, arg1):\n      \"\"\"fn doc.\n\n      Args:\n        arg0: Arg 0.\n        arg1: Arg 1.\n\n      Returns:\n        Sum of args.\n      \"\"\"\n      return arg0 + arg1\n\n    # Assert function docs are properly updated.\n    self.assertEqual(\"_fn\", _fn.__name__)\n    self.assertEqual(\n        \"fn doc. (experimental)\"\n        \"\\n\"\n        \"\\nWarning: THIS FUNCTION IS EXPERIMENTAL. It may change \"\n        \"or be removed at any time, and without warning.\"\n        \"\\n\"\n        \"\\nArgs:\"\n        \"\\n  arg0: Arg 0.\"\n        \"\\n  arg1: Arg 1.\"\n        \"\\n\"\n        \"\\nReturns:\"\n        \"\\n  Sum of args.\", _fn.__doc__)\n\n    # Assert calling new fn issues log warning.\n    self.assertEqual(3, _fn(1, 2))\n    self.assertEqual(1, mock_warning.call_count)\n    (args, _) = mock_warning.call_args\n    self.assertRegexpMatches(args[0], r\"is experimental and may change\")\n\n\nif __name__ == \"__main__\":\n  test.main()\n", "license": "apache-2.0"}
{"id": "b79c8edd09b9804ff128ef2c6e75a986c6a66e5e", "path": "sql_grammar.py", "repo_name": "j127/aenea-grammars", "content": "# Created for aenea using libraries from the Dictation Toolbox\n# https://github.com/dictation-toolbox/dragonfly-scripts\n#\n# Commands for writing SQL queries\n#\n# Author: Tony Grosinger\n#\n# Licensed under LGPL\n\nimport aenea\nimport aenea.configuration\nfrom aenea import Choice\nfrom aenea.lax import Text\nimport dragonfly\n\n\nsql_map = {\n    \"update\": \"UPDATE \",\n    \"select\": \"SELECT \",\n    \"from\": \"FROM \",\n    \"count\": \"COUNT \",\n    \"values\": \"VALUES \",\n    \"as\": \"AS \",\n    \"when\": \"WHEN \",\n    \"in\": \"IN \",\n    \"into\": \"INTO \",\n    \"and\": \"AND \",\n    \"all\": \"ALL \",\n    \"similar to\": \"SIMILAR TO \",\n    \"like\": \"LIKE \",\n    \"set\": \"SET \",\n}\n\n\nsql_mapping = aenea.configuration.make_grammar_commands('sql', {\n    '<sqlKeyword>': Text(\"%(text)s\"),\n})\n\n\nclass SQL(dragonfly.MappingRule):\n    mapping = sql_mapping\n    extras = [\n        Choice('sqlKeyword', sql_map,)\n    ]\n\n\ndef get_grammar(context):\n    sql_grammar = dragonfly.Grammar('sql', context=context)\n    sql_grammar.add_rule(SQL())\n    return sql_grammar\n", "license": "lgpl-3.0"}
{"id": "c8ff531ebe3d811c5d3e3c24a67f247b577f315b", "path": "mapproxy/test/unit/test_seed_cachelock.py", "repo_name": "camptocamp/mapproxy", "content": "# This file is part of the MapProxy project.\n# Copyright (C) 2012 Omniscale <http://omniscale.de>\n# \n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n# \n#    http://www.apache.org/licenses/LICENSE-2.0\n# \n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\nfrom __future__ import with_statement\n\nimport multiprocessing\nimport os\nimport shutil\nimport tempfile\nimport time\n\nfrom mapproxy.seed.cachelock import CacheLocker, CacheLockedError\n\nclass TestCacheLock(object):\n    \n    def setup(self):\n        self.tmp_dir = tempfile.mkdtemp()\n        self.lock_file = os.path.join(self.tmp_dir, 'lock')\n        \n    def teardown(self):\n        shutil.rmtree(self.tmp_dir)\n    \n    def test_free_lock(self):\n        locker = CacheLocker(self.lock_file)\n        with locker.lock('foo'):\n            assert True\n    \n    def test_locked_by_process_no_block(self):\n        proc_is_locked = multiprocessing.Event()\n        def lock():\n            locker = CacheLocker(self.lock_file)\n            with locker.lock('foo'):\n                proc_is_locked.set()\n                time.sleep(10)\n        \n        p = multiprocessing.Process(target=lock)\n        p.start()\n        # wait for process to start\n        proc_is_locked.wait()\n        \n        locker = CacheLocker(self.lock_file)\n        \n        # test unlocked bar\n        with locker.lock('bar', no_block=True):\n            assert True\n        \n        # test locked foo\n        try:\n            with locker.lock('foo', no_block=True):\n                assert False\n        except CacheLockedError:\n            pass\n        finally:\n            p.terminate()\n            p.join()\n    \n    def test_locked_by_process_waiting(self):\n        proc_is_locked = multiprocessing.Event()\n        def lock():\n            locker = CacheLocker(self.lock_file)\n            with locker.lock('foo'):\n                proc_is_locked.set()\n                time.sleep(.1)\n        \n        p = multiprocessing.Process(target=lock)\n        start_time = time.time()\n        p.start()\n        # wait for process to start\n        proc_is_locked.wait()\n        \n        locker = CacheLocker(self.lock_file, polltime=0.02)\n        try:\n            with locker.lock('foo', no_block=False):\n                diff = time.time() - start_time\n                assert diff > 0.1\n        finally:\n            p.terminate()\n            p.join()", "license": "apache-2.0"}
{"id": "cace0d5cac22624b35b8c10381ec41f37c38e1cb", "path": "suds/bindings/document.py", "repo_name": "arnaudsj/suds", "content": "# This program is free software; you can redistribute it and/or modify\n# it under the terms of the (LGPL) GNU Lesser General Public License as\n# published by the Free Software Foundation; either version 3 of the \n# License, or (at your option) any later version.\n#\n# This program is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the\n# GNU Library Lesser General Public License for more details at\n# ( http://www.gnu.org/licenses/lgpl.html ).\n#\n# You should have received a copy of the GNU Lesser General Public License\n# along with this program; if not, write to the Free Software\n# Foundation, Inc., 59 Temple Place - Suite 330, Boston, MA 02111-1307, USA.\n# written by: Jeff Ortel ( jortel@redhat.com )\n\n\"\"\"\nProvides classes for the (WS) SOAP I{document/literal}.\n\"\"\"\n\nfrom logging import getLogger\nfrom suds import *\nfrom suds.bindings.binding import Binding\nfrom suds.sax.element import Element\n\nlog = getLogger(__name__)\n\n\nclass Document(Binding):\n    \"\"\"\n    The document/literal style.  Literal is the only (@use) supported\n    since document/encoded is pretty much dead.\n    Although the soap specification supports multiple documents within the soap\n    <body/>, it is very uncommon.  As such, suds presents an I{RPC} view of\n    service methods defined with a single document parameter.  This is done so \n    that the user can pass individual parameters instead of one, single document.\n    To support the complete specification, service methods defined with multiple documents\n    (multiple message parts), must present a I{document} view for that method.\n    \"\"\"\n        \n    def bodycontent(self, method, args, kwargs):\n        #\n        # The I{wrapped} vs I{bare} style is detected in 2 ways.\n        # If there is 2+ parts in the message then it is I{bare}.\n        # If there is only (1) part and that part resolves to a builtin then\n        # it is I{bare}.  Otherwise, it is I{wrapped}.\n        #\n        if not len(method.soap.input.body.parts):\n            return ()\n        wrapped = method.soap.input.body.wrapped\n        if wrapped:\n            pts = self.bodypart_types(method)\n            root = self.document(pts[0])\n        else:\n            root = []\n        n = 0\n        for pd in self.param_defs(method):\n            if n < len(args):\n                value = args[n]\n            else:\n                value = kwargs.get(pd[0])\n            n += 1\n            p = self.mkparam(method, pd, value)\n            if p is None:\n                continue\n            if not wrapped:\n                ns = pd[1].namespace('ns0')\n                p.setPrefix(ns[0], ns[1])\n            root.append(p)\n        return root\n\n    def replycontent(self, method, body):\n        wrapped = method.soap.output.body.wrapped\n        if wrapped:\n            return body[0].children\n        else:\n            return body.children\n        \n    def document(self, wrapper):\n        \"\"\"\n        Get the document root.  For I{document/literal}, this is the\n        name of the wrapper element qualifed by the schema tns.\n        @param wrapper: The method name.\n        @type wrapper: L{xsd.sxbase.SchemaObject}\n        @return: A root element.\n        @rtype: L{Element}\n        \"\"\"\n        tag = wrapper[1].name\n        ns = wrapper[1].namespace('ns0')\n        d = Element(tag, ns=ns)\n        return d\n    \n    def mkparam(self, method, pdef, object):\n        #\n        # Expand list parameters into individual parameters\n        # each with the type information.  This is because in document\n        # arrays are simply unbounded elements.\n        #\n        if isinstance(object, (list, tuple)):\n            tags = []\n            for item in object:\n                tags.append(self.mkparam(method, pdef, item))\n            return tags\n        else:\n            return Binding.mkparam(self, method, pdef, object)\n        \n    def param_defs(self, method):\n        #\n        # Get parameter definitions for document literal.\n        # The I{wrapped} vs I{bare} style is detected in 2 ways.\n        # If there is 2+ parts in the message then it is I{bare}.\n        # If there is only (1) part and that part resolves to a builtin then\n        # it is I{bare}.  Otherwise, it is I{wrapped}.\n        #\n        pts = self.bodypart_types(method)\n        wrapped = method.soap.input.body.wrapped\n        if not wrapped:\n            return pts\n        result = []\n        # wrapped\n        for p in pts:\n            resolved = p[1].resolve()\n            for child, ancestry in resolved:\n                if child.isattr():\n                    continue\n                if self.bychoice(ancestry):\n                    log.debug(\n                        '%s\\ncontained by <choice/>, excluded as param for %s()',\n                        child,\n                        method.name)\n                    continue\n                result.append((child.name, child))\n        return result\n    \n    def returned_types(self, method):\n        result = []\n        wrapped = method.soap.output.body.wrapped\n        rts = self.bodypart_types(method, input=False)\n        if wrapped:\n            for pt in rts:\n                resolved = pt.resolve(nobuiltin=True)\n                for child, ancestry in resolved:\n                    result.append(child)\n                break\n        else:\n            result += rts\n        return result\n    \n    def bychoice(self, ancestry):\n        \"\"\"\n        The ancestry contains a <choice/>\n        @param ancestry: A list of ancestors.\n        @type ancestry: list\n        @return: True if contains <choice/>\n        @rtype: boolean\n        \"\"\"\n        for x in ancestry:\n            if x.choice():\n                return True\n        return False", "license": "lgpl-3.0"}
{"id": "a120bc6cc3975a3d4559d018c8aa74ff42a16d2d", "path": "tensorflow/contrib/learn/python/learn/estimators/tensor_signature.py", "repo_name": "sandeepdsouza93/TensorFlow-15712", "content": "# Copyright 2016 The TensorFlow Authors. All Rights Reserved.\n#\n# Licensed under the Apache License, Version 2.0 (the \"License\");\n# you may not use this file except in compliance with the License.\n# You may obtain a copy of the License at\n#\n#     http://www.apache.org/licenses/LICENSE-2.0\n#\n# Unless required by applicable law or agreed to in writing, software\n# distributed under the License is distributed on an \"AS IS\" BASIS,\n# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n# See the License for the specific language governing permissions and\n# limitations under the License.\n# ==============================================================================\n\n\"\"\"TensorSignature class and utilities.\"\"\"\n\nfrom __future__ import absolute_import\nfrom __future__ import division\nfrom __future__ import print_function\n\nimport collections\n\nfrom tensorflow.python.framework import dtypes\nfrom tensorflow.python.framework import sparse_tensor\nfrom tensorflow.python.framework import tensor_shape\nfrom tensorflow.python.ops import array_ops\nfrom tensorflow.python.ops import math_ops\nfrom tensorflow.python.ops import parsing_ops\n\n\nclass TensorSignature(collections.namedtuple(\n    \"TensorSignature\", [\"dtype\", \"shape\", \"is_sparse\"])):\n  \"\"\"Signature of the `Tensor` object.\n\n  Useful to check compatibility of tensors.\n\n  Example:\n\n  ```python\n  examples = tf.placeholder(...)\n  inputs = {'a': var_a, 'b': var_b}\n  signatures = tensor_signature.create_signatures(inputs)\n  result = tensor_signature.create_example_parser_from_signatures(\n      signatures, examples)\n  self.assertTrue(tensor_signature.tensors_compatible(result, signatures))\n  ```\n\n  Attributes:\n    dtype: `DType` object.\n    shape: `TensorShape` object.\n  \"\"\"\n\n  def __new__(cls, tensor):\n    if isinstance(tensor, sparse_tensor.SparseTensor):\n      return super(TensorSignature, cls).__new__(\n          cls, dtype=tensor.values.dtype, shape=None, is_sparse=True)\n    return super(TensorSignature, cls).__new__(\n        cls, dtype=tensor.dtype, shape=tensor.get_shape(), is_sparse=False)\n\n  def is_compatible_with(self, other):\n    \"\"\"Returns True if signatures are compatible.\"\"\"\n\n    def _shape_is_compatible_0dim(this, other):\n      \"\"\"Checks that shapes are compatible skipping dim 0.\"\"\"\n      other = tensor_shape.as_shape(other)\n      # If shapes are None (unknown) they may be compatible.\n      if this.dims is None or other.dims is None:\n        return True\n      if this.ndims != other.ndims:\n        return False\n      for dim, (x_dim, y_dim) in enumerate(zip(this.dims, other.dims)):\n        if dim == 0:\n          continue\n        if not x_dim.is_compatible_with(y_dim):\n          return False\n      return True\n\n    if other.is_sparse:\n      return self.is_sparse and self.dtype.is_compatible_with(other.dtype)\n    return (self.dtype.is_compatible_with(other.dtype) and\n            _shape_is_compatible_0dim(self.shape, other.shape) and\n            not self.is_sparse)\n\n  def get_placeholder(self):\n    if self.is_sparse:\n      return array_ops.sparse_placeholder(dtype=self.dtype)\n    return array_ops.placeholder(dtype=self.dtype,\n                                 shape=[None] + list(self.shape[1:]))\n\n  def get_feature_spec(self):\n    dtype = self.dtype\n    # Convert, because example parser only supports float32, int64 and string.\n    if dtype == dtypes.int32:\n      dtype = dtypes.int64\n    if dtype == dtypes.float64:\n      dtype = dtypes.float32\n    if self.is_sparse:\n      return parsing_ops.VarLenFeature(dtype=dtype)\n    return parsing_ops.FixedLenFeature(shape=self.shape[1:], dtype=dtype)\n\n\ndef tensors_compatible(tensors, signatures):\n  \"\"\"Check that tensors are compatible with signatures.\n\n  Args:\n    tensors: Dict of `Tensor` objects or single `Tensor` object.\n    signatures: Dict of `TensorSignature` objects or\n                single `TensorSignature` object.\n\n  Returns:\n    True if all tensors are compatible, False otherwise.\n  \"\"\"\n  # Dict of Tensors as input.\n  if tensors is None:\n    return signatures is None\n\n  if isinstance(tensors, dict):\n    if not isinstance(signatures, dict):\n      return False\n    for key in signatures:\n      if key not in tensors:\n        return False\n      if not TensorSignature(tensors[key]).is_compatible_with(signatures[key]):\n        return False\n    return True\n\n  # Single tensor as input.\n  if signatures is None or isinstance(signatures, dict):\n    return False\n  return TensorSignature(tensors).is_compatible_with(signatures)\n\n\ndef create_signatures(tensors):\n  \"\"\"Creates TensorSignature objects for given tensors.\n\n  Args:\n    tensors: Dict of `Tensor` objects or single `Tensor`.\n\n  Returns:\n    Dict of `TensorSignature` objects or single `TensorSignature`.\n  \"\"\"\n  if isinstance(tensors, dict):\n    return {\n        key: TensorSignature(tensors[key]) for key in tensors}\n  if tensors is None:\n    return None\n  return TensorSignature(tensors)\n\n\ndef create_placeholders_from_signatures(signatures):\n  \"\"\"Creates placeholders from given signatures.\n\n  Args:\n    signatures: Dict of `TensorSignature` objects or single `TensorSignature`,\n      or `None`.\n\n  Returns:\n    Dict of `tf.placeholder` objects or single `tf.placeholder`, or `None`.\n  \"\"\"\n  if signatures is None:\n    return None\n  if not isinstance(signatures, dict):\n    return signatures.get_placeholder()\n  return {\n      key: signatures[key].get_placeholder()\n      for key in signatures}\n\n\ndef create_example_parser_from_signatures(signatures, examples_batch,\n                                          single_feature_name=\"feature\"):\n  \"\"\"Creates example parser from given signatures.\n\n  Args:\n    signatures: Dict of `TensorSignature` objects or single `TensorSignature`.\n    examples_batch: string `Tensor` of serialized `Example` proto.\n    single_feature_name: string, single feature name.\n\n  Returns:\n    features: `Tensor` or `dict` of `Tensor` objects.\n  \"\"\"\n  feature_spec = {}\n  if not isinstance(signatures, dict):\n    feature_spec[single_feature_name] = signatures.get_feature_spec()\n  else:\n    feature_spec = {key: signatures[key].get_feature_spec()\n                    for key in signatures}\n  features = parsing_ops.parse_example(examples_batch, feature_spec)\n  if not isinstance(signatures, dict):\n    # Returns single feature, casts if needed.\n    features = features[single_feature_name]\n    if not signatures.dtype.is_compatible_with(features.dtype):\n      features = math_ops.cast(features, signatures.dtype)\n    return features\n  # Returns dict of features, casts if needed.\n  for name in features:\n    if not signatures[name].dtype.is_compatible_with(features[name].dtype):\n      features[name] = math_ops.cast(features[name], signatures[name].dtype)\n  return features\n", "license": "apache-2.0"}
{"id": "ddb5837655d9c3adb3113cccf57e6c700b4c7a92", "path": "venv/lib/python2.7/site-packages/requests/packages/chardet/langgreekmodel.py", "repo_name": "JFriel/honours_project", "content": "######################## BEGIN LICENSE BLOCK ########################\n# The Original Code is Mozilla Communicator client code.\n#\n# The Initial Developer of the Original Code is\n# Netscape Communications Corporation.\n# Portions created by the Initial Developer are Copyright (C) 1998\n# the Initial Developer. All Rights Reserved.\n#\n# Contributor(s):\n#   Mark Pilgrim - port to Python\n#\n# This library is free software; you can redistribute it and/or\n# modify it under the terms of the GNU Lesser General Public\n# License as published by the Free Software Foundation; either\n# version 2.1 of the License, or (at your option) any later version.\n#\n# This library is distributed in the hope that it will be useful,\n# but WITHOUT ANY WARRANTY; without even the implied warranty of\n# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\n# Lesser General Public License for more details.\n#\n# You should have received a copy of the GNU Lesser General Public\n# License along with this library; if not, write to the Free Software\n# Foundation, Inc., 51 Franklin St, Fifth Floor, Boston, MA\n# 02110-1301  USA\n######################### END LICENSE BLOCK #########################\n\n# 255: Control characters that usually does not exist in any text\n# 254: Carriage/Return\n# 253: symbol (punctuation) that does not belong to word\n# 252: 0 - 9\n\n# Character Mapping Table:\nLatin7_CharToOrderMap = (\n255,255,255,255,255,255,255,255,255,255,254,255,255,254,255,255,  # 00\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 10\n253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,  # 20\n252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,  # 30\n253, 82,100,104, 94, 98,101,116,102,111,187,117, 92, 88,113, 85,  # 40\n 79,118,105, 83, 67,114,119, 95, 99,109,188,253,253,253,253,253,  # 50\n253, 72, 70, 80, 81, 60, 96, 93, 89, 68,120, 97, 77, 86, 69, 55,  # 60\n 78,115, 65, 66, 58, 76,106,103, 87,107,112,253,253,253,253,253,  # 70\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 80\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 90\n253,233, 90,253,253,253,253,253,253,253,253,253,253, 74,253,253,  # a0\n253,253,253,253,247,248, 61, 36, 46, 71, 73,253, 54,253,108,123,  # b0\n110, 31, 51, 43, 41, 34, 91, 40, 52, 47, 44, 53, 38, 49, 59, 39,  # c0\n 35, 48,250, 37, 33, 45, 56, 50, 84, 57,120,121, 17, 18, 22, 15,  # d0\n124,  1, 29, 20, 21,  3, 32, 13, 25,  5, 11, 16, 10,  6, 30,  4,  # e0\n  9,  8, 14,  7,  2, 12, 28, 23, 42, 24, 64, 75, 19, 26, 27,253,  # f0\n)\n\nwin1253_CharToOrderMap = (\n255,255,255,255,255,255,255,255,255,255,254,255,255,254,255,255,  # 00\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 10\n253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,253,  # 20\n252,252,252,252,252,252,252,252,252,252,253,253,253,253,253,253,  # 30\n253, 82,100,104, 94, 98,101,116,102,111,187,117, 92, 88,113, 85,  # 40\n 79,118,105, 83, 67,114,119, 95, 99,109,188,253,253,253,253,253,  # 50\n253, 72, 70, 80, 81, 60, 96, 93, 89, 68,120, 97, 77, 86, 69, 55,  # 60\n 78,115, 65, 66, 58, 76,106,103, 87,107,112,253,253,253,253,253,  # 70\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 80\n255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,255,  # 90\n253,233, 61,253,253,253,253,253,253,253,253,253,253, 74,253,253,  # a0\n253,253,253,253,247,253,253, 36, 46, 71, 73,253, 54,253,108,123,  # b0\n110, 31, 51, 43, 41, 34, 91, 40, 52, 47, 44, 53, 38, 49, 59, 39,  # c0\n 35, 48,250, 37, 33, 45, 56, 50, 84, 57,120,121, 17, 18, 22, 15,  # d0\n124,  1, 29, 20, 21,  3, 32, 13, 25,  5, 11, 16, 10,  6, 30,  4,  # e0\n  9,  8, 14,  7,  2, 12, 28, 23, 42, 24, 64, 75, 19, 26, 27,253,  # f0\n)\n\n# Model Table:\n# total sequences: 100%\n# first 512 sequences: 98.2851%\n# first 1024 sequences:1.7001%\n# rest  sequences:     0.0359%\n# negative sequences:  0.0148%\nGreekLangModel = (\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,2,2,3,3,3,3,3,3,3,3,1,3,3,3,0,2,2,3,3,0,3,0,3,2,0,3,3,3,0,\n3,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,0,3,3,0,3,2,3,3,0,3,2,3,3,3,0,0,3,0,3,0,3,3,2,0,0,0,\n2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,\n0,2,3,2,2,3,3,3,3,3,3,3,3,0,3,3,3,3,0,2,3,3,0,3,3,3,3,2,3,3,3,0,\n2,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,3,3,2,3,3,3,3,3,3,3,3,3,3,3,3,0,2,1,3,3,3,3,2,3,3,2,3,3,2,0,\n0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,0,3,3,3,3,3,3,0,3,3,0,3,3,3,3,3,3,3,3,3,3,0,3,2,3,3,0,\n2,0,1,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,2,3,0,0,0,0,3,3,0,3,1,3,3,3,0,3,3,0,3,3,3,3,0,0,0,0,\n2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,0,3,0,3,3,3,3,3,0,3,2,2,2,3,0,2,3,3,3,3,3,2,3,3,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,3,2,2,2,3,3,3,3,0,3,1,3,3,3,3,2,3,3,3,3,3,3,3,2,2,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,2,0,3,0,0,0,3,3,2,3,3,3,3,3,0,0,3,2,3,0,2,3,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,3,3,3,3,0,0,3,3,0,2,3,0,3,0,3,3,3,0,0,3,0,3,0,2,2,3,3,0,0,\n0,0,1,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,2,0,3,2,3,3,3,3,0,3,3,3,3,3,0,3,3,2,3,2,3,3,2,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,2,3,2,3,3,3,3,3,3,0,2,3,2,3,2,2,2,3,2,3,3,2,3,0,2,2,2,3,0,\n2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,0,0,0,3,3,3,2,3,3,0,0,3,0,3,0,0,0,3,2,0,3,0,3,0,0,2,0,2,0,\n0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,0,3,3,3,3,3,3,0,3,3,0,3,0,0,0,3,3,0,3,3,3,0,0,1,2,3,0,\n3,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,2,0,0,3,2,2,3,3,0,3,3,3,3,3,2,1,3,0,3,2,3,3,2,1,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,3,0,2,3,3,3,3,3,3,0,0,3,0,3,0,0,0,3,3,0,3,2,3,0,0,3,3,3,0,\n3,0,0,0,2,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,0,3,3,3,3,3,3,0,0,3,0,3,0,0,0,3,2,0,3,2,3,0,0,3,2,3,0,\n2,0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,1,2,2,3,3,3,3,3,3,0,2,3,0,3,0,0,0,3,3,0,3,0,2,0,0,2,3,1,0,\n2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,3,3,3,3,0,3,0,3,3,2,3,0,3,3,3,3,3,3,0,3,3,3,0,2,3,0,0,3,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,3,3,3,0,0,3,0,0,0,3,3,0,3,0,2,3,3,0,0,3,0,3,0,3,3,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,0,0,0,3,3,3,3,3,3,0,0,3,0,2,0,0,0,3,3,0,3,0,3,0,0,2,0,2,0,\n0,0,0,0,1,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,3,0,3,0,2,0,3,2,0,3,2,3,2,3,0,0,3,2,3,2,3,3,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,0,0,2,3,3,3,3,3,0,0,0,3,0,2,1,0,0,3,2,2,2,0,3,0,0,2,2,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,3,3,3,2,0,3,0,3,0,3,3,0,2,1,2,3,3,0,0,3,0,3,0,3,3,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,3,3,3,0,3,3,3,3,3,3,0,2,3,0,3,0,0,0,2,1,0,2,2,3,0,0,2,2,2,0,\n0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,3,0,0,2,3,3,3,2,3,0,0,1,3,0,2,0,0,0,0,3,0,1,0,2,0,0,1,1,1,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,3,1,0,3,0,0,0,3,2,0,3,2,3,3,3,0,0,3,0,3,2,2,2,1,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,3,3,3,0,0,3,0,0,0,0,2,0,2,3,3,2,2,2,2,3,0,2,0,2,2,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,3,3,3,2,0,0,0,0,0,0,2,3,0,2,0,2,3,2,0,0,3,0,3,0,3,1,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,3,2,3,3,2,2,3,0,2,0,3,0,0,0,2,0,0,0,0,1,2,0,2,0,2,0,\n0,2,0,2,0,2,2,0,0,1,0,2,2,2,0,2,2,2,0,2,2,2,0,0,2,0,0,1,0,0,0,0,\n0,2,0,3,3,2,0,0,0,0,0,0,1,3,0,2,0,2,2,2,0,0,2,0,3,0,0,2,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,3,0,2,3,2,0,2,2,0,2,0,2,2,0,2,0,2,2,2,0,0,0,0,0,0,2,3,0,0,0,2,\n0,1,2,0,0,0,0,2,2,0,0,0,2,1,0,2,2,0,0,0,0,0,0,1,0,2,0,0,0,0,0,0,\n0,0,2,1,0,2,3,2,2,3,2,3,2,0,0,3,3,3,0,0,3,2,0,0,0,1,1,0,2,0,2,2,\n0,2,0,2,0,2,2,0,0,2,0,2,2,2,0,2,2,2,2,0,0,2,0,0,0,2,0,1,0,0,0,0,\n0,3,0,3,3,2,2,0,3,0,0,0,2,2,0,2,2,2,1,2,0,0,1,2,2,0,0,3,0,0,0,2,\n0,1,2,0,0,0,1,2,0,0,0,0,0,0,0,2,2,0,1,0,0,2,0,0,0,2,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,3,3,2,2,0,0,0,2,0,2,3,3,0,2,0,0,0,0,0,0,2,2,2,0,2,2,0,2,0,2,\n0,2,2,0,0,2,2,2,2,1,0,0,2,2,0,2,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,\n0,2,0,3,2,3,0,0,0,3,0,0,2,2,0,2,0,2,2,2,0,0,2,0,0,0,0,0,0,0,0,2,\n0,0,2,2,0,0,2,2,2,0,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,2,0,0,3,2,0,2,2,2,2,2,0,0,0,2,0,0,0,0,2,0,1,0,0,2,0,1,0,0,0,\n0,2,2,2,0,2,2,0,1,2,0,2,2,2,0,2,2,2,2,1,2,2,0,0,2,0,0,0,0,0,0,0,\n0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,\n0,2,0,2,0,2,2,0,0,0,0,1,2,1,0,0,2,2,0,0,2,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,3,2,3,0,0,2,0,0,0,2,2,0,2,0,0,0,1,0,0,2,0,2,0,2,2,0,0,0,0,\n0,0,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,\n0,2,2,3,2,2,0,0,0,0,0,0,1,3,0,2,0,2,2,0,0,0,1,0,2,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,0,2,0,3,2,0,2,0,0,0,0,0,0,2,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,\n0,0,2,0,0,0,0,1,1,0,0,2,1,2,0,2,2,0,1,0,0,1,0,0,0,2,0,0,0,0,0,0,\n0,3,0,2,2,2,0,0,2,0,0,0,2,0,0,0,2,3,0,2,0,0,0,0,0,0,2,2,0,0,0,2,\n0,1,2,0,0,0,1,2,2,1,0,0,0,2,0,0,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,3,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,1,2,0,2,2,0,2,0,0,2,0,0,0,0,1,2,1,0,2,1,0,0,0,0,0,0,0,0,0,0,\n0,0,2,0,0,0,3,1,2,2,0,2,0,0,0,0,2,0,0,0,2,0,0,3,0,0,0,0,2,2,2,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,1,0,2,0,1,2,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,1,0,0,0,0,0,0,2,\n0,2,2,0,0,2,2,2,2,2,0,1,2,0,0,0,2,2,0,1,0,2,0,0,2,2,0,0,0,0,0,0,\n0,0,0,0,1,0,0,0,0,0,0,0,3,0,0,2,0,0,0,0,0,0,0,0,2,0,2,0,0,0,0,2,\n0,1,2,0,0,0,0,2,2,1,0,1,0,1,0,2,2,2,1,0,0,0,0,0,0,1,0,0,0,0,0,0,\n0,2,0,1,2,0,0,0,0,0,0,0,0,0,0,2,0,0,2,2,0,0,0,0,1,0,0,0,0,0,0,2,\n0,2,2,0,0,0,0,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,\n0,2,2,2,2,0,0,0,3,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,1,\n0,0,2,0,0,0,0,1,2,0,0,0,0,0,0,2,2,1,1,0,0,0,0,0,0,1,0,0,0,0,0,0,\n0,2,0,2,2,2,0,0,2,0,0,0,0,0,0,0,2,2,2,0,0,0,2,0,0,0,0,0,0,0,0,2,\n0,0,1,0,0,0,0,2,1,0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,\n0,3,0,2,0,0,0,0,0,0,0,0,2,0,0,0,0,0,2,0,0,0,0,0,0,0,2,0,0,0,0,2,\n0,0,2,0,0,0,0,2,2,0,0,0,0,1,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,2,0,2,2,1,0,0,0,0,0,0,2,0,0,2,0,2,2,2,0,0,0,0,0,0,2,0,0,0,0,2,\n0,0,2,0,0,2,0,2,2,0,0,0,0,2,0,2,0,0,0,0,0,2,0,0,0,2,0,0,0,0,0,0,\n0,0,3,0,0,0,2,2,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,1,0,0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,2,0,0,0,0,0,\n0,2,2,2,2,2,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,1,1,0,0,0,1,\n0,0,0,0,0,0,0,2,1,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,2,2,0,0,0,0,0,2,0,0,0,0,0,0,0,0,1,0,0,0,0,0,0,0,0,\n0,2,0,0,0,2,0,0,0,0,0,1,0,0,0,0,2,2,0,0,0,1,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,1,0,2,0,0,0,\n0,2,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,0,2,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,1,0,0,0,0,1,1,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,1,0,0,0,0,0,0,0,0,0,0,0,0,0,1,0,0,0,0,1,0,0,2,0,2,0,0,0,\n0,0,0,0,0,0,0,0,2,1,0,0,0,0,0,0,2,0,0,0,1,2,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,\n)\n\nLatin7GreekModel = {\n  'charToOrderMap': Latin7_CharToOrderMap,\n  'precedenceMatrix': GreekLangModel,\n  'mTypicalPositiveRatio': 0.982851,\n  'keepEnglishLetter': False,\n  'charsetName': \"ISO-8859-7\"\n}\n\nWin1253GreekModel = {\n  'charToOrderMap': win1253_CharToOrderMap,\n  'precedenceMatrix': GreekLangModel,\n  'mTypicalPositiveRatio': 0.982851,\n  'keepEnglishLetter': False,\n  'charsetName': \"windows-1253\"\n}\n\n# flake8: noqa\n", "license": "gpl-3.0"}
{"id": "fb947e88c138a4539b14ea942985797f7f963f3d", "path": "lib/ansible/config/data.py", "repo_name": "shanemcd/ansible", "content": "# Copyright (c) 2017 Ansible Project\n# GNU General Public License v3.0+ (see COPYING or https://www.gnu.org/licenses/gpl-3.0.txt)\n\n# Make coding more python3-ish\nfrom __future__ import (absolute_import, division, print_function)\n__metaclass__ = type\n\n\nclass ConfigData(object):\n\n    def __init__(self):\n        self._global_settings = {}\n        self._plugins = {}\n\n    def get_setting(self, name, plugin=None):\n\n        setting = None\n        if plugin is None:\n            setting = self._global_settings.get(name)\n        elif plugin.type in self._plugins and plugin.name in self._plugins[plugin.type]:\n            setting = self._plugins[plugin.type][plugin.name].get(name)\n\n        return setting\n\n    def get_settings(self, plugin=None):\n\n        settings = []\n        if plugin is None:\n            settings = [ self._global_settings[k] for k in self._global_settings ]\n        elif plugin.type in self._plugins and plugin.name in self._plugins[plugin.type]:\n            settings = [ self._plugins[plugin.type][plugin.name][k] for k in self._plugins[plugin.type][plugin.name] ]\n\n        return settings\n\n    def update_setting(self, setting, plugin=None):\n\n        if plugin is None:\n            self._global_settings[setting.name] = setting\n        else:\n            if plugin.type not in self._plugins:\n                self._plugins[plugin.type] = {}\n            if plugin.name not in self._plugins[plugin.type]:\n                self._plugins[plugin.type][plugin.name] = {}\n            self._plugins[plugin.type][plugin.name][setting.name] = setting\n\n", "license": "gpl-3.0"}
{"id": "957a4a52d64cace4be6b4a4064fc56219b0d37dc", "path": "gyp/test/same-gyp-name/gyptest-library.py", "repo_name": "sballesteros/node-gyp", "content": "#!/usr/bin/env python\n\n# Copyright (c) 2012 Google Inc. All rights reserved.\n# Use of this source code is governed by a BSD-style license that can be\n# found in the LICENSE file.\n\n\"\"\"\nVerifies that a dependency on two gyp files with the same name do not create a\nuid collision in the resulting generated xcode file.\n\"\"\"\n\nimport TestGyp\n\nimport sys\n\ntest = TestGyp.TestGyp()\n\ntest.run_gyp('test.gyp', chdir='library')\n\ntest.pass_test()\n", "license": "mit"}
{"id": "2bf41aae6da0b4b762ecbea90b05dd7905826a25", "path": "plugins/hg4idea/testData/bin/hgext/acl.py", "repo_name": "allotria/intellij-community", "content": "# acl.py - changeset access control for mercurial\n#\n# Copyright 2006 Vadim Gelfer <vadim.gelfer@gmail.com>\n#\n# This software may be used and distributed according to the terms of the\n# GNU General Public License version 2 or any later version.\n\n'''hooks for controlling repository access\n\nThis hook makes it possible to allow or deny write access to given\nbranches and paths of a repository when receiving incoming changesets\nvia pretxnchangegroup and pretxncommit.\n\nThe authorization is matched based on the local user name on the\nsystem where the hook runs, and not the committer of the original\nchangeset (since the latter is merely informative).\n\nThe acl hook is best used along with a restricted shell like hgsh,\npreventing authenticating users from doing anything other than pushing\nor pulling. The hook is not safe to use if users have interactive\nshell access, as they can then disable the hook. Nor is it safe if\nremote users share an account, because then there is no way to\ndistinguish them.\n\nThe order in which access checks are performed is:\n\n1) Deny  list for branches (section ``acl.deny.branches``)\n2) Allow list for branches (section ``acl.allow.branches``)\n3) Deny  list for paths    (section ``acl.deny``)\n4) Allow list for paths    (section ``acl.allow``)\n\nThe allow and deny sections take key-value pairs.\n\nBranch-based Access Control\n---------------------------\n\nUse the ``acl.deny.branches`` and ``acl.allow.branches`` sections to\nhave branch-based access control. Keys in these sections can be\neither:\n\n- a branch name, or\n- an asterisk, to match any branch;\n\nThe corresponding values can be either:\n\n- a comma-separated list containing users and groups, or\n- an asterisk, to match anyone;\n\nYou can add the \"!\" prefix to a user or group name to invert the sense\nof the match.\n\nPath-based Access Control\n-------------------------\n\nUse the ``acl.deny`` and ``acl.allow`` sections to have path-based\naccess control. Keys in these sections accept a subtree pattern (with\na glob syntax by default). The corresponding values follow the same\nsyntax as the other sections above.\n\nGroups\n------\n\nGroup names must be prefixed with an ``@`` symbol. Specifying a group\nname has the same effect as specifying all the users in that group.\n\nYou can define group members in the ``acl.groups`` section.\nIf a group name is not defined there, and Mercurial is running under\na Unix-like system, the list of users will be taken from the OS.\nOtherwise, an exception will be raised.\n\nExample Configuration\n---------------------\n\n::\n\n  [hooks]\n\n  # Use this if you want to check access restrictions at commit time\n  pretxncommit.acl = python:hgext.acl.hook\n\n  # Use this if you want to check access restrictions for pull, push,\n  # bundle and serve.\n  pretxnchangegroup.acl = python:hgext.acl.hook\n\n  [acl]\n  # Allow or deny access for incoming changes only if their source is\n  # listed here, let them pass otherwise. Source is \"serve\" for all\n  # remote access (http or ssh), \"push\", \"pull\" or \"bundle\" when the\n  # related commands are run locally.\n  # Default: serve\n  sources = serve\n\n  [acl.deny.branches]\n\n  # Everyone is denied to the frozen branch:\n  frozen-branch = *\n\n  # A bad user is denied on all branches:\n  * = bad-user\n\n  [acl.allow.branches]\n\n  # A few users are allowed on branch-a:\n  branch-a = user-1, user-2, user-3\n\n  # Only one user is allowed on branch-b:\n  branch-b = user-1\n\n  # The super user is allowed on any branch:\n  * = super-user\n\n  # Everyone is allowed on branch-for-tests:\n  branch-for-tests = *\n\n  [acl.deny]\n  # This list is checked first. If a match is found, acl.allow is not\n  # checked. All users are granted access if acl.deny is not present.\n  # Format for both lists: glob pattern = user, ..., @group, ...\n\n  # To match everyone, use an asterisk for the user:\n  # my/glob/pattern = *\n\n  # user6 will not have write access to any file:\n  ** = user6\n\n  # Group \"hg-denied\" will not have write access to any file:\n  ** = @hg-denied\n\n  # Nobody will be able to change \"DONT-TOUCH-THIS.txt\", despite\n  # everyone being able to change all other files. See below.\n  src/main/resources/DONT-TOUCH-THIS.txt = *\n\n  [acl.allow]\n  # if acl.allow is not present, all users are allowed by default\n  # empty acl.allow = no users allowed\n\n  # User \"doc_writer\" has write access to any file under the \"docs\"\n  # folder:\n  docs/** = doc_writer\n\n  # User \"jack\" and group \"designers\" have write access to any file\n  # under the \"images\" folder:\n  images/** = jack, @designers\n\n  # Everyone (except for \"user6\" and \"@hg-denied\" - see acl.deny above)\n  # will have write access to any file under the \"resources\" folder\n  # (except for 1 file. See acl.deny):\n  src/main/resources/** = *\n\n  .hgtags = release_engineer\n\nExamples using the \"!\" prefix\n.............................\n\nSuppose there's a branch that only a given user (or group) should be able to\npush to, and you don't want to restrict access to any other branch that may\nbe created.\n\nThe \"!\" prefix allows you to prevent anyone except a given user or group to\npush changesets in a given branch or path.\n\nIn the examples below, we will:\n1) Deny access to branch \"ring\" to anyone but user \"gollum\"\n2) Deny access to branch \"lake\" to anyone but members of the group \"hobbit\"\n3) Deny access to a file to anyone but user \"gollum\"\n\n::\n\n  [acl.allow.branches]\n  # Empty\n\n  [acl.deny.branches]\n\n  # 1) only 'gollum' can commit to branch 'ring';\n  # 'gollum' and anyone else can still commit to any other branch.\n  ring = !gollum\n\n  # 2) only members of the group 'hobbit' can commit to branch 'lake';\n  # 'hobbit' members and anyone else can still commit to any other branch.\n  lake = !@hobbit\n\n  # You can also deny access based on file paths:\n\n  [acl.allow]\n  # Empty\n\n  [acl.deny]\n  # 3) only 'gollum' can change the file below;\n  # 'gollum' and anyone else can still change any other file.\n  /misty/mountains/cave/ring = !gollum\n\n'''\n\nfrom mercurial.i18n import _\nfrom mercurial import util, match\nimport getpass, urllib\n\ntestedwith = 'internal'\n\ndef _getusers(ui, group):\n\n    # First, try to use group definition from section [acl.groups]\n    hgrcusers = ui.configlist('acl.groups', group)\n    if hgrcusers:\n        return hgrcusers\n\n    ui.debug('acl: \"%s\" not defined in [acl.groups]\\n' % group)\n    # If no users found in group definition, get users from OS-level group\n    try:\n        return util.groupmembers(group)\n    except KeyError:\n        raise util.Abort(_(\"group '%s' is undefined\") % group)\n\ndef _usermatch(ui, user, usersorgroups):\n\n    if usersorgroups == '*':\n        return True\n\n    for ug in usersorgroups.replace(',', ' ').split():\n\n        if ug.startswith('!'):\n            # Test for excluded user or group. Format:\n            # if ug is a user  name: !username\n            # if ug is a group name: !@groupname\n            ug = ug[1:]\n            if not ug.startswith('@') and user != ug \\\n                or ug.startswith('@') and user not in _getusers(ui, ug[1:]):\n                return True\n\n        # Test for user or group. Format:\n        # if ug is a user  name: username\n        # if ug is a group name: @groupname\n        elif user == ug \\\n             or ug.startswith('@') and user in _getusers(ui, ug[1:]):\n            return True\n\n    return False\n\ndef buildmatch(ui, repo, user, key):\n    '''return tuple of (match function, list enabled).'''\n    if not ui.has_section(key):\n        ui.debug('acl: %s not enabled\\n' % key)\n        return None\n\n    pats = [pat for pat, users in ui.configitems(key)\n            if _usermatch(ui, user, users)]\n    ui.debug('acl: %s enabled, %d entries for user %s\\n' %\n             (key, len(pats), user))\n\n    # Branch-based ACL\n    if not repo:\n        if pats:\n            # If there's an asterisk (meaning \"any branch\"), always return True;\n            # Otherwise, test if b is in pats\n            if '*' in pats:\n                return util.always\n            return lambda b: b in pats\n        return util.never\n\n    # Path-based ACL\n    if pats:\n        return match.match(repo.root, '', pats)\n    return util.never\n\ndef hook(ui, repo, hooktype, node=None, source=None, **kwargs):\n    if hooktype not in ['pretxnchangegroup', 'pretxncommit']:\n        raise util.Abort(_('config error - hook type \"%s\" cannot stop '\n                           'incoming changesets nor commits') % hooktype)\n    if (hooktype == 'pretxnchangegroup' and\n        source not in ui.config('acl', 'sources', 'serve').split()):\n        ui.debug('acl: changes have source \"%s\" - skipping\\n' % source)\n        return\n\n    user = None\n    if source == 'serve' and 'url' in kwargs:\n        url = kwargs['url'].split(':')\n        if url[0] == 'remote' and url[1].startswith('http'):\n            user = urllib.unquote(url[3])\n\n    if user is None:\n        user = getpass.getuser()\n\n    ui.debug('acl: checking access for user \"%s\"\\n' % user)\n\n    cfg = ui.config('acl', 'config')\n    if cfg:\n        ui.readconfig(cfg, sections = ['acl.groups', 'acl.allow.branches',\n        'acl.deny.branches', 'acl.allow', 'acl.deny'])\n\n    allowbranches = buildmatch(ui, None, user, 'acl.allow.branches')\n    denybranches = buildmatch(ui, None, user, 'acl.deny.branches')\n    allow = buildmatch(ui, repo, user, 'acl.allow')\n    deny = buildmatch(ui, repo, user, 'acl.deny')\n\n    for rev in xrange(repo[node], len(repo)):\n        ctx = repo[rev]\n        branch = ctx.branch()\n        if denybranches and denybranches(branch):\n            raise util.Abort(_('acl: user \"%s\" denied on branch \"%s\"'\n                               ' (changeset \"%s\")')\n                               % (user, branch, ctx))\n        if allowbranches and not allowbranches(branch):\n            raise util.Abort(_('acl: user \"%s\" not allowed on branch \"%s\"'\n                               ' (changeset \"%s\")')\n                               % (user, branch, ctx))\n        ui.debug('acl: branch access granted: \"%s\" on branch \"%s\"\\n'\n        % (ctx, branch))\n\n        for f in ctx.files():\n            if deny and deny(f):\n                raise util.Abort(_('acl: user \"%s\" denied on \"%s\"'\n                ' (changeset \"%s\")') % (user, f, ctx))\n            if allow and not allow(f):\n                raise util.Abort(_('acl: user \"%s\" not allowed on \"%s\"'\n                ' (changeset \"%s\")') % (user, f, ctx))\n        ui.debug('acl: path access granted: \"%s\"\\n' % ctx)\n", "license": "apache-2.0"}
{"id": "efeae61d9916162c85c7879073fdb3fff383c942", "path": "desktop/core/ext-py/Paste-2.0.1/tests/test_errordocument.py", "repo_name": "vmax-feihu/hue", "content": "from paste.errordocument import forward\nfrom paste.fixture import *\nfrom paste.recursive import RecursiveMiddleware\n\ndef simple_app(environ, start_response):\n    start_response(\"200 OK\", [('Content-type', 'text/plain')])\n    return [b'requested page returned']\n\ndef not_found_app(environ, start_response):\n    start_response(\"404 Not found\", [('Content-type', 'text/plain')])\n    return [b'requested page returned']\n\ndef test_ok():\n    app = TestApp(simple_app)\n    res = app.get('')\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '200 OK'\n    assert 'requested page returned' in res\n\ndef error_docs_app(environ, start_response):\n    if environ['PATH_INFO'] == '/not_found':\n        start_response(\"404 Not found\", [('Content-type', 'text/plain')])\n        return [b'Not found']\n    elif environ['PATH_INFO'] == '/error':\n        start_response(\"200 OK\", [('Content-type', 'text/plain')])\n        return [b'Page not found']\n    else:\n        return simple_app(environ, start_response)\n\ndef test_error_docs_app():\n    app = TestApp(error_docs_app)\n    res = app.get('')\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '200 OK'\n    assert 'requested page returned' in res\n    res = app.get('/error')\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '200 OK'\n    assert 'Page not found' in res\n    res = app.get('/not_found', status=404)\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '404 Not found'\n    assert 'Not found' in res\n\ndef test_forward():\n    app = forward(error_docs_app, codes={404:'/error'})\n    app = TestApp(RecursiveMiddleware(app))\n    res = app.get('')\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '200 OK'\n    assert 'requested page returned' in res\n    res = app.get('/error')\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '200 OK'\n    assert 'Page not found' in res\n    res = app.get('/not_found', status=404)\n    assert res.header('content-type') == 'text/plain'\n    assert res.full_status == '404 Not found'\n    # Note changed response\n    assert 'Page not found' in res\n\ndef auth_required_app(environ, start_response):\n    start_response('401 Unauthorized', [('content-type', 'text/plain'), ('www-authenticate', 'Basic realm=\"Foo\"')])\n    return ['Sign in!']\n\ndef auth_docs_app(environ, start_response):\n    if environ['PATH_INFO'] == '/auth':\n        return auth_required_app(environ, start_response)\n    elif environ['PATH_INFO'] == '/auth_doc':\n        start_response(\"200 OK\", [('Content-type', 'text/html')])\n        return [b'<html>Login!</html>']\n    else:\n        return simple_app(environ, start_response)\n\ndef test_auth_docs_app():\n    wsgi_app = forward(auth_docs_app, codes={401: '/auth_doc'})\n    app = TestApp(wsgi_app)\n    res = app.get('/auth_doc')\n    assert res.header('content-type') == 'text/html'\n    res = app.get('/auth', status=401)\n    assert res.header('content-type') == 'text/html'\n    assert res.header('www-authenticate') == 'Basic realm=\"Foo\"'\n    assert res.body == b'<html>Login!</html>'\n\ndef test_bad_error():\n    def app(environ, start_response):\n        start_response('404 Not Found', [('content-type', 'text/plain')])\n        return ['not found']\n    app = forward(app, {404: '/404.html'})\n    app = TestApp(app)\n    resp = app.get('/test', expect_errors=True)\n    print(resp)\n", "license": "apache-2.0"}
{"id": "27dcb33ebabcbbc92c64b111603f6f015779c41b", "path": "nova/compute/flavors.py", "repo_name": "alexandrucoman/vbox-nova-driver", "content": "# Copyright 2010 United States Government as represented by the\n# Administrator of the National Aeronautics and Space Administration.\n# All Rights Reserved.\n# Copyright (c) 2010 Citrix Systems, Inc.\n# Copyright 2011 Ken Pepple\n#\n#    Licensed under the Apache License, Version 2.0 (the \"License\"); you may\n#    not use this file except in compliance with the License. You may obtain\n#    a copy of the License at\n#\n#         http://www.apache.org/licenses/LICENSE-2.0\n#\n#    Unless required by applicable law or agreed to in writing, software\n#    distributed under the License is distributed on an \"AS IS\" BASIS, WITHOUT\n#    WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the\n#    License for the specific language governing permissions and limitations\n#    under the License.\n\n\"\"\"Built-in instance properties.\"\"\"\n\nimport re\nimport uuid\n\nfrom oslo_config import cfg\nfrom oslo_log import log as logging\nfrom oslo_utils import strutils\nimport six\n\nfrom nova.api.validation import parameter_types\nfrom nova import context\nfrom nova import db\nfrom nova import exception\nfrom nova.i18n import _\nfrom nova.i18n import _LE\nfrom nova import objects\nfrom nova import utils\n\nflavor_opts = [\n    cfg.StrOpt('default_flavor',\n               default='m1.small',\n               help='Default flavor to use for the EC2 API only. The Nova API '\n               'does not support a default flavor.'),\n]\n\nCONF = cfg.CONF\nCONF.register_opts(flavor_opts)\n\nLOG = logging.getLogger(__name__)\n\n# NOTE(luisg): Flavor names can include non-ascii characters so that users can\n# create flavor names in locales that use them, however flavor IDs are limited\n# to ascii characters.\nVALID_ID_REGEX = re.compile(\"^[\\w\\.\\- ]*$\")\nVALID_NAME_REGEX = re.compile(parameter_types.valid_name_regex, re.UNICODE)\n\n# NOTE(dosaboy): This is supposed to represent the maximum value that we can\n# place into a SQL single precision float so that we can check whether values\n# are oversize. Postgres and MySQL both define this as their max whereas Sqlite\n# uses dynamic typing so this would not apply. Different dbs react in different\n# ways to oversize values e.g. postgres will raise an exception while mysql\n# will round off the value. Nevertheless we may still want to know prior to\n# insert whether the value is oversize.\nSQL_SP_FLOAT_MAX = 3.40282e+38\n\n# Validate extra specs key names.\nVALID_EXTRASPEC_NAME_REGEX = re.compile(r\"[\\w\\.\\- :]+$\", re.UNICODE)\n\n\ndef _int_or_none(val):\n    if val is not None:\n        return int(val)\n\n\nsystem_metadata_flavor_props = {\n    'id': int,\n    'name': str,\n    'memory_mb': int,\n    'vcpus': int,\n    'root_gb': int,\n    'ephemeral_gb': int,\n    'flavorid': str,\n    'swap': int,\n    'rxtx_factor': float,\n    'vcpu_weight': _int_or_none,\n    }\n\n\nsystem_metadata_flavor_extra_props = [\n    'hw:numa_cpus.', 'hw:numa_mem.',\n]\n\n\ndef create(name, memory, vcpus, root_gb, ephemeral_gb=0, flavorid=None,\n           swap=0, rxtx_factor=1.0, is_public=True):\n    \"\"\"Creates flavors.\"\"\"\n    if not flavorid:\n        flavorid = uuid.uuid4()\n\n    kwargs = {\n        'memory_mb': memory,\n        'vcpus': vcpus,\n        'root_gb': root_gb,\n        'ephemeral_gb': ephemeral_gb,\n        'swap': swap,\n        'rxtx_factor': rxtx_factor,\n    }\n\n    if isinstance(name, six.string_types):\n        name = name.strip()\n    # ensure name do not exceed 255 characters\n    utils.check_string_length(name, 'name', min_length=1, max_length=255)\n\n    # ensure name does not contain any special characters\n    valid_name = VALID_NAME_REGEX.search(name)\n    if not valid_name:\n        msg = _(\"Flavor names can only contain printable characters \"\n                \"and horizontal spaces.\")\n        raise exception.InvalidInput(reason=msg)\n\n    # NOTE(vish): Internally, flavorid is stored as a string but it comes\n    #             in through json as an integer, so we convert it here.\n    flavorid = unicode(flavorid)\n\n    # ensure leading/trailing whitespaces not present.\n    if flavorid.strip() != flavorid:\n        msg = _(\"id cannot contain leading and/or trailing whitespace(s)\")\n        raise exception.InvalidInput(reason=msg)\n\n    # ensure flavor id does not exceed 255 characters\n    utils.check_string_length(flavorid, 'id', min_length=1,\n                              max_length=255)\n\n    # ensure flavor id does not contain any special characters\n    valid_flavor_id = VALID_ID_REGEX.search(flavorid)\n    if not valid_flavor_id:\n        msg = _(\"Flavor id can only contain letters from A-Z (both cases), \"\n                \"periods, dashes, underscores and spaces.\")\n        raise exception.InvalidInput(reason=msg)\n\n    # NOTE(wangbo): validate attributes of the creating flavor.\n    # ram and vcpus should be positive ( > 0) integers.\n    # disk, ephemeral and swap should be non-negative ( >= 0) integers.\n    flavor_attributes = {\n        'memory_mb': ('ram', 1),\n        'vcpus': ('vcpus', 1),\n        'root_gb': ('disk', 0),\n        'ephemeral_gb': ('ephemeral', 0),\n        'swap': ('swap', 0)\n    }\n\n    for key, value in flavor_attributes.items():\n        kwargs[key] = utils.validate_integer(kwargs[key], value[0], value[1],\n                                             db.MAX_INT)\n\n    # rxtx_factor should be a positive float\n    try:\n        kwargs['rxtx_factor'] = float(kwargs['rxtx_factor'])\n        if (kwargs['rxtx_factor'] <= 0 or\n                kwargs['rxtx_factor'] > SQL_SP_FLOAT_MAX):\n            raise ValueError()\n    except ValueError:\n        msg = (_(\"'rxtx_factor' argument must be a float between 0 and %g\") %\n               SQL_SP_FLOAT_MAX)\n        raise exception.InvalidInput(reason=msg)\n\n    kwargs['name'] = name\n    kwargs['flavorid'] = flavorid\n    # ensure is_public attribute is boolean\n    try:\n        kwargs['is_public'] = strutils.bool_from_string(\n            is_public, strict=True)\n    except ValueError:\n        raise exception.InvalidInput(reason=_(\"is_public must be a boolean\"))\n\n    flavor = objects.Flavor(context=context.get_admin_context(), **kwargs)\n    flavor.create()\n    return flavor\n\n\ndef destroy(name):\n    \"\"\"Marks flavor as deleted.\"\"\"\n    try:\n        if not name:\n            raise ValueError()\n        flavor = objects.Flavor(context=context.get_admin_context(), name=name)\n        flavor.destroy()\n    except (ValueError, exception.NotFound):\n        LOG.exception(_LE('Instance type %s not found for deletion'), name)\n        raise exception.FlavorNotFoundByName(flavor_name=name)\n\n\ndef get_all_flavors(ctxt=None, inactive=False, filters=None):\n    \"\"\"Get all non-deleted flavors as a dict.\n\n    Pass inactive=True if you want deleted flavors returned also.\n    \"\"\"\n    if ctxt is None:\n        ctxt = context.get_admin_context()\n\n    inst_types = objects.FlavorList.get_all(ctxt, inactive=inactive,\n                                            filters=filters)\n\n    inst_type_dict = {}\n    for inst_type in inst_types:\n        inst_type_dict[inst_type.id] = inst_type\n    return inst_type_dict\n\n\ndef get_all_flavors_sorted_list(ctxt=None, filters=None, sort_key='flavorid',\n                                sort_dir='asc', limit=None, marker=None):\n    \"\"\"Get all non-deleted flavors as a sorted list.\n    \"\"\"\n    if ctxt is None:\n        ctxt = context.get_admin_context()\n\n    return objects.FlavorList.get_all(ctxt, filters=filters, sort_key=sort_key,\n                                      sort_dir=sort_dir, limit=limit,\n                                      marker=marker)\n\n\ndef get_default_flavor():\n    \"\"\"Get the default flavor.\"\"\"\n    name = CONF.default_flavor\n    return get_flavor_by_name(name)\n\n\ndef get_flavor(instance_type_id, ctxt=None, inactive=False):\n    \"\"\"Retrieves single flavor by id.\"\"\"\n    if instance_type_id is None:\n        return get_default_flavor()\n\n    if ctxt is None:\n        ctxt = context.get_admin_context()\n\n    if inactive:\n        ctxt = ctxt.elevated(read_deleted=\"yes\")\n\n    return objects.Flavor.get_by_id(ctxt, instance_type_id)\n\n\ndef get_flavor_by_name(name, ctxt=None):\n    \"\"\"Retrieves single flavor by name.\"\"\"\n    if name is None:\n        return get_default_flavor()\n\n    if ctxt is None:\n        ctxt = context.get_admin_context()\n\n    return objects.Flavor.get_by_name(ctxt, name)\n\n\n# TODO(termie): flavor-specific code should probably be in the API that uses\n#               flavors.\ndef get_flavor_by_flavor_id(flavorid, ctxt=None, read_deleted=\"yes\"):\n    \"\"\"Retrieve flavor by flavorid.\n\n    :raises: FlavorNotFound\n    \"\"\"\n    if ctxt is None:\n        ctxt = context.get_admin_context(read_deleted=read_deleted)\n\n    return objects.Flavor.get_by_flavor_id(ctxt, flavorid, read_deleted)\n\n\ndef get_flavor_access_by_flavor_id(flavorid, ctxt=None):\n    \"\"\"Retrieve flavor access list by flavor id.\"\"\"\n    if ctxt is None:\n        ctxt = context.get_admin_context()\n\n    flavor = objects.Flavor.get_by_flavor_id(ctxt, flavorid)\n    return flavor.projects\n\n\n# NOTE(danms): This method is deprecated, do not use it!\n# Use instance.{old_,new_,}flavor instead, as instances no longer\n# have flavor information in system_metadata.\ndef extract_flavor(instance, prefix=''):\n    \"\"\"Create a Flavor object from instance's system_metadata\n    information.\n    \"\"\"\n\n    flavor = objects.Flavor()\n    sys_meta = utils.instance_sys_meta(instance)\n\n    if not sys_meta:\n        return None\n\n    for key in system_metadata_flavor_props.keys():\n        type_key = '%sinstance_type_%s' % (prefix, key)\n        setattr(flavor, key, sys_meta[type_key])\n\n    # NOTE(danms): We do NOT save all of extra_specs, but only the\n    # NUMA-related ones that we need to avoid an uglier alternative. This\n    # should be replaced by a general split-out of flavor information from\n    # system_metadata very soon.\n    extra_specs = [(k, v) for k, v in sys_meta.items()\n                   if k.startswith('%sinstance_type_extra_' % prefix)]\n    if extra_specs:\n        flavor.extra_specs = {}\n        for key, value in extra_specs:\n            extra_key = key[len('%sinstance_type_extra_' % prefix):]\n            flavor.extra_specs[extra_key] = value\n\n    return flavor\n\n\n# NOTE(danms): This method is deprecated, do not use it!\n# Use instance.{old_,new_,}flavor instead, as instances no longer\n# have flavor information in system_metadata.\ndef save_flavor_info(metadata, instance_type, prefix=''):\n    \"\"\"Save properties from instance_type into instance's system_metadata,\n    in the format of:\n\n      [prefix]instance_type_[key]\n\n    This can be used to update system_metadata in place from a type, as well\n    as stash information about another instance_type for later use (such as\n    during resize).\n    \"\"\"\n\n    for key in system_metadata_flavor_props.keys():\n        to_key = '%sinstance_type_%s' % (prefix, key)\n        metadata[to_key] = instance_type[key]\n\n    # NOTE(danms): We do NOT save all of extra_specs here, but only the\n    # NUMA-related ones that we need to avoid an uglier alternative. This\n    # should be replaced by a general split-out of flavor information from\n    # system_metadata very soon.\n    extra_specs = instance_type.get('extra_specs', {})\n    for extra_prefix in system_metadata_flavor_extra_props:\n        for key in extra_specs:\n            if key.startswith(extra_prefix):\n                to_key = '%sinstance_type_extra_%s' % (prefix, key)\n                metadata[to_key] = extra_specs[key]\n\n    return metadata\n\n\n# NOTE(danms): This method is deprecated, do not use it!\n# Instances no longer store flavor information in system_metadata\ndef delete_flavor_info(metadata, *prefixes):\n    \"\"\"Delete flavor instance_type information from instance's system_metadata\n    by prefix.\n    \"\"\"\n\n    for key in system_metadata_flavor_props.keys():\n        for prefix in prefixes:\n            to_key = '%sinstance_type_%s' % (prefix, key)\n            del metadata[to_key]\n\n    # NOTE(danms): We do NOT save all of extra_specs, but only the\n    # NUMA-related ones that we need to avoid an uglier alternative. This\n    # should be replaced by a general split-out of flavor information from\n    # system_metadata very soon.\n    for key in metadata.keys():\n        for prefix in prefixes:\n            if key.startswith('%sinstance_type_extra_' % prefix):\n                del metadata[key]\n\n    return metadata\n\n\ndef validate_extra_spec_keys(key_names_list):\n    for key_name in key_names_list:\n        if not VALID_EXTRASPEC_NAME_REGEX.match(key_name):\n            expl = _('Key Names can only contain alphanumeric characters, '\n                     'periods, dashes, underscores, colons and spaces.')\n            raise exception.InvalidInput(message=expl)\n", "license": "apache-2.0"}
{"id": "0015d5a52d4e7954344fe5811a37ee713139f74b", "path": "openerp/addons/test_uninstall/__openerp__.py", "repo_name": "xzYue/odoo", "content": "# -*- coding: utf-8 -*-\n{\n    'name': 'test-uninstall',\n    'version': '0.1',\n    'category': 'Tests',\n    'description': \"\"\"A module to test the uninstall feature.\"\"\",\n    'author': 'OpenERP SA',\n    'maintainer': 'OpenERP SA',\n    'website': 'http://www.openerp.com',\n    'depends': ['base'],\n    'data': ['ir.model.access.csv'],\n    'installable': True,\n    'auto_install': False,\n}\n# vim:expandtab:smartindent:tabstop=4:softtabstop=4:shiftwidth=4:\n", "license": "agpl-3.0"}
{"id": "cd1f9ec820abcff12292f0ec0f9836a5acaed4ca", "path": "NS3/bindings/python/ns3__init__.py", "repo_name": "Chiru/NVE_Simulation", "content": "\nfrom _ns3 import *\n\nimport atexit\natexit.register(Simulator.Destroy)\ndel atexit\n\n", "license": "bsd-3-clause"}