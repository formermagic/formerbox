model:
  name: transformers.RobertaForMaskedLM
  config: transformers.RobertaConfig
  config_params:
    hidden_size: 768
    num_hidden_layers: 2
    num_attention_heads: 8
    intermediate_size: 3072
    hidden_act: gelu
    max_position_embeddings: 514
    hidden_dropout_prob: 0.1
    attention_probs_dropout_prob: 0.1
    type_vocab_size: 2
    initializer_range: 0.02
    layer_norm_eps: 1.0e-12
    gradient_checkpointing: false

tokenizer:
  name: gitnetic.tasks.codebert.CodeBertTokenizerFast
  params:
    add_prefix_space: false
    trim_offsets: true
    lowercase: true
