TOKENIZERS_PARALLELISM=true WANDB_API_KEY=476445f974f0ccad33c3c5529d3f1c3a0d8cea00 python -m src.tasks.codebert.run_codebert_pretraining \
    --wandb_project test_proj \
    --wandb_name test_name \
    --save_dir /Users/mozharovsky/GitHub/gitnetic-ml/models/2020-07-02-codeberta-exp01 \
    --tokenizer_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K \
    --weight_decay 0.01 \
    --warmup_steps 1000 \
    --train_data_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K/train.src \
    --val_data_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K/val.src \
    --batch_size 8 \
    --learning_rate 0.00068 \
    --num_hidden_layers 2 \
    --num_attention_heads 12 \
    --power 2 \
    --num_workers 4 \
    --gpus 0 \
    --num_nodes 1 \
    --accumulate_grad_batches 1 \
    --max_steps 10000 \
    --gradient_clip_val 0 \
    --save_interval_updates 20 \
    --resume_from_checkpoint /Users/mozharovsky/GitHub/gitnetic-ml/models/2020-07-02-codeberta-exp01/epoch=0-step=39.ckpt
    --auto_scale_batch_size binsearch --find_batch_size

TOKENIZERS_PARALLELISM=true WANDB_API_KEY=476445f974f0ccad33c3c5529d3f1c3a0d8cea00 python -m src.tasks.codebert.run_codebert_pretraining \
    --wandb_project test_proj \
    --wandb_name test_name \
    --save_dir /Users/mozharovsky/GitHub/gitnetic-ml/models/2020-07-02-codeberta-exp01 \
    --tokenizer_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K \
    --weight_decay 0.01 \
    --warmup_steps 7000 \
    --train_data_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K/train.src \
    --val_data_path /Users/mozharovsky/GitHub/gitnetic-ml/data/2020-05-19-dataset-byte-bpe-tokenizer-vocab10K/val.src \
    --batch_size 8 \
    --learning_rate 0.0001 \
    --num_hidden_layers 4 \
    --num_attention_heads 8 \
    --power 2 \
    --num_workers 4 \
    --num_nodes 1 \
    --accumulate_grad_batches 1 \
    --max_steps 150000 \
    --gradient_clip_val 0 \
    --save_interval_updates 100 \
    --tokenizer_add_prefix_space false \
    --tokenizer_trim_offsets true \
    --tokenizer_lowercase true