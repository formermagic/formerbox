model:
  name: transformers.BartForConditionalGeneration
  config: transformers.BartConfig
  config_params:
    activation_dropout: 0.0
    extra_pos_embeddings: 2
    activation_function: gelu
    d_model: 1024
    encoder_ffn_dim: 4096
    encoder_layers: 12
    encoder_attention_heads: 16
    decoder_ffn_dim: 4096
    decoder_layers: 12
    decoder_attention_heads: 16
    encoder_layerdrop: 0.0
    decoder_layerdrop: 0.0
    attention_dropout: 0.0
    dropout: 0.1
    max_position_embeddings: 1024
    init_std: 0.02
    classifier_dropout: 0.0
    num_labels: 3
    is_encoder_decoder: true
    normalize_before: false
    add_final_layer_norm: false
    do_blenderbot_90_layernorm: false
    scale_embedding: false
    normalize_embedding: true
    static_position_embeddings: false
    add_bias_logits: false
    force_bos_token_to_be_generated: false

tokenizer:
  name: formerbox.tasks.BartTokenizer
  params: {}
